I0313 00:22:02.247025      25 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-940252648
I0313 00:22:02.247206      25 e2e.go:92] Starting e2e run "a9c64acf-0d86-4149-ac12-b76fa9d6c87d" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1584058918 - Will randomize all specs
Will run 274 of 4897 specs

Mar 13 00:22:02.338: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:22:02.348: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 13 00:22:02.383: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 13 00:22:02.461: INFO: 5 / 5 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 13 00:22:02.461: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Mar 13 00:22:02.461: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 13 00:22:02.483: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Mar 13 00:22:02.483: INFO: e2e test version: v1.16.0
Mar 13 00:22:02.486: INFO: kube-apiserver version: v1.16.0
Mar 13 00:22:02.486: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:22:02.503: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:22:02.505: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename svc-latency
Mar 13 00:22:02.578: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6335
I0313 00:22:02.592437      25 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6335, replica count: 1
I0313 00:22:03.643449      25 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0313 00:22:04.643912      25 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0313 00:22:05.644370      25 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 13 00:22:05.761: INFO: Created: latency-svc-45stv
Mar 13 00:22:05.773: INFO: Got endpoints: latency-svc-45stv [28.149233ms]
Mar 13 00:22:05.820: INFO: Created: latency-svc-qjrn7
Mar 13 00:22:05.830: INFO: Got endpoints: latency-svc-qjrn7 [55.978711ms]
Mar 13 00:22:05.838: INFO: Created: latency-svc-n6hjr
Mar 13 00:22:05.849: INFO: Created: latency-svc-fgtjd
Mar 13 00:22:05.852: INFO: Got endpoints: latency-svc-n6hjr [77.780195ms]
Mar 13 00:22:05.862: INFO: Created: latency-svc-96tsc
Mar 13 00:22:05.874: INFO: Got endpoints: latency-svc-fgtjd [98.108623ms]
Mar 13 00:22:05.882: INFO: Got endpoints: latency-svc-96tsc [106.962072ms]
Mar 13 00:22:05.904: INFO: Created: latency-svc-p7xcn
Mar 13 00:22:05.914: INFO: Got endpoints: latency-svc-p7xcn [138.23307ms]
Mar 13 00:22:05.930: INFO: Created: latency-svc-4n7mv
Mar 13 00:22:05.936: INFO: Got endpoints: latency-svc-4n7mv [160.845734ms]
Mar 13 00:22:05.952: INFO: Created: latency-svc-b88vv
Mar 13 00:22:05.962: INFO: Created: latency-svc-bgrnd
Mar 13 00:22:05.978: INFO: Created: latency-svc-qjk5k
Mar 13 00:22:05.992: INFO: Got endpoints: latency-svc-b88vv [215.682075ms]
Mar 13 00:22:06.004: INFO: Got endpoints: latency-svc-qjk5k [225.619977ms]
Mar 13 00:22:06.013: INFO: Got endpoints: latency-svc-bgrnd [235.524116ms]
Mar 13 00:22:06.016: INFO: Created: latency-svc-qdzdn
Mar 13 00:22:06.024: INFO: Got endpoints: latency-svc-qdzdn [245.228155ms]
Mar 13 00:22:06.039: INFO: Created: latency-svc-6dzkj
Mar 13 00:22:06.046: INFO: Created: latency-svc-4s5f7
Mar 13 00:22:06.046: INFO: Got endpoints: latency-svc-6dzkj [273.088115ms]
Mar 13 00:22:06.066: INFO: Got endpoints: latency-svc-4s5f7 [286.678738ms]
Mar 13 00:22:06.078: INFO: Created: latency-svc-mnc4n
Mar 13 00:22:06.088: INFO: Got endpoints: latency-svc-mnc4n [308.672154ms]
Mar 13 00:22:06.093: INFO: Created: latency-svc-mddpw
Mar 13 00:22:06.109: INFO: Got endpoints: latency-svc-mddpw [330.026802ms]
Mar 13 00:22:06.114: INFO: Created: latency-svc-5kmdg
Mar 13 00:22:06.128: INFO: Got endpoints: latency-svc-5kmdg [349.706587ms]
Mar 13 00:22:06.130: INFO: Created: latency-svc-mk456
Mar 13 00:22:06.137: INFO: Created: latency-svc-7c95l
Mar 13 00:22:06.143: INFO: Got endpoints: latency-svc-mk456 [312.655971ms]
Mar 13 00:22:06.147: INFO: Got endpoints: latency-svc-7c95l [294.900481ms]
Mar 13 00:22:06.151: INFO: Created: latency-svc-q9sw4
Mar 13 00:22:06.164: INFO: Created: latency-svc-kmlkt
Mar 13 00:22:06.165: INFO: Got endpoints: latency-svc-q9sw4 [291.080345ms]
Mar 13 00:22:06.174: INFO: Created: latency-svc-dtdfn
Mar 13 00:22:06.177: INFO: Got endpoints: latency-svc-kmlkt [294.520984ms]
Mar 13 00:22:06.185: INFO: Got endpoints: latency-svc-dtdfn [270.933277ms]
Mar 13 00:22:06.189: INFO: Created: latency-svc-n2b8t
Mar 13 00:22:06.197: INFO: Created: latency-svc-lkkdq
Mar 13 00:22:06.199: INFO: Got endpoints: latency-svc-n2b8t [262.926673ms]
Mar 13 00:22:06.212: INFO: Created: latency-svc-5rjjx
Mar 13 00:22:06.222: INFO: Got endpoints: latency-svc-lkkdq [229.510378ms]
Mar 13 00:22:06.223: INFO: Got endpoints: latency-svc-5rjjx [218.091892ms]
Mar 13 00:22:06.232: INFO: Created: latency-svc-5vnfh
Mar 13 00:22:06.238: INFO: Got endpoints: latency-svc-5vnfh [225.48912ms]
Mar 13 00:22:06.249: INFO: Created: latency-svc-zbmjv
Mar 13 00:22:06.259: INFO: Created: latency-svc-bww8w
Mar 13 00:22:06.262: INFO: Got endpoints: latency-svc-zbmjv [237.896744ms]
Mar 13 00:22:06.269: INFO: Got endpoints: latency-svc-bww8w [223.152844ms]
Mar 13 00:22:06.279: INFO: Created: latency-svc-h2cvz
Mar 13 00:22:06.286: INFO: Got endpoints: latency-svc-h2cvz [220.41321ms]
Mar 13 00:22:06.288: INFO: Created: latency-svc-n9bcs
Mar 13 00:22:06.294: INFO: Got endpoints: latency-svc-n9bcs [185.009764ms]
Mar 13 00:22:06.296: INFO: Created: latency-svc-chdxn
Mar 13 00:22:06.311: INFO: Got endpoints: latency-svc-chdxn [223.39023ms]
Mar 13 00:22:06.315: INFO: Created: latency-svc-555sz
Mar 13 00:22:06.324: INFO: Got endpoints: latency-svc-555sz [195.573972ms]
Mar 13 00:22:06.329: INFO: Created: latency-svc-kpdmj
Mar 13 00:22:06.344: INFO: Created: latency-svc-2d27q
Mar 13 00:22:06.349: INFO: Got endpoints: latency-svc-kpdmj [205.689299ms]
Mar 13 00:22:06.359: INFO: Created: latency-svc-m9gh5
Mar 13 00:22:06.360: INFO: Got endpoints: latency-svc-2d27q [212.503485ms]
Mar 13 00:22:06.369: INFO: Got endpoints: latency-svc-m9gh5 [203.137054ms]
Mar 13 00:22:06.377: INFO: Created: latency-svc-zztm6
Mar 13 00:22:06.383: INFO: Got endpoints: latency-svc-zztm6 [206.31629ms]
Mar 13 00:22:06.391: INFO: Created: latency-svc-h6s2p
Mar 13 00:22:06.401: INFO: Got endpoints: latency-svc-h6s2p [216.049549ms]
Mar 13 00:22:06.410: INFO: Created: latency-svc-x8wvd
Mar 13 00:22:06.423: INFO: Created: latency-svc-b27cq
Mar 13 00:22:06.427: INFO: Got endpoints: latency-svc-x8wvd [227.410678ms]
Mar 13 00:22:06.431: INFO: Created: latency-svc-svhk7
Mar 13 00:22:06.432: INFO: Got endpoints: latency-svc-b27cq [209.446744ms]
Mar 13 00:22:06.443: INFO: Got endpoints: latency-svc-svhk7 [220.886662ms]
Mar 13 00:22:06.450: INFO: Created: latency-svc-vdrkn
Mar 13 00:22:06.460: INFO: Created: latency-svc-psdxs
Mar 13 00:22:06.461: INFO: Got endpoints: latency-svc-vdrkn [222.813375ms]
Mar 13 00:22:06.469: INFO: Got endpoints: latency-svc-psdxs [206.853642ms]
Mar 13 00:22:06.474: INFO: Created: latency-svc-jlqzx
Mar 13 00:22:06.484: INFO: Got endpoints: latency-svc-jlqzx [214.395625ms]
Mar 13 00:22:06.489: INFO: Created: latency-svc-gw47w
Mar 13 00:22:06.497: INFO: Got endpoints: latency-svc-gw47w [211.270909ms]
Mar 13 00:22:06.499: INFO: Created: latency-svc-cdw8z
Mar 13 00:22:06.508: INFO: Created: latency-svc-np7ng
Mar 13 00:22:06.512: INFO: Got endpoints: latency-svc-cdw8z [217.741498ms]
Mar 13 00:22:06.522: INFO: Created: latency-svc-knqsc
Mar 13 00:22:06.526: INFO: Got endpoints: latency-svc-np7ng [215.323303ms]
Mar 13 00:22:06.541: INFO: Created: latency-svc-8fmpf
Mar 13 00:22:06.542: INFO: Created: latency-svc-nlgnz
Mar 13 00:22:06.546: INFO: Created: latency-svc-6lcxd
Mar 13 00:22:06.554: INFO: Created: latency-svc-65b2v
Mar 13 00:22:06.565: INFO: Created: latency-svc-qxclv
Mar 13 00:22:06.572: INFO: Got endpoints: latency-svc-knqsc [247.623278ms]
Mar 13 00:22:06.579: INFO: Created: latency-svc-5tlw4
Mar 13 00:22:06.594: INFO: Created: latency-svc-qg8qw
Mar 13 00:22:06.602: INFO: Created: latency-svc-fdcf9
Mar 13 00:22:06.611: INFO: Created: latency-svc-ntqcd
Mar 13 00:22:06.627: INFO: Created: latency-svc-ck48p
Mar 13 00:22:06.631: INFO: Got endpoints: latency-svc-nlgnz [282.574442ms]
Mar 13 00:22:06.649: INFO: Created: latency-svc-6x92r
Mar 13 00:22:06.655: INFO: Created: latency-svc-p7669
Mar 13 00:22:06.665: INFO: Created: latency-svc-n6x85
Mar 13 00:22:06.677: INFO: Got endpoints: latency-svc-8fmpf [317.641224ms]
Mar 13 00:22:06.685: INFO: Created: latency-svc-l4n5g
Mar 13 00:22:06.692: INFO: Created: latency-svc-kr949
Mar 13 00:22:06.712: INFO: Created: latency-svc-6ldck
Mar 13 00:22:06.720: INFO: Created: latency-svc-v82sv
Mar 13 00:22:06.722: INFO: Got endpoints: latency-svc-6lcxd [353.7876ms]
Mar 13 00:22:06.740: INFO: Created: latency-svc-fqmmm
Mar 13 00:22:06.777: INFO: Got endpoints: latency-svc-65b2v [393.319858ms]
Mar 13 00:22:06.798: INFO: Created: latency-svc-8ckkt
Mar 13 00:22:06.824: INFO: Got endpoints: latency-svc-qxclv [422.726923ms]
Mar 13 00:22:06.841: INFO: Created: latency-svc-4cdfx
Mar 13 00:22:06.875: INFO: Got endpoints: latency-svc-5tlw4 [447.735598ms]
Mar 13 00:22:06.892: INFO: Created: latency-svc-mg8tb
Mar 13 00:22:06.923: INFO: Got endpoints: latency-svc-qg8qw [490.514963ms]
Mar 13 00:22:06.941: INFO: Created: latency-svc-krqvr
Mar 13 00:22:06.973: INFO: Got endpoints: latency-svc-fdcf9 [530.123296ms]
Mar 13 00:22:06.987: INFO: Created: latency-svc-zzx2t
Mar 13 00:22:07.023: INFO: Got endpoints: latency-svc-ntqcd [561.63177ms]
Mar 13 00:22:07.041: INFO: Created: latency-svc-5q99g
Mar 13 00:22:07.076: INFO: Got endpoints: latency-svc-ck48p [606.706467ms]
Mar 13 00:22:07.095: INFO: Created: latency-svc-jkxnc
Mar 13 00:22:07.126: INFO: Got endpoints: latency-svc-6x92r [642.151301ms]
Mar 13 00:22:07.144: INFO: Created: latency-svc-cxpdv
Mar 13 00:22:07.175: INFO: Got endpoints: latency-svc-p7669 [543.267219ms]
Mar 13 00:22:07.188: INFO: Created: latency-svc-959lk
Mar 13 00:22:07.225: INFO: Got endpoints: latency-svc-n6x85 [727.698191ms]
Mar 13 00:22:07.251: INFO: Created: latency-svc-2r994
Mar 13 00:22:07.275: INFO: Got endpoints: latency-svc-l4n5g [762.481698ms]
Mar 13 00:22:07.294: INFO: Created: latency-svc-7q9sc
Mar 13 00:22:07.324: INFO: Got endpoints: latency-svc-kr949 [797.445269ms]
Mar 13 00:22:07.341: INFO: Created: latency-svc-92ssk
Mar 13 00:22:07.374: INFO: Got endpoints: latency-svc-6ldck [802.2898ms]
Mar 13 00:22:07.391: INFO: Created: latency-svc-rdvzq
Mar 13 00:22:07.425: INFO: Got endpoints: latency-svc-v82sv [746.901167ms]
Mar 13 00:22:07.445: INFO: Created: latency-svc-792s9
Mar 13 00:22:07.475: INFO: Got endpoints: latency-svc-fqmmm [752.588322ms]
Mar 13 00:22:07.493: INFO: Created: latency-svc-jg5tf
Mar 13 00:22:07.528: INFO: Got endpoints: latency-svc-8ckkt [751.567775ms]
Mar 13 00:22:07.544: INFO: Created: latency-svc-l6hwb
Mar 13 00:22:07.573: INFO: Got endpoints: latency-svc-4cdfx [748.72018ms]
Mar 13 00:22:07.589: INFO: Created: latency-svc-mj2xc
Mar 13 00:22:07.625: INFO: Got endpoints: latency-svc-mg8tb [750.776236ms]
Mar 13 00:22:07.645: INFO: Created: latency-svc-wkmkf
Mar 13 00:22:07.676: INFO: Got endpoints: latency-svc-krqvr [752.984249ms]
Mar 13 00:22:07.695: INFO: Created: latency-svc-mdx2g
Mar 13 00:22:07.726: INFO: Got endpoints: latency-svc-zzx2t [752.615999ms]
Mar 13 00:22:07.743: INFO: Created: latency-svc-ddn66
Mar 13 00:22:07.779: INFO: Got endpoints: latency-svc-5q99g [755.638673ms]
Mar 13 00:22:07.795: INFO: Created: latency-svc-xn55p
Mar 13 00:22:07.823: INFO: Got endpoints: latency-svc-jkxnc [747.06811ms]
Mar 13 00:22:07.842: INFO: Created: latency-svc-cvktc
Mar 13 00:22:07.874: INFO: Got endpoints: latency-svc-cxpdv [747.703088ms]
Mar 13 00:22:07.889: INFO: Created: latency-svc-vq6tz
Mar 13 00:22:07.928: INFO: Got endpoints: latency-svc-959lk [753.049141ms]
Mar 13 00:22:07.946: INFO: Created: latency-svc-xtzh4
Mar 13 00:22:07.974: INFO: Got endpoints: latency-svc-2r994 [748.256272ms]
Mar 13 00:22:07.995: INFO: Created: latency-svc-rtwrm
Mar 13 00:22:08.025: INFO: Got endpoints: latency-svc-7q9sc [750.270634ms]
Mar 13 00:22:08.043: INFO: Created: latency-svc-6nmjp
Mar 13 00:22:08.075: INFO: Got endpoints: latency-svc-92ssk [750.888913ms]
Mar 13 00:22:08.097: INFO: Created: latency-svc-9qdc7
Mar 13 00:22:08.127: INFO: Got endpoints: latency-svc-rdvzq [752.878688ms]
Mar 13 00:22:08.142: INFO: Created: latency-svc-5crfs
Mar 13 00:22:08.174: INFO: Got endpoints: latency-svc-792s9 [749.153374ms]
Mar 13 00:22:08.188: INFO: Created: latency-svc-8xzh9
Mar 13 00:22:08.223: INFO: Got endpoints: latency-svc-jg5tf [747.908672ms]
Mar 13 00:22:08.239: INFO: Created: latency-svc-kctgd
Mar 13 00:22:08.273: INFO: Got endpoints: latency-svc-l6hwb [744.546288ms]
Mar 13 00:22:08.291: INFO: Created: latency-svc-5w7qx
Mar 13 00:22:08.325: INFO: Got endpoints: latency-svc-mj2xc [751.762639ms]
Mar 13 00:22:08.346: INFO: Created: latency-svc-7n5qn
Mar 13 00:22:08.374: INFO: Got endpoints: latency-svc-wkmkf [748.535608ms]
Mar 13 00:22:08.388: INFO: Created: latency-svc-dnrzg
Mar 13 00:22:08.424: INFO: Got endpoints: latency-svc-mdx2g [748.394259ms]
Mar 13 00:22:08.438: INFO: Created: latency-svc-t4c2d
Mar 13 00:22:08.472: INFO: Got endpoints: latency-svc-ddn66 [745.767051ms]
Mar 13 00:22:08.487: INFO: Created: latency-svc-v9j7t
Mar 13 00:22:08.522: INFO: Got endpoints: latency-svc-xn55p [743.613343ms]
Mar 13 00:22:08.543: INFO: Created: latency-svc-4chb5
Mar 13 00:22:08.573: INFO: Got endpoints: latency-svc-cvktc [750.004616ms]
Mar 13 00:22:08.589: INFO: Created: latency-svc-t67vk
Mar 13 00:22:08.624: INFO: Got endpoints: latency-svc-vq6tz [750.192098ms]
Mar 13 00:22:08.639: INFO: Created: latency-svc-dpghd
Mar 13 00:22:08.676: INFO: Got endpoints: latency-svc-xtzh4 [748.350101ms]
Mar 13 00:22:08.695: INFO: Created: latency-svc-dp6br
Mar 13 00:22:08.722: INFO: Got endpoints: latency-svc-rtwrm [748.352657ms]
Mar 13 00:22:08.737: INFO: Created: latency-svc-wl2kn
Mar 13 00:22:08.776: INFO: Got endpoints: latency-svc-6nmjp [750.369382ms]
Mar 13 00:22:08.798: INFO: Created: latency-svc-jx2k8
Mar 13 00:22:08.823: INFO: Got endpoints: latency-svc-9qdc7 [747.28289ms]
Mar 13 00:22:08.835: INFO: Created: latency-svc-v6r9d
Mar 13 00:22:08.876: INFO: Got endpoints: latency-svc-5crfs [748.133538ms]
Mar 13 00:22:08.892: INFO: Created: latency-svc-l4d56
Mar 13 00:22:08.924: INFO: Got endpoints: latency-svc-8xzh9 [750.308214ms]
Mar 13 00:22:08.945: INFO: Created: latency-svc-tkz5s
Mar 13 00:22:08.975: INFO: Got endpoints: latency-svc-kctgd [751.605279ms]
Mar 13 00:22:08.990: INFO: Created: latency-svc-s9t4x
Mar 13 00:22:09.023: INFO: Got endpoints: latency-svc-5w7qx [750.238176ms]
Mar 13 00:22:09.039: INFO: Created: latency-svc-5z7p5
Mar 13 00:22:09.075: INFO: Got endpoints: latency-svc-7n5qn [750.047627ms]
Mar 13 00:22:09.091: INFO: Created: latency-svc-7lzfm
Mar 13 00:22:09.125: INFO: Got endpoints: latency-svc-dnrzg [750.645216ms]
Mar 13 00:22:09.144: INFO: Created: latency-svc-mwq69
Mar 13 00:22:09.176: INFO: Got endpoints: latency-svc-t4c2d [751.824558ms]
Mar 13 00:22:09.195: INFO: Created: latency-svc-n52zw
Mar 13 00:22:09.224: INFO: Got endpoints: latency-svc-v9j7t [752.709361ms]
Mar 13 00:22:09.243: INFO: Created: latency-svc-w8mv8
Mar 13 00:22:09.276: INFO: Got endpoints: latency-svc-4chb5 [753.17602ms]
Mar 13 00:22:09.291: INFO: Created: latency-svc-9kv9q
Mar 13 00:22:09.322: INFO: Got endpoints: latency-svc-t67vk [748.606958ms]
Mar 13 00:22:09.341: INFO: Created: latency-svc-rlwgf
Mar 13 00:22:09.375: INFO: Got endpoints: latency-svc-dpghd [750.320635ms]
Mar 13 00:22:09.398: INFO: Created: latency-svc-7mgg4
Mar 13 00:22:09.424: INFO: Got endpoints: latency-svc-dp6br [747.194858ms]
Mar 13 00:22:09.440: INFO: Created: latency-svc-72gd8
Mar 13 00:22:09.476: INFO: Got endpoints: latency-svc-wl2kn [754.015738ms]
Mar 13 00:22:09.496: INFO: Created: latency-svc-pfgr8
Mar 13 00:22:09.526: INFO: Got endpoints: latency-svc-jx2k8 [750.31418ms]
Mar 13 00:22:09.547: INFO: Created: latency-svc-4mk6g
Mar 13 00:22:09.577: INFO: Got endpoints: latency-svc-v6r9d [754.070478ms]
Mar 13 00:22:09.597: INFO: Created: latency-svc-67jjq
Mar 13 00:22:09.628: INFO: Got endpoints: latency-svc-l4d56 [752.147731ms]
Mar 13 00:22:09.644: INFO: Created: latency-svc-zdksw
Mar 13 00:22:09.675: INFO: Got endpoints: latency-svc-tkz5s [750.758185ms]
Mar 13 00:22:09.696: INFO: Created: latency-svc-q2jfc
Mar 13 00:22:09.725: INFO: Got endpoints: latency-svc-s9t4x [749.778447ms]
Mar 13 00:22:09.750: INFO: Created: latency-svc-n4c24
Mar 13 00:22:09.775: INFO: Got endpoints: latency-svc-5z7p5 [751.255424ms]
Mar 13 00:22:09.792: INFO: Created: latency-svc-mx9rr
Mar 13 00:22:09.829: INFO: Got endpoints: latency-svc-7lzfm [753.831136ms]
Mar 13 00:22:09.847: INFO: Created: latency-svc-s7b8t
Mar 13 00:22:09.877: INFO: Got endpoints: latency-svc-mwq69 [752.361947ms]
Mar 13 00:22:09.896: INFO: Created: latency-svc-mmc8q
Mar 13 00:22:09.925: INFO: Got endpoints: latency-svc-n52zw [748.093803ms]
Mar 13 00:22:09.944: INFO: Created: latency-svc-rgvx6
Mar 13 00:22:09.984: INFO: Got endpoints: latency-svc-w8mv8 [759.258865ms]
Mar 13 00:22:10.006: INFO: Created: latency-svc-px2bt
Mar 13 00:22:10.021: INFO: Got endpoints: latency-svc-9kv9q [745.629539ms]
Mar 13 00:22:10.036: INFO: Created: latency-svc-5c2zg
Mar 13 00:22:10.077: INFO: Got endpoints: latency-svc-rlwgf [754.586622ms]
Mar 13 00:22:10.100: INFO: Created: latency-svc-2v8j6
Mar 13 00:22:10.126: INFO: Got endpoints: latency-svc-7mgg4 [751.156437ms]
Mar 13 00:22:10.140: INFO: Created: latency-svc-gxljk
Mar 13 00:22:10.173: INFO: Got endpoints: latency-svc-72gd8 [749.360554ms]
Mar 13 00:22:10.192: INFO: Created: latency-svc-f6chm
Mar 13 00:22:10.224: INFO: Got endpoints: latency-svc-pfgr8 [747.866959ms]
Mar 13 00:22:10.238: INFO: Created: latency-svc-8kkpp
Mar 13 00:22:10.273: INFO: Got endpoints: latency-svc-4mk6g [747.199549ms]
Mar 13 00:22:10.289: INFO: Created: latency-svc-5bgfz
Mar 13 00:22:10.324: INFO: Got endpoints: latency-svc-67jjq [747.536198ms]
Mar 13 00:22:10.342: INFO: Created: latency-svc-2lbbj
Mar 13 00:22:10.374: INFO: Got endpoints: latency-svc-zdksw [745.653797ms]
Mar 13 00:22:10.394: INFO: Created: latency-svc-c8dkg
Mar 13 00:22:10.426: INFO: Got endpoints: latency-svc-q2jfc [750.862164ms]
Mar 13 00:22:10.439: INFO: Created: latency-svc-b5z8k
Mar 13 00:22:10.477: INFO: Got endpoints: latency-svc-n4c24 [752.337692ms]
Mar 13 00:22:10.497: INFO: Created: latency-svc-2qjjw
Mar 13 00:22:10.523: INFO: Got endpoints: latency-svc-mx9rr [748.267791ms]
Mar 13 00:22:10.540: INFO: Created: latency-svc-b8tt8
Mar 13 00:22:10.571: INFO: Got endpoints: latency-svc-s7b8t [742.040861ms]
Mar 13 00:22:10.586: INFO: Created: latency-svc-76zd8
Mar 13 00:22:10.627: INFO: Got endpoints: latency-svc-mmc8q [749.712279ms]
Mar 13 00:22:10.642: INFO: Created: latency-svc-w8v7c
Mar 13 00:22:10.676: INFO: Got endpoints: latency-svc-rgvx6 [751.18714ms]
Mar 13 00:22:10.694: INFO: Created: latency-svc-n7q58
Mar 13 00:22:10.724: INFO: Got endpoints: latency-svc-px2bt [740.055444ms]
Mar 13 00:22:10.742: INFO: Created: latency-svc-cg76x
Mar 13 00:22:10.772: INFO: Got endpoints: latency-svc-5c2zg [751.014397ms]
Mar 13 00:22:10.789: INFO: Created: latency-svc-rjll9
Mar 13 00:22:10.824: INFO: Got endpoints: latency-svc-2v8j6 [746.962959ms]
Mar 13 00:22:10.843: INFO: Created: latency-svc-rfrmn
Mar 13 00:22:10.875: INFO: Got endpoints: latency-svc-gxljk [748.367514ms]
Mar 13 00:22:10.894: INFO: Created: latency-svc-6k22q
Mar 13 00:22:10.923: INFO: Got endpoints: latency-svc-f6chm [749.456141ms]
Mar 13 00:22:10.946: INFO: Created: latency-svc-tz2tp
Mar 13 00:22:10.975: INFO: Got endpoints: latency-svc-8kkpp [750.188048ms]
Mar 13 00:22:10.997: INFO: Created: latency-svc-rg4dt
Mar 13 00:22:11.027: INFO: Got endpoints: latency-svc-5bgfz [753.493537ms]
Mar 13 00:22:11.049: INFO: Created: latency-svc-qr86b
Mar 13 00:22:11.073: INFO: Got endpoints: latency-svc-2lbbj [748.839681ms]
Mar 13 00:22:11.090: INFO: Created: latency-svc-zw8gx
Mar 13 00:22:11.133: INFO: Got endpoints: latency-svc-c8dkg [759.147842ms]
Mar 13 00:22:11.167: INFO: Created: latency-svc-ls6t5
Mar 13 00:22:11.180: INFO: Got endpoints: latency-svc-b5z8k [753.601702ms]
Mar 13 00:22:11.200: INFO: Created: latency-svc-h9tq5
Mar 13 00:22:11.226: INFO: Got endpoints: latency-svc-2qjjw [748.217286ms]
Mar 13 00:22:11.256: INFO: Created: latency-svc-vdl7l
Mar 13 00:22:11.274: INFO: Got endpoints: latency-svc-b8tt8 [750.922649ms]
Mar 13 00:22:11.297: INFO: Created: latency-svc-7bsb2
Mar 13 00:22:11.329: INFO: Got endpoints: latency-svc-76zd8 [757.531411ms]
Mar 13 00:22:11.361: INFO: Created: latency-svc-vlbdc
Mar 13 00:22:11.374: INFO: Got endpoints: latency-svc-w8v7c [746.834242ms]
Mar 13 00:22:11.391: INFO: Created: latency-svc-8dkwj
Mar 13 00:22:11.437: INFO: Got endpoints: latency-svc-n7q58 [760.735441ms]
Mar 13 00:22:11.473: INFO: Created: latency-svc-swmx4
Mar 13 00:22:11.478: INFO: Got endpoints: latency-svc-cg76x [753.361554ms]
Mar 13 00:22:11.490: INFO: Created: latency-svc-5z974
Mar 13 00:22:11.524: INFO: Got endpoints: latency-svc-rjll9 [751.467257ms]
Mar 13 00:22:11.541: INFO: Created: latency-svc-pmc82
Mar 13 00:22:11.575: INFO: Got endpoints: latency-svc-rfrmn [751.27769ms]
Mar 13 00:22:11.587: INFO: Created: latency-svc-wkw5t
Mar 13 00:22:11.626: INFO: Got endpoints: latency-svc-6k22q [751.408791ms]
Mar 13 00:22:11.645: INFO: Created: latency-svc-qndlb
Mar 13 00:22:11.675: INFO: Got endpoints: latency-svc-tz2tp [751.971031ms]
Mar 13 00:22:11.689: INFO: Created: latency-svc-t6l2r
Mar 13 00:22:11.722: INFO: Got endpoints: latency-svc-rg4dt [747.452294ms]
Mar 13 00:22:11.737: INFO: Created: latency-svc-7b2kc
Mar 13 00:22:11.778: INFO: Got endpoints: latency-svc-qr86b [750.777335ms]
Mar 13 00:22:11.793: INFO: Created: latency-svc-fg6jv
Mar 13 00:22:11.824: INFO: Got endpoints: latency-svc-zw8gx [750.384755ms]
Mar 13 00:22:11.837: INFO: Created: latency-svc-fqz8t
Mar 13 00:22:11.874: INFO: Got endpoints: latency-svc-ls6t5 [741.306166ms]
Mar 13 00:22:11.905: INFO: Created: latency-svc-rfdxs
Mar 13 00:22:11.924: INFO: Got endpoints: latency-svc-h9tq5 [743.914763ms]
Mar 13 00:22:11.943: INFO: Created: latency-svc-hlg7n
Mar 13 00:22:11.978: INFO: Got endpoints: latency-svc-vdl7l [751.780128ms]
Mar 13 00:22:11.993: INFO: Created: latency-svc-bswhx
Mar 13 00:22:12.023: INFO: Got endpoints: latency-svc-7bsb2 [748.73547ms]
Mar 13 00:22:12.057: INFO: Created: latency-svc-6pcjt
Mar 13 00:22:12.077: INFO: Got endpoints: latency-svc-vlbdc [748.5629ms]
Mar 13 00:22:12.103: INFO: Created: latency-svc-mxf7g
Mar 13 00:22:12.124: INFO: Got endpoints: latency-svc-8dkwj [749.39017ms]
Mar 13 00:22:12.141: INFO: Created: latency-svc-kbv5g
Mar 13 00:22:12.177: INFO: Got endpoints: latency-svc-swmx4 [739.672333ms]
Mar 13 00:22:12.193: INFO: Created: latency-svc-lsggf
Mar 13 00:22:12.224: INFO: Got endpoints: latency-svc-5z974 [746.201716ms]
Mar 13 00:22:12.239: INFO: Created: latency-svc-rqsqr
Mar 13 00:22:12.274: INFO: Got endpoints: latency-svc-pmc82 [749.937096ms]
Mar 13 00:22:12.288: INFO: Created: latency-svc-6sl7w
Mar 13 00:22:12.325: INFO: Got endpoints: latency-svc-wkw5t [749.828456ms]
Mar 13 00:22:12.340: INFO: Created: latency-svc-lcsq5
Mar 13 00:22:12.373: INFO: Got endpoints: latency-svc-qndlb [746.910934ms]
Mar 13 00:22:12.391: INFO: Created: latency-svc-qvdsx
Mar 13 00:22:12.424: INFO: Got endpoints: latency-svc-t6l2r [748.998802ms]
Mar 13 00:22:12.440: INFO: Created: latency-svc-kv98h
Mar 13 00:22:12.478: INFO: Got endpoints: latency-svc-7b2kc [755.704254ms]
Mar 13 00:22:12.501: INFO: Created: latency-svc-m85qj
Mar 13 00:22:12.525: INFO: Got endpoints: latency-svc-fg6jv [746.500337ms]
Mar 13 00:22:12.541: INFO: Created: latency-svc-ndfq6
Mar 13 00:22:12.573: INFO: Got endpoints: latency-svc-fqz8t [748.582533ms]
Mar 13 00:22:12.589: INFO: Created: latency-svc-kcdkd
Mar 13 00:22:12.624: INFO: Got endpoints: latency-svc-rfdxs [749.362002ms]
Mar 13 00:22:12.639: INFO: Created: latency-svc-6swhq
Mar 13 00:22:12.675: INFO: Got endpoints: latency-svc-hlg7n [750.82753ms]
Mar 13 00:22:12.692: INFO: Created: latency-svc-9lr4p
Mar 13 00:22:12.724: INFO: Got endpoints: latency-svc-bswhx [745.825807ms]
Mar 13 00:22:12.739: INFO: Created: latency-svc-slzrf
Mar 13 00:22:12.773: INFO: Got endpoints: latency-svc-6pcjt [749.994374ms]
Mar 13 00:22:12.802: INFO: Created: latency-svc-bddtk
Mar 13 00:22:12.823: INFO: Got endpoints: latency-svc-mxf7g [745.331346ms]
Mar 13 00:22:12.838: INFO: Created: latency-svc-lqhk8
Mar 13 00:22:12.875: INFO: Got endpoints: latency-svc-kbv5g [750.904293ms]
Mar 13 00:22:12.892: INFO: Created: latency-svc-mvgb5
Mar 13 00:22:12.928: INFO: Got endpoints: latency-svc-lsggf [750.889214ms]
Mar 13 00:22:12.956: INFO: Created: latency-svc-gxzpf
Mar 13 00:22:12.978: INFO: Got endpoints: latency-svc-rqsqr [754.07427ms]
Mar 13 00:22:13.003: INFO: Created: latency-svc-zntrs
Mar 13 00:22:13.037: INFO: Got endpoints: latency-svc-6sl7w [762.291761ms]
Mar 13 00:22:13.054: INFO: Created: latency-svc-2hvlf
Mar 13 00:22:13.081: INFO: Got endpoints: latency-svc-lcsq5 [755.24416ms]
Mar 13 00:22:13.098: INFO: Created: latency-svc-l4gcc
Mar 13 00:22:13.127: INFO: Got endpoints: latency-svc-qvdsx [753.292496ms]
Mar 13 00:22:13.143: INFO: Created: latency-svc-7jjms
Mar 13 00:22:13.176: INFO: Got endpoints: latency-svc-kv98h [751.05124ms]
Mar 13 00:22:13.193: INFO: Created: latency-svc-x9mhn
Mar 13 00:22:13.225: INFO: Got endpoints: latency-svc-m85qj [746.164463ms]
Mar 13 00:22:13.245: INFO: Created: latency-svc-5qgfn
Mar 13 00:22:13.280: INFO: Got endpoints: latency-svc-ndfq6 [754.916639ms]
Mar 13 00:22:13.294: INFO: Created: latency-svc-69dtx
Mar 13 00:22:13.322: INFO: Got endpoints: latency-svc-kcdkd [749.412582ms]
Mar 13 00:22:13.341: INFO: Created: latency-svc-jf22m
Mar 13 00:22:13.375: INFO: Got endpoints: latency-svc-6swhq [751.401081ms]
Mar 13 00:22:13.394: INFO: Created: latency-svc-rwd52
Mar 13 00:22:13.423: INFO: Got endpoints: latency-svc-9lr4p [748.055414ms]
Mar 13 00:22:13.439: INFO: Created: latency-svc-47bv9
Mar 13 00:22:13.477: INFO: Got endpoints: latency-svc-slzrf [753.598795ms]
Mar 13 00:22:13.501: INFO: Created: latency-svc-dr52h
Mar 13 00:22:13.533: INFO: Got endpoints: latency-svc-bddtk [760.163255ms]
Mar 13 00:22:13.550: INFO: Created: latency-svc-qwpn9
Mar 13 00:22:13.574: INFO: Got endpoints: latency-svc-lqhk8 [750.914875ms]
Mar 13 00:22:13.586: INFO: Created: latency-svc-8rfpd
Mar 13 00:22:13.624: INFO: Got endpoints: latency-svc-mvgb5 [749.056502ms]
Mar 13 00:22:13.675: INFO: Got endpoints: latency-svc-gxzpf [747.373394ms]
Mar 13 00:22:13.726: INFO: Got endpoints: latency-svc-zntrs [748.019581ms]
Mar 13 00:22:13.773: INFO: Got endpoints: latency-svc-2hvlf [736.071117ms]
Mar 13 00:22:13.825: INFO: Got endpoints: latency-svc-l4gcc [744.26613ms]
Mar 13 00:22:13.874: INFO: Got endpoints: latency-svc-7jjms [746.567394ms]
Mar 13 00:22:13.924: INFO: Got endpoints: latency-svc-x9mhn [748.439928ms]
Mar 13 00:22:13.975: INFO: Got endpoints: latency-svc-5qgfn [750.330769ms]
Mar 13 00:22:14.026: INFO: Got endpoints: latency-svc-69dtx [745.785548ms]
Mar 13 00:22:14.079: INFO: Got endpoints: latency-svc-jf22m [756.958226ms]
Mar 13 00:22:14.125: INFO: Got endpoints: latency-svc-rwd52 [748.826947ms]
Mar 13 00:22:14.175: INFO: Got endpoints: latency-svc-47bv9 [751.193833ms]
Mar 13 00:22:14.224: INFO: Got endpoints: latency-svc-dr52h [746.354222ms]
Mar 13 00:22:14.274: INFO: Got endpoints: latency-svc-qwpn9 [739.994501ms]
Mar 13 00:22:14.323: INFO: Got endpoints: latency-svc-8rfpd [748.524626ms]
Mar 13 00:22:14.323: INFO: Latencies: [55.978711ms 77.780195ms 98.108623ms 106.962072ms 138.23307ms 160.845734ms 185.009764ms 195.573972ms 203.137054ms 205.689299ms 206.31629ms 206.853642ms 209.446744ms 211.270909ms 212.503485ms 214.395625ms 215.323303ms 215.682075ms 216.049549ms 217.741498ms 218.091892ms 220.41321ms 220.886662ms 222.813375ms 223.152844ms 223.39023ms 225.48912ms 225.619977ms 227.410678ms 229.510378ms 235.524116ms 237.896744ms 245.228155ms 247.623278ms 262.926673ms 270.933277ms 273.088115ms 282.574442ms 286.678738ms 291.080345ms 294.520984ms 294.900481ms 308.672154ms 312.655971ms 317.641224ms 330.026802ms 349.706587ms 353.7876ms 393.319858ms 422.726923ms 447.735598ms 490.514963ms 530.123296ms 543.267219ms 561.63177ms 606.706467ms 642.151301ms 727.698191ms 736.071117ms 739.672333ms 739.994501ms 740.055444ms 741.306166ms 742.040861ms 743.613343ms 743.914763ms 744.26613ms 744.546288ms 745.331346ms 745.629539ms 745.653797ms 745.767051ms 745.785548ms 745.825807ms 746.164463ms 746.201716ms 746.354222ms 746.500337ms 746.567394ms 746.834242ms 746.901167ms 746.910934ms 746.962959ms 747.06811ms 747.194858ms 747.199549ms 747.28289ms 747.373394ms 747.452294ms 747.536198ms 747.703088ms 747.866959ms 747.908672ms 748.019581ms 748.055414ms 748.093803ms 748.133538ms 748.217286ms 748.256272ms 748.267791ms 748.350101ms 748.352657ms 748.367514ms 748.394259ms 748.439928ms 748.524626ms 748.535608ms 748.5629ms 748.582533ms 748.606958ms 748.72018ms 748.73547ms 748.826947ms 748.839681ms 748.998802ms 749.056502ms 749.153374ms 749.360554ms 749.362002ms 749.39017ms 749.412582ms 749.456141ms 749.712279ms 749.778447ms 749.828456ms 749.937096ms 749.994374ms 750.004616ms 750.047627ms 750.188048ms 750.192098ms 750.238176ms 750.270634ms 750.308214ms 750.31418ms 750.320635ms 750.330769ms 750.369382ms 750.384755ms 750.645216ms 750.758185ms 750.776236ms 750.777335ms 750.82753ms 750.862164ms 750.888913ms 750.889214ms 750.904293ms 750.914875ms 750.922649ms 751.014397ms 751.05124ms 751.156437ms 751.18714ms 751.193833ms 751.255424ms 751.27769ms 751.401081ms 751.408791ms 751.467257ms 751.567775ms 751.605279ms 751.762639ms 751.780128ms 751.824558ms 751.971031ms 752.147731ms 752.337692ms 752.361947ms 752.588322ms 752.615999ms 752.709361ms 752.878688ms 752.984249ms 753.049141ms 753.17602ms 753.292496ms 753.361554ms 753.493537ms 753.598795ms 753.601702ms 753.831136ms 754.015738ms 754.070478ms 754.07427ms 754.586622ms 754.916639ms 755.24416ms 755.638673ms 755.704254ms 756.958226ms 757.531411ms 759.147842ms 759.258865ms 760.163255ms 760.735441ms 762.291761ms 762.481698ms 797.445269ms 802.2898ms]
Mar 13 00:22:14.323: INFO: 50 %ile: 748.350101ms
Mar 13 00:22:14.323: INFO: 90 %ile: 753.601702ms
Mar 13 00:22:14.323: INFO: 99 %ile: 797.445269ms
Mar 13 00:22:14.323: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:22:14.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6335" for this suite.
Mar 13 00:22:28.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:22:28.529: INFO: namespace svc-latency-6335 deletion completed in 14.195040132s

• [SLOW TEST:26.024 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:22:28.530: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 00:22:30.529: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 00:22:32.549: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719655750, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719655750, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719655750, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719655750, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 00:22:34.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719655750, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719655750, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719655750, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719655750, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 00:22:36.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719655750, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719655750, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719655750, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719655750, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 00:22:39.568: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 00:22:39.576: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8436-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:22:40.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2900" for this suite.
Mar 13 00:22:46.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:22:47.033: INFO: namespace webhook-2900 deletion completed in 6.192777627s
STEP: Destroying namespace "webhook-2900-markers" for this suite.
Mar 13 00:22:53.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:22:53.217: INFO: namespace webhook-2900-markers deletion completed in 6.184803058s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.749 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:22:53.281: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Mar 13 00:22:53.374: INFO: Waiting up to 5m0s for pod "client-containers-7519070c-8bba-4efe-9f54-6c8901f8446b" in namespace "containers-7444" to be "success or failure"
Mar 13 00:22:53.382: INFO: Pod "client-containers-7519070c-8bba-4efe-9f54-6c8901f8446b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.876818ms
Mar 13 00:22:55.389: INFO: Pod "client-containers-7519070c-8bba-4efe-9f54-6c8901f8446b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013981369s
Mar 13 00:22:57.393: INFO: Pod "client-containers-7519070c-8bba-4efe-9f54-6c8901f8446b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018029262s
STEP: Saw pod success
Mar 13 00:22:57.393: INFO: Pod "client-containers-7519070c-8bba-4efe-9f54-6c8901f8446b" satisfied condition "success or failure"
Mar 13 00:22:57.396: INFO: Trying to get logs from node silbory-nirmata0 pod client-containers-7519070c-8bba-4efe-9f54-6c8901f8446b container test-container: <nil>
STEP: delete the pod
Mar 13 00:22:57.424: INFO: Waiting for pod client-containers-7519070c-8bba-4efe-9f54-6c8901f8446b to disappear
Mar 13 00:22:57.434: INFO: Pod client-containers-7519070c-8bba-4efe-9f54-6c8901f8446b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:22:57.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7444" for this suite.
Mar 13 00:23:03.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:23:03.622: INFO: namespace containers-7444 deletion completed in 6.176762274s

• [SLOW TEST:10.342 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:23:03.624: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 00:23:03.716: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aca09721-0807-4774-95d9-e7c1b4b3ad59" in namespace "downward-api-2351" to be "success or failure"
Mar 13 00:23:03.723: INFO: Pod "downwardapi-volume-aca09721-0807-4774-95d9-e7c1b4b3ad59": Phase="Pending", Reason="", readiness=false. Elapsed: 6.893684ms
Mar 13 00:23:05.730: INFO: Pod "downwardapi-volume-aca09721-0807-4774-95d9-e7c1b4b3ad59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013944738s
Mar 13 00:23:07.737: INFO: Pod "downwardapi-volume-aca09721-0807-4774-95d9-e7c1b4b3ad59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020969166s
STEP: Saw pod success
Mar 13 00:23:07.738: INFO: Pod "downwardapi-volume-aca09721-0807-4774-95d9-e7c1b4b3ad59" satisfied condition "success or failure"
Mar 13 00:23:07.743: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-aca09721-0807-4774-95d9-e7c1b4b3ad59 container client-container: <nil>
STEP: delete the pod
Mar 13 00:23:07.785: INFO: Waiting for pod downwardapi-volume-aca09721-0807-4774-95d9-e7c1b4b3ad59 to disappear
Mar 13 00:23:07.791: INFO: Pod downwardapi-volume-aca09721-0807-4774-95d9-e7c1b4b3ad59 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:23:07.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2351" for this suite.
Mar 13 00:23:13.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:23:13.984: INFO: namespace downward-api-2351 deletion completed in 6.183483029s

• [SLOW TEST:10.360 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:23:13.985: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-2f0a7664-964a-45d4-8176-d111bf379040 in namespace container-probe-9063
Mar 13 00:23:18.092: INFO: Started pod liveness-2f0a7664-964a-45d4-8176-d111bf379040 in namespace container-probe-9063
STEP: checking the pod's current state and verifying that restartCount is present
Mar 13 00:23:18.098: INFO: Initial restart count of pod liveness-2f0a7664-964a-45d4-8176-d111bf379040 is 0
Mar 13 00:23:34.163: INFO: Restart count of pod container-probe-9063/liveness-2f0a7664-964a-45d4-8176-d111bf379040 is now 1 (16.065628024s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:23:34.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9063" for this suite.
Mar 13 00:23:40.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:23:40.434: INFO: namespace container-probe-9063 deletion completed in 6.225559131s

• [SLOW TEST:26.450 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:23:40.445: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 00:23:40.516: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:23:46.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2152" for this suite.
Mar 13 00:23:52.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:23:53.088: INFO: namespace custom-resource-definition-2152 deletion completed in 6.165353389s

• [SLOW TEST:12.644 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:23:53.088: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-17fb92cc-652c-45dc-8756-43a7732442e1
STEP: Creating a pod to test consume secrets
Mar 13 00:23:53.169: INFO: Waiting up to 5m0s for pod "pod-secrets-736060e9-89ed-409c-891d-0d0719eabb00" in namespace "secrets-4309" to be "success or failure"
Mar 13 00:23:53.175: INFO: Pod "pod-secrets-736060e9-89ed-409c-891d-0d0719eabb00": Phase="Pending", Reason="", readiness=false. Elapsed: 6.164326ms
Mar 13 00:23:55.181: INFO: Pod "pod-secrets-736060e9-89ed-409c-891d-0d0719eabb00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012406768s
Mar 13 00:23:57.187: INFO: Pod "pod-secrets-736060e9-89ed-409c-891d-0d0719eabb00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018508529s
Mar 13 00:23:59.195: INFO: Pod "pod-secrets-736060e9-89ed-409c-891d-0d0719eabb00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026063759s
STEP: Saw pod success
Mar 13 00:23:59.195: INFO: Pod "pod-secrets-736060e9-89ed-409c-891d-0d0719eabb00" satisfied condition "success or failure"
Mar 13 00:23:59.201: INFO: Trying to get logs from node silbory-nirmata0 pod pod-secrets-736060e9-89ed-409c-891d-0d0719eabb00 container secret-env-test: <nil>
STEP: delete the pod
Mar 13 00:23:59.254: INFO: Waiting for pod pod-secrets-736060e9-89ed-409c-891d-0d0719eabb00 to disappear
Mar 13 00:23:59.259: INFO: Pod pod-secrets-736060e9-89ed-409c-891d-0d0719eabb00 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:23:59.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4309" for this suite.
Mar 13 00:24:05.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:24:05.449: INFO: namespace secrets-4309 deletion completed in 6.181511577s

• [SLOW TEST:12.361 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:24:05.450: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 13 00:24:11.591: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0313 00:24:11.591607      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:24:11.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4438" for this suite.
Mar 13 00:24:17.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:24:17.815: INFO: namespace gc-4438 deletion completed in 6.200869487s

• [SLOW TEST:12.365 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:24:17.816: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 13 00:24:22.428: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1006 pod-service-account-02bfcd39-d2c6-43a7-a0af-65da2a37b1ed -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 13 00:24:23.217: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1006 pod-service-account-02bfcd39-d2c6-43a7-a0af-65da2a37b1ed -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 13 00:24:23.810: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1006 pod-service-account-02bfcd39-d2c6-43a7-a0af-65da2a37b1ed -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:24:24.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1006" for this suite.
Mar 13 00:24:30.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:24:30.614: INFO: namespace svcaccounts-1006 deletion completed in 6.181013315s

• [SLOW TEST:12.799 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:24:30.615: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 13 00:24:38.755: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 00:24:38.763: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 00:24:40.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 00:24:40.772: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 00:24:42.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 00:24:42.770: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 00:24:44.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 00:24:44.771: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 00:24:46.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 00:24:46.769: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:24:46.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2050" for this suite.
Mar 13 00:24:58.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:24:58.973: INFO: namespace container-lifecycle-hook-2050 deletion completed in 12.178674031s

• [SLOW TEST:28.358 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:24:58.974: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 00:24:59.036: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31daf136-fc84-40d6-b2ae-349bb3967d13" in namespace "projected-5087" to be "success or failure"
Mar 13 00:24:59.040: INFO: Pod "downwardapi-volume-31daf136-fc84-40d6-b2ae-349bb3967d13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.286468ms
Mar 13 00:25:01.046: INFO: Pod "downwardapi-volume-31daf136-fc84-40d6-b2ae-349bb3967d13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010152813s
Mar 13 00:25:03.053: INFO: Pod "downwardapi-volume-31daf136-fc84-40d6-b2ae-349bb3967d13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017667824s
STEP: Saw pod success
Mar 13 00:25:03.053: INFO: Pod "downwardapi-volume-31daf136-fc84-40d6-b2ae-349bb3967d13" satisfied condition "success or failure"
Mar 13 00:25:03.058: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-31daf136-fc84-40d6-b2ae-349bb3967d13 container client-container: <nil>
STEP: delete the pod
Mar 13 00:25:03.098: INFO: Waiting for pod downwardapi-volume-31daf136-fc84-40d6-b2ae-349bb3967d13 to disappear
Mar 13 00:25:03.111: INFO: Pod downwardapi-volume-31daf136-fc84-40d6-b2ae-349bb3967d13 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:25:03.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5087" for this suite.
Mar 13 00:25:09.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:25:09.296: INFO: namespace projected-5087 deletion completed in 6.177203847s

• [SLOW TEST:10.322 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:25:09.297: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-fa7cbced-4064-4c6e-b34c-a27cc23cf5b0
STEP: Creating a pod to test consume secrets
Mar 13 00:25:09.443: INFO: Waiting up to 5m0s for pod "pod-secrets-ae396bc7-2c30-4ece-90d0-1e593253f4ae" in namespace "secrets-6178" to be "success or failure"
Mar 13 00:25:09.448: INFO: Pod "pod-secrets-ae396bc7-2c30-4ece-90d0-1e593253f4ae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.073214ms
Mar 13 00:25:11.456: INFO: Pod "pod-secrets-ae396bc7-2c30-4ece-90d0-1e593253f4ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013554853s
Mar 13 00:25:13.463: INFO: Pod "pod-secrets-ae396bc7-2c30-4ece-90d0-1e593253f4ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02070704s
STEP: Saw pod success
Mar 13 00:25:13.464: INFO: Pod "pod-secrets-ae396bc7-2c30-4ece-90d0-1e593253f4ae" satisfied condition "success or failure"
Mar 13 00:25:13.469: INFO: Trying to get logs from node silbory-nirmata0 pod pod-secrets-ae396bc7-2c30-4ece-90d0-1e593253f4ae container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 00:25:13.510: INFO: Waiting for pod pod-secrets-ae396bc7-2c30-4ece-90d0-1e593253f4ae to disappear
Mar 13 00:25:13.515: INFO: Pod pod-secrets-ae396bc7-2c30-4ece-90d0-1e593253f4ae no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:25:13.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6178" for this suite.
Mar 13 00:25:19.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:25:19.713: INFO: namespace secrets-6178 deletion completed in 6.19047572s
STEP: Destroying namespace "secret-namespace-3482" for this suite.
Mar 13 00:25:25.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:25:25.889: INFO: namespace secret-namespace-3482 deletion completed in 6.176355531s

• [SLOW TEST:16.592 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:25:25.891: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Mar 13 00:25:25.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-9581'
Mar 13 00:25:26.540: INFO: stderr: ""
Mar 13 00:25:26.540: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 13 00:25:27.547: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:25:27.547: INFO: Found 0 / 1
Mar 13 00:25:28.548: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:25:28.548: INFO: Found 0 / 1
Mar 13 00:25:29.548: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:25:29.548: INFO: Found 0 / 1
Mar 13 00:25:30.548: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:25:30.548: INFO: Found 0 / 1
Mar 13 00:25:31.548: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:25:31.548: INFO: Found 0 / 1
Mar 13 00:25:32.548: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:25:32.548: INFO: Found 0 / 1
Mar 13 00:25:33.548: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:25:33.548: INFO: Found 1 / 1
Mar 13 00:25:33.548: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 13 00:25:33.554: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:25:33.554: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 13 00:25:33.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 patch pod redis-master-ljsbb --namespace=kubectl-9581 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 13 00:25:33.783: INFO: stderr: ""
Mar 13 00:25:33.783: INFO: stdout: "pod/redis-master-ljsbb patched\n"
STEP: checking annotations
Mar 13 00:25:33.790: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:25:33.790: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:25:33.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9581" for this suite.
Mar 13 00:26:01.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:26:01.978: INFO: namespace kubectl-9581 deletion completed in 28.181922932s

• [SLOW TEST:36.087 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:26:01.981: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 13 00:26:02.079: INFO: Waiting up to 5m0s for pod "pod-9990a960-2b7a-4880-a2b4-8f8a0820be05" in namespace "emptydir-7110" to be "success or failure"
Mar 13 00:26:02.085: INFO: Pod "pod-9990a960-2b7a-4880-a2b4-8f8a0820be05": Phase="Pending", Reason="", readiness=false. Elapsed: 5.711473ms
Mar 13 00:26:04.095: INFO: Pod "pod-9990a960-2b7a-4880-a2b4-8f8a0820be05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015940073s
Mar 13 00:26:06.103: INFO: Pod "pod-9990a960-2b7a-4880-a2b4-8f8a0820be05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023238111s
STEP: Saw pod success
Mar 13 00:26:06.103: INFO: Pod "pod-9990a960-2b7a-4880-a2b4-8f8a0820be05" satisfied condition "success or failure"
Mar 13 00:26:06.108: INFO: Trying to get logs from node silbory-nirmata0 pod pod-9990a960-2b7a-4880-a2b4-8f8a0820be05 container test-container: <nil>
STEP: delete the pod
Mar 13 00:26:06.156: INFO: Waiting for pod pod-9990a960-2b7a-4880-a2b4-8f8a0820be05 to disappear
Mar 13 00:26:06.162: INFO: Pod pod-9990a960-2b7a-4880-a2b4-8f8a0820be05 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:26:06.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7110" for this suite.
Mar 13 00:26:12.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:26:12.344: INFO: namespace emptydir-7110 deletion completed in 6.174459254s

• [SLOW TEST:10.363 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:26:12.344: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 13 00:26:12.447: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3467 /api/v1/namespaces/watch-3467/configmaps/e2e-watch-test-label-changed 23b68c7b-1480-4bc8-a33d-54390e65fea6 3163 0 2020-03-13 00:26:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 13 00:26:12.448: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3467 /api/v1/namespaces/watch-3467/configmaps/e2e-watch-test-label-changed 23b68c7b-1480-4bc8-a33d-54390e65fea6 3164 0 2020-03-13 00:26:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 13 00:26:12.448: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3467 /api/v1/namespaces/watch-3467/configmaps/e2e-watch-test-label-changed 23b68c7b-1480-4bc8-a33d-54390e65fea6 3165 0 2020-03-13 00:26:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 13 00:26:22.497: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3467 /api/v1/namespaces/watch-3467/configmaps/e2e-watch-test-label-changed 23b68c7b-1480-4bc8-a33d-54390e65fea6 3173 0 2020-03-13 00:26:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 13 00:26:22.498: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3467 /api/v1/namespaces/watch-3467/configmaps/e2e-watch-test-label-changed 23b68c7b-1480-4bc8-a33d-54390e65fea6 3174 0 2020-03-13 00:26:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar 13 00:26:22.498: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3467 /api/v1/namespaces/watch-3467/configmaps/e2e-watch-test-label-changed 23b68c7b-1480-4bc8-a33d-54390e65fea6 3175 0 2020-03-13 00:26:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:26:22.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3467" for this suite.
Mar 13 00:26:28.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:26:28.675: INFO: namespace watch-3467 deletion completed in 6.169145541s

• [SLOW TEST:16.331 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:26:28.676: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2464.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2464.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2464.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2464.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2464.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2464.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2464.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2464.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2464.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2464.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2464.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2464.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2464.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2464.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2464.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2464.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2464.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2464.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 13 00:26:52.801: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2464.svc.cluster.local from pod dns-2464/dns-test-f6d55ec7-5215-49e4-a033-e43c3f79bb64: the server could not find the requested resource (get pods dns-test-f6d55ec7-5215-49e4-a033-e43c3f79bb64)
Mar 13 00:26:52.808: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2464.svc.cluster.local from pod dns-2464/dns-test-f6d55ec7-5215-49e4-a033-e43c3f79bb64: the server could not find the requested resource (get pods dns-test-f6d55ec7-5215-49e4-a033-e43c3f79bb64)
Mar 13 00:26:52.816: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2464.svc.cluster.local from pod dns-2464/dns-test-f6d55ec7-5215-49e4-a033-e43c3f79bb64: the server could not find the requested resource (get pods dns-test-f6d55ec7-5215-49e4-a033-e43c3f79bb64)
Mar 13 00:26:52.824: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2464.svc.cluster.local from pod dns-2464/dns-test-f6d55ec7-5215-49e4-a033-e43c3f79bb64: the server could not find the requested resource (get pods dns-test-f6d55ec7-5215-49e4-a033-e43c3f79bb64)
Mar 13 00:26:52.868: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2464.svc.cluster.local from pod dns-2464/dns-test-f6d55ec7-5215-49e4-a033-e43c3f79bb64: the server could not find the requested resource (get pods dns-test-f6d55ec7-5215-49e4-a033-e43c3f79bb64)
Mar 13 00:26:52.883: INFO: Lookups using dns-2464/dns-test-f6d55ec7-5215-49e4-a033-e43c3f79bb64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2464.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2464.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2464.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2464.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2464.svc.cluster.local]

Mar 13 00:26:57.974: INFO: DNS probes using dns-2464/dns-test-f6d55ec7-5215-49e4-a033-e43c3f79bb64 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:26:58.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2464" for this suite.
Mar 13 00:27:04.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:27:04.306: INFO: namespace dns-2464 deletion completed in 6.216131011s

• [SLOW TEST:35.630 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:27:04.308: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Mar 13 00:27:04.382: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar 13 00:27:04.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-2738'
Mar 13 00:27:04.797: INFO: stderr: ""
Mar 13 00:27:04.797: INFO: stdout: "service/redis-slave created\n"
Mar 13 00:27:04.797: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar 13 00:27:04.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-2738'
Mar 13 00:27:05.190: INFO: stderr: ""
Mar 13 00:27:05.190: INFO: stdout: "service/redis-master created\n"
Mar 13 00:27:05.191: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 13 00:27:05.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-2738'
Mar 13 00:27:05.596: INFO: stderr: ""
Mar 13 00:27:05.596: INFO: stdout: "service/frontend created\n"
Mar 13 00:27:05.596: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar 13 00:27:05.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-2738'
Mar 13 00:27:06.010: INFO: stderr: ""
Mar 13 00:27:06.010: INFO: stdout: "deployment.apps/frontend created\n"
Mar 13 00:27:06.010: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 13 00:27:06.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-2738'
Mar 13 00:27:06.408: INFO: stderr: ""
Mar 13 00:27:06.408: INFO: stdout: "deployment.apps/redis-master created\n"
Mar 13 00:27:06.408: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar 13 00:27:06.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-2738'
Mar 13 00:27:06.838: INFO: stderr: ""
Mar 13 00:27:06.838: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Mar 13 00:27:06.838: INFO: Waiting for all frontend pods to be Running.
Mar 13 00:27:36.892: INFO: Waiting for frontend to serve content.
Mar 13 00:27:36.934: INFO: Trying to add a new entry to the guestbook.
Mar 13 00:27:36.962: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar 13 00:27:36.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete --grace-period=0 --force -f - --namespace=kubectl-2738'
Mar 13 00:27:37.230: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 00:27:37.230: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar 13 00:27:37.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete --grace-period=0 --force -f - --namespace=kubectl-2738'
Mar 13 00:27:37.476: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 00:27:37.476: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 13 00:27:37.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete --grace-period=0 --force -f - --namespace=kubectl-2738'
Mar 13 00:27:37.732: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 00:27:37.732: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 13 00:27:37.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete --grace-period=0 --force -f - --namespace=kubectl-2738'
Mar 13 00:27:37.955: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 00:27:37.955: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 13 00:27:37.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete --grace-period=0 --force -f - --namespace=kubectl-2738'
Mar 13 00:27:38.218: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 00:27:38.218: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 13 00:27:38.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete --grace-period=0 --force -f - --namespace=kubectl-2738'
Mar 13 00:27:38.506: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 00:27:38.506: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:27:38.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2738" for this suite.
Mar 13 00:28:06.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:28:06.696: INFO: namespace kubectl-2738 deletion completed in 28.177542328s

• [SLOW TEST:62.388 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:28:06.698: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-fab44246-20be-420c-94a2-a02ba94a99bf in namespace container-probe-1088
Mar 13 00:28:10.796: INFO: Started pod test-webserver-fab44246-20be-420c-94a2-a02ba94a99bf in namespace container-probe-1088
STEP: checking the pod's current state and verifying that restartCount is present
Mar 13 00:28:10.801: INFO: Initial restart count of pod test-webserver-fab44246-20be-420c-94a2-a02ba94a99bf is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:32:11.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1088" for this suite.
Mar 13 00:32:17.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:32:17.869: INFO: namespace container-probe-1088 deletion completed in 6.179329627s

• [SLOW TEST:251.171 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:32:17.870: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2834
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 13 00:32:17.940: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 13 00:32:42.126: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.29 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2834 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 00:32:42.126: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:32:43.487: INFO: Found all expected endpoints: [netserver-0]
Mar 13 00:32:43.493: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.7 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2834 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 00:32:43.493: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:32:44.940: INFO: Found all expected endpoints: [netserver-1]
Mar 13 00:32:44.946: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.8 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2834 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 00:32:44.946: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:32:46.389: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:32:46.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2834" for this suite.
Mar 13 00:32:58.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:32:58.574: INFO: namespace pod-network-test-2834 deletion completed in 12.174303331s

• [SLOW TEST:40.704 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:32:58.575: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 13 00:33:06.730: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1345 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 00:33:06.730: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:33:07.100: INFO: Exec stderr: ""
Mar 13 00:33:07.100: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1345 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 00:33:07.101: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:33:07.538: INFO: Exec stderr: ""
Mar 13 00:33:07.538: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1345 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 00:33:07.538: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:33:07.949: INFO: Exec stderr: ""
Mar 13 00:33:07.949: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1345 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 00:33:07.949: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:33:08.413: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 13 00:33:08.414: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1345 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 00:33:08.414: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:33:08.796: INFO: Exec stderr: ""
Mar 13 00:33:08.796: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1345 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 00:33:08.796: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:33:09.256: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 13 00:33:09.256: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1345 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 00:33:09.256: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:33:09.660: INFO: Exec stderr: ""
Mar 13 00:33:09.660: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1345 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 00:33:09.660: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:33:10.136: INFO: Exec stderr: ""
Mar 13 00:33:10.136: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1345 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 00:33:10.136: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:33:10.468: INFO: Exec stderr: ""
Mar 13 00:33:10.468: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1345 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 00:33:10.468: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:33:10.941: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:33:10.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1345" for this suite.
Mar 13 00:33:56.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:33:57.129: INFO: namespace e2e-kubelet-etc-hosts-1345 deletion completed in 46.179238435s

• [SLOW TEST:58.554 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:33:57.130: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-9706325c-c9e3-499a-a052-ac2d418d374e
STEP: Creating configMap with name cm-test-opt-upd-4ca5d483-b039-449e-8bfa-114e6a955c54
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9706325c-c9e3-499a-a052-ac2d418d374e
STEP: Updating configmap cm-test-opt-upd-4ca5d483-b039-449e-8bfa-114e6a955c54
STEP: Creating configMap with name cm-test-opt-create-193b1e39-26e6-44d4-8059-987b241876ae
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:34:03.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1548" for this suite.
Mar 13 00:34:15.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:34:15.612: INFO: namespace projected-1548 deletion completed in 12.185679501s

• [SLOW TEST:18.483 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:34:15.613: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-q9f6
STEP: Creating a pod to test atomic-volume-subpath
Mar 13 00:34:15.725: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-q9f6" in namespace "subpath-2364" to be "success or failure"
Mar 13 00:34:15.733: INFO: Pod "pod-subpath-test-projected-q9f6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.202709ms
Mar 13 00:34:17.741: INFO: Pod "pod-subpath-test-projected-q9f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015817319s
Mar 13 00:34:19.748: INFO: Pod "pod-subpath-test-projected-q9f6": Phase="Running", Reason="", readiness=true. Elapsed: 4.023320085s
Mar 13 00:34:21.755: INFO: Pod "pod-subpath-test-projected-q9f6": Phase="Running", Reason="", readiness=true. Elapsed: 6.029935507s
Mar 13 00:34:23.761: INFO: Pod "pod-subpath-test-projected-q9f6": Phase="Running", Reason="", readiness=true. Elapsed: 8.035881736s
Mar 13 00:34:25.768: INFO: Pod "pod-subpath-test-projected-q9f6": Phase="Running", Reason="", readiness=true. Elapsed: 10.042646177s
Mar 13 00:34:27.774: INFO: Pod "pod-subpath-test-projected-q9f6": Phase="Running", Reason="", readiness=true. Elapsed: 12.04948673s
Mar 13 00:34:29.782: INFO: Pod "pod-subpath-test-projected-q9f6": Phase="Running", Reason="", readiness=true. Elapsed: 14.057202433s
Mar 13 00:34:31.789: INFO: Pod "pod-subpath-test-projected-q9f6": Phase="Running", Reason="", readiness=true. Elapsed: 16.064092128s
Mar 13 00:34:33.796: INFO: Pod "pod-subpath-test-projected-q9f6": Phase="Running", Reason="", readiness=true. Elapsed: 18.071510977s
Mar 13 00:34:35.803: INFO: Pod "pod-subpath-test-projected-q9f6": Phase="Running", Reason="", readiness=true. Elapsed: 20.077971208s
Mar 13 00:34:37.811: INFO: Pod "pod-subpath-test-projected-q9f6": Phase="Running", Reason="", readiness=true. Elapsed: 22.085723574s
Mar 13 00:34:39.817: INFO: Pod "pod-subpath-test-projected-q9f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.092318136s
STEP: Saw pod success
Mar 13 00:34:39.817: INFO: Pod "pod-subpath-test-projected-q9f6" satisfied condition "success or failure"
Mar 13 00:34:39.823: INFO: Trying to get logs from node silbory-nirmata0 pod pod-subpath-test-projected-q9f6 container test-container-subpath-projected-q9f6: <nil>
STEP: delete the pod
Mar 13 00:34:39.856: INFO: Waiting for pod pod-subpath-test-projected-q9f6 to disappear
Mar 13 00:34:39.864: INFO: Pod pod-subpath-test-projected-q9f6 no longer exists
STEP: Deleting pod pod-subpath-test-projected-q9f6
Mar 13 00:34:39.864: INFO: Deleting pod "pod-subpath-test-projected-q9f6" in namespace "subpath-2364"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:34:39.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2364" for this suite.
Mar 13 00:34:45.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:34:46.058: INFO: namespace subpath-2364 deletion completed in 6.180261162s

• [SLOW TEST:30.445 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:34:46.059: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-78660edd-3c14-489f-a318-841bcd6ece35
STEP: Creating a pod to test consume secrets
Mar 13 00:34:46.162: INFO: Waiting up to 5m0s for pod "pod-secrets-ccfcd2bd-8292-41ec-bd60-60297a035dc7" in namespace "secrets-8809" to be "success or failure"
Mar 13 00:34:46.168: INFO: Pod "pod-secrets-ccfcd2bd-8292-41ec-bd60-60297a035dc7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.857168ms
Mar 13 00:34:48.175: INFO: Pod "pod-secrets-ccfcd2bd-8292-41ec-bd60-60297a035dc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013704063s
Mar 13 00:34:50.183: INFO: Pod "pod-secrets-ccfcd2bd-8292-41ec-bd60-60297a035dc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020994741s
STEP: Saw pod success
Mar 13 00:34:50.183: INFO: Pod "pod-secrets-ccfcd2bd-8292-41ec-bd60-60297a035dc7" satisfied condition "success or failure"
Mar 13 00:34:50.189: INFO: Trying to get logs from node silbory-nirmata0 pod pod-secrets-ccfcd2bd-8292-41ec-bd60-60297a035dc7 container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 00:34:50.229: INFO: Waiting for pod pod-secrets-ccfcd2bd-8292-41ec-bd60-60297a035dc7 to disappear
Mar 13 00:34:50.236: INFO: Pod pod-secrets-ccfcd2bd-8292-41ec-bd60-60297a035dc7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:34:50.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8809" for this suite.
Mar 13 00:34:56.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:34:56.414: INFO: namespace secrets-8809 deletion completed in 6.170214729s

• [SLOW TEST:10.356 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:34:56.416: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:35:56.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9503" for this suite.
Mar 13 00:36:08.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:36:08.685: INFO: namespace container-probe-9503 deletion completed in 12.175612938s

• [SLOW TEST:72.270 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:36:08.688: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 00:36:08.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-1780'
Mar 13 00:36:09.290: INFO: stderr: ""
Mar 13 00:36:09.290: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar 13 00:36:09.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-1780'
Mar 13 00:36:09.730: INFO: stderr: ""
Mar 13 00:36:09.730: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 13 00:36:10.741: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:36:10.742: INFO: Found 0 / 1
Mar 13 00:36:11.739: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:36:11.740: INFO: Found 0 / 1
Mar 13 00:36:12.738: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:36:12.738: INFO: Found 1 / 1
Mar 13 00:36:12.738: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 13 00:36:12.744: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:36:12.744: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 13 00:36:12.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 describe pod redis-master-b4mqr --namespace=kubectl-1780'
Mar 13 00:36:13.005: INFO: stderr: ""
Mar 13 00:36:13.005: INFO: stdout: "Name:         redis-master-b4mqr\nNamespace:    kubectl-1780\nPriority:     0\nNode:         silbory-nirmata0/10.10.1.79\nStart Time:   Fri, 13 Mar 2020 00:36:09 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           10.244.2.36\nIPs:\n  IP:           10.244.2.36\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://83c8b4f95e8596695422594bdabe8e543f82875f1fa47182bc434fb6fc08e635\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 13 Mar 2020 00:36:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-68mn7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-68mn7:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-68mn7\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                       Message\n  ----    ------     ----       ----                       -------\n  Normal  Scheduled  <unknown>  default-scheduler          Successfully assigned kubectl-1780/redis-master-b4mqr to silbory-nirmata0\n  Normal  Pulled     3s         kubelet, silbory-nirmata0  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    3s         kubelet, silbory-nirmata0  Created container redis-master\n  Normal  Started    2s         kubelet, silbory-nirmata0  Started container redis-master\n"
Mar 13 00:36:13.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 describe rc redis-master --namespace=kubectl-1780'
Mar 13 00:36:13.279: INFO: stderr: ""
Mar 13 00:36:13.279: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1780\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-b4mqr\n"
Mar 13 00:36:13.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 describe service redis-master --namespace=kubectl-1780'
Mar 13 00:36:13.559: INFO: stderr: ""
Mar 13 00:36:13.559: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1780\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.10.131.199\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.2.36:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 13 00:36:13.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 describe node sam-node2'
Mar 13 00:36:13.875: INFO: stderr: ""
Mar 13 00:36:13.875: INFO: stdout: "Name:               sam-node2\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=sam-node2\n                    kubernetes.io/os=linux\n                    nirmata.io/cluster.name=sam-test-conf\n                    nirmata.io/cluster.role=worker\n                    node-role.kubernetes.io/worker=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"3a:fd:c5:7b:7f:69\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.10.1.75\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 13 Mar 2020 00:11:44 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 13 Mar 2020 00:35:44 +0000   Fri, 13 Mar 2020 00:11:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 13 Mar 2020 00:35:44 +0000   Fri, 13 Mar 2020 00:11:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 13 Mar 2020 00:35:44 +0000   Fri, 13 Mar 2020 00:11:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 13 Mar 2020 00:35:44 +0000   Fri, 13 Mar 2020 00:13:25 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.10.1.75\n  Hostname:    sam-node2\nCapacity:\n cpu:                2\n ephemeral-storage:  31166436Ki\n hugepages-2Mi:      0\n memory:             8149868Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  28722987371\n hugepages-2Mi:      0\n memory:             8047468Ki\n pods:               110\nSystem Info:\n Machine ID:                 cf9039846e817bf110c3933d5c3e0c56\n System UUID:                1FC902A2-4392-3854-7433-39AA23B7C2D0\n Boot ID:                    7a334495-509d-49a0-ba1f-c19785341d01\n Kernel Version:             4.4.0-131-generic\n OS Image:                   Debian GNU/Linux 9 (stretch)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://19.3.5\n Kubelet Version:            v1.16.0\n Kube-Proxy Version:         v1.16.0\nPodCIDR:                     10.244.1.0/24\nPodCIDRs:                    10.244.1.0/24\nNon-terminated Pods:         (4 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                kube-dns-869c556677-bfkcm                                  260m (13%)    0 (0%)      110Mi (1%)       170Mi (2%)     22m\n  kube-system                kube-flannel-ds-d9nq5                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m\n  nirmata                    nirmata-cni-installer-nkzv7                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-5206d17636204e77-w6d2v    0 (0%)        0 (0%)      0 (0%)           0 (0%)         14m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                260m (13%)  0 (0%)\n  memory             110Mi (1%)  170Mi (2%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                From                Message\n  ----    ------                   ----               ----                -------\n  Normal  NodeHasSufficientMemory  24m (x7 over 24m)  kubelet, sam-node2  Node sam-node2 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    24m (x7 over 24m)  kubelet, sam-node2  Node sam-node2 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     24m (x7 over 24m)  kubelet, sam-node2  Node sam-node2 status is now: NodeHasSufficientPID\n  Normal  NodeReady                22m                kubelet, sam-node2  Node sam-node2 status is now: NodeReady\n"
Mar 13 00:36:13.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 describe namespace kubectl-1780'
Mar 13 00:36:14.118: INFO: stderr: ""
Mar 13 00:36:14.118: INFO: stdout: "Name:         kubectl-1780\nLabels:       e2e-framework=kubectl\n              e2e-run=a9c64acf-0d86-4149-ac12-b76fa9d6c87d\nAnnotations:  nirmata.io:\n                {\"environment.modelId\":\"81a79eb1-db49-4e67-88f8-49572d9a6b41\",\"environment.name\":\"kubectl-1780-sam-test-conf\",\"modelId\":\"a388581f-7443-406...\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:36:14.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1780" for this suite.
Mar 13 00:36:26.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:36:26.309: INFO: namespace kubectl-1780 deletion completed in 12.181071426s

• [SLOW TEST:17.621 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:36:26.310: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 13 00:36:26.391: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 13 00:36:31.399: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:36:31.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6400" for this suite.
Mar 13 00:36:37.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:36:37.679: INFO: namespace replication-controller-6400 deletion completed in 6.221946181s

• [SLOW TEST:11.370 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:36:37.682: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 13 00:36:37.755: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 13 00:37:01.013: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:37:06.929: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:37:31.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5129" for this suite.
Mar 13 00:37:37.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:37:37.881: INFO: namespace crd-publish-openapi-5129 deletion completed in 6.170989594s

• [SLOW TEST:60.199 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:37:37.881: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-92xg
STEP: Creating a pod to test atomic-volume-subpath
Mar 13 00:37:37.972: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-92xg" in namespace "subpath-9519" to be "success or failure"
Mar 13 00:37:37.977: INFO: Pod "pod-subpath-test-secret-92xg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.568414ms
Mar 13 00:37:39.984: INFO: Pod "pod-subpath-test-secret-92xg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012093979s
Mar 13 00:37:41.991: INFO: Pod "pod-subpath-test-secret-92xg": Phase="Running", Reason="", readiness=true. Elapsed: 4.019337787s
Mar 13 00:37:43.998: INFO: Pod "pod-subpath-test-secret-92xg": Phase="Running", Reason="", readiness=true. Elapsed: 6.026154564s
Mar 13 00:37:46.005: INFO: Pod "pod-subpath-test-secret-92xg": Phase="Running", Reason="", readiness=true. Elapsed: 8.032770628s
Mar 13 00:37:48.011: INFO: Pod "pod-subpath-test-secret-92xg": Phase="Running", Reason="", readiness=true. Elapsed: 10.038868706s
Mar 13 00:37:50.018: INFO: Pod "pod-subpath-test-secret-92xg": Phase="Running", Reason="", readiness=true. Elapsed: 12.045649985s
Mar 13 00:37:52.024: INFO: Pod "pod-subpath-test-secret-92xg": Phase="Running", Reason="", readiness=true. Elapsed: 14.052279595s
Mar 13 00:37:54.031: INFO: Pod "pod-subpath-test-secret-92xg": Phase="Running", Reason="", readiness=true. Elapsed: 16.059318205s
Mar 13 00:37:56.039: INFO: Pod "pod-subpath-test-secret-92xg": Phase="Running", Reason="", readiness=true. Elapsed: 18.066441864s
Mar 13 00:37:58.046: INFO: Pod "pod-subpath-test-secret-92xg": Phase="Running", Reason="", readiness=true. Elapsed: 20.073758359s
Mar 13 00:38:00.053: INFO: Pod "pod-subpath-test-secret-92xg": Phase="Running", Reason="", readiness=true. Elapsed: 22.081041267s
Mar 13 00:38:02.060: INFO: Pod "pod-subpath-test-secret-92xg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.087598648s
STEP: Saw pod success
Mar 13 00:38:02.060: INFO: Pod "pod-subpath-test-secret-92xg" satisfied condition "success or failure"
Mar 13 00:38:02.065: INFO: Trying to get logs from node silbory-nirmata0 pod pod-subpath-test-secret-92xg container test-container-subpath-secret-92xg: <nil>
STEP: delete the pod
Mar 13 00:38:02.113: INFO: Waiting for pod pod-subpath-test-secret-92xg to disappear
Mar 13 00:38:02.119: INFO: Pod pod-subpath-test-secret-92xg no longer exists
STEP: Deleting pod pod-subpath-test-secret-92xg
Mar 13 00:38:02.119: INFO: Deleting pod "pod-subpath-test-secret-92xg" in namespace "subpath-9519"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:38:02.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9519" for this suite.
Mar 13 00:38:08.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:38:08.314: INFO: namespace subpath-9519 deletion completed in 6.1785301s

• [SLOW TEST:30.433 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:38:08.318: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Mar 13 00:38:08.382: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-940252648 proxy --unix-socket=/tmp/kubectl-proxy-unix227017291/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:38:08.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2064" for this suite.
Mar 13 00:38:14.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:38:14.730: INFO: namespace kubectl-2064 deletion completed in 6.176225664s

• [SLOW TEST:6.412 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:38:14.730: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 00:38:14.810: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3460dad-bf57-4ef4-8593-135b1efee52f" in namespace "projected-3407" to be "success or failure"
Mar 13 00:38:14.820: INFO: Pod "downwardapi-volume-c3460dad-bf57-4ef4-8593-135b1efee52f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020513ms
Mar 13 00:38:16.827: INFO: Pod "downwardapi-volume-c3460dad-bf57-4ef4-8593-135b1efee52f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016483385s
Mar 13 00:38:18.833: INFO: Pod "downwardapi-volume-c3460dad-bf57-4ef4-8593-135b1efee52f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023082906s
STEP: Saw pod success
Mar 13 00:38:18.834: INFO: Pod "downwardapi-volume-c3460dad-bf57-4ef4-8593-135b1efee52f" satisfied condition "success or failure"
Mar 13 00:38:18.839: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-c3460dad-bf57-4ef4-8593-135b1efee52f container client-container: <nil>
STEP: delete the pod
Mar 13 00:38:18.870: INFO: Waiting for pod downwardapi-volume-c3460dad-bf57-4ef4-8593-135b1efee52f to disappear
Mar 13 00:38:18.877: INFO: Pod downwardapi-volume-c3460dad-bf57-4ef4-8593-135b1efee52f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:38:18.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3407" for this suite.
Mar 13 00:38:24.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:38:25.064: INFO: namespace projected-3407 deletion completed in 6.180305277s

• [SLOW TEST:10.334 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:38:25.065: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 00:38:25.143: INFO: Creating deployment "webserver-deployment"
Mar 13 00:38:25.155: INFO: Waiting for observed generation 1
Mar 13 00:38:27.171: INFO: Waiting for all required pods to come up
Mar 13 00:38:27.183: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 13 00:38:41.201: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 13 00:38:41.214: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 13 00:38:41.226: INFO: Updating deployment webserver-deployment
Mar 13 00:38:41.226: INFO: Waiting for observed generation 2
Mar 13 00:38:43.239: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 13 00:38:43.245: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 13 00:38:43.250: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 13 00:38:43.271: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 13 00:38:43.272: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 13 00:38:43.277: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 13 00:38:43.285: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 13 00:38:43.285: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 13 00:38:43.296: INFO: Updating deployment webserver-deployment
Mar 13 00:38:43.297: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 13 00:38:43.308: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 13 00:38:43.318: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 13 00:38:43.353: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3084 /apis/apps/v1/namespaces/deployment-3084/deployments/webserver-deployment e6be5344-6218-4357-b825-025ff2127d1a 4560 3 2020-03-13 00:38:25 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00746aed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-13 00:38:38 +0000 UTC,LastTransitionTime:2020-03-13 00:38:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-03-13 00:38:41 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 13 00:38:43.381: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-3084 /apis/apps/v1/namespaces/deployment-3084/replicasets/webserver-deployment-c7997dcc8 da6a3a7e-00ca-430a-9271-17ee390541ea 4563 3 2020-03-13 00:38:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e6be5344-6218-4357-b825-025ff2127d1a 0xc00746b617 0xc00746b618}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00746b6a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 13 00:38:43.381: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 13 00:38:43.382: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-3084 /apis/apps/v1/namespaces/deployment-3084/replicasets/webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 4561 3 2020-03-13 00:38:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e6be5344-6218-4357-b825-025ff2127d1a 0xc00746b4b7 0xc00746b4b8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00746b528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 13 00:38:43.402: INFO: Pod "webserver-deployment-595b5b9587-4lw2t" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4lw2t webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-4lw2t 7c3763ce-119d-44a7-ab1f-c794fe42bbbb 4575 0 2020-03-13 00:38:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc0050dd4a0 0xc0050dd4a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.403: INFO: Pod "webserver-deployment-595b5b9587-4q5vx" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4q5vx webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-4q5vx ab5393a5-79b6-4db7-bec9-44aaf905ce6a 4482 0 2020-03-13 00:38:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc0050dd607 0xc0050dd608}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sam-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.75,PodIP:10.244.1.9,StartTime:2020-03-13 00:38:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-13 00:38:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b51a9dc8c9b4e7cee6c9236f5c1be4699680633245f00ecf09ab221616c2abe0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.403: INFO: Pod "webserver-deployment-595b5b9587-5rzgl" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5rzgl webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-5rzgl 1d695b6b-1e92-490b-8939-15c5f7e3da9e 4565 0 2020-03-13 00:38:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc0050dd860 0xc0050dd861}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sam-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.404: INFO: Pod "webserver-deployment-595b5b9587-5vzvs" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5vzvs webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-5vzvs 07445e83-514a-4b76-b32a-1b7922be69dc 4472 0 2020-03-13 00:38:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc0050dd9f0 0xc0050dd9f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sam-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.75,PodIP:10.244.1.8,StartTime:2020-03-13 00:38:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-13 00:38:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://84156bb94455910aad3a1ef8c6f18741f364a7c3163b2fe5814b3027348d4481,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.405: INFO: Pod "webserver-deployment-595b5b9587-bglbn" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bglbn webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-bglbn 7f4b93d5-371b-4897-a425-c2b60137c030 4579 0 2020-03-13 00:38:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc0050ddb90 0xc0050ddb91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.405: INFO: Pod "webserver-deployment-595b5b9587-bsllf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bsllf webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-bsllf a8b7add8-35da-46b3-978f-94bc27451582 4573 0 2020-03-13 00:38:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc0050ddc87 0xc0050ddc88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sam-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.406: INFO: Pod "webserver-deployment-595b5b9587-fmhs6" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fmhs6 webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-fmhs6 9358b9c5-c43d-4fec-8835-ef3e2b643ba9 4449 0 2020-03-13 00:38:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc0050ddde0 0xc0050ddde1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:silbory-nirmata0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.79,PodIP:10.244.2.44,StartTime:2020-03-13 00:38:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-13 00:38:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f7c0c55b34b88c036a5f498293626f0f432ed56f8d0ec298d1a14becdf78b20b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.406: INFO: Pod "webserver-deployment-595b5b9587-l2fvx" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-l2fvx webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-l2fvx ccd32d53-f6aa-44f8-9605-dcda3362afac 4455 0 2020-03-13 00:38:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc0050ddf80 0xc0050ddf81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:silbory-nirmata0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.79,PodIP:10.244.2.41,StartTime:2020-03-13 00:38:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-13 00:38:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://05c6a1b9a792541247f06159de0c9ed271dd5e7c0eb60249d869e8f4f0de592f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.407: INFO: Pod "webserver-deployment-595b5b9587-l7fkg" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-l7fkg webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-l7fkg d1fdf2e9-0da0-4587-be38-a2d60f975485 4476 0 2020-03-13 00:38:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc007a34250 0xc007a34251}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sam-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.238,PodIP:10.244.0.9,StartTime:2020-03-13 00:38:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-13 00:38:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://594b2980bae15ccb629daf78797b23279a9157b61f16484a8171f156fc6c3b2c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.407: INFO: Pod "webserver-deployment-595b5b9587-mxz6n" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mxz6n webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-mxz6n 68078cc0-0ae9-408a-a31f-580860a99975 4578 0 2020-03-13 00:38:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc007a34440 0xc007a34441}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:silbory-nirmata0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.407: INFO: Pod "webserver-deployment-595b5b9587-n9xxk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-n9xxk webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-n9xxk 40224120-716a-4064-bb5d-392bad3a4f76 4574 0 2020-03-13 00:38:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc007a345b0 0xc007a345b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.408: INFO: Pod "webserver-deployment-595b5b9587-rwm7w" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rwm7w webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-rwm7w 8696f6eb-9450-4005-9581-abb41eccb731 4452 0 2020-03-13 00:38:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc007a346c7 0xc007a346c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:silbory-nirmata0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.79,PodIP:10.244.2.42,StartTime:2020-03-13 00:38:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-13 00:38:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c711e800d66df26a015a364ddf9c7ee06f9cafabaf064f69ccb512bc2d8dcef8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.408: INFO: Pod "webserver-deployment-595b5b9587-tzht8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tzht8 webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-tzht8 bcc88fec-0a6f-40b6-ad91-b096e0020109 4446 0 2020-03-13 00:38:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc007a34850 0xc007a34851}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:silbory-nirmata0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.79,PodIP:10.244.2.43,StartTime:2020-03-13 00:38:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-13 00:38:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b22b5068b4cd73766b7b9c58276ea7c1bd79e3b2d4e572e328d30f4ae98bd5ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.408: INFO: Pod "webserver-deployment-595b5b9587-wtpp4" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wtpp4 webserver-deployment-595b5b9587- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-595b5b9587-wtpp4 56e4d5ac-2063-402b-ba41-f9c2665b9450 4490 0 2020-03-13 00:38:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 5485e8e7-26a6-4134-95d4-c2ed20bc9ba6 0xc007a349c0 0xc007a349c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sam-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.238,PodIP:10.244.0.10,StartTime:2020-03-13 00:38:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-13 00:38:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9fded97c28492733f1d10ef2e82e549c5c73d7b0ae32b01f5d503e73c6fca06b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.409: INFO: Pod "webserver-deployment-c7997dcc8-95r9r" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-95r9r webserver-deployment-c7997dcc8- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-c7997dcc8-95r9r 28c866e8-14bd-4090-a455-2b5805ea6618 4576 0 2020-03-13 00:38:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 da6a3a7e-00ca-430a-9271-17ee390541ea 0xc007a34bd0 0xc007a34bd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sam-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.409: INFO: Pod "webserver-deployment-c7997dcc8-dj48h" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-dj48h webserver-deployment-c7997dcc8- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-c7997dcc8-dj48h 68bbd9d3-94d9-4266-af54-60041d92a180 4525 0 2020-03-13 00:38:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 da6a3a7e-00ca-430a-9271-17ee390541ea 0xc007a34d30 0xc007a34d31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:silbory-nirmata0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.79,PodIP:,StartTime:2020-03-13 00:38:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.409: INFO: Pod "webserver-deployment-c7997dcc8-h2gf7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-h2gf7 webserver-deployment-c7997dcc8- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-c7997dcc8-h2gf7 a61c88f7-aa6b-4d6c-a533-3cbce268dda5 4527 0 2020-03-13 00:38:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 da6a3a7e-00ca-430a-9271-17ee390541ea 0xc007a34f20 0xc007a34f21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sam-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.75,PodIP:,StartTime:2020-03-13 00:38:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.410: INFO: Pod "webserver-deployment-c7997dcc8-h455j" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-h455j webserver-deployment-c7997dcc8- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-c7997dcc8-h455j 4c38a3c4-5927-4229-803f-9d10bf0915b1 4537 0 2020-03-13 00:38:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 da6a3a7e-00ca-430a-9271-17ee390541ea 0xc007a350f0 0xc007a350f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sam-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.238,PodIP:,StartTime:2020-03-13 00:38:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.410: INFO: Pod "webserver-deployment-c7997dcc8-p2ghk" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-p2ghk webserver-deployment-c7997dcc8- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-c7997dcc8-p2ghk 7db05c1a-8738-4704-85a6-cd7d2c959e48 4549 0 2020-03-13 00:38:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 da6a3a7e-00ca-430a-9271-17ee390541ea 0xc007a35280 0xc007a35281}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:silbory-nirmata0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.79,PodIP:,StartTime:2020-03-13 00:38:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 13 00:38:43.411: INFO: Pod "webserver-deployment-c7997dcc8-qnskd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qnskd webserver-deployment-c7997dcc8- deployment-3084 /api/v1/namespaces/deployment-3084/pods/webserver-deployment-c7997dcc8-qnskd 0270b9f3-978c-44ef-8d85-fcc435a6efd1 4551 0 2020-03-13 00:38:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 da6a3a7e-00ca-430a-9271-17ee390541ea 0xc007a35430 0xc007a35431}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sam-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:38:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.238,PodIP:,StartTime:2020-03-13 00:38:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:38:43.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3084" for this suite.
Mar 13 00:38:51.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:38:51.669: INFO: namespace deployment-3084 deletion completed in 8.247645596s

• [SLOW TEST:26.605 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:38:51.672: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 00:38:51.778: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20c5fb12-eb86-4f48-a90d-423b6c8f0799" in namespace "downward-api-312" to be "success or failure"
Mar 13 00:38:51.792: INFO: Pod "downwardapi-volume-20c5fb12-eb86-4f48-a90d-423b6c8f0799": Phase="Pending", Reason="", readiness=false. Elapsed: 13.694143ms
Mar 13 00:38:53.798: INFO: Pod "downwardapi-volume-20c5fb12-eb86-4f48-a90d-423b6c8f0799": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020371471s
Mar 13 00:38:55.804: INFO: Pod "downwardapi-volume-20c5fb12-eb86-4f48-a90d-423b6c8f0799": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02644788s
STEP: Saw pod success
Mar 13 00:38:55.804: INFO: Pod "downwardapi-volume-20c5fb12-eb86-4f48-a90d-423b6c8f0799" satisfied condition "success or failure"
Mar 13 00:38:55.810: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-20c5fb12-eb86-4f48-a90d-423b6c8f0799 container client-container: <nil>
STEP: delete the pod
Mar 13 00:38:55.848: INFO: Waiting for pod downwardapi-volume-20c5fb12-eb86-4f48-a90d-423b6c8f0799 to disappear
Mar 13 00:38:55.856: INFO: Pod downwardapi-volume-20c5fb12-eb86-4f48-a90d-423b6c8f0799 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:38:55.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-312" for this suite.
Mar 13 00:39:01.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:39:02.063: INFO: namespace downward-api-312 deletion completed in 6.198377017s

• [SLOW TEST:10.392 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:39:02.066: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 00:39:02.158: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f2874cf7-8f67-4724-af2c-1782943a538b" in namespace "downward-api-9126" to be "success or failure"
Mar 13 00:39:02.168: INFO: Pod "downwardapi-volume-f2874cf7-8f67-4724-af2c-1782943a538b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.821295ms
Mar 13 00:39:04.177: INFO: Pod "downwardapi-volume-f2874cf7-8f67-4724-af2c-1782943a538b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018737146s
Mar 13 00:39:06.183: INFO: Pod "downwardapi-volume-f2874cf7-8f67-4724-af2c-1782943a538b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024934411s
STEP: Saw pod success
Mar 13 00:39:06.183: INFO: Pod "downwardapi-volume-f2874cf7-8f67-4724-af2c-1782943a538b" satisfied condition "success or failure"
Mar 13 00:39:06.189: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-f2874cf7-8f67-4724-af2c-1782943a538b container client-container: <nil>
STEP: delete the pod
Mar 13 00:39:06.222: INFO: Waiting for pod downwardapi-volume-f2874cf7-8f67-4724-af2c-1782943a538b to disappear
Mar 13 00:39:06.227: INFO: Pod downwardapi-volume-f2874cf7-8f67-4724-af2c-1782943a538b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:39:06.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9126" for this suite.
Mar 13 00:39:12.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:39:12.414: INFO: namespace downward-api-9126 deletion completed in 6.175991865s

• [SLOW TEST:10.348 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:39:12.418: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 00:39:12.506: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d330af27-7e95-409d-b538-8db325ba081d" in namespace "projected-6891" to be "success or failure"
Mar 13 00:39:12.513: INFO: Pod "downwardapi-volume-d330af27-7e95-409d-b538-8db325ba081d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.852274ms
Mar 13 00:39:14.519: INFO: Pod "downwardapi-volume-d330af27-7e95-409d-b538-8db325ba081d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012620593s
Mar 13 00:39:16.535: INFO: Pod "downwardapi-volume-d330af27-7e95-409d-b538-8db325ba081d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028997816s
STEP: Saw pod success
Mar 13 00:39:16.535: INFO: Pod "downwardapi-volume-d330af27-7e95-409d-b538-8db325ba081d" satisfied condition "success or failure"
Mar 13 00:39:16.541: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-d330af27-7e95-409d-b538-8db325ba081d container client-container: <nil>
STEP: delete the pod
Mar 13 00:39:16.578: INFO: Waiting for pod downwardapi-volume-d330af27-7e95-409d-b538-8db325ba081d to disappear
Mar 13 00:39:16.582: INFO: Pod downwardapi-volume-d330af27-7e95-409d-b538-8db325ba081d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:39:16.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6891" for this suite.
Mar 13 00:39:22.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:39:22.778: INFO: namespace projected-6891 deletion completed in 6.18608644s

• [SLOW TEST:10.360 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:39:22.778: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 13 00:39:30.924: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 00:39:30.932: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 00:39:32.933: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 00:39:32.940: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 00:39:34.933: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 00:39:34.939: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 00:39:36.933: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 00:39:36.941: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:39:36.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3383" for this suite.
Mar 13 00:39:48.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:39:49.132: INFO: namespace container-lifecycle-hook-3383 deletion completed in 12.181175965s

• [SLOW TEST:26.354 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:39:49.132: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 13 00:39:49.208: INFO: Waiting up to 5m0s for pod "pod-95740658-3cfe-4a6c-a2ee-66b21b639d84" in namespace "emptydir-4190" to be "success or failure"
Mar 13 00:39:49.213: INFO: Pod "pod-95740658-3cfe-4a6c-a2ee-66b21b639d84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.896753ms
Mar 13 00:39:51.220: INFO: Pod "pod-95740658-3cfe-4a6c-a2ee-66b21b639d84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012170682s
Mar 13 00:39:53.226: INFO: Pod "pod-95740658-3cfe-4a6c-a2ee-66b21b639d84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018174763s
STEP: Saw pod success
Mar 13 00:39:53.226: INFO: Pod "pod-95740658-3cfe-4a6c-a2ee-66b21b639d84" satisfied condition "success or failure"
Mar 13 00:39:53.231: INFO: Trying to get logs from node silbory-nirmata0 pod pod-95740658-3cfe-4a6c-a2ee-66b21b639d84 container test-container: <nil>
STEP: delete the pod
Mar 13 00:39:53.266: INFO: Waiting for pod pod-95740658-3cfe-4a6c-a2ee-66b21b639d84 to disappear
Mar 13 00:39:53.272: INFO: Pod pod-95740658-3cfe-4a6c-a2ee-66b21b639d84 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:39:53.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4190" for this suite.
Mar 13 00:39:59.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:39:59.445: INFO: namespace emptydir-4190 deletion completed in 6.166292181s

• [SLOW TEST:10.313 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:39:59.446: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar 13 00:39:59.516: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:40:05.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3564" for this suite.
Mar 13 00:40:17.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:40:17.782: INFO: namespace init-container-3564 deletion completed in 12.174488021s

• [SLOW TEST:18.336 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:40:17.783: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 13 00:40:17.850: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 00:40:23.058: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:40:45.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4293" for this suite.
Mar 13 00:40:51.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:40:51.848: INFO: namespace crd-publish-openapi-4293 deletion completed in 6.171780202s

• [SLOW TEST:34.066 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:40:51.849: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-f99dd535-5f6d-4606-9382-4c240885baf6
STEP: Creating a pod to test consume configMaps
Mar 13 00:40:51.956: INFO: Waiting up to 5m0s for pod "pod-configmaps-5b1c9364-f893-4d28-9e63-75d0b18df39a" in namespace "configmap-9118" to be "success or failure"
Mar 13 00:40:51.985: INFO: Pod "pod-configmaps-5b1c9364-f893-4d28-9e63-75d0b18df39a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.261172ms
Mar 13 00:40:53.991: INFO: Pod "pod-configmaps-5b1c9364-f893-4d28-9e63-75d0b18df39a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035071867s
Mar 13 00:40:55.998: INFO: Pod "pod-configmaps-5b1c9364-f893-4d28-9e63-75d0b18df39a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042067986s
STEP: Saw pod success
Mar 13 00:40:55.999: INFO: Pod "pod-configmaps-5b1c9364-f893-4d28-9e63-75d0b18df39a" satisfied condition "success or failure"
Mar 13 00:40:56.004: INFO: Trying to get logs from node silbory-nirmata0 pod pod-configmaps-5b1c9364-f893-4d28-9e63-75d0b18df39a container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 00:40:56.042: INFO: Waiting for pod pod-configmaps-5b1c9364-f893-4d28-9e63-75d0b18df39a to disappear
Mar 13 00:40:56.050: INFO: Pod pod-configmaps-5b1c9364-f893-4d28-9e63-75d0b18df39a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:40:56.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9118" for this suite.
Mar 13 00:41:02.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:41:02.233: INFO: namespace configmap-9118 deletion completed in 6.174951375s

• [SLOW TEST:10.384 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:41:02.235: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0313 00:41:42.375614      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 13 00:41:42.375: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:41:42.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8254" for this suite.
Mar 13 00:41:50.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:41:50.564: INFO: namespace gc-8254 deletion completed in 8.181933122s

• [SLOW TEST:48.329 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:41:50.564: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:41:50.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6325" for this suite.
Mar 13 00:41:56.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:41:56.844: INFO: namespace custom-resource-definition-6325 deletion completed in 6.192921392s

• [SLOW TEST:6.280 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:41:56.845: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 00:41:56.909: INFO: Creating ReplicaSet my-hostname-basic-fdc56a18-5c40-434d-af95-01d28b4587e3
Mar 13 00:41:56.924: INFO: Pod name my-hostname-basic-fdc56a18-5c40-434d-af95-01d28b4587e3: Found 0 pods out of 1
Mar 13 00:42:01.931: INFO: Pod name my-hostname-basic-fdc56a18-5c40-434d-af95-01d28b4587e3: Found 1 pods out of 1
Mar 13 00:42:01.931: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-fdc56a18-5c40-434d-af95-01d28b4587e3" is running
Mar 13 00:42:01.936: INFO: Pod "my-hostname-basic-fdc56a18-5c40-434d-af95-01d28b4587e3-jnt9b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-13 00:41:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-13 00:41:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-13 00:41:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-13 00:41:56 +0000 UTC Reason: Message:}])
Mar 13 00:42:01.936: INFO: Trying to dial the pod
Mar 13 00:42:06.957: INFO: Controller my-hostname-basic-fdc56a18-5c40-434d-af95-01d28b4587e3: Got expected result from replica 1 [my-hostname-basic-fdc56a18-5c40-434d-af95-01d28b4587e3-jnt9b]: "my-hostname-basic-fdc56a18-5c40-434d-af95-01d28b4587e3-jnt9b", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:42:06.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7775" for this suite.
Mar 13 00:42:12.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:42:13.156: INFO: namespace replicaset-7775 deletion completed in 6.191955647s

• [SLOW TEST:16.311 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:42:13.157: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:42:17.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6862" for this suite.
Mar 13 00:42:23.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:42:23.447: INFO: namespace kubelet-test-6862 deletion completed in 6.16782404s

• [SLOW TEST:10.290 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:42:23.448: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 13 00:42:27.555: INFO: &Pod{ObjectMeta:{send-events-561469ab-7830-4f9e-a179-b3e5cb32e142  events-1139 /api/v1/namespaces/events-1139/pods/send-events-561469ab-7830-4f9e-a179-b3e5cb32e142 254b4e33-c160-4005-9158-54e6196b206e 5499 0 2020-03-13 00:42:23 +0000 UTC <nil> <nil> map[name:foo time:518177562] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c766v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c766v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c766v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:silbory-nirmata0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:42:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 00:42:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.79,PodIP:10.244.2.64,StartTime:2020-03-13 00:42:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-13 00:42:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://19c3ab071141f298c37387c08bd5f85490990cd2329265530be90d2b540c66e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar 13 00:42:29.563: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 13 00:42:31.571: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:42:31.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1139" for this suite.
Mar 13 00:43:15.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:43:15.781: INFO: namespace events-1139 deletion completed in 44.185059781s

• [SLOW TEST:52.333 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:43:15.781: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:43:26.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3201" for this suite.
Mar 13 00:43:32.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:43:33.119: INFO: namespace resourcequota-3201 deletion completed in 6.192641578s

• [SLOW TEST:17.338 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:43:33.120: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-73e4b56d-a51d-4fc1-bf0a-a791a816d510
STEP: Creating a pod to test consume secrets
Mar 13 00:43:33.212: INFO: Waiting up to 5m0s for pod "pod-secrets-5c299471-66a0-4c29-9cf6-cc7031d5e40c" in namespace "secrets-3409" to be "success or failure"
Mar 13 00:43:33.219: INFO: Pod "pod-secrets-5c299471-66a0-4c29-9cf6-cc7031d5e40c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.512634ms
Mar 13 00:43:35.224: INFO: Pod "pod-secrets-5c299471-66a0-4c29-9cf6-cc7031d5e40c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011932244s
Mar 13 00:43:37.231: INFO: Pod "pod-secrets-5c299471-66a0-4c29-9cf6-cc7031d5e40c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019072436s
STEP: Saw pod success
Mar 13 00:43:37.231: INFO: Pod "pod-secrets-5c299471-66a0-4c29-9cf6-cc7031d5e40c" satisfied condition "success or failure"
Mar 13 00:43:37.236: INFO: Trying to get logs from node silbory-nirmata0 pod pod-secrets-5c299471-66a0-4c29-9cf6-cc7031d5e40c container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 00:43:37.290: INFO: Waiting for pod pod-secrets-5c299471-66a0-4c29-9cf6-cc7031d5e40c to disappear
Mar 13 00:43:37.297: INFO: Pod pod-secrets-5c299471-66a0-4c29-9cf6-cc7031d5e40c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:43:37.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3409" for this suite.
Mar 13 00:43:43.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:43:43.482: INFO: namespace secrets-3409 deletion completed in 6.176096777s

• [SLOW TEST:10.363 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:43:43.483: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-7vtl
STEP: Creating a pod to test atomic-volume-subpath
Mar 13 00:43:43.576: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7vtl" in namespace "subpath-6593" to be "success or failure"
Mar 13 00:43:43.587: INFO: Pod "pod-subpath-test-configmap-7vtl": Phase="Pending", Reason="", readiness=false. Elapsed: 11.150913ms
Mar 13 00:43:45.593: INFO: Pod "pod-subpath-test-configmap-7vtl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017110515s
Mar 13 00:43:47.601: INFO: Pod "pod-subpath-test-configmap-7vtl": Phase="Running", Reason="", readiness=true. Elapsed: 4.025140318s
Mar 13 00:43:49.608: INFO: Pod "pod-subpath-test-configmap-7vtl": Phase="Running", Reason="", readiness=true. Elapsed: 6.031730979s
Mar 13 00:43:51.615: INFO: Pod "pod-subpath-test-configmap-7vtl": Phase="Running", Reason="", readiness=true. Elapsed: 8.038851672s
Mar 13 00:43:53.622: INFO: Pod "pod-subpath-test-configmap-7vtl": Phase="Running", Reason="", readiness=true. Elapsed: 10.046179242s
Mar 13 00:43:55.629: INFO: Pod "pod-subpath-test-configmap-7vtl": Phase="Running", Reason="", readiness=true. Elapsed: 12.052611729s
Mar 13 00:43:57.636: INFO: Pod "pod-subpath-test-configmap-7vtl": Phase="Running", Reason="", readiness=true. Elapsed: 14.059665528s
Mar 13 00:43:59.643: INFO: Pod "pod-subpath-test-configmap-7vtl": Phase="Running", Reason="", readiness=true. Elapsed: 16.066995402s
Mar 13 00:44:01.650: INFO: Pod "pod-subpath-test-configmap-7vtl": Phase="Running", Reason="", readiness=true. Elapsed: 18.074301987s
Mar 13 00:44:03.657: INFO: Pod "pod-subpath-test-configmap-7vtl": Phase="Running", Reason="", readiness=true. Elapsed: 20.08090968s
Mar 13 00:44:05.667: INFO: Pod "pod-subpath-test-configmap-7vtl": Phase="Running", Reason="", readiness=true. Elapsed: 22.09066142s
Mar 13 00:44:07.674: INFO: Pod "pod-subpath-test-configmap-7vtl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.097416383s
STEP: Saw pod success
Mar 13 00:44:07.674: INFO: Pod "pod-subpath-test-configmap-7vtl" satisfied condition "success or failure"
Mar 13 00:44:07.683: INFO: Trying to get logs from node silbory-nirmata0 pod pod-subpath-test-configmap-7vtl container test-container-subpath-configmap-7vtl: <nil>
STEP: delete the pod
Mar 13 00:44:07.723: INFO: Waiting for pod pod-subpath-test-configmap-7vtl to disappear
Mar 13 00:44:07.728: INFO: Pod pod-subpath-test-configmap-7vtl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7vtl
Mar 13 00:44:07.728: INFO: Deleting pod "pod-subpath-test-configmap-7vtl" in namespace "subpath-6593"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:44:07.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6593" for this suite.
Mar 13 00:44:13.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:44:13.921: INFO: namespace subpath-6593 deletion completed in 6.17735069s

• [SLOW TEST:30.438 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:44:13.923: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-a9cfd6e8-bb8c-4e31-9acf-058726c04ad1
STEP: Creating secret with name secret-projected-all-test-volume-cb3672ba-8ba9-4378-b4db-175692875100
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 13 00:44:14.017: INFO: Waiting up to 5m0s for pod "projected-volume-910cbb3f-1df0-43cd-98bd-ec1ba0e99cb4" in namespace "projected-1439" to be "success or failure"
Mar 13 00:44:14.025: INFO: Pod "projected-volume-910cbb3f-1df0-43cd-98bd-ec1ba0e99cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.089436ms
Mar 13 00:44:16.032: INFO: Pod "projected-volume-910cbb3f-1df0-43cd-98bd-ec1ba0e99cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015046923s
Mar 13 00:44:18.038: INFO: Pod "projected-volume-910cbb3f-1df0-43cd-98bd-ec1ba0e99cb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021108462s
STEP: Saw pod success
Mar 13 00:44:18.038: INFO: Pod "projected-volume-910cbb3f-1df0-43cd-98bd-ec1ba0e99cb4" satisfied condition "success or failure"
Mar 13 00:44:18.043: INFO: Trying to get logs from node silbory-nirmata0 pod projected-volume-910cbb3f-1df0-43cd-98bd-ec1ba0e99cb4 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 13 00:44:18.083: INFO: Waiting for pod projected-volume-910cbb3f-1df0-43cd-98bd-ec1ba0e99cb4 to disappear
Mar 13 00:44:18.089: INFO: Pod projected-volume-910cbb3f-1df0-43cd-98bd-ec1ba0e99cb4 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:44:18.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1439" for this suite.
Mar 13 00:44:24.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:44:24.275: INFO: namespace projected-1439 deletion completed in 6.177076237s

• [SLOW TEST:10.351 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:44:24.275: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 13 00:44:24.338: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 13 00:44:24.369: INFO: Waiting for terminating namespaces to be deleted...
Mar 13 00:44:24.379: INFO: 
Logging pods the kubelet thinks is on node sam-node2 before test
Mar 13 00:44:24.411: INFO: kube-dns-869c556677-bfkcm from kube-system started at 2020-03-13 00:13:29 +0000 UTC (3 container statuses recorded)
Mar 13 00:44:24.411: INFO: 	Container dnsmasq ready: true, restart count 0
Mar 13 00:44:24.411: INFO: 	Container kubedns ready: true, restart count 0
Mar 13 00:44:24.411: INFO: 	Container sidecar ready: true, restart count 0
Mar 13 00:44:24.411: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-w6d2v from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 00:44:24.411: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 00:44:24.411: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 13 00:44:24.411: INFO: nirmata-cni-installer-nkzv7 from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 00:44:24.411: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 00:44:24.411: INFO: kube-flannel-ds-d9nq5 from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 00:44:24.411: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 00:44:24.411: INFO: 
Logging pods the kubelet thinks is on node sam-node3 before test
Mar 13 00:44:24.433: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-f9nz4 from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 00:44:24.433: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 00:44:24.433: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 13 00:44:24.433: INFO: kube-flannel-ds-2rr2t from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 00:44:24.433: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 00:44:24.433: INFO: nirmata-kube-controller-666cf5cf5f-j4mmx from nirmata started at 2020-03-13 00:13:27 +0000 UTC (1 container statuses recorded)
Mar 13 00:44:24.433: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
Mar 13 00:44:24.433: INFO: nirmata-cni-installer-bf5qr from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 00:44:24.433: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 00:44:24.433: INFO: 
Logging pods the kubelet thinks is on node silbory-nirmata0 before test
Mar 13 00:44:24.446: INFO: kube-flannel-ds-959x5 from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 00:44:24.446: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 00:44:24.446: INFO: ingress-default-backend-cf675c575-nttt9 from ingress-haproxy started at 2020-03-13 00:14:00 +0000 UTC (1 container statuses recorded)
Mar 13 00:44:24.446: INFO: 	Container ingress-default-backend ready: true, restart count 0
Mar 13 00:44:24.446: INFO: sonobuoy-e2e-job-fce51d5a97b943f8 from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 00:44:24.446: INFO: 	Container e2e ready: true, restart count 0
Mar 13 00:44:24.446: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 00:44:24.446: INFO: nirmata-cni-installer-lnhv6 from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 00:44:24.446: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 00:44:24.446: INFO: haproxy-ingress-8688f746b4-2xbx5 from ingress-haproxy started at 2020-03-13 00:14:00 +0000 UTC (1 container statuses recorded)
Mar 13 00:44:24.446: INFO: 	Container haproxy-ingress ready: true, restart count 0
Mar 13 00:44:24.446: INFO: sonobuoy from sonobuoy started at 2020-03-13 00:21:12 +0000 UTC (1 container statuses recorded)
Mar 13 00:44:24.446: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 13 00:44:24.446: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-qf4zs from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 00:44:24.446: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 00:44:24.446: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 13 00:44:24.446: INFO: metrics-server-56c7b465d6-xr59t from kube-system started at 2020-03-13 00:13:52 +0000 UTC (1 container statuses recorded)
Mar 13 00:44:24.446: INFO: 	Container metrics-server ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-49113732-e324-4728-a2af-ad7dfa6ed66e 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-49113732-e324-4728-a2af-ad7dfa6ed66e off the node silbory-nirmata0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-49113732-e324-4728-a2af-ad7dfa6ed66e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:49:32.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7429" for this suite.
Mar 13 00:49:48.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:49:48.835: INFO: namespace sched-pred-7429 deletion completed in 16.196226784s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:324.560 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:49:48.836: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-9af5a412-1d05-40d3-b933-be6a0aec8114
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:49:52.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6706" for this suite.
Mar 13 00:50:07.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:50:07.187: INFO: namespace configmap-6706 deletion completed in 14.18959212s

• [SLOW TEST:18.352 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:50:07.188: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Mar 13 00:50:07.262: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-9128" to be "success or failure"
Mar 13 00:50:07.269: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.718716ms
Mar 13 00:50:09.275: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012789946s
Mar 13 00:50:11.293: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030204973s
STEP: Saw pod success
Mar 13 00:50:11.293: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar 13 00:50:11.298: INFO: Trying to get logs from node silbory-nirmata0 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar 13 00:50:11.364: INFO: Waiting for pod pod-host-path-test to disappear
Mar 13 00:50:11.369: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:50:11.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-9128" for this suite.
Mar 13 00:50:17.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:50:17.557: INFO: namespace hostpath-9128 deletion completed in 6.179825325s

• [SLOW TEST:10.369 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:50:17.558: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6672.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6672.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6672.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6672.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6672.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6672.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 13 00:50:21.739: INFO: DNS probes using dns-6672/dns-test-b7c7c36a-37d5-4453-a51a-de6fb4d728ef succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:50:21.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6672" for this suite.
Mar 13 00:50:27.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:50:28.039: INFO: namespace dns-6672 deletion completed in 6.212768364s

• [SLOW TEST:10.481 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:50:28.039: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-2c2b52f8-119f-43d8-924c-61f3a8262915
STEP: Creating a pod to test consume secrets
Mar 13 00:50:28.149: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-de0027bd-d3b8-4a68-8ddc-fe72768b86eb" in namespace "projected-2683" to be "success or failure"
Mar 13 00:50:28.154: INFO: Pod "pod-projected-secrets-de0027bd-d3b8-4a68-8ddc-fe72768b86eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.455923ms
Mar 13 00:50:30.166: INFO: Pod "pod-projected-secrets-de0027bd-d3b8-4a68-8ddc-fe72768b86eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016365359s
Mar 13 00:50:32.174: INFO: Pod "pod-projected-secrets-de0027bd-d3b8-4a68-8ddc-fe72768b86eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024055849s
STEP: Saw pod success
Mar 13 00:50:32.174: INFO: Pod "pod-projected-secrets-de0027bd-d3b8-4a68-8ddc-fe72768b86eb" satisfied condition "success or failure"
Mar 13 00:50:32.179: INFO: Trying to get logs from node silbory-nirmata0 pod pod-projected-secrets-de0027bd-d3b8-4a68-8ddc-fe72768b86eb container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 13 00:50:32.218: INFO: Waiting for pod pod-projected-secrets-de0027bd-d3b8-4a68-8ddc-fe72768b86eb to disappear
Mar 13 00:50:32.223: INFO: Pod pod-projected-secrets-de0027bd-d3b8-4a68-8ddc-fe72768b86eb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:50:32.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2683" for this suite.
Mar 13 00:50:38.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:50:38.415: INFO: namespace projected-2683 deletion completed in 6.182888107s

• [SLOW TEST:10.376 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:50:38.416: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 13 00:50:38.491: INFO: Waiting up to 5m0s for pod "pod-ad194446-c36f-4726-b681-3615ea2c27ab" in namespace "emptydir-2254" to be "success or failure"
Mar 13 00:50:38.498: INFO: Pod "pod-ad194446-c36f-4726-b681-3615ea2c27ab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.841564ms
Mar 13 00:50:40.507: INFO: Pod "pod-ad194446-c36f-4726-b681-3615ea2c27ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015497497s
Mar 13 00:50:42.514: INFO: Pod "pod-ad194446-c36f-4726-b681-3615ea2c27ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023128029s
STEP: Saw pod success
Mar 13 00:50:42.515: INFO: Pod "pod-ad194446-c36f-4726-b681-3615ea2c27ab" satisfied condition "success or failure"
Mar 13 00:50:42.520: INFO: Trying to get logs from node silbory-nirmata0 pod pod-ad194446-c36f-4726-b681-3615ea2c27ab container test-container: <nil>
STEP: delete the pod
Mar 13 00:50:42.555: INFO: Waiting for pod pod-ad194446-c36f-4726-b681-3615ea2c27ab to disappear
Mar 13 00:50:42.565: INFO: Pod pod-ad194446-c36f-4726-b681-3615ea2c27ab no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:50:42.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2254" for this suite.
Mar 13 00:50:48.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:50:48.793: INFO: namespace emptydir-2254 deletion completed in 6.211375133s

• [SLOW TEST:10.377 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:50:48.795: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-186c801b-238b-4cd5-9e61-93c379beb727
STEP: Creating a pod to test consume configMaps
Mar 13 00:50:48.881: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-55c6132f-6365-4df1-82fd-e7061ca0931a" in namespace "projected-431" to be "success or failure"
Mar 13 00:50:48.888: INFO: Pod "pod-projected-configmaps-55c6132f-6365-4df1-82fd-e7061ca0931a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.735426ms
Mar 13 00:50:50.894: INFO: Pod "pod-projected-configmaps-55c6132f-6365-4df1-82fd-e7061ca0931a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013004657s
Mar 13 00:50:52.901: INFO: Pod "pod-projected-configmaps-55c6132f-6365-4df1-82fd-e7061ca0931a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019084004s
STEP: Saw pod success
Mar 13 00:50:52.901: INFO: Pod "pod-projected-configmaps-55c6132f-6365-4df1-82fd-e7061ca0931a" satisfied condition "success or failure"
Mar 13 00:50:52.906: INFO: Trying to get logs from node silbory-nirmata0 pod pod-projected-configmaps-55c6132f-6365-4df1-82fd-e7061ca0931a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 00:50:52.943: INFO: Waiting for pod pod-projected-configmaps-55c6132f-6365-4df1-82fd-e7061ca0931a to disappear
Mar 13 00:50:52.948: INFO: Pod pod-projected-configmaps-55c6132f-6365-4df1-82fd-e7061ca0931a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:50:52.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-431" for this suite.
Mar 13 00:50:58.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:50:59.145: INFO: namespace projected-431 deletion completed in 6.178264071s

• [SLOW TEST:10.350 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:50:59.146: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 00:50:59.214: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 13 00:51:05.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-6998 create -f -'
Mar 13 00:51:06.156: INFO: stderr: ""
Mar 13 00:51:06.156: INFO: stdout: "e2e-test-crd-publish-openapi-8007-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 13 00:51:06.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-6998 delete e2e-test-crd-publish-openapi-8007-crds test-cr'
Mar 13 00:51:06.388: INFO: stderr: ""
Mar 13 00:51:06.388: INFO: stdout: "e2e-test-crd-publish-openapi-8007-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 13 00:51:06.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-6998 apply -f -'
Mar 13 00:51:06.807: INFO: stderr: ""
Mar 13 00:51:06.807: INFO: stdout: "e2e-test-crd-publish-openapi-8007-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 13 00:51:06.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-6998 delete e2e-test-crd-publish-openapi-8007-crds test-cr'
Mar 13 00:51:07.038: INFO: stderr: ""
Mar 13 00:51:07.038: INFO: stdout: "e2e-test-crd-publish-openapi-8007-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 13 00:51:07.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 explain e2e-test-crd-publish-openapi-8007-crds'
Mar 13 00:51:07.463: INFO: stderr: ""
Mar 13 00:51:07.463: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8007-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:51:13.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6998" for this suite.
Mar 13 00:51:19.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:51:19.579: INFO: namespace crd-publish-openapi-6998 deletion completed in 6.17758092s

• [SLOW TEST:20.434 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:51:19.580: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-4542/configmap-test-ed8a65d8-bfa1-4fd3-aa13-24ba7d27255f
STEP: Creating a pod to test consume configMaps
Mar 13 00:51:19.685: INFO: Waiting up to 5m0s for pod "pod-configmaps-88e8ddce-b48a-449c-80e6-466bcb598f14" in namespace "configmap-4542" to be "success or failure"
Mar 13 00:51:19.692: INFO: Pod "pod-configmaps-88e8ddce-b48a-449c-80e6-466bcb598f14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.944329ms
Mar 13 00:51:21.699: INFO: Pod "pod-configmaps-88e8ddce-b48a-449c-80e6-466bcb598f14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013433043s
Mar 13 00:51:23.706: INFO: Pod "pod-configmaps-88e8ddce-b48a-449c-80e6-466bcb598f14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020216948s
STEP: Saw pod success
Mar 13 00:51:23.706: INFO: Pod "pod-configmaps-88e8ddce-b48a-449c-80e6-466bcb598f14" satisfied condition "success or failure"
Mar 13 00:51:23.711: INFO: Trying to get logs from node silbory-nirmata0 pod pod-configmaps-88e8ddce-b48a-449c-80e6-466bcb598f14 container env-test: <nil>
STEP: delete the pod
Mar 13 00:51:23.743: INFO: Waiting for pod pod-configmaps-88e8ddce-b48a-449c-80e6-466bcb598f14 to disappear
Mar 13 00:51:23.749: INFO: Pod pod-configmaps-88e8ddce-b48a-449c-80e6-466bcb598f14 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:51:23.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4542" for this suite.
Mar 13 00:51:29.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:51:29.972: INFO: namespace configmap-4542 deletion completed in 6.214355686s

• [SLOW TEST:10.393 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:51:29.973: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-7fbf1e3f-86f2-4124-949b-9565a85a53a4
STEP: Creating a pod to test consume secrets
Mar 13 00:51:30.066: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-92aa4e42-3189-4c33-a04e-8b08fea08997" in namespace "projected-8774" to be "success or failure"
Mar 13 00:51:30.071: INFO: Pod "pod-projected-secrets-92aa4e42-3189-4c33-a04e-8b08fea08997": Phase="Pending", Reason="", readiness=false. Elapsed: 4.632767ms
Mar 13 00:51:32.078: INFO: Pod "pod-projected-secrets-92aa4e42-3189-4c33-a04e-8b08fea08997": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011733103s
Mar 13 00:51:34.087: INFO: Pod "pod-projected-secrets-92aa4e42-3189-4c33-a04e-8b08fea08997": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021175357s
STEP: Saw pod success
Mar 13 00:51:34.087: INFO: Pod "pod-projected-secrets-92aa4e42-3189-4c33-a04e-8b08fea08997" satisfied condition "success or failure"
Mar 13 00:51:34.093: INFO: Trying to get logs from node silbory-nirmata0 pod pod-projected-secrets-92aa4e42-3189-4c33-a04e-8b08fea08997 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 13 00:51:34.138: INFO: Waiting for pod pod-projected-secrets-92aa4e42-3189-4c33-a04e-8b08fea08997 to disappear
Mar 13 00:51:34.145: INFO: Pod pod-projected-secrets-92aa4e42-3189-4c33-a04e-8b08fea08997 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:51:34.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8774" for this suite.
Mar 13 00:51:40.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:51:40.352: INFO: namespace projected-8774 deletion completed in 6.199140755s

• [SLOW TEST:10.380 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:51:40.362: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Mar 13 00:51:40.433: INFO: namespace kubectl-9266
Mar 13 00:51:40.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-9266'
Mar 13 00:51:40.993: INFO: stderr: ""
Mar 13 00:51:40.993: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 13 00:51:42.001: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:51:42.001: INFO: Found 0 / 1
Mar 13 00:51:43.001: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:51:43.001: INFO: Found 0 / 1
Mar 13 00:51:43.999: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:51:43.999: INFO: Found 1 / 1
Mar 13 00:51:43.999: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 13 00:51:44.006: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 00:51:44.006: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 13 00:51:44.006: INFO: wait on redis-master startup in kubectl-9266 
Mar 13 00:51:44.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 logs redis-master-plc99 redis-master --namespace=kubectl-9266'
Mar 13 00:51:44.253: INFO: stderr: ""
Mar 13 00:51:44.253: INFO: stdout: "1:C 13 Mar 2020 00:51:42.721 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 13 Mar 2020 00:51:42.721 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 13 Mar 2020 00:51:42.721 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 13 Mar 2020 00:51:42.726 * Running mode=standalone, port=6379.\n1:M 13 Mar 2020 00:51:42.726 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Mar 2020 00:51:42.727 # Server initialized\n1:M 13 Mar 2020 00:51:42.727 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Mar 2020 00:51:42.727 * Ready to accept connections\n"
STEP: exposing RC
Mar 13 00:51:44.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9266'
Mar 13 00:51:44.518: INFO: stderr: ""
Mar 13 00:51:44.518: INFO: stdout: "service/rm2 exposed\n"
Mar 13 00:51:44.526: INFO: Service rm2 in namespace kubectl-9266 found.
STEP: exposing service
Mar 13 00:51:46.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9266'
Mar 13 00:51:46.801: INFO: stderr: ""
Mar 13 00:51:46.801: INFO: stdout: "service/rm3 exposed\n"
Mar 13 00:51:46.809: INFO: Service rm3 in namespace kubectl-9266 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:51:48.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9266" for this suite.
Mar 13 00:52:16.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:52:17.009: INFO: namespace kubectl-9266 deletion completed in 28.177884657s

• [SLOW TEST:36.648 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:52:17.010: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 13 00:52:17.470: INFO: Pod name wrapped-volume-race-f2a6f041-e0eb-466c-8840-e2f97fc7ff03: Found 0 pods out of 5
Mar 13 00:52:22.483: INFO: Pod name wrapped-volume-race-f2a6f041-e0eb-466c-8840-e2f97fc7ff03: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f2a6f041-e0eb-466c-8840-e2f97fc7ff03 in namespace emptydir-wrapper-629, will wait for the garbage collector to delete the pods
Mar 13 00:52:34.652: INFO: Deleting ReplicationController wrapped-volume-race-f2a6f041-e0eb-466c-8840-e2f97fc7ff03 took: 28.039783ms
Mar 13 00:52:35.053: INFO: Terminating ReplicationController wrapped-volume-race-f2a6f041-e0eb-466c-8840-e2f97fc7ff03 pods took: 400.553643ms
STEP: Creating RC which spawns configmap-volume pods
Mar 13 00:53:12.903: INFO: Pod name wrapped-volume-race-3bd27d93-1b2e-4aab-8a4a-c7cd9b96dc41: Found 0 pods out of 5
Mar 13 00:53:17.915: INFO: Pod name wrapped-volume-race-3bd27d93-1b2e-4aab-8a4a-c7cd9b96dc41: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3bd27d93-1b2e-4aab-8a4a-c7cd9b96dc41 in namespace emptydir-wrapper-629, will wait for the garbage collector to delete the pods
Mar 13 00:53:30.028: INFO: Deleting ReplicationController wrapped-volume-race-3bd27d93-1b2e-4aab-8a4a-c7cd9b96dc41 took: 14.817874ms
Mar 13 00:53:30.429: INFO: Terminating ReplicationController wrapped-volume-race-3bd27d93-1b2e-4aab-8a4a-c7cd9b96dc41 pods took: 400.713935ms
STEP: Creating RC which spawns configmap-volume pods
Mar 13 00:54:12.861: INFO: Pod name wrapped-volume-race-19664b05-de2e-482f-bacf-a4f916a2f68d: Found 0 pods out of 5
Mar 13 00:54:17.875: INFO: Pod name wrapped-volume-race-19664b05-de2e-482f-bacf-a4f916a2f68d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-19664b05-de2e-482f-bacf-a4f916a2f68d in namespace emptydir-wrapper-629, will wait for the garbage collector to delete the pods
Mar 13 00:54:29.992: INFO: Deleting ReplicationController wrapped-volume-race-19664b05-de2e-482f-bacf-a4f916a2f68d took: 17.132188ms
Mar 13 00:54:30.393: INFO: Terminating ReplicationController wrapped-volume-race-19664b05-de2e-482f-bacf-a4f916a2f68d pods took: 400.653098ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:55:12.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-629" for this suite.
Mar 13 00:55:20.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:55:20.709: INFO: namespace emptydir-wrapper-629 deletion completed in 8.177880265s

• [SLOW TEST:183.699 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:55:20.714: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 00:55:20.786: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 13 00:55:25.795: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 13 00:55:25.795: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 13 00:55:25.828: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8119 /apis/apps/v1/namespaces/deployment-8119/deployments/test-cleanup-deployment 0ceb0a77-fe0c-4271-88ca-d959b795cd23 7332 1 2020-03-13 00:55:25 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007247778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 13 00:55:25.834: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:55:25.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8119" for this suite.
Mar 13 00:55:31.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:55:32.073: INFO: namespace deployment-8119 deletion completed in 6.22333245s

• [SLOW TEST:11.359 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:55:32.073: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-434c0387-5b24-48d6-aca8-6e34995e5fe0
STEP: Creating a pod to test consume configMaps
Mar 13 00:55:32.176: INFO: Waiting up to 5m0s for pod "pod-configmaps-63c6f309-afd0-4e20-a60c-10c9d5beb426" in namespace "configmap-8756" to be "success or failure"
Mar 13 00:55:32.184: INFO: Pod "pod-configmaps-63c6f309-afd0-4e20-a60c-10c9d5beb426": Phase="Pending", Reason="", readiness=false. Elapsed: 8.409747ms
Mar 13 00:55:34.192: INFO: Pod "pod-configmaps-63c6f309-afd0-4e20-a60c-10c9d5beb426": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015827415s
Mar 13 00:55:36.199: INFO: Pod "pod-configmaps-63c6f309-afd0-4e20-a60c-10c9d5beb426": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023433284s
STEP: Saw pod success
Mar 13 00:55:36.199: INFO: Pod "pod-configmaps-63c6f309-afd0-4e20-a60c-10c9d5beb426" satisfied condition "success or failure"
Mar 13 00:55:36.204: INFO: Trying to get logs from node silbory-nirmata0 pod pod-configmaps-63c6f309-afd0-4e20-a60c-10c9d5beb426 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 00:55:36.254: INFO: Waiting for pod pod-configmaps-63c6f309-afd0-4e20-a60c-10c9d5beb426 to disappear
Mar 13 00:55:36.259: INFO: Pod pod-configmaps-63c6f309-afd0-4e20-a60c-10c9d5beb426 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:55:36.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8756" for this suite.
Mar 13 00:55:42.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:55:42.451: INFO: namespace configmap-8756 deletion completed in 6.184325749s

• [SLOW TEST:10.378 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:55:42.452: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 00:55:42.541: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a3e453a-0e3c-4b5a-84bc-d8604c8aa971" in namespace "downward-api-9222" to be "success or failure"
Mar 13 00:55:42.553: INFO: Pod "downwardapi-volume-4a3e453a-0e3c-4b5a-84bc-d8604c8aa971": Phase="Pending", Reason="", readiness=false. Elapsed: 11.613878ms
Mar 13 00:55:44.561: INFO: Pod "downwardapi-volume-4a3e453a-0e3c-4b5a-84bc-d8604c8aa971": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020080897s
Mar 13 00:55:46.568: INFO: Pod "downwardapi-volume-4a3e453a-0e3c-4b5a-84bc-d8604c8aa971": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027292724s
STEP: Saw pod success
Mar 13 00:55:46.568: INFO: Pod "downwardapi-volume-4a3e453a-0e3c-4b5a-84bc-d8604c8aa971" satisfied condition "success or failure"
Mar 13 00:55:46.573: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-4a3e453a-0e3c-4b5a-84bc-d8604c8aa971 container client-container: <nil>
STEP: delete the pod
Mar 13 00:55:46.609: INFO: Waiting for pod downwardapi-volume-4a3e453a-0e3c-4b5a-84bc-d8604c8aa971 to disappear
Mar 13 00:55:46.615: INFO: Pod downwardapi-volume-4a3e453a-0e3c-4b5a-84bc-d8604c8aa971 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:55:46.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9222" for this suite.
Mar 13 00:55:52.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:55:52.814: INFO: namespace downward-api-9222 deletion completed in 6.190535194s

• [SLOW TEST:10.362 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:55:52.818: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 00:55:52.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b012e573-62a7-4e66-be54-14eaa6f08e81" in namespace "downward-api-4282" to be "success or failure"
Mar 13 00:55:52.905: INFO: Pod "downwardapi-volume-b012e573-62a7-4e66-be54-14eaa6f08e81": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077808ms
Mar 13 00:55:54.911: INFO: Pod "downwardapi-volume-b012e573-62a7-4e66-be54-14eaa6f08e81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012766871s
Mar 13 00:55:56.919: INFO: Pod "downwardapi-volume-b012e573-62a7-4e66-be54-14eaa6f08e81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020480644s
STEP: Saw pod success
Mar 13 00:55:56.919: INFO: Pod "downwardapi-volume-b012e573-62a7-4e66-be54-14eaa6f08e81" satisfied condition "success or failure"
Mar 13 00:55:56.924: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-b012e573-62a7-4e66-be54-14eaa6f08e81 container client-container: <nil>
STEP: delete the pod
Mar 13 00:55:56.961: INFO: Waiting for pod downwardapi-volume-b012e573-62a7-4e66-be54-14eaa6f08e81 to disappear
Mar 13 00:55:56.968: INFO: Pod downwardapi-volume-b012e573-62a7-4e66-be54-14eaa6f08e81 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:55:56.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4282" for this suite.
Mar 13 00:56:03.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:56:03.157: INFO: namespace downward-api-4282 deletion completed in 6.178876618s

• [SLOW TEST:10.340 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:56:03.158: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 13 00:56:03.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-5462'
Mar 13 00:56:03.474: INFO: stderr: ""
Mar 13 00:56:03.474: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 13 00:56:08.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pod e2e-test-httpd-pod --namespace=kubectl-5462 -o json'
Mar 13 00:56:08.753: INFO: stderr: ""
Mar 13 00:56:08.753: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-03-13T00:56:03Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5462\",\n        \"resourceVersion\": \"7468\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5462/pods/e2e-test-httpd-pod\",\n        \"uid\": \"cf73157b-ac79-41f4-940e-5c78398eb55e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-vm886\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"silbory-nirmata0\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-vm886\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-vm886\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-13T00:56:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-13T00:56:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-13T00:56:05Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-13T00:56:03Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://c6fe3bdaea20b29b857467247a5e1bc7081a95e18e4e9e0e5980ab55e13592d7\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-13T00:56:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.1.79\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.2.83\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.2.83\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-13T00:56:03Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 13 00:56:08.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 replace -f - --namespace=kubectl-5462'
Mar 13 00:56:09.168: INFO: stderr: ""
Mar 13 00:56:09.168: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Mar 13 00:56:09.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete pods e2e-test-httpd-pod --namespace=kubectl-5462'
Mar 13 00:56:12.193: INFO: stderr: ""
Mar 13 00:56:12.194: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:56:12.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5462" for this suite.
Mar 13 00:56:18.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:56:18.387: INFO: namespace kubectl-5462 deletion completed in 6.181592794s

• [SLOW TEST:15.229 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:56:18.389: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-9783/configmap-test-30fb753f-1713-461e-906e-7e8a4b5812c4
STEP: Creating a pod to test consume configMaps
Mar 13 00:56:18.487: INFO: Waiting up to 5m0s for pod "pod-configmaps-5b2c6bb5-0172-48a1-a63f-197733be0294" in namespace "configmap-9783" to be "success or failure"
Mar 13 00:56:18.494: INFO: Pod "pod-configmaps-5b2c6bb5-0172-48a1-a63f-197733be0294": Phase="Pending", Reason="", readiness=false. Elapsed: 6.990678ms
Mar 13 00:56:20.501: INFO: Pod "pod-configmaps-5b2c6bb5-0172-48a1-a63f-197733be0294": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013561986s
Mar 13 00:56:22.508: INFO: Pod "pod-configmaps-5b2c6bb5-0172-48a1-a63f-197733be0294": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020389238s
STEP: Saw pod success
Mar 13 00:56:22.508: INFO: Pod "pod-configmaps-5b2c6bb5-0172-48a1-a63f-197733be0294" satisfied condition "success or failure"
Mar 13 00:56:22.513: INFO: Trying to get logs from node silbory-nirmata0 pod pod-configmaps-5b2c6bb5-0172-48a1-a63f-197733be0294 container env-test: <nil>
STEP: delete the pod
Mar 13 00:56:22.553: INFO: Waiting for pod pod-configmaps-5b2c6bb5-0172-48a1-a63f-197733be0294 to disappear
Mar 13 00:56:22.560: INFO: Pod pod-configmaps-5b2c6bb5-0172-48a1-a63f-197733be0294 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:56:22.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9783" for this suite.
Mar 13 00:56:28.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:56:28.759: INFO: namespace configmap-9783 deletion completed in 6.1907696s

• [SLOW TEST:10.370 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:56:28.760: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 13 00:56:33.408: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e1163872-2a6f-4aec-98e7-32edd7c32538"
Mar 13 00:56:33.408: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e1163872-2a6f-4aec-98e7-32edd7c32538" in namespace "pods-6641" to be "terminated due to deadline exceeded"
Mar 13 00:56:33.414: INFO: Pod "pod-update-activedeadlineseconds-e1163872-2a6f-4aec-98e7-32edd7c32538": Phase="Running", Reason="", readiness=true. Elapsed: 5.74013ms
Mar 13 00:56:35.421: INFO: Pod "pod-update-activedeadlineseconds-e1163872-2a6f-4aec-98e7-32edd7c32538": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.012512275s
Mar 13 00:56:35.421: INFO: Pod "pod-update-activedeadlineseconds-e1163872-2a6f-4aec-98e7-32edd7c32538" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:56:35.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6641" for this suite.
Mar 13 00:56:41.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:56:41.648: INFO: namespace pods-6641 deletion completed in 6.214378977s

• [SLOW TEST:12.888 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:56:41.650: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-3ec3ef8d-b6cb-4e48-b11b-ef22e7bcc844 in namespace container-probe-8198
Mar 13 00:56:45.792: INFO: Started pod busybox-3ec3ef8d-b6cb-4e48-b11b-ef22e7bcc844 in namespace container-probe-8198
STEP: checking the pod's current state and verifying that restartCount is present
Mar 13 00:56:45.798: INFO: Initial restart count of pod busybox-3ec3ef8d-b6cb-4e48-b11b-ef22e7bcc844 is 0
Mar 13 00:57:33.969: INFO: Restart count of pod container-probe-8198/busybox-3ec3ef8d-b6cb-4e48-b11b-ef22e7bcc844 is now 1 (48.171508237s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:57:33.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8198" for this suite.
Mar 13 00:57:40.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:57:40.221: INFO: namespace container-probe-8198 deletion completed in 6.213491478s

• [SLOW TEST:58.571 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:57:40.222: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-s2bb
STEP: Creating a pod to test atomic-volume-subpath
Mar 13 00:57:40.332: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-s2bb" in namespace "subpath-3435" to be "success or failure"
Mar 13 00:57:40.339: INFO: Pod "pod-subpath-test-downwardapi-s2bb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.193786ms
Mar 13 00:57:42.345: INFO: Pod "pod-subpath-test-downwardapi-s2bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012815475s
Mar 13 00:57:44.352: INFO: Pod "pod-subpath-test-downwardapi-s2bb": Phase="Running", Reason="", readiness=true. Elapsed: 4.019904168s
Mar 13 00:57:46.359: INFO: Pod "pod-subpath-test-downwardapi-s2bb": Phase="Running", Reason="", readiness=true. Elapsed: 6.027560379s
Mar 13 00:57:48.366: INFO: Pod "pod-subpath-test-downwardapi-s2bb": Phase="Running", Reason="", readiness=true. Elapsed: 8.034128302s
Mar 13 00:57:50.379: INFO: Pod "pod-subpath-test-downwardapi-s2bb": Phase="Running", Reason="", readiness=true. Elapsed: 10.046830809s
Mar 13 00:57:52.386: INFO: Pod "pod-subpath-test-downwardapi-s2bb": Phase="Running", Reason="", readiness=true. Elapsed: 12.054432309s
Mar 13 00:57:54.391: INFO: Pod "pod-subpath-test-downwardapi-s2bb": Phase="Running", Reason="", readiness=true. Elapsed: 14.059074783s
Mar 13 00:57:56.398: INFO: Pod "pod-subpath-test-downwardapi-s2bb": Phase="Running", Reason="", readiness=true. Elapsed: 16.065906674s
Mar 13 00:57:58.405: INFO: Pod "pod-subpath-test-downwardapi-s2bb": Phase="Running", Reason="", readiness=true. Elapsed: 18.073310193s
Mar 13 00:58:00.413: INFO: Pod "pod-subpath-test-downwardapi-s2bb": Phase="Running", Reason="", readiness=true. Elapsed: 20.080581539s
Mar 13 00:58:02.419: INFO: Pod "pod-subpath-test-downwardapi-s2bb": Phase="Running", Reason="", readiness=true. Elapsed: 22.087025908s
Mar 13 00:58:04.426: INFO: Pod "pod-subpath-test-downwardapi-s2bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.094446901s
STEP: Saw pod success
Mar 13 00:58:04.426: INFO: Pod "pod-subpath-test-downwardapi-s2bb" satisfied condition "success or failure"
Mar 13 00:58:04.432: INFO: Trying to get logs from node silbory-nirmata0 pod pod-subpath-test-downwardapi-s2bb container test-container-subpath-downwardapi-s2bb: <nil>
STEP: delete the pod
Mar 13 00:58:04.472: INFO: Waiting for pod pod-subpath-test-downwardapi-s2bb to disappear
Mar 13 00:58:04.479: INFO: Pod pod-subpath-test-downwardapi-s2bb no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-s2bb
Mar 13 00:58:04.480: INFO: Deleting pod "pod-subpath-test-downwardapi-s2bb" in namespace "subpath-3435"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:58:04.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3435" for this suite.
Mar 13 00:58:10.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:58:10.662: INFO: namespace subpath-3435 deletion completed in 6.170396494s

• [SLOW TEST:30.441 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:58:10.667: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 13 00:58:10.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-7491'
Mar 13 00:58:10.961: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 13 00:58:10.961: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Mar 13 00:58:10.984: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-j26q9]
Mar 13 00:58:10.985: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-j26q9" in namespace "kubectl-7491" to be "running and ready"
Mar 13 00:58:10.990: INFO: Pod "e2e-test-httpd-rc-j26q9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.851861ms
Mar 13 00:58:12.997: INFO: Pod "e2e-test-httpd-rc-j26q9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012454571s
Mar 13 00:58:15.005: INFO: Pod "e2e-test-httpd-rc-j26q9": Phase="Running", Reason="", readiness=true. Elapsed: 4.019917151s
Mar 13 00:58:15.005: INFO: Pod "e2e-test-httpd-rc-j26q9" satisfied condition "running and ready"
Mar 13 00:58:15.005: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-j26q9]
Mar 13 00:58:15.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 logs rc/e2e-test-httpd-rc --namespace=kubectl-7491'
Mar 13 00:58:15.287: INFO: stderr: ""
Mar 13 00:58:15.287: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.244.2.88. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.244.2.88. Set the 'ServerName' directive globally to suppress this message\n[Fri Mar 13 00:58:12.835761 2020] [mpm_event:notice] [pid 1:tid 139852524616552] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Fri Mar 13 00:58:12.836062 2020] [core:notice] [pid 1:tid 139852524616552] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Mar 13 00:58:15.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete rc e2e-test-httpd-rc --namespace=kubectl-7491'
Mar 13 00:58:15.526: INFO: stderr: ""
Mar 13 00:58:15.526: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:58:15.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7491" for this suite.
Mar 13 00:58:21.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:58:21.713: INFO: namespace kubectl-7491 deletion completed in 6.179789963s

• [SLOW TEST:11.046 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:58:21.713: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 00:58:21.786: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f07fb063-c9fe-4056-9061-e2b375f0834b" in namespace "downward-api-5131" to be "success or failure"
Mar 13 00:58:21.795: INFO: Pod "downwardapi-volume-f07fb063-c9fe-4056-9061-e2b375f0834b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.790094ms
Mar 13 00:58:23.802: INFO: Pod "downwardapi-volume-f07fb063-c9fe-4056-9061-e2b375f0834b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016319047s
Mar 13 00:58:25.809: INFO: Pod "downwardapi-volume-f07fb063-c9fe-4056-9061-e2b375f0834b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023418587s
STEP: Saw pod success
Mar 13 00:58:25.809: INFO: Pod "downwardapi-volume-f07fb063-c9fe-4056-9061-e2b375f0834b" satisfied condition "success or failure"
Mar 13 00:58:25.816: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-f07fb063-c9fe-4056-9061-e2b375f0834b container client-container: <nil>
STEP: delete the pod
Mar 13 00:58:25.850: INFO: Waiting for pod downwardapi-volume-f07fb063-c9fe-4056-9061-e2b375f0834b to disappear
Mar 13 00:58:25.859: INFO: Pod downwardapi-volume-f07fb063-c9fe-4056-9061-e2b375f0834b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:58:25.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5131" for this suite.
Mar 13 00:58:31.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:58:32.055: INFO: namespace downward-api-5131 deletion completed in 6.187428961s

• [SLOW TEST:10.341 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:58:32.056: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 13 00:58:32.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-4319'
Mar 13 00:58:32.363: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 13 00:58:32.363: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Mar 13 00:58:32.398: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar 13 00:58:32.430: INFO: scanned /root for discovery docs: <nil>
Mar 13 00:58:32.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4319'
Mar 13 00:58:48.497: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 13 00:58:48.497: INFO: stdout: "Created e2e-test-httpd-rc-f570f4a326f2bad39c7f83865ed5cba4\nScaling up e2e-test-httpd-rc-f570f4a326f2bad39c7f83865ed5cba4 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-f570f4a326f2bad39c7f83865ed5cba4 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-f570f4a326f2bad39c7f83865ed5cba4 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Mar 13 00:58:48.497: INFO: stdout: "Created e2e-test-httpd-rc-f570f4a326f2bad39c7f83865ed5cba4\nScaling up e2e-test-httpd-rc-f570f4a326f2bad39c7f83865ed5cba4 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-f570f4a326f2bad39c7f83865ed5cba4 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-f570f4a326f2bad39c7f83865ed5cba4 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Mar 13 00:58:48.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-4319'
Mar 13 00:58:48.728: INFO: stderr: ""
Mar 13 00:58:48.728: INFO: stdout: "e2e-test-httpd-rc-f570f4a326f2bad39c7f83865ed5cba4-rvpgr "
Mar 13 00:58:48.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods e2e-test-httpd-rc-f570f4a326f2bad39c7f83865ed5cba4-rvpgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4319'
Mar 13 00:58:48.943: INFO: stderr: ""
Mar 13 00:58:48.943: INFO: stdout: "true"
Mar 13 00:58:48.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods e2e-test-httpd-rc-f570f4a326f2bad39c7f83865ed5cba4-rvpgr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4319'
Mar 13 00:58:49.160: INFO: stderr: ""
Mar 13 00:58:49.160: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Mar 13 00:58:49.160: INFO: e2e-test-httpd-rc-f570f4a326f2bad39c7f83865ed5cba4-rvpgr is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Mar 13 00:58:49.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete rc e2e-test-httpd-rc --namespace=kubectl-4319'
Mar 13 00:58:49.418: INFO: stderr: ""
Mar 13 00:58:49.418: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:58:49.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4319" for this suite.
Mar 13 00:58:55.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:58:55.608: INFO: namespace kubectl-4319 deletion completed in 6.181136198s

• [SLOW TEST:23.552 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:58:55.610: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 00:58:56.862: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 13 00:58:58.883: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719657936, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719657936, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719657936, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719657936, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 00:59:01.904: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 00:59:01.910: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:59:03.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6651" for this suite.
Mar 13 00:59:09.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:59:09.387: INFO: namespace webhook-6651 deletion completed in 6.191560754s
STEP: Destroying namespace "webhook-6651-markers" for this suite.
Mar 13 00:59:15.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:59:15.586: INFO: namespace webhook-6651-markers deletion completed in 6.198071148s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:20.000 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:59:15.610: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-6565
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6565 to expose endpoints map[]
Mar 13 00:59:15.695: INFO: Get endpoints failed (8.762808ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar 13 00:59:16.703: INFO: successfully validated that service multi-endpoint-test in namespace services-6565 exposes endpoints map[] (1.016774073s elapsed)
STEP: Creating pod pod1 in namespace services-6565
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6565 to expose endpoints map[pod1:[100]]
Mar 13 00:59:19.777: INFO: successfully validated that service multi-endpoint-test in namespace services-6565 exposes endpoints map[pod1:[100]] (3.056707189s elapsed)
STEP: Creating pod pod2 in namespace services-6565
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6565 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 13 00:59:22.860: INFO: successfully validated that service multi-endpoint-test in namespace services-6565 exposes endpoints map[pod1:[100] pod2:[101]] (3.075605602s elapsed)
STEP: Deleting pod pod1 in namespace services-6565
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6565 to expose endpoints map[pod2:[101]]
Mar 13 00:59:23.895: INFO: successfully validated that service multi-endpoint-test in namespace services-6565 exposes endpoints map[pod2:[101]] (1.027611408s elapsed)
STEP: Deleting pod pod2 in namespace services-6565
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6565 to expose endpoints map[]
Mar 13 00:59:24.922: INFO: successfully validated that service multi-endpoint-test in namespace services-6565 exposes endpoints map[] (1.015617303s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:59:24.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6565" for this suite.
Mar 13 00:59:52.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 00:59:53.138: INFO: namespace services-6565 deletion completed in 28.174281441s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:37.527 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 00:59:53.138: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-7298/secret-test-2133ea98-8d19-4d84-8cf3-42176d5da9d0
STEP: Creating a pod to test consume secrets
Mar 13 00:59:53.222: INFO: Waiting up to 5m0s for pod "pod-configmaps-38e3d421-fadb-4f93-bb77-d6403436c0dc" in namespace "secrets-7298" to be "success or failure"
Mar 13 00:59:53.229: INFO: Pod "pod-configmaps-38e3d421-fadb-4f93-bb77-d6403436c0dc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.302807ms
Mar 13 00:59:55.235: INFO: Pod "pod-configmaps-38e3d421-fadb-4f93-bb77-d6403436c0dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012563669s
Mar 13 00:59:57.240: INFO: Pod "pod-configmaps-38e3d421-fadb-4f93-bb77-d6403436c0dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017873357s
STEP: Saw pod success
Mar 13 00:59:57.240: INFO: Pod "pod-configmaps-38e3d421-fadb-4f93-bb77-d6403436c0dc" satisfied condition "success or failure"
Mar 13 00:59:57.245: INFO: Trying to get logs from node silbory-nirmata0 pod pod-configmaps-38e3d421-fadb-4f93-bb77-d6403436c0dc container env-test: <nil>
STEP: delete the pod
Mar 13 00:59:57.300: INFO: Waiting for pod pod-configmaps-38e3d421-fadb-4f93-bb77-d6403436c0dc to disappear
Mar 13 00:59:57.309: INFO: Pod pod-configmaps-38e3d421-fadb-4f93-bb77-d6403436c0dc no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 00:59:57.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7298" for this suite.
Mar 13 01:00:03.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:00:03.494: INFO: namespace secrets-7298 deletion completed in 6.175865646s

• [SLOW TEST:10.356 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:00:03.495: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8963
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8963
I0313 01:00:03.610560      25 runners.go:184] Created replication controller with name: externalname-service, namespace: services-8963, replica count: 2
Mar 13 01:00:06.661: INFO: Creating new exec pod
I0313 01:00:06.661696      25 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 13 01:00:11.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=services-8963 execpodlvjjf -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 13 01:00:12.326: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 13 01:00:12.326: INFO: stdout: ""
Mar 13 01:00:12.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=services-8963 execpodlvjjf -- /bin/sh -x -c nc -zv -t -w 2 10.10.29.168 80'
Mar 13 01:00:12.932: INFO: stderr: "+ nc -zv -t -w 2 10.10.29.168 80\nConnection to 10.10.29.168 80 port [tcp/http] succeeded!\n"
Mar 13 01:00:12.932: INFO: stdout: ""
Mar 13 01:00:12.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=services-8963 execpodlvjjf -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.75 30269'
Mar 13 01:00:13.500: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.75 30269\nConnection to 10.10.1.75 30269 port [tcp/30269] succeeded!\n"
Mar 13 01:00:13.500: INFO: stdout: ""
Mar 13 01:00:13.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=services-8963 execpodlvjjf -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.238 30269'
Mar 13 01:00:14.091: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.238 30269\nConnection to 10.10.1.238 30269 port [tcp/30269] succeeded!\n"
Mar 13 01:00:14.091: INFO: stdout: ""
Mar 13 01:00:14.091: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:00:14.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8963" for this suite.
Mar 13 01:00:20.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:00:20.378: INFO: namespace services-8963 deletion completed in 6.226159933s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:16.884 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:00:20.379: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 13 01:00:20.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3060'
Mar 13 01:00:20.720: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 13 01:00:20.720: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Mar 13 01:00:20.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete jobs e2e-test-httpd-job --namespace=kubectl-3060'
Mar 13 01:00:20.965: INFO: stderr: ""
Mar 13 01:00:20.965: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:00:20.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3060" for this suite.
Mar 13 01:00:26.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:00:27.154: INFO: namespace kubectl-3060 deletion completed in 6.178830977s

• [SLOW TEST:6.776 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:00:27.158: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:00:27.892: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 01:00:29.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719658027, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719658027, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719658027, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719658027, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:00:32.936: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:00:32.953: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8159-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:00:34.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8237" for this suite.
Mar 13 01:00:40.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:00:40.450: INFO: namespace webhook-8237 deletion completed in 6.21531381s
STEP: Destroying namespace "webhook-8237-markers" for this suite.
Mar 13 01:00:46.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:00:46.613: INFO: namespace webhook-8237-markers deletion completed in 6.162884717s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.483 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:00:46.642: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9013
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 13 01:00:46.712: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 13 01:01:12.882: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.100:8080/dial?request=hostName&protocol=udp&host=10.244.0.21&port=8081&tries=1'] Namespace:pod-network-test-9013 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 01:01:12.882: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 01:01:13.284: INFO: Waiting for endpoints: map[]
Mar 13 01:01:13.290: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.100:8080/dial?request=hostName&protocol=udp&host=10.244.1.33&port=8081&tries=1'] Namespace:pod-network-test-9013 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 01:01:13.290: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 01:01:13.743: INFO: Waiting for endpoints: map[]
Mar 13 01:01:13.752: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.100:8080/dial?request=hostName&protocol=udp&host=10.244.2.99&port=8081&tries=1'] Namespace:pod-network-test-9013 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 01:01:13.752: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 01:01:14.218: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:01:14.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9013" for this suite.
Mar 13 01:01:26.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:01:26.394: INFO: namespace pod-network-test-9013 deletion completed in 12.168306262s

• [SLOW TEST:39.752 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:01:26.398: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:01:30.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3673" for this suite.
Mar 13 01:02:18.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:02:18.706: INFO: namespace kubelet-test-3673 deletion completed in 48.173455524s

• [SLOW TEST:52.308 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:02:18.707: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-c269b632-ebe1-4d04-b9e7-3b86cb06a789
STEP: Creating a pod to test consume secrets
Mar 13 01:02:18.785: INFO: Waiting up to 5m0s for pod "pod-secrets-2d2eff3a-fd46-4bec-8a0e-d1f5cd5d586d" in namespace "secrets-887" to be "success or failure"
Mar 13 01:02:18.790: INFO: Pod "pod-secrets-2d2eff3a-fd46-4bec-8a0e-d1f5cd5d586d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.412293ms
Mar 13 01:02:20.796: INFO: Pod "pod-secrets-2d2eff3a-fd46-4bec-8a0e-d1f5cd5d586d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0104728s
Mar 13 01:02:22.801: INFO: Pod "pod-secrets-2d2eff3a-fd46-4bec-8a0e-d1f5cd5d586d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015494902s
STEP: Saw pod success
Mar 13 01:02:22.801: INFO: Pod "pod-secrets-2d2eff3a-fd46-4bec-8a0e-d1f5cd5d586d" satisfied condition "success or failure"
Mar 13 01:02:22.806: INFO: Trying to get logs from node silbory-nirmata0 pod pod-secrets-2d2eff3a-fd46-4bec-8a0e-d1f5cd5d586d container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 01:02:22.839: INFO: Waiting for pod pod-secrets-2d2eff3a-fd46-4bec-8a0e-d1f5cd5d586d to disappear
Mar 13 01:02:22.845: INFO: Pod pod-secrets-2d2eff3a-fd46-4bec-8a0e-d1f5cd5d586d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:02:22.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-887" for this suite.
Mar 13 01:02:28.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:02:29.028: INFO: namespace secrets-887 deletion completed in 6.173977347s

• [SLOW TEST:10.321 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:02:29.028: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:02:46.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5970" for this suite.
Mar 13 01:02:52.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:02:52.347: INFO: namespace resourcequota-5970 deletion completed in 6.171998895s

• [SLOW TEST:23.319 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:02:52.352: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1861.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1861.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1861.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1861.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1861.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1861.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1861.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1861.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1861.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1861.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1861.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1861.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1861.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 246.243.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.243.246_udp@PTR;check="$$(dig +tcp +noall +answer +search 246.243.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.243.246_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1861.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1861.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1861.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1861.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1861.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1861.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1861.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1861.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1861.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1861.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1861.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1861.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1861.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 246.243.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.243.246_udp@PTR;check="$$(dig +tcp +noall +answer +search 246.243.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.243.246_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 13 01:02:56.595: INFO: Unable to read jessie_udp@dns-test-service.dns-1861.svc.cluster.local from pod dns-1861/dns-test-bb8b971b-5bcf-4f69-b676-96ae4eb5d78b: the server could not find the requested resource (get pods dns-test-bb8b971b-5bcf-4f69-b676-96ae4eb5d78b)
Mar 13 01:02:56.606: INFO: Unable to read jessie_tcp@dns-test-service.dns-1861.svc.cluster.local from pod dns-1861/dns-test-bb8b971b-5bcf-4f69-b676-96ae4eb5d78b: the server could not find the requested resource (get pods dns-test-bb8b971b-5bcf-4f69-b676-96ae4eb5d78b)
Mar 13 01:02:56.614: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1861.svc.cluster.local from pod dns-1861/dns-test-bb8b971b-5bcf-4f69-b676-96ae4eb5d78b: the server could not find the requested resource (get pods dns-test-bb8b971b-5bcf-4f69-b676-96ae4eb5d78b)
Mar 13 01:02:56.623: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1861.svc.cluster.local from pod dns-1861/dns-test-bb8b971b-5bcf-4f69-b676-96ae4eb5d78b: the server could not find the requested resource (get pods dns-test-bb8b971b-5bcf-4f69-b676-96ae4eb5d78b)
Mar 13 01:02:56.673: INFO: Lookups using dns-1861/dns-test-bb8b971b-5bcf-4f69-b676-96ae4eb5d78b failed for: [jessie_udp@dns-test-service.dns-1861.svc.cluster.local jessie_tcp@dns-test-service.dns-1861.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1861.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1861.svc.cluster.local]

Mar 13 01:03:01.820: INFO: DNS probes using dns-1861/dns-test-bb8b971b-5bcf-4f69-b676-96ae4eb5d78b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:03:02.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1861" for this suite.
Mar 13 01:03:08.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:03:08.221: INFO: namespace dns-1861 deletion completed in 6.196723195s

• [SLOW TEST:15.869 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:03:08.222: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:03:08.318: INFO: (0) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 22.498906ms)
Mar 13 01:03:08.326: INFO: (1) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.642875ms)
Mar 13 01:03:08.334: INFO: (2) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.205993ms)
Mar 13 01:03:08.341: INFO: (3) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.373075ms)
Mar 13 01:03:08.349: INFO: (4) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.56574ms)
Mar 13 01:03:08.357: INFO: (5) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.773529ms)
Mar 13 01:03:08.365: INFO: (6) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.197032ms)
Mar 13 01:03:08.373: INFO: (7) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.773018ms)
Mar 13 01:03:08.381: INFO: (8) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.215329ms)
Mar 13 01:03:08.390: INFO: (9) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.979318ms)
Mar 13 01:03:08.398: INFO: (10) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.225577ms)
Mar 13 01:03:08.407: INFO: (11) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.333056ms)
Mar 13 01:03:08.415: INFO: (12) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.452649ms)
Mar 13 01:03:08.423: INFO: (13) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.008234ms)
Mar 13 01:03:08.431: INFO: (14) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.937761ms)
Mar 13 01:03:08.438: INFO: (15) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.097376ms)
Mar 13 01:03:08.446: INFO: (16) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.993231ms)
Mar 13 01:03:08.453: INFO: (17) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.846373ms)
Mar 13 01:03:08.460: INFO: (18) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.451933ms)
Mar 13 01:03:08.467: INFO: (19) /api/v1/nodes/sam-node2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.697776ms)
[AfterEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:03:08.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7301" for this suite.
Mar 13 01:03:14.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:03:14.661: INFO: namespace proxy-7301 deletion completed in 6.186953214s

• [SLOW TEST:6.439 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:03:14.662: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 13 01:03:19.775: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:03:20.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9378" for this suite.
Mar 13 01:03:48.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:03:48.986: INFO: namespace replicaset-9378 deletion completed in 28.171374841s

• [SLOW TEST:34.325 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:03:48.987: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:03:49.051: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:03:53.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4148" for this suite.
Mar 13 01:04:37.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:04:37.624: INFO: namespace pods-4148 deletion completed in 44.175922427s

• [SLOW TEST:48.637 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:04:37.624: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 13 01:04:37.690: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 13 01:04:37.713: INFO: Waiting for terminating namespaces to be deleted...
Mar 13 01:04:37.718: INFO: 
Logging pods the kubelet thinks is on node sam-node2 before test
Mar 13 01:04:37.730: INFO: kube-dns-869c556677-bfkcm from kube-system started at 2020-03-13 00:13:29 +0000 UTC (3 container statuses recorded)
Mar 13 01:04:37.731: INFO: 	Container dnsmasq ready: true, restart count 0
Mar 13 01:04:37.731: INFO: 	Container kubedns ready: true, restart count 0
Mar 13 01:04:37.731: INFO: 	Container sidecar ready: true, restart count 0
Mar 13 01:04:37.731: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-w6d2v from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 01:04:37.732: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 01:04:37.732: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 13 01:04:37.732: INFO: nirmata-cni-installer-nkzv7 from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 01:04:37.732: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 01:04:37.733: INFO: kube-flannel-ds-d9nq5 from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 01:04:37.733: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 01:04:37.733: INFO: 
Logging pods the kubelet thinks is on node sam-node3 before test
Mar 13 01:04:37.755: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-f9nz4 from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 01:04:37.755: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 01:04:37.756: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 13 01:04:37.756: INFO: nirmata-cni-installer-bf5qr from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 01:04:37.756: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 01:04:37.756: INFO: kube-flannel-ds-2rr2t from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 01:04:37.756: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 01:04:37.756: INFO: nirmata-kube-controller-666cf5cf5f-j4mmx from nirmata started at 2020-03-13 00:13:27 +0000 UTC (1 container statuses recorded)
Mar 13 01:04:37.756: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
Mar 13 01:04:37.756: INFO: 
Logging pods the kubelet thinks is on node silbory-nirmata0 before test
Mar 13 01:04:37.784: INFO: metrics-server-56c7b465d6-xr59t from kube-system started at 2020-03-13 00:13:52 +0000 UTC (1 container statuses recorded)
Mar 13 01:04:37.785: INFO: 	Container metrics-server ready: true, restart count 0
Mar 13 01:04:37.785: INFO: kube-flannel-ds-959x5 from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 01:04:37.785: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 01:04:37.785: INFO: ingress-default-backend-cf675c575-nttt9 from ingress-haproxy started at 2020-03-13 00:14:00 +0000 UTC (1 container statuses recorded)
Mar 13 01:04:37.785: INFO: 	Container ingress-default-backend ready: true, restart count 0
Mar 13 01:04:37.785: INFO: sonobuoy-e2e-job-fce51d5a97b943f8 from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 01:04:37.786: INFO: 	Container e2e ready: true, restart count 0
Mar 13 01:04:37.786: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 01:04:37.786: INFO: nirmata-cni-installer-lnhv6 from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 01:04:37.786: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 01:04:37.786: INFO: haproxy-ingress-8688f746b4-2xbx5 from ingress-haproxy started at 2020-03-13 00:14:00 +0000 UTC (1 container statuses recorded)
Mar 13 01:04:37.787: INFO: 	Container haproxy-ingress ready: true, restart count 0
Mar 13 01:04:37.787: INFO: sonobuoy from sonobuoy started at 2020-03-13 00:21:12 +0000 UTC (1 container statuses recorded)
Mar 13 01:04:37.787: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 13 01:04:37.787: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-qf4zs from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 01:04:37.787: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 01:04:37.787: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15fbb78578197e07], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:04:38.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7189" for this suite.
Mar 13 01:04:44.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:04:45.011: INFO: namespace sched-pred-7189 deletion completed in 6.171154213s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.387 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:04:45.014: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3398
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3398
STEP: Creating statefulset with conflicting port in namespace statefulset-3398
STEP: Waiting until pod test-pod will start running in namespace statefulset-3398
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3398
Mar 13 01:04:49.168: INFO: Observed stateful pod in namespace: statefulset-3398, name: ss-0, uid: 01d6c9d5-dcca-4a69-8dd4-a540513230bb, status phase: Pending. Waiting for statefulset controller to delete.
Mar 13 01:04:51.793: INFO: Observed stateful pod in namespace: statefulset-3398, name: ss-0, uid: 01d6c9d5-dcca-4a69-8dd4-a540513230bb, status phase: Failed. Waiting for statefulset controller to delete.
Mar 13 01:04:51.808: INFO: Observed stateful pod in namespace: statefulset-3398, name: ss-0, uid: 01d6c9d5-dcca-4a69-8dd4-a540513230bb, status phase: Failed. Waiting for statefulset controller to delete.
Mar 13 01:04:51.819: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3398
STEP: Removing pod with conflicting port in namespace statefulset-3398
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3398 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 13 01:04:55.878: INFO: Deleting all statefulset in ns statefulset-3398
Mar 13 01:04:55.884: INFO: Scaling statefulset ss to 0
Mar 13 01:05:05.916: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 01:05:05.919: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:05:05.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3398" for this suite.
Mar 13 01:05:11.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:05:12.114: INFO: namespace statefulset-3398 deletion completed in 6.180095179s

• [SLOW TEST:27.100 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:05:12.116: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 13 01:05:12.209: INFO: Waiting up to 5m0s for pod "pod-537277ca-865b-47c0-b40c-d92ebd2dc6b6" in namespace "emptydir-9772" to be "success or failure"
Mar 13 01:05:12.215: INFO: Pod "pod-537277ca-865b-47c0-b40c-d92ebd2dc6b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.394766ms
Mar 13 01:05:14.224: INFO: Pod "pod-537277ca-865b-47c0-b40c-d92ebd2dc6b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015253178s
Mar 13 01:05:16.231: INFO: Pod "pod-537277ca-865b-47c0-b40c-d92ebd2dc6b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021694036s
STEP: Saw pod success
Mar 13 01:05:16.231: INFO: Pod "pod-537277ca-865b-47c0-b40c-d92ebd2dc6b6" satisfied condition "success or failure"
Mar 13 01:05:16.235: INFO: Trying to get logs from node silbory-nirmata0 pod pod-537277ca-865b-47c0-b40c-d92ebd2dc6b6 container test-container: <nil>
STEP: delete the pod
Mar 13 01:05:16.281: INFO: Waiting for pod pod-537277ca-865b-47c0-b40c-d92ebd2dc6b6 to disappear
Mar 13 01:05:16.289: INFO: Pod pod-537277ca-865b-47c0-b40c-d92ebd2dc6b6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:05:16.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9772" for this suite.
Mar 13 01:05:22.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:05:22.481: INFO: namespace emptydir-9772 deletion completed in 6.182654874s

• [SLOW TEST:10.365 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:05:22.483: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:05:38.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-791" for this suite.
Mar 13 01:05:44.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:05:44.873: INFO: namespace resourcequota-791 deletion completed in 6.174267027s

• [SLOW TEST:22.390 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:05:44.873: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-a32e0d2e-68e3-4166-b601-8262b7368c18
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:05:44.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6108" for this suite.
Mar 13 01:05:51.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:05:51.166: INFO: namespace secrets-6108 deletion completed in 6.164774509s

• [SLOW TEST:6.293 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:05:51.166: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8384
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 13 01:05:51.233: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 13 01:06:13.388: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.111:8080/dial?request=hostName&protocol=http&host=10.244.2.110&port=8080&tries=1'] Namespace:pod-network-test-8384 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 01:06:13.388: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 01:06:13.756: INFO: Waiting for endpoints: map[]
Mar 13 01:06:13.763: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.111:8080/dial?request=hostName&protocol=http&host=10.244.1.36&port=8080&tries=1'] Namespace:pod-network-test-8384 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 01:06:13.763: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 01:06:14.213: INFO: Waiting for endpoints: map[]
Mar 13 01:06:14.220: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.111:8080/dial?request=hostName&protocol=http&host=10.244.0.22&port=8080&tries=1'] Namespace:pod-network-test-8384 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 01:06:14.220: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 01:06:14.683: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:06:14.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8384" for this suite.
Mar 13 01:06:26.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:06:26.879: INFO: namespace pod-network-test-8384 deletion completed in 12.185833945s

• [SLOW TEST:35.713 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:06:26.880: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-fc6e0c1d-e9e1-46f3-aae0-a3fa9d081b71
STEP: Creating a pod to test consume configMaps
Mar 13 01:06:26.982: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-344e78cf-267b-4b74-80f9-a6da48dde0d7" in namespace "projected-7296" to be "success or failure"
Mar 13 01:06:26.990: INFO: Pod "pod-projected-configmaps-344e78cf-267b-4b74-80f9-a6da48dde0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.700269ms
Mar 13 01:06:28.999: INFO: Pod "pod-projected-configmaps-344e78cf-267b-4b74-80f9-a6da48dde0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017298942s
Mar 13 01:06:31.006: INFO: Pod "pod-projected-configmaps-344e78cf-267b-4b74-80f9-a6da48dde0d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024202412s
STEP: Saw pod success
Mar 13 01:06:31.007: INFO: Pod "pod-projected-configmaps-344e78cf-267b-4b74-80f9-a6da48dde0d7" satisfied condition "success or failure"
Mar 13 01:06:31.012: INFO: Trying to get logs from node silbory-nirmata0 pod pod-projected-configmaps-344e78cf-267b-4b74-80f9-a6da48dde0d7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 01:06:31.045: INFO: Waiting for pod pod-projected-configmaps-344e78cf-267b-4b74-80f9-a6da48dde0d7 to disappear
Mar 13 01:06:31.053: INFO: Pod pod-projected-configmaps-344e78cf-267b-4b74-80f9-a6da48dde0d7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:06:31.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7296" for this suite.
Mar 13 01:06:37.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:06:37.248: INFO: namespace projected-7296 deletion completed in 6.184989346s

• [SLOW TEST:10.367 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:06:37.248: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-222fe4ec-6b60-48ba-a8ee-6e560337c679
STEP: Creating a pod to test consume configMaps
Mar 13 01:06:37.339: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-34e047d9-5cfd-427b-a783-28051cdfcfdf" in namespace "projected-6696" to be "success or failure"
Mar 13 01:06:37.346: INFO: Pod "pod-projected-configmaps-34e047d9-5cfd-427b-a783-28051cdfcfdf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.591222ms
Mar 13 01:06:39.353: INFO: Pod "pod-projected-configmaps-34e047d9-5cfd-427b-a783-28051cdfcfdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013392367s
Mar 13 01:06:41.360: INFO: Pod "pod-projected-configmaps-34e047d9-5cfd-427b-a783-28051cdfcfdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020596876s
STEP: Saw pod success
Mar 13 01:06:41.360: INFO: Pod "pod-projected-configmaps-34e047d9-5cfd-427b-a783-28051cdfcfdf" satisfied condition "success or failure"
Mar 13 01:06:41.366: INFO: Trying to get logs from node silbory-nirmata0 pod pod-projected-configmaps-34e047d9-5cfd-427b-a783-28051cdfcfdf container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 01:06:41.407: INFO: Waiting for pod pod-projected-configmaps-34e047d9-5cfd-427b-a783-28051cdfcfdf to disappear
Mar 13 01:06:41.413: INFO: Pod pod-projected-configmaps-34e047d9-5cfd-427b-a783-28051cdfcfdf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:06:41.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6696" for this suite.
Mar 13 01:06:47.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:06:47.605: INFO: namespace projected-6696 deletion completed in 6.185079989s

• [SLOW TEST:10.357 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:06:47.606: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 13 01:06:47.684: INFO: Waiting up to 5m0s for pod "pod-ff567083-769d-40a9-9e3f-f4578d29b014" in namespace "emptydir-5041" to be "success or failure"
Mar 13 01:06:47.690: INFO: Pod "pod-ff567083-769d-40a9-9e3f-f4578d29b014": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030066ms
Mar 13 01:06:49.700: INFO: Pod "pod-ff567083-769d-40a9-9e3f-f4578d29b014": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015771702s
Mar 13 01:06:51.708: INFO: Pod "pod-ff567083-769d-40a9-9e3f-f4578d29b014": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023748052s
STEP: Saw pod success
Mar 13 01:06:51.708: INFO: Pod "pod-ff567083-769d-40a9-9e3f-f4578d29b014" satisfied condition "success or failure"
Mar 13 01:06:51.713: INFO: Trying to get logs from node silbory-nirmata0 pod pod-ff567083-769d-40a9-9e3f-f4578d29b014 container test-container: <nil>
STEP: delete the pod
Mar 13 01:06:51.754: INFO: Waiting for pod pod-ff567083-769d-40a9-9e3f-f4578d29b014 to disappear
Mar 13 01:06:51.763: INFO: Pod pod-ff567083-769d-40a9-9e3f-f4578d29b014 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:06:51.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5041" for this suite.
Mar 13 01:06:57.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:06:57.956: INFO: namespace emptydir-5041 deletion completed in 6.185314507s

• [SLOW TEST:10.351 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:06:57.957: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 13 01:06:58.047: INFO: Waiting up to 5m0s for pod "pod-65a5fd2f-3d65-4236-8cbb-f5efd935eb48" in namespace "emptydir-4871" to be "success or failure"
Mar 13 01:06:58.054: INFO: Pod "pod-65a5fd2f-3d65-4236-8cbb-f5efd935eb48": Phase="Pending", Reason="", readiness=false. Elapsed: 6.634802ms
Mar 13 01:07:00.074: INFO: Pod "pod-65a5fd2f-3d65-4236-8cbb-f5efd935eb48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026442261s
Mar 13 01:07:02.081: INFO: Pod "pod-65a5fd2f-3d65-4236-8cbb-f5efd935eb48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034181876s
STEP: Saw pod success
Mar 13 01:07:02.082: INFO: Pod "pod-65a5fd2f-3d65-4236-8cbb-f5efd935eb48" satisfied condition "success or failure"
Mar 13 01:07:02.096: INFO: Trying to get logs from node silbory-nirmata0 pod pod-65a5fd2f-3d65-4236-8cbb-f5efd935eb48 container test-container: <nil>
STEP: delete the pod
Mar 13 01:07:02.138: INFO: Waiting for pod pod-65a5fd2f-3d65-4236-8cbb-f5efd935eb48 to disappear
Mar 13 01:07:02.148: INFO: Pod pod-65a5fd2f-3d65-4236-8cbb-f5efd935eb48 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:07:02.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4871" for this suite.
Mar 13 01:07:08.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:07:08.339: INFO: namespace emptydir-4871 deletion completed in 6.182957049s

• [SLOW TEST:10.383 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:07:08.340: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar 13 01:07:12.991: INFO: Successfully updated pod "labelsupdatef86807e2-03f9-4298-a4ad-f80365b645df"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:07:15.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-228" for this suite.
Mar 13 01:07:35.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:07:35.211: INFO: namespace downward-api-228 deletion completed in 20.174006609s

• [SLOW TEST:26.872 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:07:35.212: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:07:35.271: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:07:39.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-223" for this suite.
Mar 13 01:08:23.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:08:23.519: INFO: namespace pods-223 deletion completed in 44.172438036s

• [SLOW TEST:48.307 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:08:23.521: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:08:55.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4915" for this suite.
Mar 13 01:09:01.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:09:01.936: INFO: namespace namespaces-4915 deletion completed in 6.176515202s
STEP: Destroying namespace "nsdeletetest-8652" for this suite.
Mar 13 01:09:01.940: INFO: Namespace nsdeletetest-8652 was already deleted
STEP: Destroying namespace "nsdeletetest-2876" for this suite.
Mar 13 01:09:07.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:09:08.139: INFO: namespace nsdeletetest-2876 deletion completed in 6.199168149s

• [SLOW TEST:44.619 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:09:08.141: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3194.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3194.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 13 01:09:12.329: INFO: DNS probes using dns-3194/dns-test-47dbeb24-b32d-4136-9c72-a7ea1ca1a9f0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:09:12.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3194" for this suite.
Mar 13 01:09:18.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:09:18.567: INFO: namespace dns-3194 deletion completed in 6.204763674s

• [SLOW TEST:10.427 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:09:18.568: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Mar 13 01:09:18.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=kubectl-5838 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar 13 01:09:21.720: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar 13 01:09:21.720: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:09:23.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5838" for this suite.
Mar 13 01:09:29.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:09:29.914: INFO: namespace kubectl-5838 deletion completed in 6.174318124s

• [SLOW TEST:11.346 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:09:29.915: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:09:30.071: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"163c42cd-32d2-452e-899e-3de858b4c0bb", Controller:(*bool)(0xc003384506), BlockOwnerDeletion:(*bool)(0xc003384507)}}
Mar 13 01:09:30.093: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3882b0f6-104c-4e64-bead-9fd9bd1f581e", Controller:(*bool)(0xc0036d5ece), BlockOwnerDeletion:(*bool)(0xc0036d5ecf)}}
Mar 13 01:09:30.108: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"12d03373-e5da-43e4-baf6-44f14d111b47", Controller:(*bool)(0xc00344d866), BlockOwnerDeletion:(*bool)(0xc00344d867)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:09:35.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8304" for this suite.
Mar 13 01:09:41.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:09:41.314: INFO: namespace gc-8304 deletion completed in 6.17798421s

• [SLOW TEST:11.399 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:09:41.314: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:09:47.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1343" for this suite.
Mar 13 01:09:53.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:09:53.732: INFO: namespace namespaces-1343 deletion completed in 6.187335318s
STEP: Destroying namespace "nsdeletetest-6348" for this suite.
Mar 13 01:09:53.739: INFO: Namespace nsdeletetest-6348 was already deleted
STEP: Destroying namespace "nsdeletetest-7277" for this suite.
Mar 13 01:09:59.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:09:59.897: INFO: namespace nsdeletetest-7277 deletion completed in 6.157205024s

• [SLOW TEST:18.582 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:09:59.897: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-8fab7698-3db8-4266-b060-ca5d6127a0e6
STEP: Creating configMap with name cm-test-opt-upd-c16337d0-bcb4-462d-809d-84a9633b77cb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-8fab7698-3db8-4266-b060-ca5d6127a0e6
STEP: Updating configmap cm-test-opt-upd-c16337d0-bcb4-462d-809d-84a9633b77cb
STEP: Creating configMap with name cm-test-opt-create-5184653d-1d72-426f-b8ff-9b3ef942bcea
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:11:20.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-93" for this suite.
Mar 13 01:11:33.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:11:33.159: INFO: namespace configmap-93 deletion completed in 12.175750169s

• [SLOW TEST:93.263 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:11:33.164: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 13 01:11:37.766: INFO: Successfully updated pod "adopt-release-2rlrq"
STEP: Checking that the Job readopts the Pod
Mar 13 01:11:37.766: INFO: Waiting up to 15m0s for pod "adopt-release-2rlrq" in namespace "job-9438" to be "adopted"
Mar 13 01:11:37.771: INFO: Pod "adopt-release-2rlrq": Phase="Running", Reason="", readiness=true. Elapsed: 5.487721ms
Mar 13 01:11:39.777: INFO: Pod "adopt-release-2rlrq": Phase="Running", Reason="", readiness=true. Elapsed: 2.011373352s
Mar 13 01:11:39.777: INFO: Pod "adopt-release-2rlrq" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 13 01:11:40.296: INFO: Successfully updated pod "adopt-release-2rlrq"
STEP: Checking that the Job releases the Pod
Mar 13 01:11:40.297: INFO: Waiting up to 15m0s for pod "adopt-release-2rlrq" in namespace "job-9438" to be "released"
Mar 13 01:11:40.301: INFO: Pod "adopt-release-2rlrq": Phase="Running", Reason="", readiness=true. Elapsed: 4.550204ms
Mar 13 01:11:42.308: INFO: Pod "adopt-release-2rlrq": Phase="Running", Reason="", readiness=true. Elapsed: 2.011503353s
Mar 13 01:11:42.308: INFO: Pod "adopt-release-2rlrq" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:11:42.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9438" for this suite.
Mar 13 01:12:26.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:12:26.516: INFO: namespace job-9438 deletion completed in 44.199106577s

• [SLOW TEST:53.352 seconds]
[sig-apps] Job
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:12:26.517: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:12:37.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-43" for this suite.
Mar 13 01:12:43.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:12:43.820: INFO: namespace resourcequota-43 deletion completed in 6.159690464s

• [SLOW TEST:17.303 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:12:43.823: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-672, will wait for the garbage collector to delete the pods
Mar 13 01:12:47.988: INFO: Deleting Job.batch foo took: 14.25144ms
Mar 13 01:12:48.289: INFO: Terminating Job.batch foo pods took: 300.430882ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:13:26.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-672" for this suite.
Mar 13 01:13:32.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:13:32.371: INFO: namespace job-672 deletion completed in 6.168528265s

• [SLOW TEST:48.549 seconds]
[sig-apps] Job
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:13:32.373: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1706
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 13 01:13:32.435: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 13 01:13:56.587: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.37:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1706 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 01:13:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 01:13:56.996: INFO: Found all expected endpoints: [netserver-0]
Mar 13 01:13:57.004: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.23:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1706 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 01:13:57.004: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 01:13:57.431: INFO: Found all expected endpoints: [netserver-1]
Mar 13 01:13:57.437: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.127:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1706 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 01:13:57.437: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 01:13:57.888: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:13:57.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1706" for this suite.
Mar 13 01:14:09.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:14:10.105: INFO: namespace pod-network-test-1706 deletion completed in 12.205997846s

• [SLOW TEST:37.732 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:14:10.106: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 01:14:10.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d04d596-dc8c-47a4-a140-73df1c8c0c1f" in namespace "projected-205" to be "success or failure"
Mar 13 01:14:10.172: INFO: Pod "downwardapi-volume-9d04d596-dc8c-47a4-a140-73df1c8c0c1f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.74393ms
Mar 13 01:14:12.179: INFO: Pod "downwardapi-volume-9d04d596-dc8c-47a4-a140-73df1c8c0c1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009928904s
Mar 13 01:14:14.185: INFO: Pod "downwardapi-volume-9d04d596-dc8c-47a4-a140-73df1c8c0c1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01644603s
STEP: Saw pod success
Mar 13 01:14:14.185: INFO: Pod "downwardapi-volume-9d04d596-dc8c-47a4-a140-73df1c8c0c1f" satisfied condition "success or failure"
Mar 13 01:14:14.192: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-9d04d596-dc8c-47a4-a140-73df1c8c0c1f container client-container: <nil>
STEP: delete the pod
Mar 13 01:14:14.250: INFO: Waiting for pod downwardapi-volume-9d04d596-dc8c-47a4-a140-73df1c8c0c1f to disappear
Mar 13 01:14:14.255: INFO: Pod downwardapi-volume-9d04d596-dc8c-47a4-a140-73df1c8c0c1f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:14:14.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-205" for this suite.
Mar 13 01:14:20.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:14:20.444: INFO: namespace projected-205 deletion completed in 6.180949078s

• [SLOW TEST:10.339 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:14:20.445: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:14:20.512: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 13 01:14:26.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-8041 create -f -'
Mar 13 01:14:27.138: INFO: stderr: ""
Mar 13 01:14:27.138: INFO: stdout: "e2e-test-crd-publish-openapi-3776-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 13 01:14:27.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-8041 delete e2e-test-crd-publish-openapi-3776-crds test-cr'
Mar 13 01:14:27.362: INFO: stderr: ""
Mar 13 01:14:27.362: INFO: stdout: "e2e-test-crd-publish-openapi-3776-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 13 01:14:27.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-8041 apply -f -'
Mar 13 01:14:27.768: INFO: stderr: ""
Mar 13 01:14:27.768: INFO: stdout: "e2e-test-crd-publish-openapi-3776-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 13 01:14:27.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-8041 delete e2e-test-crd-publish-openapi-3776-crds test-cr'
Mar 13 01:14:27.992: INFO: stderr: ""
Mar 13 01:14:27.992: INFO: stdout: "e2e-test-crd-publish-openapi-3776-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 13 01:14:27.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 explain e2e-test-crd-publish-openapi-3776-crds'
Mar 13 01:14:28.385: INFO: stderr: ""
Mar 13 01:14:28.385: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3776-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:14:34.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8041" for this suite.
Mar 13 01:14:40.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:14:41.068: INFO: namespace crd-publish-openapi-8041 deletion completed in 6.191751242s

• [SLOW TEST:20.622 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:14:41.068: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Mar 13 01:14:41.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-2835 -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 13 01:14:41.389: INFO: stderr: ""
Mar 13 01:14:41.390: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Mar 13 01:14:41.390: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 13 01:14:41.390: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2835" to be "running and ready, or succeeded"
Mar 13 01:14:41.398: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.133672ms
Mar 13 01:14:43.405: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015028003s
Mar 13 01:14:45.412: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.02235303s
Mar 13 01:14:45.412: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 13 01:14:45.412: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 13 01:14:45.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 logs logs-generator logs-generator --namespace=kubectl-2835'
Mar 13 01:14:45.662: INFO: stderr: ""
Mar 13 01:14:45.662: INFO: stdout: "I0313 01:14:43.156376       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/zhn7 280\nI0313 01:14:43.356713       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/cmc 292\nI0313 01:14:43.556690       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/bxhb 260\nI0313 01:14:43.756695       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/cdb8 249\nI0313 01:14:43.956696       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/948t 248\nI0313 01:14:44.156782       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/b2n 383\nI0313 01:14:44.356696       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/xn7 495\nI0313 01:14:44.556691       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/5mxx 463\nI0313 01:14:44.756713       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/27m 373\nI0313 01:14:44.956670       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/f6kw 246\nI0313 01:14:45.156693       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/9f7x 431\nI0313 01:14:45.356728       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/8bsh 536\nI0313 01:14:45.556664       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/s6cr 445\n"
STEP: limiting log lines
Mar 13 01:14:45.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 logs logs-generator logs-generator --namespace=kubectl-2835 --tail=1'
Mar 13 01:14:45.918: INFO: stderr: ""
Mar 13 01:14:45.918: INFO: stdout: "I0313 01:14:45.756742       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/4mfj 214\n"
STEP: limiting log bytes
Mar 13 01:14:45.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 logs logs-generator logs-generator --namespace=kubectl-2835 --limit-bytes=1'
Mar 13 01:14:46.156: INFO: stderr: ""
Mar 13 01:14:46.156: INFO: stdout: "I"
STEP: exposing timestamps
Mar 13 01:14:46.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 logs logs-generator logs-generator --namespace=kubectl-2835 --tail=1 --timestamps'
Mar 13 01:14:46.418: INFO: stderr: ""
Mar 13 01:14:46.418: INFO: stdout: "2020-03-13T01:14:46.357179921Z I0313 01:14:46.356681       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/c4vd 497\n"
STEP: restricting to a time range
Mar 13 01:14:48.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 logs logs-generator logs-generator --namespace=kubectl-2835 --since=1s'
Mar 13 01:14:49.160: INFO: stderr: ""
Mar 13 01:14:49.161: INFO: stdout: "I0313 01:14:48.156643       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/grkb 520\nI0313 01:14:48.356681       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/wkt 347\nI0313 01:14:48.556638       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/2j66 416\nI0313 01:14:48.756701       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/6xg 566\nI0313 01:14:48.956766       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/x6f 201\n"
Mar 13 01:14:49.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 logs logs-generator logs-generator --namespace=kubectl-2835 --since=24h'
Mar 13 01:14:49.424: INFO: stderr: ""
Mar 13 01:14:49.424: INFO: stdout: "I0313 01:14:43.156376       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/zhn7 280\nI0313 01:14:43.356713       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/cmc 292\nI0313 01:14:43.556690       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/bxhb 260\nI0313 01:14:43.756695       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/cdb8 249\nI0313 01:14:43.956696       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/948t 248\nI0313 01:14:44.156782       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/b2n 383\nI0313 01:14:44.356696       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/xn7 495\nI0313 01:14:44.556691       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/5mxx 463\nI0313 01:14:44.756713       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/27m 373\nI0313 01:14:44.956670       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/f6kw 246\nI0313 01:14:45.156693       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/9f7x 431\nI0313 01:14:45.356728       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/8bsh 536\nI0313 01:14:45.556664       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/s6cr 445\nI0313 01:14:45.756742       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/4mfj 214\nI0313 01:14:45.956680       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/qjpp 292\nI0313 01:14:46.156651       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/jcq 218\nI0313 01:14:46.356681       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/c4vd 497\nI0313 01:14:46.556670       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/gkwz 449\nI0313 01:14:46.756712       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/w4vg 556\nI0313 01:14:46.956695       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/qz4 331\nI0313 01:14:47.156816       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/tq6 367\nI0313 01:14:47.356692       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/76w 567\nI0313 01:14:47.556705       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/hbjm 384\nI0313 01:14:47.756665       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/wzvq 364\nI0313 01:14:47.956724       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/crnb 335\nI0313 01:14:48.156643       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/grkb 520\nI0313 01:14:48.356681       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/wkt 347\nI0313 01:14:48.556638       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/2j66 416\nI0313 01:14:48.756701       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/6xg 566\nI0313 01:14:48.956766       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/x6f 201\nI0313 01:14:49.156607       1 logs_generator.go:76] 30 GET /api/v1/namespaces/ns/pods/4kr 276\nI0313 01:14:49.356728       1 logs_generator.go:76] 31 POST /api/v1/namespaces/kube-system/pods/ksfs 543\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Mar 13 01:14:49.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete pod logs-generator --namespace=kubectl-2835'
Mar 13 01:14:56.090: INFO: stderr: ""
Mar 13 01:14:56.090: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:14:56.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2835" for this suite.
Mar 13 01:15:02.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:15:02.297: INFO: namespace kubectl-2835 deletion completed in 6.196101384s

• [SLOW TEST:21.229 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:15:02.301: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:15:18.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6026" for this suite.
Mar 13 01:15:24.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:15:24.674: INFO: namespace resourcequota-6026 deletion completed in 6.215906614s

• [SLOW TEST:22.374 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:15:24.674: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar 13 01:15:29.299: INFO: Successfully updated pod "labelsupdatef27678c7-e5a2-46dd-835a-45984c51ec14"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:15:31.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8424" for this suite.
Mar 13 01:15:59.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:15:59.573: INFO: namespace projected-8424 deletion completed in 28.231236906s

• [SLOW TEST:34.898 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:15:59.575: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-231.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-231.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-231.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-231.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-231.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-231.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 13 01:16:03.754: INFO: DNS probes using dns-231/dns-test-a10c8af9-3a17-429a-9fdc-e8f90ea16d46 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:16:03.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-231" for this suite.
Mar 13 01:16:09.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:16:09.974: INFO: namespace dns-231 deletion completed in 6.179168197s

• [SLOW TEST:10.399 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:16:09.975: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:16:10.898: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 01:16:12.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719658970, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719658970, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719658970, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719658970, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:16:15.936: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:16:16.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8538" for this suite.
Mar 13 01:16:28.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:16:28.242: INFO: namespace webhook-8538 deletion completed in 12.184221627s
STEP: Destroying namespace "webhook-8538-markers" for this suite.
Mar 13 01:16:34.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:16:34.412: INFO: namespace webhook-8538-markers deletion completed in 6.169709419s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.461 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:16:34.436: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-545e16ed-5949-4ff1-84f0-64923fa661c3
STEP: Creating a pod to test consume configMaps
Mar 13 01:16:34.518: INFO: Waiting up to 5m0s for pod "pod-configmaps-f75a32ec-d829-40d7-8347-f64fa4acad36" in namespace "configmap-7681" to be "success or failure"
Mar 13 01:16:34.523: INFO: Pod "pod-configmaps-f75a32ec-d829-40d7-8347-f64fa4acad36": Phase="Pending", Reason="", readiness=false. Elapsed: 5.431254ms
Mar 13 01:16:36.529: INFO: Pod "pod-configmaps-f75a32ec-d829-40d7-8347-f64fa4acad36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011594004s
Mar 13 01:16:38.537: INFO: Pod "pod-configmaps-f75a32ec-d829-40d7-8347-f64fa4acad36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018844027s
STEP: Saw pod success
Mar 13 01:16:38.537: INFO: Pod "pod-configmaps-f75a32ec-d829-40d7-8347-f64fa4acad36" satisfied condition "success or failure"
Mar 13 01:16:38.542: INFO: Trying to get logs from node silbory-nirmata0 pod pod-configmaps-f75a32ec-d829-40d7-8347-f64fa4acad36 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 01:16:38.582: INFO: Waiting for pod pod-configmaps-f75a32ec-d829-40d7-8347-f64fa4acad36 to disappear
Mar 13 01:16:38.587: INFO: Pod pod-configmaps-f75a32ec-d829-40d7-8347-f64fa4acad36 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:16:38.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7681" for this suite.
Mar 13 01:16:44.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:16:44.782: INFO: namespace configmap-7681 deletion completed in 6.186738379s

• [SLOW TEST:10.346 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:16:44.782: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 13 01:16:47.894: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:16:47.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9759" for this suite.
Mar 13 01:16:53.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:16:54.131: INFO: namespace container-runtime-9759 deletion completed in 6.187621854s

• [SLOW TEST:9.349 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:16:54.132: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 13 01:16:57.240: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:16:57.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4230" for this suite.
Mar 13 01:17:03.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:17:03.466: INFO: namespace container-runtime-4230 deletion completed in 6.186057798s

• [SLOW TEST:9.334 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:17:03.469: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 13 01:17:03.550: INFO: Waiting up to 5m0s for pod "pod-af2bd51a-a110-4350-8e6d-20ce5ca92b65" in namespace "emptydir-5720" to be "success or failure"
Mar 13 01:17:03.554: INFO: Pod "pod-af2bd51a-a110-4350-8e6d-20ce5ca92b65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.672168ms
Mar 13 01:17:05.561: INFO: Pod "pod-af2bd51a-a110-4350-8e6d-20ce5ca92b65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011211587s
Mar 13 01:17:07.568: INFO: Pod "pod-af2bd51a-a110-4350-8e6d-20ce5ca92b65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017873999s
STEP: Saw pod success
Mar 13 01:17:07.568: INFO: Pod "pod-af2bd51a-a110-4350-8e6d-20ce5ca92b65" satisfied condition "success or failure"
Mar 13 01:17:07.574: INFO: Trying to get logs from node silbory-nirmata0 pod pod-af2bd51a-a110-4350-8e6d-20ce5ca92b65 container test-container: <nil>
STEP: delete the pod
Mar 13 01:17:07.610: INFO: Waiting for pod pod-af2bd51a-a110-4350-8e6d-20ce5ca92b65 to disappear
Mar 13 01:17:07.624: INFO: Pod pod-af2bd51a-a110-4350-8e6d-20ce5ca92b65 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:17:07.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5720" for this suite.
Mar 13 01:17:13.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:17:13.808: INFO: namespace emptydir-5720 deletion completed in 6.174836357s

• [SLOW TEST:10.340 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:17:13.809: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 13 01:17:13.887: INFO: Waiting up to 5m0s for pod "pod-41001baa-b4de-4adc-9b9d-cc05b1babf3a" in namespace "emptydir-6523" to be "success or failure"
Mar 13 01:17:13.892: INFO: Pod "pod-41001baa-b4de-4adc-9b9d-cc05b1babf3a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.182211ms
Mar 13 01:17:15.899: INFO: Pod "pod-41001baa-b4de-4adc-9b9d-cc05b1babf3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011618877s
Mar 13 01:17:17.905: INFO: Pod "pod-41001baa-b4de-4adc-9b9d-cc05b1babf3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01831981s
STEP: Saw pod success
Mar 13 01:17:17.906: INFO: Pod "pod-41001baa-b4de-4adc-9b9d-cc05b1babf3a" satisfied condition "success or failure"
Mar 13 01:17:17.911: INFO: Trying to get logs from node silbory-nirmata0 pod pod-41001baa-b4de-4adc-9b9d-cc05b1babf3a container test-container: <nil>
STEP: delete the pod
Mar 13 01:17:17.952: INFO: Waiting for pod pod-41001baa-b4de-4adc-9b9d-cc05b1babf3a to disappear
Mar 13 01:17:17.958: INFO: Pod pod-41001baa-b4de-4adc-9b9d-cc05b1babf3a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:17:17.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6523" for this suite.
Mar 13 01:17:23.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:17:24.151: INFO: namespace emptydir-6523 deletion completed in 6.181316481s

• [SLOW TEST:10.342 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:17:24.159: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Mar 13 01:17:24.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-1150'
Mar 13 01:17:24.787: INFO: stderr: ""
Mar 13 01:17:24.787: INFO: stdout: "pod/pause created\n"
Mar 13 01:17:24.787: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 13 01:17:24.787: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1150" to be "running and ready"
Mar 13 01:17:24.803: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 15.710359ms
Mar 13 01:17:26.811: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023989948s
Mar 13 01:17:28.818: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.030808706s
Mar 13 01:17:28.818: INFO: Pod "pause" satisfied condition "running and ready"
Mar 13 01:17:28.818: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 13 01:17:28.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 label pods pause testing-label=testing-label-value --namespace=kubectl-1150'
Mar 13 01:17:29.065: INFO: stderr: ""
Mar 13 01:17:29.065: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 13 01:17:29.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pod pause -L testing-label --namespace=kubectl-1150'
Mar 13 01:17:29.284: INFO: stderr: ""
Mar 13 01:17:29.284: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 13 01:17:29.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 label pods pause testing-label- --namespace=kubectl-1150'
Mar 13 01:17:29.514: INFO: stderr: ""
Mar 13 01:17:29.514: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 13 01:17:29.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pod pause -L testing-label --namespace=kubectl-1150'
Mar 13 01:17:29.737: INFO: stderr: ""
Mar 13 01:17:29.737: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Mar 13 01:17:29.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete --grace-period=0 --force -f - --namespace=kubectl-1150'
Mar 13 01:17:29.975: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 01:17:29.975: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 13 01:17:29.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get rc,svc -l name=pause --no-headers --namespace=kubectl-1150'
Mar 13 01:17:30.223: INFO: stderr: "No resources found in kubectl-1150 namespace.\n"
Mar 13 01:17:30.223: INFO: stdout: ""
Mar 13 01:17:30.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -l name=pause --namespace=kubectl-1150 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 13 01:17:30.453: INFO: stderr: ""
Mar 13 01:17:30.453: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:17:30.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1150" for this suite.
Mar 13 01:17:36.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:17:36.644: INFO: namespace kubectl-1150 deletion completed in 6.178588195s

• [SLOW TEST:12.486 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:17:36.649: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 01:17:36.724: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bccd05ce-efce-4645-a973-90d4155c9881" in namespace "projected-2173" to be "success or failure"
Mar 13 01:17:36.730: INFO: Pod "downwardapi-volume-bccd05ce-efce-4645-a973-90d4155c9881": Phase="Pending", Reason="", readiness=false. Elapsed: 5.950703ms
Mar 13 01:17:38.737: INFO: Pod "downwardapi-volume-bccd05ce-efce-4645-a973-90d4155c9881": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012755164s
Mar 13 01:17:40.744: INFO: Pod "downwardapi-volume-bccd05ce-efce-4645-a973-90d4155c9881": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019466353s
STEP: Saw pod success
Mar 13 01:17:40.744: INFO: Pod "downwardapi-volume-bccd05ce-efce-4645-a973-90d4155c9881" satisfied condition "success or failure"
Mar 13 01:17:40.750: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-bccd05ce-efce-4645-a973-90d4155c9881 container client-container: <nil>
STEP: delete the pod
Mar 13 01:17:40.791: INFO: Waiting for pod downwardapi-volume-bccd05ce-efce-4645-a973-90d4155c9881 to disappear
Mar 13 01:17:40.796: INFO: Pod downwardapi-volume-bccd05ce-efce-4645-a973-90d4155c9881 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:17:40.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2173" for this suite.
Mar 13 01:17:46.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:17:46.991: INFO: namespace projected-2173 deletion completed in 6.187537102s

• [SLOW TEST:10.343 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:17:46.992: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1887
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1887
STEP: creating replication controller externalsvc in namespace services-1887
I0313 01:17:47.094074      25 runners.go:184] Created replication controller with name: externalsvc, namespace: services-1887, replica count: 2
I0313 01:17:50.145103      25 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 13 01:17:50.171: INFO: Creating new exec pod
Mar 13 01:17:54.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=services-1887 execpodh8696 -- /bin/sh -x -c nslookup clusterip-service'
Mar 13 01:17:54.793: INFO: stderr: "+ nslookup clusterip-service\n"
Mar 13 01:17:54.793: INFO: stdout: "Server:\t\t10.10.0.10\nAddress:\t10.10.0.10#53\n\nclusterip-service.services-1887.svc.cluster.local\tcanonical name = externalsvc.services-1887.svc.cluster.local.\nName:\texternalsvc.services-1887.svc.cluster.local\nAddress: 10.10.120.50\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1887, will wait for the garbage collector to delete the pods
Mar 13 01:17:54.865: INFO: Deleting ReplicationController externalsvc took: 16.081972ms
Mar 13 01:17:55.166: INFO: Terminating ReplicationController externalsvc pods took: 300.532823ms
Mar 13 01:18:06.198: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:18:06.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1887" for this suite.
Mar 13 01:18:12.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:18:12.411: INFO: namespace services-1887 deletion completed in 6.171349427s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:25.419 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:18:12.412: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:18:12.515: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 13 01:18:12.542: INFO: Number of nodes with available pods: 0
Mar 13 01:18:12.542: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:18:13.560: INFO: Number of nodes with available pods: 0
Mar 13 01:18:13.560: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:18:14.555: INFO: Number of nodes with available pods: 0
Mar 13 01:18:14.555: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:18:15.557: INFO: Number of nodes with available pods: 2
Mar 13 01:18:15.557: INFO: Node silbory-nirmata0 is running more than one daemon pod
Mar 13 01:18:16.557: INFO: Number of nodes with available pods: 3
Mar 13 01:18:16.557: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 13 01:18:16.611: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:16.612: INFO: Wrong image for pod: daemon-set-7xmlb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:16.612: INFO: Wrong image for pod: daemon-set-mcwcz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:17.625: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:17.625: INFO: Wrong image for pod: daemon-set-7xmlb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:17.625: INFO: Wrong image for pod: daemon-set-mcwcz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:18.628: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:18.628: INFO: Wrong image for pod: daemon-set-7xmlb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:18.628: INFO: Wrong image for pod: daemon-set-mcwcz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:19.625: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:19.625: INFO: Wrong image for pod: daemon-set-7xmlb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:19.625: INFO: Pod daemon-set-7xmlb is not available
Mar 13 01:18:19.625: INFO: Wrong image for pod: daemon-set-mcwcz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:20.626: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:20.626: INFO: Wrong image for pod: daemon-set-7xmlb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:20.626: INFO: Pod daemon-set-7xmlb is not available
Mar 13 01:18:20.626: INFO: Wrong image for pod: daemon-set-mcwcz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:21.628: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:21.628: INFO: Wrong image for pod: daemon-set-7xmlb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:21.628: INFO: Pod daemon-set-7xmlb is not available
Mar 13 01:18:21.628: INFO: Wrong image for pod: daemon-set-mcwcz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:22.626: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:22.626: INFO: Wrong image for pod: daemon-set-7xmlb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:22.626: INFO: Pod daemon-set-7xmlb is not available
Mar 13 01:18:22.626: INFO: Wrong image for pod: daemon-set-mcwcz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:23.627: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:23.627: INFO: Wrong image for pod: daemon-set-7xmlb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:23.627: INFO: Pod daemon-set-7xmlb is not available
Mar 13 01:18:23.627: INFO: Wrong image for pod: daemon-set-mcwcz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:24.626: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:24.626: INFO: Pod daemon-set-cdldn is not available
Mar 13 01:18:24.626: INFO: Wrong image for pod: daemon-set-mcwcz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:25.626: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:25.626: INFO: Pod daemon-set-cdldn is not available
Mar 13 01:18:25.626: INFO: Wrong image for pod: daemon-set-mcwcz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:26.624: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:26.624: INFO: Wrong image for pod: daemon-set-mcwcz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:27.626: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:27.626: INFO: Wrong image for pod: daemon-set-mcwcz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:27.626: INFO: Pod daemon-set-mcwcz is not available
Mar 13 01:18:28.626: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:28.626: INFO: Pod daemon-set-cw5n7 is not available
Mar 13 01:18:29.627: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:29.627: INFO: Pod daemon-set-cw5n7 is not available
Mar 13 01:18:30.625: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:31.626: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:32.625: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:32.625: INFO: Pod daemon-set-6g2lc is not available
Mar 13 01:18:33.625: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:33.625: INFO: Pod daemon-set-6g2lc is not available
Mar 13 01:18:34.625: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:34.625: INFO: Pod daemon-set-6g2lc is not available
Mar 13 01:18:35.626: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:35.626: INFO: Pod daemon-set-6g2lc is not available
Mar 13 01:18:36.625: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:36.625: INFO: Pod daemon-set-6g2lc is not available
Mar 13 01:18:37.625: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:37.625: INFO: Pod daemon-set-6g2lc is not available
Mar 13 01:18:38.630: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:38.630: INFO: Pod daemon-set-6g2lc is not available
Mar 13 01:18:39.626: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:39.626: INFO: Pod daemon-set-6g2lc is not available
Mar 13 01:18:40.627: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:40.627: INFO: Pod daemon-set-6g2lc is not available
Mar 13 01:18:41.627: INFO: Wrong image for pod: daemon-set-6g2lc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 13 01:18:41.627: INFO: Pod daemon-set-6g2lc is not available
Mar 13 01:18:42.626: INFO: Pod daemon-set-l7n5t is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 13 01:18:42.652: INFO: Number of nodes with available pods: 2
Mar 13 01:18:42.652: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:18:43.671: INFO: Number of nodes with available pods: 2
Mar 13 01:18:43.671: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:18:44.671: INFO: Number of nodes with available pods: 2
Mar 13 01:18:44.671: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:18:45.670: INFO: Number of nodes with available pods: 2
Mar 13 01:18:45.670: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:18:46.666: INFO: Number of nodes with available pods: 2
Mar 13 01:18:46.666: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:18:47.670: INFO: Number of nodes with available pods: 3
Mar 13 01:18:47.671: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7796, will wait for the garbage collector to delete the pods
Mar 13 01:18:47.768: INFO: Deleting DaemonSet.extensions daemon-set took: 15.253667ms
Mar 13 01:18:48.169: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.651621ms
Mar 13 01:19:01.875: INFO: Number of nodes with available pods: 0
Mar 13 01:19:01.875: INFO: Number of running nodes: 0, number of available pods: 0
Mar 13 01:19:01.880: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7796/daemonsets","resourceVersion":"10744"},"items":null}

Mar 13 01:19:01.886: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7796/pods","resourceVersion":"10744"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:19:01.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7796" for this suite.
Mar 13 01:19:07.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:19:08.110: INFO: namespace daemonsets-7796 deletion completed in 6.191262619s

• [SLOW TEST:55.698 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:19:08.110: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-8634c5d5-a8bc-46e9-ad73-c8ed87132352
STEP: Creating secret with name s-test-opt-upd-161332b6-8b0e-4b37-8f3c-dd8550362953
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8634c5d5-a8bc-46e9-ad73-c8ed87132352
STEP: Updating secret s-test-opt-upd-161332b6-8b0e-4b37-8f3c-dd8550362953
STEP: Creating secret with name s-test-opt-create-49e98a8d-9d5d-4f7d-872a-fa0dd2de2619
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:20:35.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7405" for this suite.
Mar 13 01:20:47.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:20:47.400: INFO: namespace secrets-7405 deletion completed in 12.176089094s

• [SLOW TEST:99.290 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:20:47.401: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 13 01:20:47.486: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7637 /api/v1/namespaces/watch-7637/configmaps/e2e-watch-test-watch-closed 82aeb78e-806a-4fda-b535-2fe793e11073 10899 0 2020-03-13 01:20:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 13 01:20:47.486: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7637 /api/v1/namespaces/watch-7637/configmaps/e2e-watch-test-watch-closed 82aeb78e-806a-4fda-b535-2fe793e11073 10900 0 2020-03-13 01:20:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 13 01:20:47.513: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7637 /api/v1/namespaces/watch-7637/configmaps/e2e-watch-test-watch-closed 82aeb78e-806a-4fda-b535-2fe793e11073 10901 0 2020-03-13 01:20:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 13 01:20:47.514: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7637 /api/v1/namespaces/watch-7637/configmaps/e2e-watch-test-watch-closed 82aeb78e-806a-4fda-b535-2fe793e11073 10902 0 2020-03-13 01:20:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:20:47.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7637" for this suite.
Mar 13 01:20:53.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:20:53.697: INFO: namespace watch-7637 deletion completed in 6.175770227s

• [SLOW TEST:6.296 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:20:53.698: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 13 01:20:53.772: INFO: Waiting up to 5m0s for pod "downward-api-d2c5c0e5-b89c-4377-8c2b-c59c4d3648d3" in namespace "downward-api-502" to be "success or failure"
Mar 13 01:20:53.780: INFO: Pod "downward-api-d2c5c0e5-b89c-4377-8c2b-c59c4d3648d3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.515617ms
Mar 13 01:20:55.786: INFO: Pod "downward-api-d2c5c0e5-b89c-4377-8c2b-c59c4d3648d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013985535s
Mar 13 01:20:57.794: INFO: Pod "downward-api-d2c5c0e5-b89c-4377-8c2b-c59c4d3648d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021663629s
STEP: Saw pod success
Mar 13 01:20:57.794: INFO: Pod "downward-api-d2c5c0e5-b89c-4377-8c2b-c59c4d3648d3" satisfied condition "success or failure"
Mar 13 01:20:57.799: INFO: Trying to get logs from node silbory-nirmata0 pod downward-api-d2c5c0e5-b89c-4377-8c2b-c59c4d3648d3 container dapi-container: <nil>
STEP: delete the pod
Mar 13 01:20:57.835: INFO: Waiting for pod downward-api-d2c5c0e5-b89c-4377-8c2b-c59c4d3648d3 to disappear
Mar 13 01:20:57.847: INFO: Pod downward-api-d2c5c0e5-b89c-4377-8c2b-c59c4d3648d3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:20:57.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-502" for this suite.
Mar 13 01:21:03.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:21:04.036: INFO: namespace downward-api-502 deletion completed in 6.181013556s

• [SLOW TEST:10.338 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:21:04.036: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-bca0b36d-8d9c-4933-9505-e03fa070b9b0
STEP: Creating a pod to test consume secrets
Mar 13 01:21:04.148: INFO: Waiting up to 5m0s for pod "pod-secrets-838a3839-99b3-43bb-96eb-aa5dfdde91bc" in namespace "secrets-3445" to be "success or failure"
Mar 13 01:21:04.154: INFO: Pod "pod-secrets-838a3839-99b3-43bb-96eb-aa5dfdde91bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.500864ms
Mar 13 01:21:06.160: INFO: Pod "pod-secrets-838a3839-99b3-43bb-96eb-aa5dfdde91bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012693329s
Mar 13 01:21:08.168: INFO: Pod "pod-secrets-838a3839-99b3-43bb-96eb-aa5dfdde91bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019811923s
STEP: Saw pod success
Mar 13 01:21:08.168: INFO: Pod "pod-secrets-838a3839-99b3-43bb-96eb-aa5dfdde91bc" satisfied condition "success or failure"
Mar 13 01:21:08.174: INFO: Trying to get logs from node silbory-nirmata0 pod pod-secrets-838a3839-99b3-43bb-96eb-aa5dfdde91bc container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 01:21:08.209: INFO: Waiting for pod pod-secrets-838a3839-99b3-43bb-96eb-aa5dfdde91bc to disappear
Mar 13 01:21:08.215: INFO: Pod pod-secrets-838a3839-99b3-43bb-96eb-aa5dfdde91bc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:21:08.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3445" for this suite.
Mar 13 01:21:14.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:21:14.401: INFO: namespace secrets-3445 deletion completed in 6.177815634s

• [SLOW TEST:10.365 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:21:14.403: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-2747b47d-6345-4b79-ba54-ab0d43dad214
STEP: Creating a pod to test consume secrets
Mar 13 01:21:14.481: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fa23b203-5120-42bb-864a-6eae87f3d03c" in namespace "projected-5872" to be "success or failure"
Mar 13 01:21:14.486: INFO: Pod "pod-projected-secrets-fa23b203-5120-42bb-864a-6eae87f3d03c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.455226ms
Mar 13 01:21:16.494: INFO: Pod "pod-projected-secrets-fa23b203-5120-42bb-864a-6eae87f3d03c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013327954s
Mar 13 01:21:18.502: INFO: Pod "pod-projected-secrets-fa23b203-5120-42bb-864a-6eae87f3d03c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021115083s
STEP: Saw pod success
Mar 13 01:21:18.502: INFO: Pod "pod-projected-secrets-fa23b203-5120-42bb-864a-6eae87f3d03c" satisfied condition "success or failure"
Mar 13 01:21:18.507: INFO: Trying to get logs from node silbory-nirmata0 pod pod-projected-secrets-fa23b203-5120-42bb-864a-6eae87f3d03c container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 01:21:18.543: INFO: Waiting for pod pod-projected-secrets-fa23b203-5120-42bb-864a-6eae87f3d03c to disappear
Mar 13 01:21:18.549: INFO: Pod pod-projected-secrets-fa23b203-5120-42bb-864a-6eae87f3d03c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:21:18.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5872" for this suite.
Mar 13 01:21:24.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:21:24.744: INFO: namespace projected-5872 deletion completed in 6.186913486s

• [SLOW TEST:10.341 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:21:24.745: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 13 01:21:24.819: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2370 /api/v1/namespaces/watch-2370/configmaps/e2e-watch-test-configmap-a 95f01f2c-a421-4d15-806b-046845713a39 11013 0 2020-03-13 01:21:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 13 01:21:24.819: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2370 /api/v1/namespaces/watch-2370/configmaps/e2e-watch-test-configmap-a 95f01f2c-a421-4d15-806b-046845713a39 11013 0 2020-03-13 01:21:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 13 01:21:34.833: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2370 /api/v1/namespaces/watch-2370/configmaps/e2e-watch-test-configmap-a 95f01f2c-a421-4d15-806b-046845713a39 11020 0 2020-03-13 01:21:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 13 01:21:34.834: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2370 /api/v1/namespaces/watch-2370/configmaps/e2e-watch-test-configmap-a 95f01f2c-a421-4d15-806b-046845713a39 11020 0 2020-03-13 01:21:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 13 01:21:44.849: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2370 /api/v1/namespaces/watch-2370/configmaps/e2e-watch-test-configmap-a 95f01f2c-a421-4d15-806b-046845713a39 11027 0 2020-03-13 01:21:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 13 01:21:44.849: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2370 /api/v1/namespaces/watch-2370/configmaps/e2e-watch-test-configmap-a 95f01f2c-a421-4d15-806b-046845713a39 11027 0 2020-03-13 01:21:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 13 01:21:54.861: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2370 /api/v1/namespaces/watch-2370/configmaps/e2e-watch-test-configmap-a 95f01f2c-a421-4d15-806b-046845713a39 11055 0 2020-03-13 01:21:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 13 01:21:54.861: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2370 /api/v1/namespaces/watch-2370/configmaps/e2e-watch-test-configmap-a 95f01f2c-a421-4d15-806b-046845713a39 11055 0 2020-03-13 01:21:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 13 01:22:04.875: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2370 /api/v1/namespaces/watch-2370/configmaps/e2e-watch-test-configmap-b 652fa022-272e-462a-ab51-8b9d891faa14 11062 0 2020-03-13 01:22:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 13 01:22:04.875: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2370 /api/v1/namespaces/watch-2370/configmaps/e2e-watch-test-configmap-b 652fa022-272e-462a-ab51-8b9d891faa14 11062 0 2020-03-13 01:22:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 13 01:22:14.889: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2370 /api/v1/namespaces/watch-2370/configmaps/e2e-watch-test-configmap-b 652fa022-272e-462a-ab51-8b9d891faa14 11072 0 2020-03-13 01:22:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 13 01:22:14.889: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2370 /api/v1/namespaces/watch-2370/configmaps/e2e-watch-test-configmap-b 652fa022-272e-462a-ab51-8b9d891faa14 11072 0 2020-03-13 01:22:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:22:24.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2370" for this suite.
Mar 13 01:22:30.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:22:31.064: INFO: namespace watch-2370 deletion completed in 6.166267677s

• [SLOW TEST:66.319 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:22:31.065: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:22:31.129: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 13 01:22:37.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-9277 create -f -'
Mar 13 01:22:38.079: INFO: stderr: ""
Mar 13 01:22:38.079: INFO: stdout: "e2e-test-crd-publish-openapi-7675-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 13 01:22:38.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-9277 delete e2e-test-crd-publish-openapi-7675-crds test-cr'
Mar 13 01:22:38.300: INFO: stderr: ""
Mar 13 01:22:38.300: INFO: stdout: "e2e-test-crd-publish-openapi-7675-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 13 01:22:38.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-9277 apply -f -'
Mar 13 01:22:38.713: INFO: stderr: ""
Mar 13 01:22:38.713: INFO: stdout: "e2e-test-crd-publish-openapi-7675-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 13 01:22:38.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-9277 delete e2e-test-crd-publish-openapi-7675-crds test-cr'
Mar 13 01:22:38.936: INFO: stderr: ""
Mar 13 01:22:38.936: INFO: stdout: "e2e-test-crd-publish-openapi-7675-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 13 01:22:38.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 explain e2e-test-crd-publish-openapi-7675-crds'
Mar 13 01:22:39.357: INFO: stderr: ""
Mar 13 01:22:39.357: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7675-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:22:45.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9277" for this suite.
Mar 13 01:22:51.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:22:51.832: INFO: namespace crd-publish-openapi-9277 deletion completed in 6.214069023s

• [SLOW TEST:20.767 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:22:51.833: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 01:22:51.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98b308da-a777-4bbf-a0ae-00757420a94c" in namespace "downward-api-7079" to be "success or failure"
Mar 13 01:22:51.939: INFO: Pod "downwardapi-volume-98b308da-a777-4bbf-a0ae-00757420a94c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.688548ms
Mar 13 01:22:53.949: INFO: Pod "downwardapi-volume-98b308da-a777-4bbf-a0ae-00757420a94c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015474101s
Mar 13 01:22:55.957: INFO: Pod "downwardapi-volume-98b308da-a777-4bbf-a0ae-00757420a94c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022977482s
STEP: Saw pod success
Mar 13 01:22:55.957: INFO: Pod "downwardapi-volume-98b308da-a777-4bbf-a0ae-00757420a94c" satisfied condition "success or failure"
Mar 13 01:22:55.962: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-98b308da-a777-4bbf-a0ae-00757420a94c container client-container: <nil>
STEP: delete the pod
Mar 13 01:22:56.018: INFO: Waiting for pod downwardapi-volume-98b308da-a777-4bbf-a0ae-00757420a94c to disappear
Mar 13 01:22:56.025: INFO: Pod downwardapi-volume-98b308da-a777-4bbf-a0ae-00757420a94c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:22:56.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7079" for this suite.
Mar 13 01:23:02.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:23:02.269: INFO: namespace downward-api-7079 deletion completed in 6.222923839s

• [SLOW TEST:10.436 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:23:02.270: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:23:03.315: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 13 01:23:05.335: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719659383, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719659383, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719659383, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719659383, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:23:08.360: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:23:18.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1761" for this suite.
Mar 13 01:23:24.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:23:24.792: INFO: namespace webhook-1761 deletion completed in 6.195833113s
STEP: Destroying namespace "webhook-1761-markers" for this suite.
Mar 13 01:23:30.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:23:30.977: INFO: namespace webhook-1761-markers deletion completed in 6.184385474s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:28.732 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:23:31.003: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-9795
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Mar 13 01:23:31.101: INFO: Found 0 stateful pods, waiting for 3
Mar 13 01:23:41.109: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 01:23:41.109: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 01:23:41.109: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 13 01:23:41.151: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 13 01:23:51.209: INFO: Updating stateful set ss2
Mar 13 01:23:51.224: INFO: Waiting for Pod statefulset-9795/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 13 01:24:01.238: INFO: Waiting for Pod statefulset-9795/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar 13 01:24:11.324: INFO: Found 2 stateful pods, waiting for 3
Mar 13 01:24:21.332: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 01:24:21.332: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 01:24:21.332: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 13 01:24:21.370: INFO: Updating stateful set ss2
Mar 13 01:24:21.383: INFO: Waiting for Pod statefulset-9795/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 13 01:24:31.399: INFO: Waiting for Pod statefulset-9795/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 13 01:24:41.424: INFO: Updating stateful set ss2
Mar 13 01:24:41.443: INFO: Waiting for StatefulSet statefulset-9795/ss2 to complete update
Mar 13 01:24:41.443: INFO: Waiting for Pod statefulset-9795/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 13 01:24:51.457: INFO: Waiting for StatefulSet statefulset-9795/ss2 to complete update
Mar 13 01:24:51.457: INFO: Waiting for Pod statefulset-9795/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 13 01:25:01.456: INFO: Waiting for StatefulSet statefulset-9795/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 13 01:25:11.452: INFO: Deleting all statefulset in ns statefulset-9795
Mar 13 01:25:11.457: INFO: Scaling statefulset ss2 to 0
Mar 13 01:25:41.495: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 01:25:41.503: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:25:41.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9795" for this suite.
Mar 13 01:25:47.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:25:47.728: INFO: namespace statefulset-9795 deletion completed in 6.187143362s

• [SLOW TEST:136.725 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:25:47.729: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:25:51.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8316" for this suite.
Mar 13 01:25:57.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:25:58.137: INFO: namespace emptydir-wrapper-8316 deletion completed in 6.211480577s

• [SLOW TEST:10.409 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:25:58.138: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 13 01:25:58.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5658'
Mar 13 01:25:58.477: INFO: stderr: ""
Mar 13 01:25:58.477: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Mar 13 01:25:58.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete pods e2e-test-httpd-pod --namespace=kubectl-5658'
Mar 13 01:26:06.088: INFO: stderr: ""
Mar 13 01:26:06.088: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:26:06.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5658" for this suite.
Mar 13 01:26:12.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:26:12.291: INFO: namespace kubectl-5658 deletion completed in 6.190193812s

• [SLOW TEST:14.153 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:26:12.291: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:26:12.355: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:26:12.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3879" for this suite.
Mar 13 01:26:18.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:26:19.117: INFO: namespace custom-resource-definition-3879 deletion completed in 6.182534495s

• [SLOW TEST:6.826 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:26:19.134: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:26:20.557: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 01:26:22.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719659580, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719659580, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719659580, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719659580, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:26:25.597: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:26:25.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7047" for this suite.
Mar 13 01:26:31.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:26:31.919: INFO: namespace webhook-7047 deletion completed in 6.196063982s
STEP: Destroying namespace "webhook-7047-markers" for this suite.
Mar 13 01:26:37.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:26:38.116: INFO: namespace webhook-7047-markers deletion completed in 6.196598311s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.006 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:26:38.141: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-5049
STEP: creating replication controller nodeport-test in namespace services-5049
I0313 01:26:38.248692      25 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-5049, replica count: 2
Mar 13 01:26:41.300: INFO: Creating new exec pod
I0313 01:26:41.299878      25 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 13 01:26:46.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=services-5049 execpodfxpzr -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar 13 01:26:46.945: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 13 01:26:46.945: INFO: stdout: ""
Mar 13 01:26:46.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=services-5049 execpodfxpzr -- /bin/sh -x -c nc -zv -t -w 2 10.10.68.234 80'
Mar 13 01:26:47.577: INFO: stderr: "+ nc -zv -t -w 2 10.10.68.234 80\nConnection to 10.10.68.234 80 port [tcp/http] succeeded!\n"
Mar 13 01:26:47.577: INFO: stdout: ""
Mar 13 01:26:47.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=services-5049 execpodfxpzr -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.75 32190'
Mar 13 01:26:48.198: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.75 32190\nConnection to 10.10.1.75 32190 port [tcp/32190] succeeded!\n"
Mar 13 01:26:48.199: INFO: stdout: ""
Mar 13 01:26:48.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=services-5049 execpodfxpzr -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.238 32190'
Mar 13 01:26:48.829: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.238 32190\nConnection to 10.10.1.238 32190 port [tcp/32190] succeeded!\n"
Mar 13 01:26:48.829: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:26:48.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5049" for this suite.
Mar 13 01:26:54.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:26:55.050: INFO: namespace services-5049 deletion completed in 6.210687265s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:16.909 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:26:55.050: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:27:15.161: INFO: Container started at 2020-03-13 01:26:56 +0000 UTC, pod became ready at 2020-03-13 01:27:13 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:27:15.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3114" for this suite.
Mar 13 01:27:35.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:27:35.349: INFO: namespace container-probe-3114 deletion completed in 20.181027851s

• [SLOW TEST:40.299 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:27:35.351: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar 13 01:27:35.475: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:27:40.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6633" for this suite.
Mar 13 01:27:46.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:27:46.237: INFO: namespace init-container-6633 deletion completed in 6.203304249s

• [SLOW TEST:10.886 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:27:46.238: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Mar 13 01:27:50.353: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-940252648 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar 13 01:28:00.580: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:28:00.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3652" for this suite.
Mar 13 01:28:06.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:28:06.774: INFO: namespace pods-3652 deletion completed in 6.179347696s

• [SLOW TEST:20.536 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:28:06.775: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:28:32.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3363" for this suite.
Mar 13 01:28:38.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:28:38.437: INFO: namespace container-runtime-3363 deletion completed in 6.177972144s

• [SLOW TEST:31.662 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:28:38.438: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7621
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-7621
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7621
Mar 13 01:28:38.528: INFO: Found 0 stateful pods, waiting for 1
Mar 13 01:28:48.536: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 13 01:28:48.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 13 01:28:49.156: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 13 01:28:49.156: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 13 01:28:49.156: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 13 01:28:49.166: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 13 01:28:59.174: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 01:28:59.174: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 01:28:59.198: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Mar 13 01:28:59.198: INFO: ss-0  silbory-nirmata0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:38 +0000 UTC  }]
Mar 13 01:28:59.198: INFO: 
Mar 13 01:28:59.198: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 13 01:29:00.206: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993244338s
Mar 13 01:29:01.215: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985558822s
Mar 13 01:29:02.226: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976024313s
Mar 13 01:29:03.254: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965135604s
Mar 13 01:29:04.262: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.937902778s
Mar 13 01:29:05.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.929163302s
Mar 13 01:29:06.278: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.921954521s
Mar 13 01:29:07.285: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.913983545s
Mar 13 01:29:08.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 906.663417ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7621
Mar 13 01:29:09.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:29:09.954: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 13 01:29:09.954: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 13 01:29:09.954: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 13 01:29:09.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:29:10.396: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 13 01:29:10.396: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 13 01:29:10.396: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 13 01:29:10.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:29:10.877: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 13 01:29:10.877: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 13 01:29:10.877: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 13 01:29:10.884: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 01:29:10.884: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 01:29:10.884: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 13 01:29:10.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 13 01:29:11.501: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 13 01:29:11.501: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 13 01:29:11.501: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 13 01:29:11.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 13 01:29:11.918: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 13 01:29:11.918: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 13 01:29:11.918: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 13 01:29:11.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 13 01:29:12.385: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 13 01:29:12.385: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 13 01:29:12.385: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 13 01:29:12.385: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 01:29:12.392: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar 13 01:29:22.406: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 01:29:22.406: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 01:29:22.406: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 01:29:22.430: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Mar 13 01:29:22.430: INFO: ss-0  silbory-nirmata0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:38 +0000 UTC  }]
Mar 13 01:29:22.430: INFO: ss-1  sam-node3         Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  }]
Mar 13 01:29:22.430: INFO: ss-2  sam-node2         Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  }]
Mar 13 01:29:22.430: INFO: 
Mar 13 01:29:22.431: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 13 01:29:23.435: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Mar 13 01:29:23.435: INFO: ss-0  silbory-nirmata0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:38 +0000 UTC  }]
Mar 13 01:29:23.436: INFO: ss-1  sam-node3         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  }]
Mar 13 01:29:23.436: INFO: ss-2  sam-node2         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  }]
Mar 13 01:29:23.436: INFO: 
Mar 13 01:29:23.436: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 13 01:29:24.444: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Mar 13 01:29:24.444: INFO: ss-0  silbory-nirmata0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:38 +0000 UTC  }]
Mar 13 01:29:24.444: INFO: ss-1  sam-node3         Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  }]
Mar 13 01:29:24.444: INFO: ss-2  sam-node2         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  }]
Mar 13 01:29:24.445: INFO: 
Mar 13 01:29:24.445: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 13 01:29:25.451: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Mar 13 01:29:25.452: INFO: ss-0  silbory-nirmata0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:38 +0000 UTC  }]
Mar 13 01:29:25.452: INFO: ss-1  sam-node3         Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  }]
Mar 13 01:29:25.452: INFO: 
Mar 13 01:29:25.452: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 13 01:29:26.460: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Mar 13 01:29:26.460: INFO: ss-1  sam-node3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  }]
Mar 13 01:29:26.460: INFO: 
Mar 13 01:29:26.460: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 13 01:29:27.467: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Mar 13 01:29:27.467: INFO: ss-1  sam-node3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  }]
Mar 13 01:29:27.467: INFO: 
Mar 13 01:29:27.467: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 13 01:29:28.476: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Mar 13 01:29:28.476: INFO: ss-1  sam-node3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  }]
Mar 13 01:29:28.476: INFO: 
Mar 13 01:29:28.476: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 13 01:29:29.487: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Mar 13 01:29:29.488: INFO: ss-1  sam-node3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  }]
Mar 13 01:29:29.488: INFO: 
Mar 13 01:29:29.488: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 13 01:29:30.495: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Mar 13 01:29:30.495: INFO: ss-1  sam-node3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  }]
Mar 13 01:29:30.495: INFO: 
Mar 13 01:29:30.495: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 13 01:29:31.504: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Mar 13 01:29:31.504: INFO: ss-1  sam-node3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:29:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-13 01:28:59 +0000 UTC  }]
Mar 13 01:29:31.504: INFO: 
Mar 13 01:29:31.504: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7621
Mar 13 01:29:32.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:29:32.702: INFO: rc: 1
Mar 13 01:29:32.702: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc00310bc50 exit status 1 <nil> <nil> true [0xc0033d01f8 0xc0033d0210 0xc0033d0228] [0xc0033d01f8 0xc0033d0210 0xc0033d0228] [0xc0033d0208 0xc0033d0220] [0x10ef310 0x10ef310] 0xc0020df6e0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar 13 01:29:42.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:29:42.931: INFO: rc: 1
Mar 13 01:29:42.931: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003fc83c0 exit status 1 <nil> <nil> true [0xc00196c4e0 0xc00196c538 0xc00196c588] [0xc00196c4e0 0xc00196c538 0xc00196c588] [0xc00196c528 0xc00196c558] [0x10ef310 0x10ef310] 0xc003b556e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:29:52.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:29:53.175: INFO: rc: 1
Mar 13 01:29:53.175: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003fc8780 exit status 1 <nil> <nil> true [0xc00196c5b0 0xc00196c628 0xc00196c670] [0xc00196c5b0 0xc00196c628 0xc00196c670] [0xc00196c5f8 0xc00196c658] [0x10ef310 0x10ef310] 0xc003b55da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:30:03.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:30:03.419: INFO: rc: 1
Mar 13 01:30:03.420: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0066fc120 exit status 1 <nil> <nil> true [0xc0033d0230 0xc0033d0248 0xc0033d0278] [0xc0033d0230 0xc0033d0248 0xc0033d0278] [0xc0033d0240 0xc0033d0270] [0x10ef310 0x10ef310] 0xc002e6f020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:30:13.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:30:13.650: INFO: rc: 1
Mar 13 01:30:13.650: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0066fc510 exit status 1 <nil> <nil> true [0xc0033d0280 0xc0033d0298 0xc0033d02d8] [0xc0033d0280 0xc0033d0298 0xc0033d02d8] [0xc0033d0290 0xc0033d02b8] [0x10ef310 0x10ef310] 0xc0023b0420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:30:23.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:30:23.864: INFO: rc: 1
Mar 13 01:30:23.864: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00379e630 exit status 1 <nil> <nil> true [0xc000011df8 0xc000011f38 0xc005d56008] [0xc000011df8 0xc000011f38 0xc005d56008] [0xc000011f10 0xc005d56000] [0x10ef310 0x10ef310] 0xc002a17500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:30:33.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:30:34.092: INFO: rc: 1
Mar 13 01:30:34.092: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003fc8b40 exit status 1 <nil> <nil> true [0xc00196c688 0xc00196c6b0 0xc00196c700] [0xc00196c688 0xc00196c6b0 0xc00196c700] [0xc00196c6a8 0xc00196c6f8] [0x10ef310 0x10ef310] 0xc00329c600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:30:44.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:30:44.320: INFO: rc: 1
Mar 13 01:30:44.320: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00379e9f0 exit status 1 <nil> <nil> true [0xc005d56010 0xc005d56028 0xc005d56040] [0xc005d56010 0xc005d56028 0xc005d56040] [0xc005d56020 0xc005d56038] [0x10ef310 0x10ef310] 0xc002a17920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:30:54.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:30:54.544: INFO: rc: 1
Mar 13 01:30:54.545: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003fc8f90 exit status 1 <nil> <nil> true [0xc00196c710 0xc00196c748 0xc00196c7a0] [0xc00196c710 0xc00196c748 0xc00196c7a0] [0xc00196c738 0xc00196c780] [0x10ef310 0x10ef310] 0xc00216a720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:31:04.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:31:04.762: INFO: rc: 1
Mar 13 01:31:04.763: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0039de450 exit status 1 <nil> <nil> true [0xc00017fe20 0xc000010050 0xc000010790] [0xc00017fe20 0xc000010050 0xc000010790] [0xc0000d3a38 0xc000010750] [0x10ef310 0x10ef310] 0xc00329de00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:31:14.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:31:15.002: INFO: rc: 1
Mar 13 01:31:15.002: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0039de7e0 exit status 1 <nil> <nil> true [0xc000010a40 0xc000010d10 0xc0000111b0] [0xc000010a40 0xc000010d10 0xc0000111b0] [0xc000010c58 0xc000011150] [0x10ef310 0x10ef310] 0xc002e6e900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:31:25.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:31:25.236: INFO: rc: 1
Mar 13 01:31:25.236: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00310a390 exit status 1 <nil> <nil> true [0xc005d56000 0xc005d56018 0xc005d56030] [0xc005d56000 0xc005d56018 0xc005d56030] [0xc005d56010 0xc005d56028] [0x10ef310 0x10ef310] 0xc003b54060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:31:35.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:31:35.458: INFO: rc: 1
Mar 13 01:31:35.458: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011e43c0 exit status 1 <nil> <nil> true [0xc0033d0010 0xc0033d0050 0xc0033d0068] [0xc0033d0010 0xc0033d0050 0xc0033d0068] [0xc0033d0048 0xc0033d0060] [0x10ef310 0x10ef310] 0xc002d802a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:31:45.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:31:45.683: INFO: rc: 1
Mar 13 01:31:45.684: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011e4780 exit status 1 <nil> <nil> true [0xc0033d0070 0xc0033d0088 0xc0033d00a8] [0xc0033d0070 0xc0033d0088 0xc0033d00a8] [0xc0033d0080 0xc0033d00a0] [0x10ef310 0x10ef310] 0xc002d80720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:31:55.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:31:55.854: INFO: rc: 1
Mar 13 01:31:55.854: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011e4b10 exit status 1 <nil> <nil> true [0xc0033d00b0 0xc0033d00c8 0xc0033d00e0] [0xc0033d00b0 0xc0033d00c8 0xc0033d00e0] [0xc0033d00c0 0xc0033d00d8] [0x10ef310 0x10ef310] 0xc002d80b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:32:05.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:32:06.083: INFO: rc: 1
Mar 13 01:32:06.083: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00310a750 exit status 1 <nil> <nil> true [0xc005d56038 0xc005d56050 0xc005d56068] [0xc005d56038 0xc005d56050 0xc005d56068] [0xc005d56048 0xc005d56060] [0x10ef310 0x10ef310] 0xc003b546c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:32:16.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:32:16.321: INFO: rc: 1
Mar 13 01:32:16.321: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011e5110 exit status 1 <nil> <nil> true [0xc0033d00e8 0xc0033d0100 0xc0033d0118] [0xc0033d00e8 0xc0033d0100 0xc0033d0118] [0xc0033d00f8 0xc0033d0110] [0x10ef310 0x10ef310] 0xc002d80ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:32:26.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:32:26.563: INFO: rc: 1
Mar 13 01:32:26.563: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00310ba10 exit status 1 <nil> <nil> true [0xc005d56070 0xc005d56088 0xc005d560a0] [0xc005d56070 0xc005d56088 0xc005d560a0] [0xc005d56080 0xc005d56098] [0x10ef310 0x10ef310] 0xc003b55020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:32:36.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:32:36.782: INFO: rc: 1
Mar 13 01:32:36.782: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00310be30 exit status 1 <nil> <nil> true [0xc005d560a8 0xc005d560c0 0xc005d560d8] [0xc005d560a8 0xc005d560c0 0xc005d560d8] [0xc005d560b8 0xc005d560d0] [0x10ef310 0x10ef310] 0xc003b556e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:32:46.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:32:47.172: INFO: rc: 1
Mar 13 01:32:47.172: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000bddef0 exit status 1 <nil> <nil> true [0xc005d560e0 0xc005d560f8 0xc005d56110] [0xc005d560e0 0xc005d560f8 0xc005d56110] [0xc005d560f0 0xc005d56108] [0x10ef310 0x10ef310] 0xc003b55da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:32:57.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:32:57.414: INFO: rc: 1
Mar 13 01:32:57.414: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000fafc80 exit status 1 <nil> <nil> true [0xc005d56118 0xc005d56130 0xc005d56148] [0xc005d56118 0xc005d56130 0xc005d56148] [0xc005d56128 0xc005d56140] [0x10ef310 0x10ef310] 0xc002a16420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:33:07.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:33:07.646: INFO: rc: 1
Mar 13 01:33:07.646: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00310a060 exit status 1 <nil> <nil> true [0xc00017fca0 0xc000010050 0xc000010790] [0xc00017fca0 0xc000010050 0xc000010790] [0xc00017fe78 0xc000010750] [0x10ef310 0x10ef310] 0xc003b543c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:33:17.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:33:17.876: INFO: rc: 1
Mar 13 01:33:17.876: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00310a450 exit status 1 <nil> <nil> true [0xc000010a40 0xc000010d10 0xc0000111b0] [0xc000010a40 0xc000010d10 0xc0000111b0] [0xc000010c58 0xc000011150] [0x10ef310 0x10ef310] 0xc003b54d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:33:27.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:33:28.111: INFO: rc: 1
Mar 13 01:33:28.111: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0039de4b0 exit status 1 <nil> <nil> true [0xc005d56000 0xc005d56018 0xc005d56030] [0xc005d56000 0xc005d56018 0xc005d56030] [0xc005d56010 0xc005d56028] [0x10ef310 0x10ef310] 0xc002be2a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:33:38.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:33:38.317: INFO: rc: 1
Mar 13 01:33:38.317: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0039de8a0 exit status 1 <nil> <nil> true [0xc005d56038 0xc005d56050 0xc005d56068] [0xc005d56038 0xc005d56050 0xc005d56068] [0xc005d56048 0xc005d56060] [0x10ef310 0x10ef310] 0xc00329d440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:33:48.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:33:48.550: INFO: rc: 1
Mar 13 01:33:48.550: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0039dec30 exit status 1 <nil> <nil> true [0xc005d56070 0xc005d56088 0xc005d560a0] [0xc005d56070 0xc005d56088 0xc005d560a0] [0xc005d56080 0xc005d56098] [0x10ef310 0x10ef310] 0xc002e6ee40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:33:58.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:33:58.782: INFO: rc: 1
Mar 13 01:33:58.782: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0039defc0 exit status 1 <nil> <nil> true [0xc005d560a8 0xc005d560c0 0xc005d560d8] [0xc005d560a8 0xc005d560c0 0xc005d560d8] [0xc005d560b8 0xc005d560d0] [0x10ef310 0x10ef310] 0xc002e6fec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:34:08.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:34:09.022: INFO: rc: 1
Mar 13 01:34:09.022: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc005516390 exit status 1 <nil> <nil> true [0xc0033d0010 0xc0033d0050 0xc0033d0068] [0xc0033d0010 0xc0033d0050 0xc0033d0068] [0xc0033d0048 0xc0033d0060] [0x10ef310 0x10ef310] 0xc002d802a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:34:19.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:34:19.254: INFO: rc: 1
Mar 13 01:34:19.254: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc005516750 exit status 1 <nil> <nil> true [0xc0033d0070 0xc0033d0088 0xc0033d00a8] [0xc0033d0070 0xc0033d0088 0xc0033d00a8] [0xc0033d0080 0xc0033d00a0] [0x10ef310 0x10ef310] 0xc002d80720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:34:29.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:34:29.480: INFO: rc: 1
Mar 13 01:34:29.480: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00310a840 exit status 1 <nil> <nil> true [0xc0000114a8 0xc000011698 0xc0000117d8] [0xc0000114a8 0xc000011698 0xc0000117d8] [0xc000011578 0xc000011798] [0x10ef310 0x10ef310] 0xc003b554a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar 13 01:34:39.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-7621 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:34:39.706: INFO: rc: 1
Mar 13 01:34:39.706: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Mar 13 01:34:39.706: INFO: Scaling statefulset ss to 0
Mar 13 01:34:39.726: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 13 01:34:39.731: INFO: Deleting all statefulset in ns statefulset-7621
Mar 13 01:34:39.736: INFO: Scaling statefulset ss to 0
Mar 13 01:34:39.752: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 01:34:39.756: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:34:39.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7621" for this suite.
Mar 13 01:34:45.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:34:45.984: INFO: namespace statefulset-7621 deletion completed in 6.195395363s

• [SLOW TEST:367.546 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:34:45.989: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:34:46.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5650" for this suite.
Mar 13 01:34:58.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:34:58.334: INFO: namespace pods-5650 deletion completed in 12.206497229s

• [SLOW TEST:12.345 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:34:58.335: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:34:58.448: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 13 01:34:58.463: INFO: Number of nodes with available pods: 0
Mar 13 01:34:58.463: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 13 01:34:58.510: INFO: Number of nodes with available pods: 0
Mar 13 01:34:58.510: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:34:59.518: INFO: Number of nodes with available pods: 0
Mar 13 01:34:59.518: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:35:00.518: INFO: Number of nodes with available pods: 0
Mar 13 01:35:00.518: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:35:01.518: INFO: Number of nodes with available pods: 0
Mar 13 01:35:01.518: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:35:02.517: INFO: Number of nodes with available pods: 1
Mar 13 01:35:02.517: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 13 01:35:02.546: INFO: Number of nodes with available pods: 1
Mar 13 01:35:02.547: INFO: Number of running nodes: 0, number of available pods: 1
Mar 13 01:35:03.553: INFO: Number of nodes with available pods: 0
Mar 13 01:35:03.553: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 13 01:35:03.570: INFO: Number of nodes with available pods: 0
Mar 13 01:35:03.571: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:35:04.578: INFO: Number of nodes with available pods: 0
Mar 13 01:35:04.578: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:35:05.578: INFO: Number of nodes with available pods: 0
Mar 13 01:35:05.578: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:35:06.578: INFO: Number of nodes with available pods: 0
Mar 13 01:35:06.578: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:35:07.585: INFO: Number of nodes with available pods: 0
Mar 13 01:35:07.585: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 01:35:08.577: INFO: Number of nodes with available pods: 1
Mar 13 01:35:08.577: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1070, will wait for the garbage collector to delete the pods
Mar 13 01:35:08.659: INFO: Deleting DaemonSet.extensions daemon-set took: 14.228414ms
Mar 13 01:35:08.960: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.850141ms
Mar 13 01:35:12.067: INFO: Number of nodes with available pods: 0
Mar 13 01:35:12.067: INFO: Number of running nodes: 0, number of available pods: 0
Mar 13 01:35:12.075: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1070/daemonsets","resourceVersion":"12560"},"items":null}

Mar 13 01:35:12.087: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1070/pods","resourceVersion":"12560"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:35:12.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1070" for this suite.
Mar 13 01:35:18.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:35:18.296: INFO: namespace daemonsets-1070 deletion completed in 6.167931512s

• [SLOW TEST:19.962 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:35:18.299: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6594
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6594
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6594
Mar 13 01:35:18.397: INFO: Found 0 stateful pods, waiting for 1
Mar 13 01:35:28.404: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 13 01:35:28.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-6594 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 13 01:35:29.012: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 13 01:35:29.012: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 13 01:35:29.012: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 13 01:35:29.020: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 13 01:35:39.030: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 01:35:39.030: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 01:35:39.058: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999994494s
Mar 13 01:35:40.065: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992439917s
Mar 13 01:35:41.072: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.98559509s
Mar 13 01:35:42.080: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.979186695s
Mar 13 01:35:43.088: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.97116091s
Mar 13 01:35:44.095: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.962976027s
Mar 13 01:35:45.103: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.955792866s
Mar 13 01:35:46.111: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.947726384s
Mar 13 01:35:47.117: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.940153366s
Mar 13 01:35:48.125: INFO: Verifying statefulset ss doesn't scale past 1 for another 933.361007ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6594
Mar 13 01:35:49.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-6594 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:35:49.753: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 13 01:35:49.753: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 13 01:35:49.753: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 13 01:35:49.765: INFO: Found 1 stateful pods, waiting for 3
Mar 13 01:35:59.774: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 01:35:59.774: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 01:35:59.774: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 13 01:35:59.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-6594 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 13 01:36:00.425: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 13 01:36:00.425: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 13 01:36:00.425: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 13 01:36:00.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-6594 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 13 01:36:00.921: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 13 01:36:00.921: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 13 01:36:00.921: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 13 01:36:00.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-6594 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 13 01:36:01.396: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 13 01:36:01.396: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 13 01:36:01.396: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 13 01:36:01.396: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 01:36:01.402: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 13 01:36:11.420: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 01:36:11.420: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 01:36:11.420: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 01:36:11.443: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999993461s
Mar 13 01:36:12.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988963314s
Mar 13 01:36:13.473: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.978552295s
Mar 13 01:36:14.483: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.95878485s
Mar 13 01:36:15.491: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.949564258s
Mar 13 01:36:16.499: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.941013825s
Mar 13 01:36:17.506: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.933214158s
Mar 13 01:36:18.514: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.925846313s
Mar 13 01:36:19.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.918061081s
Mar 13 01:36:20.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 910.019155ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6594
Mar 13 01:36:21.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-6594 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:36:22.181: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 13 01:36:22.181: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 13 01:36:22.181: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 13 01:36:22.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-6594 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:36:22.675: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 13 01:36:22.675: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 13 01:36:22.675: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 13 01:36:22.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-6594 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:36:23.158: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 13 01:36:23.159: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 13 01:36:23.159: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 13 01:36:23.159: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 13 01:36:53.187: INFO: Deleting all statefulset in ns statefulset-6594
Mar 13 01:36:53.192: INFO: Scaling statefulset ss to 0
Mar 13 01:36:53.211: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 01:36:53.215: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:36:53.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6594" for this suite.
Mar 13 01:36:59.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:36:59.413: INFO: namespace statefulset-6594 deletion completed in 6.167215339s

• [SLOW TEST:101.115 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:36:59.415: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:37:00.716: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 01:37:02.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660220, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660220, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660220, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660220, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:37:05.760: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:37:05.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2149" for this suite.
Mar 13 01:37:11.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:37:12.039: INFO: namespace webhook-2149 deletion completed in 6.202872906s
STEP: Destroying namespace "webhook-2149-markers" for this suite.
Mar 13 01:37:18.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:37:18.217: INFO: namespace webhook-2149-markers deletion completed in 6.178093232s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.826 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:37:18.241: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:37:18.302: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:37:19.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5894" for this suite.
Mar 13 01:37:25.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:37:25.532: INFO: namespace custom-resource-definition-5894 deletion completed in 6.178874796s

• [SLOW TEST:7.291 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:37:25.532: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 01:37:25.618: INFO: Waiting up to 5m0s for pod "downwardapi-volume-262b8bf3-5ecb-46ec-a2a4-296fcfe87bb5" in namespace "projected-8124" to be "success or failure"
Mar 13 01:37:25.629: INFO: Pod "downwardapi-volume-262b8bf3-5ecb-46ec-a2a4-296fcfe87bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.090201ms
Mar 13 01:37:27.636: INFO: Pod "downwardapi-volume-262b8bf3-5ecb-46ec-a2a4-296fcfe87bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017734721s
Mar 13 01:37:29.644: INFO: Pod "downwardapi-volume-262b8bf3-5ecb-46ec-a2a4-296fcfe87bb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026281756s
STEP: Saw pod success
Mar 13 01:37:29.644: INFO: Pod "downwardapi-volume-262b8bf3-5ecb-46ec-a2a4-296fcfe87bb5" satisfied condition "success or failure"
Mar 13 01:37:29.649: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-262b8bf3-5ecb-46ec-a2a4-296fcfe87bb5 container client-container: <nil>
STEP: delete the pod
Mar 13 01:37:29.703: INFO: Waiting for pod downwardapi-volume-262b8bf3-5ecb-46ec-a2a4-296fcfe87bb5 to disappear
Mar 13 01:37:29.716: INFO: Pod downwardapi-volume-262b8bf3-5ecb-46ec-a2a4-296fcfe87bb5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:37:29.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8124" for this suite.
Mar 13 01:37:35.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:37:35.900: INFO: namespace projected-8124 deletion completed in 6.175315397s

• [SLOW TEST:10.368 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:37:35.901: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 13 01:37:35.965: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 13 01:37:35.992: INFO: Waiting for terminating namespaces to be deleted...
Mar 13 01:37:36.003: INFO: 
Logging pods the kubelet thinks is on node sam-node2 before test
Mar 13 01:37:36.033: INFO: kube-dns-869c556677-bfkcm from kube-system started at 2020-03-13 00:13:29 +0000 UTC (3 container statuses recorded)
Mar 13 01:37:36.033: INFO: 	Container dnsmasq ready: true, restart count 0
Mar 13 01:37:36.033: INFO: 	Container kubedns ready: true, restart count 0
Mar 13 01:37:36.033: INFO: 	Container sidecar ready: true, restart count 0
Mar 13 01:37:36.033: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-w6d2v from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 01:37:36.033: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 13 01:37:36.033: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 13 01:37:36.033: INFO: nirmata-cni-installer-nkzv7 from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 01:37:36.033: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 01:37:36.033: INFO: kube-flannel-ds-d9nq5 from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 01:37:36.033: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 01:37:36.033: INFO: 
Logging pods the kubelet thinks is on node sam-node3 before test
Mar 13 01:37:36.056: INFO: nirmata-cni-installer-bf5qr from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 01:37:36.056: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 01:37:36.056: INFO: kube-flannel-ds-2rr2t from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 01:37:36.056: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 01:37:36.056: INFO: nirmata-kube-controller-666cf5cf5f-j4mmx from nirmata started at 2020-03-13 00:13:27 +0000 UTC (1 container statuses recorded)
Mar 13 01:37:36.056: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
Mar 13 01:37:36.056: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-f9nz4 from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 01:37:36.056: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 13 01:37:36.056: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 13 01:37:36.056: INFO: 
Logging pods the kubelet thinks is on node silbory-nirmata0 before test
Mar 13 01:37:36.082: INFO: metrics-server-56c7b465d6-xr59t from kube-system started at 2020-03-13 00:13:52 +0000 UTC (1 container statuses recorded)
Mar 13 01:37:36.082: INFO: 	Container metrics-server ready: true, restart count 0
Mar 13 01:37:36.082: INFO: kube-flannel-ds-959x5 from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 01:37:36.082: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 01:37:36.082: INFO: ingress-default-backend-cf675c575-nttt9 from ingress-haproxy started at 2020-03-13 00:14:00 +0000 UTC (1 container statuses recorded)
Mar 13 01:37:36.082: INFO: 	Container ingress-default-backend ready: true, restart count 0
Mar 13 01:37:36.082: INFO: sonobuoy-e2e-job-fce51d5a97b943f8 from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 01:37:36.083: INFO: 	Container e2e ready: true, restart count 0
Mar 13 01:37:36.083: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 01:37:36.083: INFO: nirmata-cni-installer-lnhv6 from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 01:37:36.083: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 01:37:36.083: INFO: haproxy-ingress-8688f746b4-2xbx5 from ingress-haproxy started at 2020-03-13 00:14:00 +0000 UTC (1 container statuses recorded)
Mar 13 01:37:36.083: INFO: 	Container haproxy-ingress ready: true, restart count 0
Mar 13 01:37:36.083: INFO: sonobuoy from sonobuoy started at 2020-03-13 00:21:12 +0000 UTC (1 container statuses recorded)
Mar 13 01:37:36.083: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 13 01:37:36.083: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-qf4zs from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 01:37:36.083: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 13 01:37:36.083: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e434dde4-d2ae-457e-b5d2-0effed1d88db 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-e434dde4-d2ae-457e-b5d2-0effed1d88db off the node silbory-nirmata0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e434dde4-d2ae-457e-b5d2-0effed1d88db
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:37:52.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8978" for this suite.
Mar 13 01:38:08.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:38:08.491: INFO: namespace sched-pred-8978 deletion completed in 16.185983166s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:32.590 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:38:08.491: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 13 01:38:08.613: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2654 /api/v1/namespaces/watch-2654/configmaps/e2e-watch-test-resource-version 14bc633e-6be6-4092-9f0e-9a01b6e53616 13090 0 2020-03-13 01:38:08 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 13 01:38:08.613: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2654 /api/v1/namespaces/watch-2654/configmaps/e2e-watch-test-resource-version 14bc633e-6be6-4092-9f0e-9a01b6e53616 13091 0 2020-03-13 01:38:08 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:38:08.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2654" for this suite.
Mar 13 01:38:14.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:38:14.808: INFO: namespace watch-2654 deletion completed in 6.187200301s

• [SLOW TEST:6.317 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:38:14.808: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Mar 13 01:38:14.900: INFO: Waiting up to 5m0s for pod "client-containers-4154e491-9908-4a6d-88b9-d9c30ce4639e" in namespace "containers-5900" to be "success or failure"
Mar 13 01:38:14.910: INFO: Pod "client-containers-4154e491-9908-4a6d-88b9-d9c30ce4639e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.663642ms
Mar 13 01:38:16.916: INFO: Pod "client-containers-4154e491-9908-4a6d-88b9-d9c30ce4639e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016453696s
Mar 13 01:38:18.925: INFO: Pod "client-containers-4154e491-9908-4a6d-88b9-d9c30ce4639e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025150169s
STEP: Saw pod success
Mar 13 01:38:18.926: INFO: Pod "client-containers-4154e491-9908-4a6d-88b9-d9c30ce4639e" satisfied condition "success or failure"
Mar 13 01:38:18.931: INFO: Trying to get logs from node silbory-nirmata0 pod client-containers-4154e491-9908-4a6d-88b9-d9c30ce4639e container test-container: <nil>
STEP: delete the pod
Mar 13 01:38:18.972: INFO: Waiting for pod client-containers-4154e491-9908-4a6d-88b9-d9c30ce4639e to disappear
Mar 13 01:38:18.979: INFO: Pod client-containers-4154e491-9908-4a6d-88b9-d9c30ce4639e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:38:18.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5900" for this suite.
Mar 13 01:38:25.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:38:25.185: INFO: namespace containers-5900 deletion completed in 6.192406787s

• [SLOW TEST:10.377 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:38:25.186: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2714
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Mar 13 01:38:25.301: INFO: Found 0 stateful pods, waiting for 3
Mar 13 01:38:35.311: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 01:38:35.311: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 01:38:35.311: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 01:38:35.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-2714 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 13 01:38:35.819: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 13 01:38:35.819: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 13 01:38:35.819: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 13 01:38:45.870: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 13 01:38:55.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-2714 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:38:56.358: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 13 01:38:56.358: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 13 01:38:56.358: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Mar 13 01:39:26.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-2714 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 13 01:39:26.859: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 13 01:39:26.859: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 13 01:39:26.859: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 13 01:39:36.918: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 13 01:39:46.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=statefulset-2714 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 13 01:39:47.425: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 13 01:39:47.425: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 13 01:39:47.425: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 13 01:40:07.466: INFO: Waiting for StatefulSet statefulset-2714/ss2 to complete update
Mar 13 01:40:07.467: INFO: Waiting for Pod statefulset-2714/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 13 01:40:17.481: INFO: Waiting for StatefulSet statefulset-2714/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 13 01:40:27.481: INFO: Deleting all statefulset in ns statefulset-2714
Mar 13 01:40:27.486: INFO: Scaling statefulset ss2 to 0
Mar 13 01:40:57.516: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 01:40:57.522: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:40:57.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2714" for this suite.
Mar 13 01:41:03.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:41:03.738: INFO: namespace statefulset-2714 deletion completed in 6.187368641s

• [SLOW TEST:158.552 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:41:03.739: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 01:41:03.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b4b2e91e-f6f9-4419-a621-bf6c110cb395" in namespace "downward-api-9033" to be "success or failure"
Mar 13 01:41:03.824: INFO: Pod "downwardapi-volume-b4b2e91e-f6f9-4419-a621-bf6c110cb395": Phase="Pending", Reason="", readiness=false. Elapsed: 7.163643ms
Mar 13 01:41:05.832: INFO: Pod "downwardapi-volume-b4b2e91e-f6f9-4419-a621-bf6c110cb395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01552445s
Mar 13 01:41:07.839: INFO: Pod "downwardapi-volume-b4b2e91e-f6f9-4419-a621-bf6c110cb395": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022612243s
STEP: Saw pod success
Mar 13 01:41:07.839: INFO: Pod "downwardapi-volume-b4b2e91e-f6f9-4419-a621-bf6c110cb395" satisfied condition "success or failure"
Mar 13 01:41:07.844: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-b4b2e91e-f6f9-4419-a621-bf6c110cb395 container client-container: <nil>
STEP: delete the pod
Mar 13 01:41:07.899: INFO: Waiting for pod downwardapi-volume-b4b2e91e-f6f9-4419-a621-bf6c110cb395 to disappear
Mar 13 01:41:07.909: INFO: Pod downwardapi-volume-b4b2e91e-f6f9-4419-a621-bf6c110cb395 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:41:07.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9033" for this suite.
Mar 13 01:41:13.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:41:14.215: INFO: namespace downward-api-9033 deletion completed in 6.292993073s

• [SLOW TEST:10.476 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:41:14.216: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:41:14.308: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-bb959787-ed52-4ffa-b5e2-3de3ce8c1044" in namespace "security-context-test-1668" to be "success or failure"
Mar 13 01:41:14.320: INFO: Pod "busybox-readonly-false-bb959787-ed52-4ffa-b5e2-3de3ce8c1044": Phase="Pending", Reason="", readiness=false. Elapsed: 11.475867ms
Mar 13 01:41:16.326: INFO: Pod "busybox-readonly-false-bb959787-ed52-4ffa-b5e2-3de3ce8c1044": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017873491s
Mar 13 01:41:18.332: INFO: Pod "busybox-readonly-false-bb959787-ed52-4ffa-b5e2-3de3ce8c1044": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02372727s
Mar 13 01:41:18.332: INFO: Pod "busybox-readonly-false-bb959787-ed52-4ffa-b5e2-3de3ce8c1044" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:41:18.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1668" for this suite.
Mar 13 01:41:24.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:41:24.513: INFO: namespace security-context-test-1668 deletion completed in 6.173287902s

• [SLOW TEST:10.298 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:41:24.515: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0313 01:41:25.664567      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 13 01:41:25.664: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:41:25.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3193" for this suite.
Mar 13 01:41:31.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:41:31.847: INFO: namespace gc-3193 deletion completed in 6.173086594s

• [SLOW TEST:7.332 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:41:31.848: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:41:32.671: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 01:41:34.689: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660492, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660492, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660492, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660492, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:41:37.715: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:41:37.721: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7834-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:41:39.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9931" for this suite.
Mar 13 01:41:45.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:41:45.361: INFO: namespace webhook-9931 deletion completed in 6.185708179s
STEP: Destroying namespace "webhook-9931-markers" for this suite.
Mar 13 01:41:51.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:41:51.539: INFO: namespace webhook-9931-markers deletion completed in 6.178493569s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.719 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:41:51.574: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:41:51.658: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-043f809e-d700-4c2a-a94a-795b46c3114a" in namespace "security-context-test-4902" to be "success or failure"
Mar 13 01:41:51.665: INFO: Pod "alpine-nnp-false-043f809e-d700-4c2a-a94a-795b46c3114a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.388904ms
Mar 13 01:41:53.673: INFO: Pod "alpine-nnp-false-043f809e-d700-4c2a-a94a-795b46c3114a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014205717s
Mar 13 01:41:55.680: INFO: Pod "alpine-nnp-false-043f809e-d700-4c2a-a94a-795b46c3114a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021196849s
Mar 13 01:41:57.686: INFO: Pod "alpine-nnp-false-043f809e-d700-4c2a-a94a-795b46c3114a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027409634s
Mar 13 01:41:57.686: INFO: Pod "alpine-nnp-false-043f809e-d700-4c2a-a94a-795b46c3114a" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:41:57.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4902" for this suite.
Mar 13 01:42:03.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:42:03.875: INFO: namespace security-context-test-4902 deletion completed in 6.163696109s

• [SLOW TEST:12.301 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:42:03.876: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 13 01:42:03.943: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 13 01:42:03.965: INFO: Waiting for terminating namespaces to be deleted...
Mar 13 01:42:03.971: INFO: 
Logging pods the kubelet thinks is on node sam-node2 before test
Mar 13 01:42:03.999: INFO: kube-dns-869c556677-bfkcm from kube-system started at 2020-03-13 00:13:29 +0000 UTC (3 container statuses recorded)
Mar 13 01:42:03.999: INFO: 	Container dnsmasq ready: true, restart count 0
Mar 13 01:42:03.999: INFO: 	Container kubedns ready: true, restart count 0
Mar 13 01:42:03.999: INFO: 	Container sidecar ready: true, restart count 0
Mar 13 01:42:03.999: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-w6d2v from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 01:42:03.999: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 13 01:42:03.999: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 13 01:42:03.999: INFO: nirmata-cni-installer-nkzv7 from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 01:42:03.999: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 01:42:03.999: INFO: kube-flannel-ds-d9nq5 from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 01:42:03.999: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 01:42:03.999: INFO: 
Logging pods the kubelet thinks is on node sam-node3 before test
Mar 13 01:42:04.022: INFO: nirmata-cni-installer-bf5qr from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 01:42:04.022: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 01:42:04.022: INFO: kube-flannel-ds-2rr2t from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 01:42:04.022: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 01:42:04.022: INFO: nirmata-kube-controller-666cf5cf5f-j4mmx from nirmata started at 2020-03-13 00:13:27 +0000 UTC (1 container statuses recorded)
Mar 13 01:42:04.022: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
Mar 13 01:42:04.022: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-f9nz4 from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 01:42:04.022: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 13 01:42:04.022: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 13 01:42:04.022: INFO: 
Logging pods the kubelet thinks is on node silbory-nirmata0 before test
Mar 13 01:42:04.034: INFO: kube-flannel-ds-959x5 from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 01:42:04.034: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 01:42:04.034: INFO: ingress-default-backend-cf675c575-nttt9 from ingress-haproxy started at 2020-03-13 00:14:00 +0000 UTC (1 container statuses recorded)
Mar 13 01:42:04.034: INFO: 	Container ingress-default-backend ready: true, restart count 0
Mar 13 01:42:04.034: INFO: sonobuoy-e2e-job-fce51d5a97b943f8 from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 01:42:04.034: INFO: 	Container e2e ready: true, restart count 0
Mar 13 01:42:04.034: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 01:42:04.034: INFO: nirmata-cni-installer-lnhv6 from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 01:42:04.034: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 01:42:04.034: INFO: haproxy-ingress-8688f746b4-2xbx5 from ingress-haproxy started at 2020-03-13 00:14:00 +0000 UTC (1 container statuses recorded)
Mar 13 01:42:04.034: INFO: 	Container haproxy-ingress ready: true, restart count 0
Mar 13 01:42:04.034: INFO: sonobuoy from sonobuoy started at 2020-03-13 00:21:12 +0000 UTC (1 container statuses recorded)
Mar 13 01:42:04.034: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 13 01:42:04.034: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-qf4zs from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 01:42:04.034: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 13 01:42:04.034: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 13 01:42:04.034: INFO: metrics-server-56c7b465d6-xr59t from kube-system started at 2020-03-13 00:13:52 +0000 UTC (1 container statuses recorded)
Mar 13 01:42:04.034: INFO: 	Container metrics-server ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-85ba76db-673a-493f-a5c6-6f8b42255fef 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-85ba76db-673a-493f-a5c6-6f8b42255fef off the node silbory-nirmata0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-85ba76db-673a-493f-a5c6-6f8b42255fef
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:42:12.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6313" for this suite.
Mar 13 01:42:32.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:42:32.386: INFO: namespace sched-pred-6313 deletion completed in 20.190529117s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:28.511 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:42:32.388: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Mar 13 01:42:32.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-8949'
Mar 13 01:42:33.028: INFO: stderr: ""
Mar 13 01:42:33.028: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 13 01:42:33.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8949'
Mar 13 01:42:33.266: INFO: stderr: ""
Mar 13 01:42:33.266: INFO: stdout: "update-demo-nautilus-27lr5 update-demo-nautilus-bxz9p "
Mar 13 01:42:33.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-27lr5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8949'
Mar 13 01:42:33.486: INFO: stderr: ""
Mar 13 01:42:33.486: INFO: stdout: ""
Mar 13 01:42:33.486: INFO: update-demo-nautilus-27lr5 is created but not running
Mar 13 01:42:38.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8949'
Mar 13 01:42:38.696: INFO: stderr: ""
Mar 13 01:42:38.696: INFO: stdout: "update-demo-nautilus-27lr5 update-demo-nautilus-bxz9p "
Mar 13 01:42:38.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-27lr5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8949'
Mar 13 01:42:38.899: INFO: stderr: ""
Mar 13 01:42:38.899: INFO: stdout: "true"
Mar 13 01:42:38.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-27lr5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8949'
Mar 13 01:42:39.121: INFO: stderr: ""
Mar 13 01:42:39.121: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 01:42:39.121: INFO: validating pod update-demo-nautilus-27lr5
Mar 13 01:42:39.132: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 01:42:39.132: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 01:42:39.132: INFO: update-demo-nautilus-27lr5 is verified up and running
Mar 13 01:42:39.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-bxz9p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8949'
Mar 13 01:42:39.355: INFO: stderr: ""
Mar 13 01:42:39.355: INFO: stdout: "true"
Mar 13 01:42:39.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-bxz9p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8949'
Mar 13 01:42:39.595: INFO: stderr: ""
Mar 13 01:42:39.596: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 01:42:39.596: INFO: validating pod update-demo-nautilus-bxz9p
Mar 13 01:42:39.611: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 01:42:39.611: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 01:42:39.611: INFO: update-demo-nautilus-bxz9p is verified up and running
STEP: scaling down the replication controller
Mar 13 01:42:39.627: INFO: scanned /root for discovery docs: <nil>
Mar 13 01:42:39.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8949'
Mar 13 01:42:40.960: INFO: stderr: ""
Mar 13 01:42:40.960: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 13 01:42:40.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8949'
Mar 13 01:42:41.191: INFO: stderr: ""
Mar 13 01:42:41.191: INFO: stdout: "update-demo-nautilus-27lr5 update-demo-nautilus-bxz9p "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 13 01:42:46.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8949'
Mar 13 01:42:46.411: INFO: stderr: ""
Mar 13 01:42:46.411: INFO: stdout: "update-demo-nautilus-27lr5 update-demo-nautilus-bxz9p "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 13 01:42:51.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8949'
Mar 13 01:42:51.810: INFO: stderr: ""
Mar 13 01:42:51.810: INFO: stdout: "update-demo-nautilus-27lr5 update-demo-nautilus-bxz9p "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 13 01:42:56.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8949'
Mar 13 01:42:57.062: INFO: stderr: ""
Mar 13 01:42:57.062: INFO: stdout: "update-demo-nautilus-bxz9p "
Mar 13 01:42:57.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-bxz9p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8949'
Mar 13 01:42:57.198: INFO: stderr: ""
Mar 13 01:42:57.198: INFO: stdout: "true"
Mar 13 01:42:57.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-bxz9p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8949'
Mar 13 01:42:57.421: INFO: stderr: ""
Mar 13 01:42:57.421: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 01:42:57.421: INFO: validating pod update-demo-nautilus-bxz9p
Mar 13 01:42:57.428: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 01:42:57.428: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 01:42:57.428: INFO: update-demo-nautilus-bxz9p is verified up and running
STEP: scaling up the replication controller
Mar 13 01:42:57.444: INFO: scanned /root for discovery docs: <nil>
Mar 13 01:42:57.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8949'
Mar 13 01:42:58.722: INFO: stderr: ""
Mar 13 01:42:58.723: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 13 01:42:58.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8949'
Mar 13 01:42:58.945: INFO: stderr: ""
Mar 13 01:42:58.945: INFO: stdout: "update-demo-nautilus-2drc9 update-demo-nautilus-bxz9p "
Mar 13 01:42:58.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-2drc9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8949'
Mar 13 01:42:59.168: INFO: stderr: ""
Mar 13 01:42:59.168: INFO: stdout: ""
Mar 13 01:42:59.168: INFO: update-demo-nautilus-2drc9 is created but not running
Mar 13 01:43:04.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8949'
Mar 13 01:43:04.411: INFO: stderr: ""
Mar 13 01:43:04.411: INFO: stdout: "update-demo-nautilus-2drc9 update-demo-nautilus-bxz9p "
Mar 13 01:43:04.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-2drc9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8949'
Mar 13 01:43:04.630: INFO: stderr: ""
Mar 13 01:43:04.630: INFO: stdout: "true"
Mar 13 01:43:04.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-2drc9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8949'
Mar 13 01:43:04.800: INFO: stderr: ""
Mar 13 01:43:04.800: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 01:43:04.800: INFO: validating pod update-demo-nautilus-2drc9
Mar 13 01:43:04.809: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 01:43:04.809: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 01:43:04.810: INFO: update-demo-nautilus-2drc9 is verified up and running
Mar 13 01:43:04.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-bxz9p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8949'
Mar 13 01:43:05.026: INFO: stderr: ""
Mar 13 01:43:05.026: INFO: stdout: "true"
Mar 13 01:43:05.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-bxz9p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8949'
Mar 13 01:43:05.212: INFO: stderr: ""
Mar 13 01:43:05.212: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 01:43:05.212: INFO: validating pod update-demo-nautilus-bxz9p
Mar 13 01:43:05.219: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 01:43:05.219: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 01:43:05.219: INFO: update-demo-nautilus-bxz9p is verified up and running
STEP: using delete to clean up resources
Mar 13 01:43:05.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete --grace-period=0 --force -f - --namespace=kubectl-8949'
Mar 13 01:43:05.441: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 01:43:05.441: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 13 01:43:05.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8949'
Mar 13 01:43:05.689: INFO: stderr: "No resources found in kubectl-8949 namespace.\n"
Mar 13 01:43:05.689: INFO: stdout: ""
Mar 13 01:43:05.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -l name=update-demo --namespace=kubectl-8949 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 13 01:43:05.927: INFO: stderr: ""
Mar 13 01:43:05.927: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:43:05.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8949" for this suite.
Mar 13 01:43:11.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:43:12.138: INFO: namespace kubectl-8949 deletion completed in 6.200073492s

• [SLOW TEST:39.751 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:43:12.143: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:43:16.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9407" for this suite.
Mar 13 01:43:26.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:43:26.487: INFO: namespace containers-9407 deletion completed in 10.226655262s

• [SLOW TEST:14.344 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:43:26.488: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar 13 01:43:31.110: INFO: Successfully updated pod "annotationupdateaa6e0978-49eb-452f-a975-d70856c81141"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:43:33.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7648" for this suite.
Mar 13 01:44:01.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:44:01.332: INFO: namespace downward-api-7648 deletion completed in 28.173587193s

• [SLOW TEST:34.845 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:44:01.333: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:44:01.413: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-cf6a9007-7a34-4050-b6f9-4258cc868ac4" in namespace "security-context-test-5951" to be "success or failure"
Mar 13 01:44:01.421: INFO: Pod "busybox-privileged-false-cf6a9007-7a34-4050-b6f9-4258cc868ac4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.389254ms
Mar 13 01:44:03.430: INFO: Pod "busybox-privileged-false-cf6a9007-7a34-4050-b6f9-4258cc868ac4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016756716s
Mar 13 01:44:05.436: INFO: Pod "busybox-privileged-false-cf6a9007-7a34-4050-b6f9-4258cc868ac4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022766192s
Mar 13 01:44:05.436: INFO: Pod "busybox-privileged-false-cf6a9007-7a34-4050-b6f9-4258cc868ac4" satisfied condition "success or failure"
Mar 13 01:44:05.452: INFO: Got logs for pod "busybox-privileged-false-cf6a9007-7a34-4050-b6f9-4258cc868ac4": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:44:05.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5951" for this suite.
Mar 13 01:44:11.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:44:11.646: INFO: namespace security-context-test-5951 deletion completed in 6.184308064s

• [SLOW TEST:10.313 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:44:11.649: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:44:12.710: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 01:44:14.728: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660652, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660652, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660652, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660652, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:44:17.754: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:44:18.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2041" for this suite.
Mar 13 01:44:24.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:44:24.287: INFO: namespace webhook-2041 deletion completed in 6.230014711s
STEP: Destroying namespace "webhook-2041-markers" for this suite.
Mar 13 01:44:30.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:44:30.487: INFO: namespace webhook-2041-markers deletion completed in 6.200021661s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.868 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:44:30.517: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:44:30.586: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 13 01:44:30.600: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 13 01:44:35.608: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 13 01:44:35.609: INFO: Creating deployment "test-rolling-update-deployment"
Mar 13 01:44:35.617: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 13 01:44:35.628: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 13 01:44:37.640: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 13 01:44:37.646: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660675, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660675, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660675, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660675, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 01:44:39.654: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 13 01:44:39.675: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2312 /apis/apps/v1/namespaces/deployment-2312/deployments/test-rolling-update-deployment 4f6392d1-5c1c-4246-b273-2571cce9fb5c 14261 1 2020-03-13 01:44:35 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a0e528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-13 01:44:35 +0000 UTC,LastTransitionTime:2020-03-13 01:44:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-03-13 01:44:38 +0000 UTC,LastTransitionTime:2020-03-13 01:44:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 13 01:44:39.682: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-2312 /apis/apps/v1/namespaces/deployment-2312/replicasets/test-rolling-update-deployment-55d946486 85660182-6ff6-482c-9546-88e93d315305 14250 1 2020-03-13 01:44:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 4f6392d1-5c1c-4246-b273-2571cce9fb5c 0xc003a0ea50 0xc003a0ea51}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a0eab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 13 01:44:39.682: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 13 01:44:39.682: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2312 /apis/apps/v1/namespaces/deployment-2312/replicasets/test-rolling-update-controller 846611d2-bad3-461d-b67f-a2bbc25510b8 14260 2 2020-03-13 01:44:30 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 4f6392d1-5c1c-4246-b273-2571cce9fb5c 0xc003a0e987 0xc003a0e988}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a0e9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 13 01:44:39.688: INFO: Pod "test-rolling-update-deployment-55d946486-vscsj" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-vscsj test-rolling-update-deployment-55d946486- deployment-2312 /api/v1/namespaces/deployment-2312/pods/test-rolling-update-deployment-55d946486-vscsj 085477c4-8e02-4187-940b-ae1646635e34 14249 0 2020-03-13 01:44:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 85660182-6ff6-482c-9546-88e93d315305 0xc0064551e0 0xc0064551e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x5pmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x5pmw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x5pmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:silbory-nirmata0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 01:44:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 01:44:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 01:44:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 01:44:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.79,PodIP:10.244.2.192,StartTime:2020-03-13 01:44:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-13 01:44:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://e6f0c6c61ae14b0ed7986e65a70b94ae06c5d76f1d8009484393149879b69b47,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:44:39.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2312" for this suite.
Mar 13 01:44:45.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:44:45.892: INFO: namespace deployment-2312 deletion completed in 6.196562146s

• [SLOW TEST:15.375 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:44:45.894: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 13 01:44:45.996: INFO: Waiting up to 5m0s for pod "pod-2789c9ba-1466-4ac6-8db4-dce59ff336de" in namespace "emptydir-2619" to be "success or failure"
Mar 13 01:44:46.007: INFO: Pod "pod-2789c9ba-1466-4ac6-8db4-dce59ff336de": Phase="Pending", Reason="", readiness=false. Elapsed: 10.273614ms
Mar 13 01:44:48.015: INFO: Pod "pod-2789c9ba-1466-4ac6-8db4-dce59ff336de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018018033s
Mar 13 01:44:50.021: INFO: Pod "pod-2789c9ba-1466-4ac6-8db4-dce59ff336de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024744608s
STEP: Saw pod success
Mar 13 01:44:50.021: INFO: Pod "pod-2789c9ba-1466-4ac6-8db4-dce59ff336de" satisfied condition "success or failure"
Mar 13 01:44:50.035: INFO: Trying to get logs from node silbory-nirmata0 pod pod-2789c9ba-1466-4ac6-8db4-dce59ff336de container test-container: <nil>
STEP: delete the pod
Mar 13 01:44:50.086: INFO: Waiting for pod pod-2789c9ba-1466-4ac6-8db4-dce59ff336de to disappear
Mar 13 01:44:50.092: INFO: Pod pod-2789c9ba-1466-4ac6-8db4-dce59ff336de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:44:50.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2619" for this suite.
Mar 13 01:44:56.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:44:56.278: INFO: namespace emptydir-2619 deletion completed in 6.167581207s

• [SLOW TEST:10.383 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:44:56.279: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:44:56.345: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Creating first CR 
Mar 13 01:44:57.012: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-13T01:44:57Z generation:1 name:name1 resourceVersion:14334 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:2e448082-5c6b-4c81-b30a-f4aab87d60f6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 13 01:45:07.023: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-13T01:45:07Z generation:1 name:name2 resourceVersion:14341 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:593b6659-130a-4227-b463-bb6931cdeff4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 13 01:45:17.036: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-13T01:44:57Z generation:2 name:name1 resourceVersion:14347 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:2e448082-5c6b-4c81-b30a-f4aab87d60f6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 13 01:45:27.048: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-13T01:45:07Z generation:2 name:name2 resourceVersion:14355 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:593b6659-130a-4227-b463-bb6931cdeff4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 13 01:45:37.062: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-13T01:44:57Z generation:2 name:name1 resourceVersion:14361 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:2e448082-5c6b-4c81-b30a-f4aab87d60f6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 13 01:45:47.076: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-13T01:45:07Z generation:2 name:name2 resourceVersion:14368 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:593b6659-130a-4227-b463-bb6931cdeff4] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:45:57.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7822" for this suite.
Mar 13 01:46:03.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:46:03.803: INFO: namespace crd-watch-7822 deletion completed in 6.191019042s

• [SLOW TEST:67.524 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:46:03.804: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar 13 01:46:07.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec pod-sharedvolume-708cd902-499a-4e3e-a241-f3440a52451d -c busybox-main-container --namespace=emptydir-1432 -- cat /usr/share/volumeshare/shareddata.txt'
Mar 13 01:46:08.531: INFO: stderr: ""
Mar 13 01:46:08.532: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:46:08.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1432" for this suite.
Mar 13 01:46:14.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:46:14.733: INFO: namespace emptydir-1432 deletion completed in 6.191150156s

• [SLOW TEST:10.929 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:46:14.734: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:46:16.180: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Mar 13 01:46:18.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660776, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660776, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660776, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660776, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:46:21.224: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:46:21.230: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:46:22.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3182" for this suite.
Mar 13 01:46:28.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:46:28.713: INFO: namespace crd-webhook-3182 deletion completed in 6.203396004s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:14.003 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:46:28.738: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-6a336097-44d6-4c1e-abd8-5e14749879f0
Mar 13 01:46:28.815: INFO: Pod name my-hostname-basic-6a336097-44d6-4c1e-abd8-5e14749879f0: Found 0 pods out of 1
Mar 13 01:46:33.823: INFO: Pod name my-hostname-basic-6a336097-44d6-4c1e-abd8-5e14749879f0: Found 1 pods out of 1
Mar 13 01:46:33.823: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6a336097-44d6-4c1e-abd8-5e14749879f0" are running
Mar 13 01:46:33.828: INFO: Pod "my-hostname-basic-6a336097-44d6-4c1e-abd8-5e14749879f0-d6c48" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-13 01:46:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-13 01:46:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-13 01:46:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-13 01:46:28 +0000 UTC Reason: Message:}])
Mar 13 01:46:33.828: INFO: Trying to dial the pod
Mar 13 01:46:38.851: INFO: Controller my-hostname-basic-6a336097-44d6-4c1e-abd8-5e14749879f0: Got expected result from replica 1 [my-hostname-basic-6a336097-44d6-4c1e-abd8-5e14749879f0-d6c48]: "my-hostname-basic-6a336097-44d6-4c1e-abd8-5e14749879f0-d6c48", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:46:38.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-516" for this suite.
Mar 13 01:46:44.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:46:45.038: INFO: namespace replication-controller-516 deletion completed in 6.17833258s

• [SLOW TEST:16.300 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:46:45.039: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 13 01:46:53.208: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 01:46:53.215: INFO: Pod pod-with-poststart-http-hook still exists
Mar 13 01:46:55.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 01:46:55.223: INFO: Pod pod-with-poststart-http-hook still exists
Mar 13 01:46:57.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 01:46:57.224: INFO: Pod pod-with-poststart-http-hook still exists
Mar 13 01:46:59.217: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 01:46:59.224: INFO: Pod pod-with-poststart-http-hook still exists
Mar 13 01:47:01.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 01:47:01.223: INFO: Pod pod-with-poststart-http-hook still exists
Mar 13 01:47:03.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 01:47:03.224: INFO: Pod pod-with-poststart-http-hook still exists
Mar 13 01:47:05.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 01:47:05.223: INFO: Pod pod-with-poststart-http-hook still exists
Mar 13 01:47:07.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 01:47:07.223: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:47:07.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-817" for this suite.
Mar 13 01:47:35.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:47:35.398: INFO: namespace container-lifecycle-hook-817 deletion completed in 28.167710304s

• [SLOW TEST:50.359 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:47:35.399: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-9672f87a-931d-4462-a658-236650c280ea
STEP: Creating a pod to test consume configMaps
Mar 13 01:47:35.486: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b6173541-d535-460f-bf54-7d07d02764e7" in namespace "projected-552" to be "success or failure"
Mar 13 01:47:35.493: INFO: Pod "pod-projected-configmaps-b6173541-d535-460f-bf54-7d07d02764e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.708348ms
Mar 13 01:47:37.499: INFO: Pod "pod-projected-configmaps-b6173541-d535-460f-bf54-7d07d02764e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01294676s
Mar 13 01:47:39.508: INFO: Pod "pod-projected-configmaps-b6173541-d535-460f-bf54-7d07d02764e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021220693s
STEP: Saw pod success
Mar 13 01:47:39.508: INFO: Pod "pod-projected-configmaps-b6173541-d535-460f-bf54-7d07d02764e7" satisfied condition "success or failure"
Mar 13 01:47:39.514: INFO: Trying to get logs from node silbory-nirmata0 pod pod-projected-configmaps-b6173541-d535-460f-bf54-7d07d02764e7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 01:47:39.556: INFO: Waiting for pod pod-projected-configmaps-b6173541-d535-460f-bf54-7d07d02764e7 to disappear
Mar 13 01:47:39.567: INFO: Pod pod-projected-configmaps-b6173541-d535-460f-bf54-7d07d02764e7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:47:39.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-552" for this suite.
Mar 13 01:47:45.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:47:45.792: INFO: namespace projected-552 deletion completed in 6.213441456s

• [SLOW TEST:10.393 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:47:45.793: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:47:47.781: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 01:47:49.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660867, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660867, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660867, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660867, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:47:52.829: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:47:52.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6017" for this suite.
Mar 13 01:47:58.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:47:59.141: INFO: namespace webhook-6017 deletion completed in 6.181363125s
STEP: Destroying namespace "webhook-6017-markers" for this suite.
Mar 13 01:48:05.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:48:05.324: INFO: namespace webhook-6017-markers deletion completed in 6.183326236s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.556 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:48:05.350: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Mar 13 01:48:05.413: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:48:40.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3465" for this suite.
Mar 13 01:48:46.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:48:46.377: INFO: namespace crd-publish-openapi-3465 deletion completed in 6.18473093s

• [SLOW TEST:41.027 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:48:46.378: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-47b72d8b-9e38-4b72-9109-e91cec0c5b9d
STEP: Creating a pod to test consume secrets
Mar 13 01:48:46.495: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bd1cbdb0-9cae-4b3a-8a82-9d27369cf5e9" in namespace "projected-9444" to be "success or failure"
Mar 13 01:48:46.500: INFO: Pod "pod-projected-secrets-bd1cbdb0-9cae-4b3a-8a82-9d27369cf5e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.807206ms
Mar 13 01:48:48.506: INFO: Pod "pod-projected-secrets-bd1cbdb0-9cae-4b3a-8a82-9d27369cf5e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010394048s
Mar 13 01:48:50.512: INFO: Pod "pod-projected-secrets-bd1cbdb0-9cae-4b3a-8a82-9d27369cf5e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016625859s
STEP: Saw pod success
Mar 13 01:48:50.512: INFO: Pod "pod-projected-secrets-bd1cbdb0-9cae-4b3a-8a82-9d27369cf5e9" satisfied condition "success or failure"
Mar 13 01:48:50.517: INFO: Trying to get logs from node silbory-nirmata0 pod pod-projected-secrets-bd1cbdb0-9cae-4b3a-8a82-9d27369cf5e9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 13 01:48:50.558: INFO: Waiting for pod pod-projected-secrets-bd1cbdb0-9cae-4b3a-8a82-9d27369cf5e9 to disappear
Mar 13 01:48:50.563: INFO: Pod pod-projected-secrets-bd1cbdb0-9cae-4b3a-8a82-9d27369cf5e9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:48:50.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9444" for this suite.
Mar 13 01:48:56.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:48:56.768: INFO: namespace projected-9444 deletion completed in 6.194548452s

• [SLOW TEST:10.390 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:48:56.768: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 13 01:48:56.850: INFO: Waiting up to 5m0s for pod "pod-f0ac1738-fbb3-4a5e-b24c-2f79ad107e52" in namespace "emptydir-8694" to be "success or failure"
Mar 13 01:48:56.856: INFO: Pod "pod-f0ac1738-fbb3-4a5e-b24c-2f79ad107e52": Phase="Pending", Reason="", readiness=false. Elapsed: 5.319905ms
Mar 13 01:48:58.862: INFO: Pod "pod-f0ac1738-fbb3-4a5e-b24c-2f79ad107e52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011469882s
Mar 13 01:49:00.868: INFO: Pod "pod-f0ac1738-fbb3-4a5e-b24c-2f79ad107e52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017740354s
STEP: Saw pod success
Mar 13 01:49:00.868: INFO: Pod "pod-f0ac1738-fbb3-4a5e-b24c-2f79ad107e52" satisfied condition "success or failure"
Mar 13 01:49:00.874: INFO: Trying to get logs from node silbory-nirmata0 pod pod-f0ac1738-fbb3-4a5e-b24c-2f79ad107e52 container test-container: <nil>
STEP: delete the pod
Mar 13 01:49:00.917: INFO: Waiting for pod pod-f0ac1738-fbb3-4a5e-b24c-2f79ad107e52 to disappear
Mar 13 01:49:00.926: INFO: Pod pod-f0ac1738-fbb3-4a5e-b24c-2f79ad107e52 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:49:00.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8694" for this suite.
Mar 13 01:49:06.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:49:07.123: INFO: namespace emptydir-8694 deletion completed in 6.190378527s

• [SLOW TEST:10.355 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:49:07.124: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Mar 13 01:49:07.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 cluster-info'
Mar 13 01:49:07.414: INFO: stderr: ""
Mar 13 01:49:07.414: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.10.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.10.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:49:07.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5385" for this suite.
Mar 13 01:49:13.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:49:13.632: INFO: namespace kubectl-5385 deletion completed in 6.209757216s

• [SLOW TEST:6.509 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:49:13.634: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 13 01:49:13.724: INFO: Waiting up to 5m0s for pod "pod-aaad886d-fe28-4063-a463-5cc123640b37" in namespace "emptydir-5634" to be "success or failure"
Mar 13 01:49:13.729: INFO: Pod "pod-aaad886d-fe28-4063-a463-5cc123640b37": Phase="Pending", Reason="", readiness=false. Elapsed: 5.392302ms
Mar 13 01:49:15.737: INFO: Pod "pod-aaad886d-fe28-4063-a463-5cc123640b37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013163287s
Mar 13 01:49:17.743: INFO: Pod "pod-aaad886d-fe28-4063-a463-5cc123640b37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019421353s
STEP: Saw pod success
Mar 13 01:49:17.743: INFO: Pod "pod-aaad886d-fe28-4063-a463-5cc123640b37" satisfied condition "success or failure"
Mar 13 01:49:17.749: INFO: Trying to get logs from node silbory-nirmata0 pod pod-aaad886d-fe28-4063-a463-5cc123640b37 container test-container: <nil>
STEP: delete the pod
Mar 13 01:49:17.792: INFO: Waiting for pod pod-aaad886d-fe28-4063-a463-5cc123640b37 to disappear
Mar 13 01:49:17.799: INFO: Pod pod-aaad886d-fe28-4063-a463-5cc123640b37 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:49:17.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5634" for this suite.
Mar 13 01:49:23.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:49:23.984: INFO: namespace emptydir-5634 deletion completed in 6.175447143s

• [SLOW TEST:10.350 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:49:23.984: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Mar 13 01:49:24.056: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Mar 13 01:49:25.645: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 13 01:49:27.724: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660965, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660965, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660965, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660965, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 01:49:29.730: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660965, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660965, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660965, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660965, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 01:49:31.732: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660965, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660965, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660965, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719660965, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 01:49:35.594: INFO: Waited 1.849036921s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:49:36.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6733" for this suite.
Mar 13 01:49:42.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:49:42.358: INFO: namespace aggregator-6733 deletion completed in 6.255845693s

• [SLOW TEST:18.374 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:49:42.360: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:49:42.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7789" for this suite.
Mar 13 01:49:48.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:49:48.640: INFO: namespace tables-7789 deletion completed in 6.201819742s

• [SLOW TEST:6.281 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:49:48.641: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-3752
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3752 to expose endpoints map[]
Mar 13 01:49:48.724: INFO: Get endpoints failed (5.634511ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar 13 01:49:49.730: INFO: successfully validated that service endpoint-test2 in namespace services-3752 exposes endpoints map[] (1.0115395s elapsed)
STEP: Creating pod pod1 in namespace services-3752
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3752 to expose endpoints map[pod1:[80]]
Mar 13 01:49:52.799: INFO: successfully validated that service endpoint-test2 in namespace services-3752 exposes endpoints map[pod1:[80]] (3.051600799s elapsed)
STEP: Creating pod pod2 in namespace services-3752
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3752 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 13 01:49:55.883: INFO: successfully validated that service endpoint-test2 in namespace services-3752 exposes endpoints map[pod1:[80] pod2:[80]] (3.07546174s elapsed)
STEP: Deleting pod pod1 in namespace services-3752
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3752 to expose endpoints map[pod2:[80]]
Mar 13 01:49:56.929: INFO: successfully validated that service endpoint-test2 in namespace services-3752 exposes endpoints map[pod2:[80]] (1.035898932s elapsed)
STEP: Deleting pod pod2 in namespace services-3752
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3752 to expose endpoints map[]
Mar 13 01:49:57.951: INFO: successfully validated that service endpoint-test2 in namespace services-3752 exposes endpoints map[] (1.013413165s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:49:57.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3752" for this suite.
Mar 13 01:50:04.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:50:04.194: INFO: namespace services-3752 deletion completed in 6.201399458s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:15.553 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:50:04.197: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7698
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-7698
Mar 13 01:50:04.290: INFO: Found 0 stateful pods, waiting for 1
Mar 13 01:50:14.299: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 13 01:50:14.334: INFO: Deleting all statefulset in ns statefulset-7698
Mar 13 01:50:14.360: INFO: Scaling statefulset ss to 0
Mar 13 01:50:44.391: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 01:50:44.396: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:50:44.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7698" for this suite.
Mar 13 01:50:50.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:50:50.583: INFO: namespace statefulset-7698 deletion completed in 6.155721282s

• [SLOW TEST:46.386 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:50:50.585: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-0a35763b-b613-4fcd-b44e-d2c6b860fe93
STEP: Creating a pod to test consume secrets
Mar 13 01:50:50.658: INFO: Waiting up to 5m0s for pod "pod-secrets-ca32881a-bea4-4ba6-b0af-e5ad78b8ff36" in namespace "secrets-286" to be "success or failure"
Mar 13 01:50:50.668: INFO: Pod "pod-secrets-ca32881a-bea4-4ba6-b0af-e5ad78b8ff36": Phase="Pending", Reason="", readiness=false. Elapsed: 9.89508ms
Mar 13 01:50:52.676: INFO: Pod "pod-secrets-ca32881a-bea4-4ba6-b0af-e5ad78b8ff36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018300591s
Mar 13 01:50:54.683: INFO: Pod "pod-secrets-ca32881a-bea4-4ba6-b0af-e5ad78b8ff36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024855408s
STEP: Saw pod success
Mar 13 01:50:54.683: INFO: Pod "pod-secrets-ca32881a-bea4-4ba6-b0af-e5ad78b8ff36" satisfied condition "success or failure"
Mar 13 01:50:54.687: INFO: Trying to get logs from node silbory-nirmata0 pod pod-secrets-ca32881a-bea4-4ba6-b0af-e5ad78b8ff36 container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 01:50:54.737: INFO: Waiting for pod pod-secrets-ca32881a-bea4-4ba6-b0af-e5ad78b8ff36 to disappear
Mar 13 01:50:54.749: INFO: Pod pod-secrets-ca32881a-bea4-4ba6-b0af-e5ad78b8ff36 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:50:54.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-286" for this suite.
Mar 13 01:51:00.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:51:00.953: INFO: namespace secrets-286 deletion completed in 6.190382024s

• [SLOW TEST:10.368 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:51:00.955: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:51:12.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1297" for this suite.
Mar 13 01:51:18.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:51:18.306: INFO: namespace resourcequota-1297 deletion completed in 6.195209799s

• [SLOW TEST:17.351 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:51:18.307: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-7343febf-fb32-4d34-b2af-64ac48a70fa1
STEP: Creating a pod to test consume configMaps
Mar 13 01:51:18.400: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-be8a4bc9-a325-4d97-b7ab-f48ffc75f3ac" in namespace "projected-3241" to be "success or failure"
Mar 13 01:51:18.408: INFO: Pod "pod-projected-configmaps-be8a4bc9-a325-4d97-b7ab-f48ffc75f3ac": Phase="Pending", Reason="", readiness=false. Elapsed: 7.644565ms
Mar 13 01:51:20.416: INFO: Pod "pod-projected-configmaps-be8a4bc9-a325-4d97-b7ab-f48ffc75f3ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015372052s
Mar 13 01:51:22.424: INFO: Pod "pod-projected-configmaps-be8a4bc9-a325-4d97-b7ab-f48ffc75f3ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023400512s
STEP: Saw pod success
Mar 13 01:51:22.424: INFO: Pod "pod-projected-configmaps-be8a4bc9-a325-4d97-b7ab-f48ffc75f3ac" satisfied condition "success or failure"
Mar 13 01:51:22.428: INFO: Trying to get logs from node silbory-nirmata0 pod pod-projected-configmaps-be8a4bc9-a325-4d97-b7ab-f48ffc75f3ac container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 01:51:22.464: INFO: Waiting for pod pod-projected-configmaps-be8a4bc9-a325-4d97-b7ab-f48ffc75f3ac to disappear
Mar 13 01:51:22.480: INFO: Pod pod-projected-configmaps-be8a4bc9-a325-4d97-b7ab-f48ffc75f3ac no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:51:22.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3241" for this suite.
Mar 13 01:51:28.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:51:28.681: INFO: namespace projected-3241 deletion completed in 6.190065874s

• [SLOW TEST:10.375 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:51:28.687: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-9fea0a71-3c43-44c8-9cd7-09c15c2fe0fe
STEP: Creating a pod to test consume secrets
Mar 13 01:51:28.769: INFO: Waiting up to 5m0s for pod "pod-secrets-8fcdcb49-9263-44f4-9302-ed63091dcf28" in namespace "secrets-9493" to be "success or failure"
Mar 13 01:51:28.776: INFO: Pod "pod-secrets-8fcdcb49-9263-44f4-9302-ed63091dcf28": Phase="Pending", Reason="", readiness=false. Elapsed: 7.364558ms
Mar 13 01:51:30.783: INFO: Pod "pod-secrets-8fcdcb49-9263-44f4-9302-ed63091dcf28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013592663s
Mar 13 01:51:32.789: INFO: Pod "pod-secrets-8fcdcb49-9263-44f4-9302-ed63091dcf28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019973643s
STEP: Saw pod success
Mar 13 01:51:32.789: INFO: Pod "pod-secrets-8fcdcb49-9263-44f4-9302-ed63091dcf28" satisfied condition "success or failure"
Mar 13 01:51:32.794: INFO: Trying to get logs from node silbory-nirmata0 pod pod-secrets-8fcdcb49-9263-44f4-9302-ed63091dcf28 container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 01:51:32.833: INFO: Waiting for pod pod-secrets-8fcdcb49-9263-44f4-9302-ed63091dcf28 to disappear
Mar 13 01:51:32.840: INFO: Pod pod-secrets-8fcdcb49-9263-44f4-9302-ed63091dcf28 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:51:32.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9493" for this suite.
Mar 13 01:51:38.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:51:39.025: INFO: namespace secrets-9493 deletion completed in 6.178426813s

• [SLOW TEST:10.339 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:51:39.030: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:51:40.331: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 01:51:42.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661100, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661100, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661100, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661100, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:51:45.373: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:51:45.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4077" for this suite.
Mar 13 01:51:51.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:51:51.685: INFO: namespace webhook-4077 deletion completed in 6.216659953s
STEP: Destroying namespace "webhook-4077-markers" for this suite.
Mar 13 01:51:57.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:51:57.856: INFO: namespace webhook-4077-markers deletion completed in 6.17107525s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.850 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:51:57.881: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-edb5195f-3dd2-42f7-98c7-4184bdce4351 in namespace container-probe-1914
Mar 13 01:52:01.976: INFO: Started pod busybox-edb5195f-3dd2-42f7-98c7-4184bdce4351 in namespace container-probe-1914
STEP: checking the pod's current state and verifying that restartCount is present
Mar 13 01:52:01.981: INFO: Initial restart count of pod busybox-edb5195f-3dd2-42f7-98c7-4184bdce4351 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:56:02.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1914" for this suite.
Mar 13 01:56:08.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:56:09.124: INFO: namespace container-probe-1914 deletion completed in 6.200651853s

• [SLOW TEST:251.244 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:56:09.129: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 13 01:56:09.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-5947'
Mar 13 01:56:09.647: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 13 01:56:09.647: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Mar 13 01:56:11.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete deployment e2e-test-httpd-deployment --namespace=kubectl-5947'
Mar 13 01:56:11.883: INFO: stderr: ""
Mar 13 01:56:11.883: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:56:11.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5947" for this suite.
Mar 13 01:56:39.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:56:40.075: INFO: namespace kubectl-5947 deletion completed in 28.180721794s

• [SLOW TEST:30.947 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:56:40.076: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-8d8ae3fc-bb75-4d54-adb5-4255835a4950
STEP: Creating a pod to test consume configMaps
Mar 13 01:56:40.173: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-757bb3c2-3334-4c43-bfd5-2df0eadbf9df" in namespace "projected-986" to be "success or failure"
Mar 13 01:56:40.178: INFO: Pod "pod-projected-configmaps-757bb3c2-3334-4c43-bfd5-2df0eadbf9df": Phase="Pending", Reason="", readiness=false. Elapsed: 5.449782ms
Mar 13 01:56:42.185: INFO: Pod "pod-projected-configmaps-757bb3c2-3334-4c43-bfd5-2df0eadbf9df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012095367s
Mar 13 01:56:44.193: INFO: Pod "pod-projected-configmaps-757bb3c2-3334-4c43-bfd5-2df0eadbf9df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019817107s
STEP: Saw pod success
Mar 13 01:56:44.193: INFO: Pod "pod-projected-configmaps-757bb3c2-3334-4c43-bfd5-2df0eadbf9df" satisfied condition "success or failure"
Mar 13 01:56:44.198: INFO: Trying to get logs from node silbory-nirmata0 pod pod-projected-configmaps-757bb3c2-3334-4c43-bfd5-2df0eadbf9df container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 01:56:44.255: INFO: Waiting for pod pod-projected-configmaps-757bb3c2-3334-4c43-bfd5-2df0eadbf9df to disappear
Mar 13 01:56:44.265: INFO: Pod pod-projected-configmaps-757bb3c2-3334-4c43-bfd5-2df0eadbf9df no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:56:44.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-986" for this suite.
Mar 13 01:56:50.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:56:50.454: INFO: namespace projected-986 deletion completed in 6.182138187s

• [SLOW TEST:10.378 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:56:50.455: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 01:56:50.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20e621c9-d8cb-4a5c-8032-62638f49055a" in namespace "projected-1861" to be "success or failure"
Mar 13 01:56:50.541: INFO: Pod "downwardapi-volume-20e621c9-d8cb-4a5c-8032-62638f49055a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.586979ms
Mar 13 01:56:52.547: INFO: Pod "downwardapi-volume-20e621c9-d8cb-4a5c-8032-62638f49055a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011142066s
Mar 13 01:56:54.554: INFO: Pod "downwardapi-volume-20e621c9-d8cb-4a5c-8032-62638f49055a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017640461s
STEP: Saw pod success
Mar 13 01:56:54.554: INFO: Pod "downwardapi-volume-20e621c9-d8cb-4a5c-8032-62638f49055a" satisfied condition "success or failure"
Mar 13 01:56:54.560: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-20e621c9-d8cb-4a5c-8032-62638f49055a container client-container: <nil>
STEP: delete the pod
Mar 13 01:56:54.592: INFO: Waiting for pod downwardapi-volume-20e621c9-d8cb-4a5c-8032-62638f49055a to disappear
Mar 13 01:56:54.600: INFO: Pod downwardapi-volume-20e621c9-d8cb-4a5c-8032-62638f49055a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:56:54.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1861" for this suite.
Mar 13 01:57:00.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:57:00.783: INFO: namespace projected-1861 deletion completed in 6.17508879s

• [SLOW TEST:10.328 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:57:00.783: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Mar 13 01:57:04.885: INFO: Pod pod-hostip-9212449b-cdc5-44f5-86db-51fc20644e77 has hostIP: 10.10.1.79
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:57:04.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7329" for this suite.
Mar 13 01:57:16.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:57:17.059: INFO: namespace pods-7329 deletion completed in 12.167297529s

• [SLOW TEST:16.276 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:57:17.060: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:57:17.125: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 13 01:57:19.179: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:57:20.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1579" for this suite.
Mar 13 01:57:26.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:57:26.403: INFO: namespace replication-controller-1579 deletion completed in 6.204895366s

• [SLOW TEST:9.344 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:57:26.404: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:57:28.855: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 01:57:30.872: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661448, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661448, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661448, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661448, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:57:33.893: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 13 01:57:37.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 attach --namespace=webhook-8834 to-be-attached-pod -i -c=container1'
Mar 13 01:57:38.247: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:57:38.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8834" for this suite.
Mar 13 01:57:50.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:57:50.451: INFO: namespace webhook-8834 deletion completed in 12.181583564s
STEP: Destroying namespace "webhook-8834-markers" for this suite.
Mar 13 01:57:56.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:57:56.627: INFO: namespace webhook-8834-markers deletion completed in 6.175819052s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.248 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:57:56.652: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:57:58.699: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar 13 01:58:00.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661478, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661478, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661478, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661478, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:58:03.740: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:58:03.747: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:58:05.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5700" for this suite.
Mar 13 01:58:11.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:58:11.479: INFO: namespace crd-webhook-5700 deletion completed in 6.218822938s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:14.862 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:58:11.514: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4590
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4590
STEP: creating replication controller externalsvc in namespace services-4590
I0313 01:58:11.655119      25 runners.go:184] Created replication controller with name: externalsvc, namespace: services-4590, replica count: 2
I0313 01:58:14.706075      25 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 13 01:58:14.745: INFO: Creating new exec pod
Mar 13 01:58:18.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=services-4590 execpods5hzr -- /bin/sh -x -c nslookup nodeport-service'
Mar 13 01:58:19.358: INFO: stderr: "+ nslookup nodeport-service\n"
Mar 13 01:58:19.358: INFO: stdout: "Server:\t\t10.10.0.10\nAddress:\t10.10.0.10#53\n\nnodeport-service.services-4590.svc.cluster.local\tcanonical name = externalsvc.services-4590.svc.cluster.local.\nName:\texternalsvc.services-4590.svc.cluster.local\nAddress: 10.10.8.39\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4590, will wait for the garbage collector to delete the pods
Mar 13 01:58:19.428: INFO: Deleting ReplicationController externalsvc took: 14.046993ms
Mar 13 01:58:19.728: INFO: Terminating ReplicationController externalsvc pods took: 300.531614ms
Mar 13 01:58:26.161: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:58:26.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4590" for this suite.
Mar 13 01:58:32.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:58:32.404: INFO: namespace services-4590 deletion completed in 6.206660374s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:20.890 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:58:32.405: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:58:34.410: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 01:58:36.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661514, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661514, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661514, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661514, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:58:39.459: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:58:51.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2333" for this suite.
Mar 13 01:58:57.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:58:57.961: INFO: namespace webhook-2333 deletion completed in 6.248863068s
STEP: Destroying namespace "webhook-2333-markers" for this suite.
Mar 13 01:59:03.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:59:04.172: INFO: namespace webhook-2333-markers deletion completed in 6.210511077s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:31.792 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:59:04.197: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Mar 13 01:59:04.804: INFO: created pod pod-service-account-defaultsa
Mar 13 01:59:04.804: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 13 01:59:04.813: INFO: created pod pod-service-account-mountsa
Mar 13 01:59:04.813: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 13 01:59:04.824: INFO: created pod pod-service-account-nomountsa
Mar 13 01:59:04.824: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 13 01:59:04.841: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 13 01:59:04.841: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 13 01:59:04.865: INFO: created pod pod-service-account-mountsa-mountspec
Mar 13 01:59:04.865: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 13 01:59:04.891: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 13 01:59:04.891: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 13 01:59:04.925: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 13 01:59:04.926: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 13 01:59:04.936: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 13 01:59:04.936: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 13 01:59:04.945: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 13 01:59:04.945: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:59:04.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7894" for this suite.
Mar 13 01:59:10.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:59:11.139: INFO: namespace svcaccounts-7894 deletion completed in 6.180539683s

• [SLOW TEST:6.941 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:59:11.142: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 01:59:13.402: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 01:59:15.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661553, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661553, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661553, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661553, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 01:59:18.444: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:59:18.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8963" for this suite.
Mar 13 01:59:24.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:59:24.796: INFO: namespace webhook-8963 deletion completed in 6.211216734s
STEP: Destroying namespace "webhook-8963-markers" for this suite.
Mar 13 01:59:30.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:59:30.987: INFO: namespace webhook-8963-markers deletion completed in 6.1901593s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.874 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:59:31.016: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 13 01:59:34.135: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:59:34.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6629" for this suite.
Mar 13 01:59:40.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:59:40.377: INFO: namespace container-runtime-6629 deletion completed in 6.188526879s

• [SLOW TEST:9.361 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:59:40.378: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Mar 13 01:59:40.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-4836'
Mar 13 01:59:40.986: INFO: stderr: ""
Mar 13 01:59:40.986: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 13 01:59:40.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4836'
Mar 13 01:59:41.260: INFO: stderr: ""
Mar 13 01:59:41.260: INFO: stdout: "update-demo-nautilus-8dcmk update-demo-nautilus-vhjqj "
Mar 13 01:59:41.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-8dcmk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4836'
Mar 13 01:59:41.490: INFO: stderr: ""
Mar 13 01:59:41.490: INFO: stdout: ""
Mar 13 01:59:41.490: INFO: update-demo-nautilus-8dcmk is created but not running
Mar 13 01:59:46.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4836'
Mar 13 01:59:46.710: INFO: stderr: ""
Mar 13 01:59:46.710: INFO: stdout: "update-demo-nautilus-8dcmk update-demo-nautilus-vhjqj "
Mar 13 01:59:46.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-8dcmk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4836'
Mar 13 01:59:46.917: INFO: stderr: ""
Mar 13 01:59:46.917: INFO: stdout: "true"
Mar 13 01:59:46.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-8dcmk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4836'
Mar 13 01:59:47.138: INFO: stderr: ""
Mar 13 01:59:47.138: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 01:59:47.138: INFO: validating pod update-demo-nautilus-8dcmk
Mar 13 01:59:47.149: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 01:59:47.149: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 01:59:47.149: INFO: update-demo-nautilus-8dcmk is verified up and running
Mar 13 01:59:47.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-vhjqj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4836'
Mar 13 01:59:47.363: INFO: stderr: ""
Mar 13 01:59:47.363: INFO: stdout: "true"
Mar 13 01:59:47.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-vhjqj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4836'
Mar 13 01:59:47.570: INFO: stderr: ""
Mar 13 01:59:47.570: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 01:59:47.571: INFO: validating pod update-demo-nautilus-vhjqj
Mar 13 01:59:47.579: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 01:59:47.579: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 01:59:47.579: INFO: update-demo-nautilus-vhjqj is verified up and running
STEP: using delete to clean up resources
Mar 13 01:59:47.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete --grace-period=0 --force -f - --namespace=kubectl-4836'
Mar 13 01:59:47.805: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 01:59:47.805: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 13 01:59:47.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4836'
Mar 13 01:59:48.049: INFO: stderr: "No resources found in kubectl-4836 namespace.\n"
Mar 13 01:59:48.049: INFO: stdout: ""
Mar 13 01:59:48.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -l name=update-demo --namespace=kubectl-4836 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 13 01:59:48.302: INFO: stderr: ""
Mar 13 01:59:48.302: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:59:48.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4836" for this suite.
Mar 13 01:59:54.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 01:59:54.498: INFO: namespace kubectl-4836 deletion completed in 6.180716892s

• [SLOW TEST:14.120 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 01:59:54.499: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 01:59:54.587: INFO: Waiting up to 5m0s for pod "busybox-user-65534-cfb60c09-68cb-4ee7-a969-84a829fb801f" in namespace "security-context-test-9513" to be "success or failure"
Mar 13 01:59:54.605: INFO: Pod "busybox-user-65534-cfb60c09-68cb-4ee7-a969-84a829fb801f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.681959ms
Mar 13 01:59:56.612: INFO: Pod "busybox-user-65534-cfb60c09-68cb-4ee7-a969-84a829fb801f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024244637s
Mar 13 01:59:58.618: INFO: Pod "busybox-user-65534-cfb60c09-68cb-4ee7-a969-84a829fb801f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030387367s
Mar 13 01:59:58.618: INFO: Pod "busybox-user-65534-cfb60c09-68cb-4ee7-a969-84a829fb801f" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 01:59:58.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9513" for this suite.
Mar 13 02:00:04.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:00:04.782: INFO: namespace security-context-test-9513 deletion completed in 6.156025474s

• [SLOW TEST:10.283 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:00:04.782: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Mar 13 02:00:04.859: INFO: Waiting up to 5m0s for pod "client-containers-5e6810e3-7bd3-465e-b59c-8bd914e251a1" in namespace "containers-9782" to be "success or failure"
Mar 13 02:00:04.864: INFO: Pod "client-containers-5e6810e3-7bd3-465e-b59c-8bd914e251a1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.437883ms
Mar 13 02:00:06.874: INFO: Pod "client-containers-5e6810e3-7bd3-465e-b59c-8bd914e251a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015508784s
Mar 13 02:00:08.881: INFO: Pod "client-containers-5e6810e3-7bd3-465e-b59c-8bd914e251a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021785424s
STEP: Saw pod success
Mar 13 02:00:08.881: INFO: Pod "client-containers-5e6810e3-7bd3-465e-b59c-8bd914e251a1" satisfied condition "success or failure"
Mar 13 02:00:08.888: INFO: Trying to get logs from node silbory-nirmata0 pod client-containers-5e6810e3-7bd3-465e-b59c-8bd914e251a1 container test-container: <nil>
STEP: delete the pod
Mar 13 02:00:08.953: INFO: Waiting for pod client-containers-5e6810e3-7bd3-465e-b59c-8bd914e251a1 to disappear
Mar 13 02:00:08.963: INFO: Pod client-containers-5e6810e3-7bd3-465e-b59c-8bd914e251a1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:00:08.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9782" for this suite.
Mar 13 02:00:14.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:00:15.153: INFO: namespace containers-9782 deletion completed in 6.181080729s

• [SLOW TEST:10.371 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:00:15.156: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 02:00:15.246: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c88aa645-829d-454f-9ea9-475b8c68fb75" in namespace "projected-5890" to be "success or failure"
Mar 13 02:00:15.252: INFO: Pod "downwardapi-volume-c88aa645-829d-454f-9ea9-475b8c68fb75": Phase="Pending", Reason="", readiness=false. Elapsed: 6.192736ms
Mar 13 02:00:17.261: INFO: Pod "downwardapi-volume-c88aa645-829d-454f-9ea9-475b8c68fb75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014435432s
Mar 13 02:00:19.267: INFO: Pod "downwardapi-volume-c88aa645-829d-454f-9ea9-475b8c68fb75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021103296s
STEP: Saw pod success
Mar 13 02:00:19.267: INFO: Pod "downwardapi-volume-c88aa645-829d-454f-9ea9-475b8c68fb75" satisfied condition "success or failure"
Mar 13 02:00:19.273: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-c88aa645-829d-454f-9ea9-475b8c68fb75 container client-container: <nil>
STEP: delete the pod
Mar 13 02:00:19.314: INFO: Waiting for pod downwardapi-volume-c88aa645-829d-454f-9ea9-475b8c68fb75 to disappear
Mar 13 02:00:19.321: INFO: Pod downwardapi-volume-c88aa645-829d-454f-9ea9-475b8c68fb75 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:00:19.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5890" for this suite.
Mar 13 02:00:25.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:00:25.512: INFO: namespace projected-5890 deletion completed in 6.183539681s

• [SLOW TEST:10.356 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:00:25.512: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 13 02:00:25.598: INFO: Waiting up to 5m0s for pod "pod-e7637fcb-6b99-4529-9854-ef3cd24c154d" in namespace "emptydir-1823" to be "success or failure"
Mar 13 02:00:25.605: INFO: Pod "pod-e7637fcb-6b99-4529-9854-ef3cd24c154d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.04857ms
Mar 13 02:00:27.613: INFO: Pod "pod-e7637fcb-6b99-4529-9854-ef3cd24c154d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015142604s
Mar 13 02:00:29.620: INFO: Pod "pod-e7637fcb-6b99-4529-9854-ef3cd24c154d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021975392s
STEP: Saw pod success
Mar 13 02:00:29.620: INFO: Pod "pod-e7637fcb-6b99-4529-9854-ef3cd24c154d" satisfied condition "success or failure"
Mar 13 02:00:29.626: INFO: Trying to get logs from node silbory-nirmata0 pod pod-e7637fcb-6b99-4529-9854-ef3cd24c154d container test-container: <nil>
STEP: delete the pod
Mar 13 02:00:29.663: INFO: Waiting for pod pod-e7637fcb-6b99-4529-9854-ef3cd24c154d to disappear
Mar 13 02:00:29.670: INFO: Pod pod-e7637fcb-6b99-4529-9854-ef3cd24c154d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:00:29.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1823" for this suite.
Mar 13 02:00:35.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:00:35.853: INFO: namespace emptydir-1823 deletion completed in 6.175886527s

• [SLOW TEST:10.341 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:00:35.854: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-s8jkt in namespace proxy-4734
I0313 02:00:35.952791      25 runners.go:184] Created replication controller with name: proxy-service-s8jkt, namespace: proxy-4734, replica count: 1
I0313 02:00:37.003654      25 runners.go:184] proxy-service-s8jkt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0313 02:00:38.004073      25 runners.go:184] proxy-service-s8jkt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0313 02:00:39.004467      25 runners.go:184] proxy-service-s8jkt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0313 02:00:40.004872      25 runners.go:184] proxy-service-s8jkt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0313 02:00:41.005219      25 runners.go:184] proxy-service-s8jkt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0313 02:00:42.005611      25 runners.go:184] proxy-service-s8jkt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0313 02:00:43.006247      25 runners.go:184] proxy-service-s8jkt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0313 02:00:44.006592      25 runners.go:184] proxy-service-s8jkt Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 13 02:00:44.014: INFO: setup took 8.087901501s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 13 02:00:44.025: INFO: (0) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 11.477528ms)
Mar 13 02:00:44.046: INFO: (0) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 32.019206ms)
Mar 13 02:00:44.046: INFO: (0) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 31.791645ms)
Mar 13 02:00:44.050: INFO: (0) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 35.679358ms)
Mar 13 02:00:44.050: INFO: (0) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 35.574795ms)
Mar 13 02:00:44.050: INFO: (0) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 35.947803ms)
Mar 13 02:00:44.051: INFO: (0) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 36.517687ms)
Mar 13 02:00:44.053: INFO: (0) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 38.049729ms)
Mar 13 02:00:44.053: INFO: (0) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 39.154826ms)
Mar 13 02:00:44.055: INFO: (0) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 40.796209ms)
Mar 13 02:00:44.055: INFO: (0) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 41.264188ms)
Mar 13 02:00:44.056: INFO: (0) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 41.553529ms)
Mar 13 02:00:44.057: INFO: (0) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 42.408518ms)
Mar 13 02:00:44.058: INFO: (0) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 43.435924ms)
Mar 13 02:00:44.060: INFO: (0) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 45.218752ms)
Mar 13 02:00:44.067: INFO: (0) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 53.159124ms)
Mar 13 02:00:44.081: INFO: (1) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 13.742413ms)
Mar 13 02:00:44.084: INFO: (1) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 15.398976ms)
Mar 13 02:00:44.085: INFO: (1) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 16.939434ms)
Mar 13 02:00:44.089: INFO: (1) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 21.092431ms)
Mar 13 02:00:44.090: INFO: (1) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 21.875207ms)
Mar 13 02:00:44.092: INFO: (1) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 23.557834ms)
Mar 13 02:00:44.094: INFO: (1) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 26.075448ms)
Mar 13 02:00:44.095: INFO: (1) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 26.969479ms)
Mar 13 02:00:44.096: INFO: (1) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 27.369696ms)
Mar 13 02:00:44.097: INFO: (1) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 28.620946ms)
Mar 13 02:00:44.097: INFO: (1) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 29.031684ms)
Mar 13 02:00:44.097: INFO: (1) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 29.524897ms)
Mar 13 02:00:44.098: INFO: (1) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 29.335341ms)
Mar 13 02:00:44.099: INFO: (1) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 30.857474ms)
Mar 13 02:00:44.099: INFO: (1) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 31.20455ms)
Mar 13 02:00:44.100: INFO: (1) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 31.944133ms)
Mar 13 02:00:44.121: INFO: (2) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 20.650146ms)
Mar 13 02:00:44.121: INFO: (2) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 20.994665ms)
Mar 13 02:00:44.122: INFO: (2) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 21.279375ms)
Mar 13 02:00:44.124: INFO: (2) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 23.594645ms)
Mar 13 02:00:44.125: INFO: (2) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 24.774581ms)
Mar 13 02:00:44.125: INFO: (2) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 25.334005ms)
Mar 13 02:00:44.125: INFO: (2) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 24.912339ms)
Mar 13 02:00:44.130: INFO: (2) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 30.075765ms)
Mar 13 02:00:44.131: INFO: (2) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 30.695788ms)
Mar 13 02:00:44.131: INFO: (2) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 31.202008ms)
Mar 13 02:00:44.131: INFO: (2) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 30.811512ms)
Mar 13 02:00:44.131: INFO: (2) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 31.011436ms)
Mar 13 02:00:44.131: INFO: (2) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 31.03851ms)
Mar 13 02:00:44.132: INFO: (2) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 31.658366ms)
Mar 13 02:00:44.132: INFO: (2) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 32.161908ms)
Mar 13 02:00:44.133: INFO: (2) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 32.580181ms)
Mar 13 02:00:44.146: INFO: (3) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 12.523014ms)
Mar 13 02:00:44.150: INFO: (3) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 16.837997ms)
Mar 13 02:00:44.151: INFO: (3) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 17.75703ms)
Mar 13 02:00:44.163: INFO: (3) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 29.448498ms)
Mar 13 02:00:44.163: INFO: (3) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 29.509407ms)
Mar 13 02:00:44.163: INFO: (3) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 29.551109ms)
Mar 13 02:00:44.163: INFO: (3) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 29.905652ms)
Mar 13 02:00:44.165: INFO: (3) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 30.883771ms)
Mar 13 02:00:44.165: INFO: (3) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 31.03026ms)
Mar 13 02:00:44.165: INFO: (3) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 31.007763ms)
Mar 13 02:00:44.165: INFO: (3) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 31.714747ms)
Mar 13 02:00:44.165: INFO: (3) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 31.005385ms)
Mar 13 02:00:44.165: INFO: (3) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 31.468803ms)
Mar 13 02:00:44.165: INFO: (3) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 31.656479ms)
Mar 13 02:00:44.165: INFO: (3) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 31.930173ms)
Mar 13 02:00:44.167: INFO: (3) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 34.095104ms)
Mar 13 02:00:44.183: INFO: (4) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 14.888646ms)
Mar 13 02:00:44.183: INFO: (4) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 15.56214ms)
Mar 13 02:00:44.185: INFO: (4) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 17.071198ms)
Mar 13 02:00:44.186: INFO: (4) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 18.790275ms)
Mar 13 02:00:44.189: INFO: (4) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 21.124007ms)
Mar 13 02:00:44.189: INFO: (4) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 21.001838ms)
Mar 13 02:00:44.189: INFO: (4) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 21.012958ms)
Mar 13 02:00:44.189: INFO: (4) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 21.303582ms)
Mar 13 02:00:44.190: INFO: (4) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 22.082435ms)
Mar 13 02:00:44.194: INFO: (4) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 25.626046ms)
Mar 13 02:00:44.198: INFO: (4) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 29.877881ms)
Mar 13 02:00:44.198: INFO: (4) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 29.944982ms)
Mar 13 02:00:44.198: INFO: (4) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 29.954669ms)
Mar 13 02:00:44.198: INFO: (4) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 30.113512ms)
Mar 13 02:00:44.199: INFO: (4) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 31.499062ms)
Mar 13 02:00:44.200: INFO: (4) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 32.234371ms)
Mar 13 02:00:44.210: INFO: (5) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 8.870059ms)
Mar 13 02:00:44.215: INFO: (5) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 12.867967ms)
Mar 13 02:00:44.216: INFO: (5) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 14.303566ms)
Mar 13 02:00:44.216: INFO: (5) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 14.098276ms)
Mar 13 02:00:44.217: INFO: (5) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 13.557141ms)
Mar 13 02:00:44.217: INFO: (5) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 14.936105ms)
Mar 13 02:00:44.217: INFO: (5) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 12.973161ms)
Mar 13 02:00:44.216: INFO: (5) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 13.698837ms)
Mar 13 02:00:44.217: INFO: (5) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 16.06734ms)
Mar 13 02:00:44.221: INFO: (5) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 18.692976ms)
Mar 13 02:00:44.221: INFO: (5) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 20.948083ms)
Mar 13 02:00:44.226: INFO: (5) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 22.886762ms)
Mar 13 02:00:44.226: INFO: (5) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 22.989475ms)
Mar 13 02:00:44.226: INFO: (5) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 22.509272ms)
Mar 13 02:00:44.228: INFO: (5) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 25.973266ms)
Mar 13 02:00:44.229: INFO: (5) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 28.330016ms)
Mar 13 02:00:44.246: INFO: (6) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 16.588103ms)
Mar 13 02:00:44.247: INFO: (6) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 17.482181ms)
Mar 13 02:00:44.247: INFO: (6) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 17.132883ms)
Mar 13 02:00:44.248: INFO: (6) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 18.133619ms)
Mar 13 02:00:44.248: INFO: (6) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 17.807632ms)
Mar 13 02:00:44.248: INFO: (6) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 18.657632ms)
Mar 13 02:00:44.252: INFO: (6) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 21.41234ms)
Mar 13 02:00:44.252: INFO: (6) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 21.830687ms)
Mar 13 02:00:44.253: INFO: (6) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 22.441167ms)
Mar 13 02:00:44.253: INFO: (6) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 22.470013ms)
Mar 13 02:00:44.253: INFO: (6) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 23.463321ms)
Mar 13 02:00:44.254: INFO: (6) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 23.822301ms)
Mar 13 02:00:44.255: INFO: (6) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 24.996381ms)
Mar 13 02:00:44.255: INFO: (6) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 24.973661ms)
Mar 13 02:00:44.256: INFO: (6) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 25.814263ms)
Mar 13 02:00:44.257: INFO: (6) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 26.690769ms)
Mar 13 02:00:44.267: INFO: (7) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 9.817447ms)
Mar 13 02:00:44.275: INFO: (7) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 17.932322ms)
Mar 13 02:00:44.277: INFO: (7) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 18.815757ms)
Mar 13 02:00:44.277: INFO: (7) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 19.383972ms)
Mar 13 02:00:44.278: INFO: (7) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 20.664252ms)
Mar 13 02:00:44.278: INFO: (7) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 21.255111ms)
Mar 13 02:00:44.280: INFO: (7) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 22.570651ms)
Mar 13 02:00:44.281: INFO: (7) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 23.177411ms)
Mar 13 02:00:44.281: INFO: (7) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 23.857696ms)
Mar 13 02:00:44.282: INFO: (7) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 24.471816ms)
Mar 13 02:00:44.282: INFO: (7) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 24.454609ms)
Mar 13 02:00:44.283: INFO: (7) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 25.67701ms)
Mar 13 02:00:44.284: INFO: (7) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 26.394833ms)
Mar 13 02:00:44.284: INFO: (7) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 26.173754ms)
Mar 13 02:00:44.285: INFO: (7) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 27.111165ms)
Mar 13 02:00:44.285: INFO: (7) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 27.927146ms)
Mar 13 02:00:44.298: INFO: (8) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 12.601222ms)
Mar 13 02:00:44.299: INFO: (8) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 14.044436ms)
Mar 13 02:00:44.308: INFO: (8) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 22.421951ms)
Mar 13 02:00:44.308: INFO: (8) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 22.140331ms)
Mar 13 02:00:44.308: INFO: (8) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 22.51258ms)
Mar 13 02:00:44.308: INFO: (8) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 23.084311ms)
Mar 13 02:00:44.310: INFO: (8) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 23.919034ms)
Mar 13 02:00:44.310: INFO: (8) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 24.275617ms)
Mar 13 02:00:44.312: INFO: (8) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 25.801513ms)
Mar 13 02:00:44.313: INFO: (8) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 26.954304ms)
Mar 13 02:00:44.313: INFO: (8) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 27.131165ms)
Mar 13 02:00:44.314: INFO: (8) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 27.896756ms)
Mar 13 02:00:44.314: INFO: (8) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 28.690472ms)
Mar 13 02:00:44.314: INFO: (8) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 28.786215ms)
Mar 13 02:00:44.314: INFO: (8) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 28.528184ms)
Mar 13 02:00:44.315: INFO: (8) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 28.982072ms)
Mar 13 02:00:44.324: INFO: (9) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 9.464795ms)
Mar 13 02:00:44.326: INFO: (9) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 10.473197ms)
Mar 13 02:00:44.327: INFO: (9) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 11.21898ms)
Mar 13 02:00:44.330: INFO: (9) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 14.340788ms)
Mar 13 02:00:44.340: INFO: (9) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 22.626313ms)
Mar 13 02:00:44.344: INFO: (9) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 28.317867ms)
Mar 13 02:00:44.344: INFO: (9) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 26.523687ms)
Mar 13 02:00:44.345: INFO: (9) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 29.056623ms)
Mar 13 02:00:44.346: INFO: (9) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 27.6165ms)
Mar 13 02:00:44.346: INFO: (9) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 28.994742ms)
Mar 13 02:00:44.347: INFO: (9) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 29.79223ms)
Mar 13 02:00:44.347: INFO: (9) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 29.582458ms)
Mar 13 02:00:44.348: INFO: (9) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 30.226492ms)
Mar 13 02:00:44.349: INFO: (9) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 31.926477ms)
Mar 13 02:00:44.349: INFO: (9) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 32.496189ms)
Mar 13 02:00:44.349: INFO: (9) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 30.810327ms)
Mar 13 02:00:44.362: INFO: (10) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 13.153249ms)
Mar 13 02:00:44.363: INFO: (10) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 13.895083ms)
Mar 13 02:00:44.365: INFO: (10) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 15.523787ms)
Mar 13 02:00:44.367: INFO: (10) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 14.604148ms)
Mar 13 02:00:44.367: INFO: (10) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 15.347125ms)
Mar 13 02:00:44.367: INFO: (10) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 17.26407ms)
Mar 13 02:00:44.367: INFO: (10) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 17.058728ms)
Mar 13 02:00:44.368: INFO: (10) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 15.558211ms)
Mar 13 02:00:44.369: INFO: (10) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 17.281788ms)
Mar 13 02:00:44.370: INFO: (10) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 20.075529ms)
Mar 13 02:00:44.374: INFO: (10) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 22.251641ms)
Mar 13 02:00:44.376: INFO: (10) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 25.310179ms)
Mar 13 02:00:44.377: INFO: (10) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 24.583708ms)
Mar 13 02:00:44.379: INFO: (10) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 28.319665ms)
Mar 13 02:00:44.379: INFO: (10) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 28.213193ms)
Mar 13 02:00:44.379: INFO: (10) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 27.029921ms)
Mar 13 02:00:44.391: INFO: (11) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 9.77388ms)
Mar 13 02:00:44.393: INFO: (11) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 12.38414ms)
Mar 13 02:00:44.394: INFO: (11) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 12.828594ms)
Mar 13 02:00:44.400: INFO: (11) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 19.011506ms)
Mar 13 02:00:44.401: INFO: (11) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 20.065064ms)
Mar 13 02:00:44.401: INFO: (11) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 19.868361ms)
Mar 13 02:00:44.401: INFO: (11) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 21.121946ms)
Mar 13 02:00:44.401: INFO: (11) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 21.193369ms)
Mar 13 02:00:44.402: INFO: (11) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 22.099697ms)
Mar 13 02:00:44.403: INFO: (11) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 22.880992ms)
Mar 13 02:00:44.403: INFO: (11) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 22.723589ms)
Mar 13 02:00:44.403: INFO: (11) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 23.319638ms)
Mar 13 02:00:44.405: INFO: (11) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 24.046504ms)
Mar 13 02:00:44.405: INFO: (11) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 24.243824ms)
Mar 13 02:00:44.406: INFO: (11) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 24.607027ms)
Mar 13 02:00:44.406: INFO: (11) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 25.008474ms)
Mar 13 02:00:44.414: INFO: (12) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 7.541693ms)
Mar 13 02:00:44.416: INFO: (12) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 9.206696ms)
Mar 13 02:00:44.422: INFO: (12) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 12.034644ms)
Mar 13 02:00:44.422: INFO: (12) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 15.430073ms)
Mar 13 02:00:44.422: INFO: (12) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 15.146362ms)
Mar 13 02:00:44.422: INFO: (12) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 13.163563ms)
Mar 13 02:00:44.425: INFO: (12) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 16.090718ms)
Mar 13 02:00:44.425: INFO: (12) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 17.262205ms)
Mar 13 02:00:44.426: INFO: (12) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 17.87503ms)
Mar 13 02:00:44.427: INFO: (12) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 18.427309ms)
Mar 13 02:00:44.427: INFO: (12) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 18.02226ms)
Mar 13 02:00:44.427: INFO: (12) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 18.444453ms)
Mar 13 02:00:44.430: INFO: (12) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 21.826267ms)
Mar 13 02:00:44.431: INFO: (12) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 22.970335ms)
Mar 13 02:00:44.432: INFO: (12) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 24.163583ms)
Mar 13 02:00:44.432: INFO: (12) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 25.53153ms)
Mar 13 02:00:44.439: INFO: (13) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 6.238813ms)
Mar 13 02:00:44.442: INFO: (13) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 8.709319ms)
Mar 13 02:00:44.444: INFO: (13) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 11.020916ms)
Mar 13 02:00:44.446: INFO: (13) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 12.755647ms)
Mar 13 02:00:44.447: INFO: (13) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 12.684548ms)
Mar 13 02:00:44.452: INFO: (13) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 18.750892ms)
Mar 13 02:00:44.453: INFO: (13) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 19.446233ms)
Mar 13 02:00:44.454: INFO: (13) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 19.859456ms)
Mar 13 02:00:44.455: INFO: (13) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 21.130233ms)
Mar 13 02:00:44.457: INFO: (13) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 22.831872ms)
Mar 13 02:00:44.458: INFO: (13) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 25.113962ms)
Mar 13 02:00:44.458: INFO: (13) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 23.924381ms)
Mar 13 02:00:44.458: INFO: (13) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 25.765055ms)
Mar 13 02:00:44.461: INFO: (13) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 28.235009ms)
Mar 13 02:00:44.463: INFO: (13) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 29.598774ms)
Mar 13 02:00:44.463: INFO: (13) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 29.537431ms)
Mar 13 02:00:44.474: INFO: (14) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 10.693357ms)
Mar 13 02:00:44.476: INFO: (14) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 12.339284ms)
Mar 13 02:00:44.486: INFO: (14) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 21.744463ms)
Mar 13 02:00:44.486: INFO: (14) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 22.199547ms)
Mar 13 02:00:44.491: INFO: (14) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 27.171571ms)
Mar 13 02:00:44.491: INFO: (14) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 27.487716ms)
Mar 13 02:00:44.491: INFO: (14) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 27.79289ms)
Mar 13 02:00:44.492: INFO: (14) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 28.054125ms)
Mar 13 02:00:44.492: INFO: (14) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 27.775281ms)
Mar 13 02:00:44.492: INFO: (14) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 28.161705ms)
Mar 13 02:00:44.492: INFO: (14) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 28.484982ms)
Mar 13 02:00:44.493: INFO: (14) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 29.351741ms)
Mar 13 02:00:44.499: INFO: (14) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 34.878385ms)
Mar 13 02:00:44.500: INFO: (14) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 35.653189ms)
Mar 13 02:00:44.501: INFO: (14) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 36.934807ms)
Mar 13 02:00:44.501: INFO: (14) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 37.444975ms)
Mar 13 02:00:44.513: INFO: (15) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 11.113611ms)
Mar 13 02:00:44.516: INFO: (15) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 14.277371ms)
Mar 13 02:00:44.516: INFO: (15) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 14.261993ms)
Mar 13 02:00:44.522: INFO: (15) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 20.841919ms)
Mar 13 02:00:44.522: INFO: (15) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 20.007431ms)
Mar 13 02:00:44.523: INFO: (15) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 22.044125ms)
Mar 13 02:00:44.526: INFO: (15) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 23.779916ms)
Mar 13 02:00:44.526: INFO: (15) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 24.166509ms)
Mar 13 02:00:44.526: INFO: (15) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 24.365235ms)
Mar 13 02:00:44.526: INFO: (15) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 24.338574ms)
Mar 13 02:00:44.526: INFO: (15) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 24.21693ms)
Mar 13 02:00:44.526: INFO: (15) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 24.432516ms)
Mar 13 02:00:44.526: INFO: (15) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 24.978856ms)
Mar 13 02:00:44.528: INFO: (15) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 26.152466ms)
Mar 13 02:00:44.529: INFO: (15) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 26.884102ms)
Mar 13 02:00:44.530: INFO: (15) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 28.43452ms)
Mar 13 02:00:44.543: INFO: (16) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 11.561341ms)
Mar 13 02:00:44.545: INFO: (16) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 13.518241ms)
Mar 13 02:00:44.545: INFO: (16) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 15.441611ms)
Mar 13 02:00:44.545: INFO: (16) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 14.916494ms)
Mar 13 02:00:44.546: INFO: (16) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 15.062838ms)
Mar 13 02:00:44.547: INFO: (16) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 15.877827ms)
Mar 13 02:00:44.548: INFO: (16) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 17.757486ms)
Mar 13 02:00:44.548: INFO: (16) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 17.162121ms)
Mar 13 02:00:44.548: INFO: (16) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 16.797437ms)
Mar 13 02:00:44.548: INFO: (16) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 17.962039ms)
Mar 13 02:00:44.551: INFO: (16) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 21.53524ms)
Mar 13 02:00:44.552: INFO: (16) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 22.186695ms)
Mar 13 02:00:44.553: INFO: (16) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 22.38006ms)
Mar 13 02:00:44.554: INFO: (16) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 23.961295ms)
Mar 13 02:00:44.554: INFO: (16) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 23.601681ms)
Mar 13 02:00:44.555: INFO: (16) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 23.784241ms)
Mar 13 02:00:44.564: INFO: (17) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 8.866272ms)
Mar 13 02:00:44.565: INFO: (17) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 10.173841ms)
Mar 13 02:00:44.570: INFO: (17) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 14.200864ms)
Mar 13 02:00:44.571: INFO: (17) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 15.70995ms)
Mar 13 02:00:44.571: INFO: (17) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 15.719687ms)
Mar 13 02:00:44.573: INFO: (17) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 16.958028ms)
Mar 13 02:00:44.573: INFO: (17) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 17.906152ms)
Mar 13 02:00:44.575: INFO: (17) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 19.227148ms)
Mar 13 02:00:44.575: INFO: (17) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 19.854659ms)
Mar 13 02:00:44.575: INFO: (17) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 19.560923ms)
Mar 13 02:00:44.575: INFO: (17) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 19.634288ms)
Mar 13 02:00:44.577: INFO: (17) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 21.03618ms)
Mar 13 02:00:44.577: INFO: (17) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 21.391313ms)
Mar 13 02:00:44.577: INFO: (17) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 21.655965ms)
Mar 13 02:00:44.579: INFO: (17) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 23.050044ms)
Mar 13 02:00:44.580: INFO: (17) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 24.197814ms)
Mar 13 02:00:44.596: INFO: (18) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 15.356298ms)
Mar 13 02:00:44.599: INFO: (18) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 19.163127ms)
Mar 13 02:00:44.600: INFO: (18) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 19.569012ms)
Mar 13 02:00:44.600: INFO: (18) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 19.652284ms)
Mar 13 02:00:44.600: INFO: (18) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 19.721172ms)
Mar 13 02:00:44.601: INFO: (18) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 20.551591ms)
Mar 13 02:00:44.601: INFO: (18) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 20.614876ms)
Mar 13 02:00:44.602: INFO: (18) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 21.821936ms)
Mar 13 02:00:44.603: INFO: (18) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 22.933548ms)
Mar 13 02:00:44.603: INFO: (18) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 22.971612ms)
Mar 13 02:00:44.605: INFO: (18) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 24.004369ms)
Mar 13 02:00:44.605: INFO: (18) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 24.513927ms)
Mar 13 02:00:44.606: INFO: (18) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 25.541309ms)
Mar 13 02:00:44.606: INFO: (18) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 25.496894ms)
Mar 13 02:00:44.608: INFO: (18) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 27.111746ms)
Mar 13 02:00:44.609: INFO: (18) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 28.61732ms)
Mar 13 02:00:44.620: INFO: (19) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 10.824243ms)
Mar 13 02:00:44.621: INFO: (19) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname2/proxy/: bar (200; 11.419653ms)
Mar 13 02:00:44.622: INFO: (19) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname1/proxy/: tls baz (200; 12.406119ms)
Mar 13 02:00:44.622: INFO: (19) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:462/proxy/: tls qux (200; 11.871801ms)
Mar 13 02:00:44.623: INFO: (19) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">test<... (200; 12.721111ms)
Mar 13 02:00:44.623: INFO: (19) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr/proxy/rewriteme">test</a> (200; 13.119751ms)
Mar 13 02:00:44.628: INFO: (19) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:443/proxy/tlsrewritem... (200; 17.614427ms)
Mar 13 02:00:44.629: INFO: (19) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:160/proxy/: foo (200; 17.597751ms)
Mar 13 02:00:44.629: INFO: (19) /api/v1/namespaces/proxy-4734/pods/proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 18.185967ms)
Mar 13 02:00:44.630: INFO: (19) /api/v1/namespaces/proxy-4734/pods/https:proxy-service-s8jkt-2kmhr:460/proxy/: tls baz (200; 19.898892ms)
Mar 13 02:00:44.631: INFO: (19) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:162/proxy/: bar (200; 20.302025ms)
Mar 13 02:00:44.634: INFO: (19) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname1/proxy/: foo (200; 23.626788ms)
Mar 13 02:00:44.636: INFO: (19) /api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4734/pods/http:proxy-service-s8jkt-2kmhr:1080/proxy/rewriteme">... (200; 24.950949ms)
Mar 13 02:00:44.636: INFO: (19) /api/v1/namespaces/proxy-4734/services/https:proxy-service-s8jkt:tlsportname2/proxy/: tls qux (200; 25.567112ms)
Mar 13 02:00:44.636: INFO: (19) /api/v1/namespaces/proxy-4734/services/http:proxy-service-s8jkt:portname2/proxy/: bar (200; 26.338652ms)
Mar 13 02:00:44.636: INFO: (19) /api/v1/namespaces/proxy-4734/services/proxy-service-s8jkt:portname1/proxy/: foo (200; 25.760785ms)
STEP: deleting ReplicationController proxy-service-s8jkt in namespace proxy-4734, will wait for the garbage collector to delete the pods
Mar 13 02:00:44.705: INFO: Deleting ReplicationController proxy-service-s8jkt took: 13.060517ms
Mar 13 02:00:45.010: INFO: Terminating ReplicationController proxy-service-s8jkt pods took: 304.790177ms
[AfterEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:00:56.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4734" for this suite.
Mar 13 02:01:02.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:01:02.289: INFO: namespace proxy-4734 deletion completed in 6.169980786s

• [SLOW TEST:26.436 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:01:02.291: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:01:15.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8374" for this suite.
Mar 13 02:01:21.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:01:21.718: INFO: namespace resourcequota-8374 deletion completed in 6.236839108s

• [SLOW TEST:19.427 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:01:21.719: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Mar 13 02:01:21.764: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:01:53.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3236" for this suite.
Mar 13 02:01:59.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:01:59.464: INFO: namespace crd-publish-openapi-3236 deletion completed in 6.181294199s

• [SLOW TEST:37.746 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:01:59.465: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-c1ee7c8b-b595-472b-ba0f-b813c5e2f66d
STEP: Creating a pod to test consume configMaps
Mar 13 02:01:59.570: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8f6fd9f4-d966-4867-aad6-1f85faf4af22" in namespace "projected-2334" to be "success or failure"
Mar 13 02:01:59.577: INFO: Pod "pod-projected-configmaps-8f6fd9f4-d966-4867-aad6-1f85faf4af22": Phase="Pending", Reason="", readiness=false. Elapsed: 7.58008ms
Mar 13 02:02:01.584: INFO: Pod "pod-projected-configmaps-8f6fd9f4-d966-4867-aad6-1f85faf4af22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014567283s
Mar 13 02:02:03.593: INFO: Pod "pod-projected-configmaps-8f6fd9f4-d966-4867-aad6-1f85faf4af22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023030527s
STEP: Saw pod success
Mar 13 02:02:03.593: INFO: Pod "pod-projected-configmaps-8f6fd9f4-d966-4867-aad6-1f85faf4af22" satisfied condition "success or failure"
Mar 13 02:02:03.598: INFO: Trying to get logs from node silbory-nirmata0 pod pod-projected-configmaps-8f6fd9f4-d966-4867-aad6-1f85faf4af22 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 02:02:03.662: INFO: Waiting for pod pod-projected-configmaps-8f6fd9f4-d966-4867-aad6-1f85faf4af22 to disappear
Mar 13 02:02:03.669: INFO: Pod pod-projected-configmaps-8f6fd9f4-d966-4867-aad6-1f85faf4af22 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:02:03.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2334" for this suite.
Mar 13 02:02:09.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:02:09.858: INFO: namespace projected-2334 deletion completed in 6.178916451s

• [SLOW TEST:10.393 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:02:09.859: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:02:09.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6895" for this suite.
Mar 13 02:02:15.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:02:16.123: INFO: namespace services-6895 deletion completed in 6.170164671s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.264 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:02:16.124: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Mar 13 02:02:16.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 create -f - --namespace=kubectl-853'
Mar 13 02:02:16.735: INFO: stderr: ""
Mar 13 02:02:16.735: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 13 02:02:16.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-853'
Mar 13 02:02:16.966: INFO: stderr: ""
Mar 13 02:02:16.966: INFO: stdout: "update-demo-nautilus-s5xkk update-demo-nautilus-sk6fr "
Mar 13 02:02:16.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-s5xkk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-853'
Mar 13 02:02:17.216: INFO: stderr: ""
Mar 13 02:02:17.216: INFO: stdout: ""
Mar 13 02:02:17.216: INFO: update-demo-nautilus-s5xkk is created but not running
Mar 13 02:02:22.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-853'
Mar 13 02:02:22.443: INFO: stderr: ""
Mar 13 02:02:22.443: INFO: stdout: "update-demo-nautilus-s5xkk update-demo-nautilus-sk6fr "
Mar 13 02:02:22.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-s5xkk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-853'
Mar 13 02:02:22.667: INFO: stderr: ""
Mar 13 02:02:22.667: INFO: stdout: "true"
Mar 13 02:02:22.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-s5xkk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-853'
Mar 13 02:02:22.890: INFO: stderr: ""
Mar 13 02:02:22.890: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 02:02:22.890: INFO: validating pod update-demo-nautilus-s5xkk
Mar 13 02:02:22.901: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 02:02:22.901: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 02:02:22.901: INFO: update-demo-nautilus-s5xkk is verified up and running
Mar 13 02:02:22.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-sk6fr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-853'
Mar 13 02:02:23.120: INFO: stderr: ""
Mar 13 02:02:23.120: INFO: stdout: "true"
Mar 13 02:02:23.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-nautilus-sk6fr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-853'
Mar 13 02:02:23.320: INFO: stderr: ""
Mar 13 02:02:23.320: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 02:02:23.320: INFO: validating pod update-demo-nautilus-sk6fr
Mar 13 02:02:23.331: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 02:02:23.331: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 02:02:23.331: INFO: update-demo-nautilus-sk6fr is verified up and running
STEP: rolling-update to new replication controller
Mar 13 02:02:23.347: INFO: scanned /root for discovery docs: <nil>
Mar 13 02:02:23.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-853'
Mar 13 02:02:47.265: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 13 02:02:47.265: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 13 02:02:47.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-853'
Mar 13 02:02:47.496: INFO: stderr: ""
Mar 13 02:02:47.496: INFO: stdout: "update-demo-kitten-85v2z update-demo-kitten-b66sl "
Mar 13 02:02:47.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-kitten-85v2z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-853'
Mar 13 02:02:47.727: INFO: stderr: ""
Mar 13 02:02:47.727: INFO: stdout: "true"
Mar 13 02:02:47.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-kitten-85v2z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-853'
Mar 13 02:02:47.957: INFO: stderr: ""
Mar 13 02:02:47.957: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 13 02:02:47.957: INFO: validating pod update-demo-kitten-85v2z
Mar 13 02:02:47.966: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 13 02:02:47.966: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 13 02:02:47.966: INFO: update-demo-kitten-85v2z is verified up and running
Mar 13 02:02:47.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-kitten-b66sl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-853'
Mar 13 02:02:48.189: INFO: stderr: ""
Mar 13 02:02:48.189: INFO: stdout: "true"
Mar 13 02:02:48.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 get pods update-demo-kitten-b66sl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-853'
Mar 13 02:02:48.391: INFO: stderr: ""
Mar 13 02:02:48.391: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 13 02:02:48.391: INFO: validating pod update-demo-kitten-b66sl
Mar 13 02:02:48.401: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 13 02:02:48.401: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 13 02:02:48.401: INFO: update-demo-kitten-b66sl is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:02:48.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-853" for this suite.
Mar 13 02:03:16.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:03:16.582: INFO: namespace kubectl-853 deletion completed in 28.174317864s

• [SLOW TEST:60.458 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:03:16.589: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 13 02:03:19.719: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:03:19.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3215" for this suite.
Mar 13 02:03:25.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:03:25.940: INFO: namespace container-runtime-3215 deletion completed in 6.182975367s

• [SLOW TEST:9.351 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:03:25.940: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 13 02:03:26.033: INFO: Waiting up to 5m0s for pod "downward-api-76a8e3f7-81de-46e9-aa8a-79992f8cdadc" in namespace "downward-api-1881" to be "success or failure"
Mar 13 02:03:26.042: INFO: Pod "downward-api-76a8e3f7-81de-46e9-aa8a-79992f8cdadc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.13413ms
Mar 13 02:03:28.048: INFO: Pod "downward-api-76a8e3f7-81de-46e9-aa8a-79992f8cdadc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015001904s
Mar 13 02:03:30.055: INFO: Pod "downward-api-76a8e3f7-81de-46e9-aa8a-79992f8cdadc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022001443s
STEP: Saw pod success
Mar 13 02:03:30.056: INFO: Pod "downward-api-76a8e3f7-81de-46e9-aa8a-79992f8cdadc" satisfied condition "success or failure"
Mar 13 02:03:30.060: INFO: Trying to get logs from node silbory-nirmata0 pod downward-api-76a8e3f7-81de-46e9-aa8a-79992f8cdadc container dapi-container: <nil>
STEP: delete the pod
Mar 13 02:03:30.099: INFO: Waiting for pod downward-api-76a8e3f7-81de-46e9-aa8a-79992f8cdadc to disappear
Mar 13 02:03:30.105: INFO: Pod downward-api-76a8e3f7-81de-46e9-aa8a-79992f8cdadc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:03:30.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1881" for this suite.
Mar 13 02:03:36.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:03:36.298: INFO: namespace downward-api-1881 deletion completed in 6.183987553s

• [SLOW TEST:10.358 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:03:36.301: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 02:03:36.367: INFO: Creating deployment "test-recreate-deployment"
Mar 13 02:03:36.374: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 13 02:03:36.390: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 13 02:03:38.402: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 13 02:03:38.408: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661816, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661816, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661816, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719661816, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 02:03:40.415: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 13 02:03:40.427: INFO: Updating deployment test-recreate-deployment
Mar 13 02:03:40.427: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 13 02:03:40.582: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-251 /apis/apps/v1/namespaces/deployment-251/deployments/test-recreate-deployment c3a1667f-c107-4d6c-a847-02ad2c4fb6d6 17028 2 2020-03-13 02:03:36 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004630908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-13 02:03:40 +0000 UTC,LastTransitionTime:2020-03-13 02:03:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-03-13 02:03:40 +0000 UTC,LastTransitionTime:2020-03-13 02:03:36 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 13 02:03:40.590: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-251 /apis/apps/v1/namespaces/deployment-251/replicasets/test-recreate-deployment-5f94c574ff 68a9cf24-0d93-4c46-b8a1-be8ccd958815 17025 1 2020-03-13 02:03:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c3a1667f-c107-4d6c-a847-02ad2c4fb6d6 0xc007627c57 0xc007627c58}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007627cd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 13 02:03:40.590: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 13 02:03:40.590: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-251 /apis/apps/v1/namespaces/deployment-251/replicasets/test-recreate-deployment-68fc85c7bb 5f6bc19d-482d-4fc9-8cd5-752e89c163bd 17016 2 2020-03-13 02:03:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c3a1667f-c107-4d6c-a847-02ad2c4fb6d6 0xc007627d47 0xc007627d48}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007627da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 13 02:03:40.601: INFO: Pod "test-recreate-deployment-5f94c574ff-674dw" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-674dw test-recreate-deployment-5f94c574ff- deployment-251 /api/v1/namespaces/deployment-251/pods/test-recreate-deployment-5f94c574ff-674dw 1e6d6662-4f5d-4059-90ac-27a3323d81ca 17029 0 2020-03-13 02:03:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 68a9cf24-0d93-4c46-b8a1-be8ccd958815 0xc007743c47 0xc007743c48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zqqfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zqqfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zqqfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:silbory-nirmata0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 02:03:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 02:03:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 02:03:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 02:03:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.79,PodIP:,StartTime:2020-03-13 02:03:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:03:40.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-251" for this suite.
Mar 13 02:03:46.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:03:46.827: INFO: namespace deployment-251 deletion completed in 6.213866546s

• [SLOW TEST:10.526 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:03:46.827: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 13 02:03:51.458: INFO: Successfully updated pod "pod-update-8c4e6c1c-7f36-457b-8fb8-0ed658c74f9c"
STEP: verifying the updated pod is in kubernetes
Mar 13 02:03:51.471: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:03:51.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3827" for this suite.
Mar 13 02:04:19.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:04:19.771: INFO: namespace pods-3827 deletion completed in 28.291371437s

• [SLOW TEST:32.944 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:04:19.772: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:04:19.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5289" for this suite.
Mar 13 02:04:25.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:04:26.080: INFO: namespace resourcequota-5289 deletion completed in 6.187925033s

• [SLOW TEST:6.308 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:04:26.080: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-87953f20-6780-471b-b387-574077b6ed37
STEP: Creating a pod to test consume configMaps
Mar 13 02:04:26.168: INFO: Waiting up to 5m0s for pod "pod-configmaps-9c92ad03-77b0-4cba-b56e-b206c82f8b85" in namespace "configmap-392" to be "success or failure"
Mar 13 02:04:26.174: INFO: Pod "pod-configmaps-9c92ad03-77b0-4cba-b56e-b206c82f8b85": Phase="Pending", Reason="", readiness=false. Elapsed: 6.294379ms
Mar 13 02:04:28.184: INFO: Pod "pod-configmaps-9c92ad03-77b0-4cba-b56e-b206c82f8b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015381287s
Mar 13 02:04:30.192: INFO: Pod "pod-configmaps-9c92ad03-77b0-4cba-b56e-b206c82f8b85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023437921s
STEP: Saw pod success
Mar 13 02:04:30.192: INFO: Pod "pod-configmaps-9c92ad03-77b0-4cba-b56e-b206c82f8b85" satisfied condition "success or failure"
Mar 13 02:04:30.199: INFO: Trying to get logs from node silbory-nirmata0 pod pod-configmaps-9c92ad03-77b0-4cba-b56e-b206c82f8b85 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 02:04:30.249: INFO: Waiting for pod pod-configmaps-9c92ad03-77b0-4cba-b56e-b206c82f8b85 to disappear
Mar 13 02:04:30.257: INFO: Pod pod-configmaps-9c92ad03-77b0-4cba-b56e-b206c82f8b85 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:04:30.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-392" for this suite.
Mar 13 02:04:36.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:04:36.464: INFO: namespace configmap-392 deletion completed in 6.196404353s

• [SLOW TEST:10.384 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:04:36.465: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 02:04:36.551: INFO: Waiting up to 5m0s for pod "downwardapi-volume-65b087b3-d8b4-4b5b-9fe8-018ed656e38e" in namespace "downward-api-2960" to be "success or failure"
Mar 13 02:04:36.559: INFO: Pod "downwardapi-volume-65b087b3-d8b4-4b5b-9fe8-018ed656e38e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.37559ms
Mar 13 02:04:38.569: INFO: Pod "downwardapi-volume-65b087b3-d8b4-4b5b-9fe8-018ed656e38e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017317991s
Mar 13 02:04:40.575: INFO: Pod "downwardapi-volume-65b087b3-d8b4-4b5b-9fe8-018ed656e38e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023888917s
STEP: Saw pod success
Mar 13 02:04:40.575: INFO: Pod "downwardapi-volume-65b087b3-d8b4-4b5b-9fe8-018ed656e38e" satisfied condition "success or failure"
Mar 13 02:04:40.580: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-65b087b3-d8b4-4b5b-9fe8-018ed656e38e container client-container: <nil>
STEP: delete the pod
Mar 13 02:04:40.614: INFO: Waiting for pod downwardapi-volume-65b087b3-d8b4-4b5b-9fe8-018ed656e38e to disappear
Mar 13 02:04:40.625: INFO: Pod downwardapi-volume-65b087b3-d8b4-4b5b-9fe8-018ed656e38e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:04:40.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2960" for this suite.
Mar 13 02:04:46.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:04:46.845: INFO: namespace downward-api-2960 deletion completed in 6.211943569s

• [SLOW TEST:10.380 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:04:46.849: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 13 02:04:46.928: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:04:56.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2063" for this suite.
Mar 13 02:05:02.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:05:02.284: INFO: namespace pods-2063 deletion completed in 6.179304945s

• [SLOW TEST:15.435 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:05:02.285: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-779edb10-a35b-4aeb-a865-3406aa0c10c2
STEP: Creating a pod to test consume configMaps
Mar 13 02:05:02.382: INFO: Waiting up to 5m0s for pod "pod-configmaps-af8bdeb5-020d-45c2-b461-956f3b8d40fe" in namespace "configmap-9568" to be "success or failure"
Mar 13 02:05:02.389: INFO: Pod "pod-configmaps-af8bdeb5-020d-45c2-b461-956f3b8d40fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.973851ms
Mar 13 02:05:04.397: INFO: Pod "pod-configmaps-af8bdeb5-020d-45c2-b461-956f3b8d40fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014889737s
Mar 13 02:05:06.403: INFO: Pod "pod-configmaps-af8bdeb5-020d-45c2-b461-956f3b8d40fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021120229s
STEP: Saw pod success
Mar 13 02:05:06.403: INFO: Pod "pod-configmaps-af8bdeb5-020d-45c2-b461-956f3b8d40fe" satisfied condition "success or failure"
Mar 13 02:05:06.409: INFO: Trying to get logs from node silbory-nirmata0 pod pod-configmaps-af8bdeb5-020d-45c2-b461-956f3b8d40fe container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 02:05:06.444: INFO: Waiting for pod pod-configmaps-af8bdeb5-020d-45c2-b461-956f3b8d40fe to disappear
Mar 13 02:05:06.450: INFO: Pod pod-configmaps-af8bdeb5-020d-45c2-b461-956f3b8d40fe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:05:06.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9568" for this suite.
Mar 13 02:05:12.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:05:12.643: INFO: namespace configmap-9568 deletion completed in 6.184993699s

• [SLOW TEST:10.358 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:05:12.643: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 13 02:05:12.765: INFO: Number of nodes with available pods: 0
Mar 13 02:05:12.765: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:05:13.787: INFO: Number of nodes with available pods: 0
Mar 13 02:05:13.787: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:05:14.784: INFO: Number of nodes with available pods: 0
Mar 13 02:05:14.784: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:05:15.780: INFO: Number of nodes with available pods: 3
Mar 13 02:05:15.780: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 13 02:05:15.824: INFO: Number of nodes with available pods: 2
Mar 13 02:05:15.824: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:05:16.840: INFO: Number of nodes with available pods: 2
Mar 13 02:05:16.840: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:05:17.841: INFO: Number of nodes with available pods: 2
Mar 13 02:05:17.842: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:05:18.841: INFO: Number of nodes with available pods: 3
Mar 13 02:05:18.841: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3922, will wait for the garbage collector to delete the pods
Mar 13 02:05:18.920: INFO: Deleting DaemonSet.extensions daemon-set took: 13.636227ms
Mar 13 02:05:19.221: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.477077ms
Mar 13 02:05:26.127: INFO: Number of nodes with available pods: 0
Mar 13 02:05:26.127: INFO: Number of running nodes: 0, number of available pods: 0
Mar 13 02:05:26.135: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3922/daemonsets","resourceVersion":"17342"},"items":null}

Mar 13 02:05:26.140: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3922/pods","resourceVersion":"17342"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:05:26.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3922" for this suite.
Mar 13 02:05:32.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:05:32.361: INFO: namespace daemonsets-3922 deletion completed in 6.184267142s

• [SLOW TEST:19.718 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:05:32.363: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-909ea681-c165-491f-a85d-5af8b0deb6b7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-909ea681-c165-491f-a85d-5af8b0deb6b7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:06:59.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-895" for this suite.
Mar 13 02:07:27.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:07:27.589: INFO: namespace projected-895 deletion completed in 28.163027136s

• [SLOW TEST:115.226 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:07:27.590: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5892.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5892.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5892.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5892.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 13 02:07:31.712: INFO: DNS probes using dns-test-331ed195-ec9e-4092-a3f8-c605475b9b14 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5892.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5892.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5892.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5892.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 13 02:07:35.803: INFO: DNS probes using dns-test-c1a01051-2651-4c8f-8870-255a4823cc21 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5892.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5892.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5892.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5892.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 13 02:07:39.909: INFO: DNS probes using dns-test-e702d6d3-05d6-4127-b356-4017796cf5bd succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:07:39.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5892" for this suite.
Mar 13 02:07:46.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:07:46.227: INFO: namespace dns-5892 deletion completed in 6.233258823s

• [SLOW TEST:18.638 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:07:46.228: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 02:07:50.416: INFO: Waiting up to 5m0s for pod "client-envvars-5eb5366a-ee2d-4da6-a2ae-23f66988fbf8" in namespace "pods-2537" to be "success or failure"
Mar 13 02:07:50.422: INFO: Pod "client-envvars-5eb5366a-ee2d-4da6-a2ae-23f66988fbf8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041996ms
Mar 13 02:07:52.430: INFO: Pod "client-envvars-5eb5366a-ee2d-4da6-a2ae-23f66988fbf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014172103s
Mar 13 02:07:54.438: INFO: Pod "client-envvars-5eb5366a-ee2d-4da6-a2ae-23f66988fbf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0216332s
STEP: Saw pod success
Mar 13 02:07:54.438: INFO: Pod "client-envvars-5eb5366a-ee2d-4da6-a2ae-23f66988fbf8" satisfied condition "success or failure"
Mar 13 02:07:54.444: INFO: Trying to get logs from node silbory-nirmata0 pod client-envvars-5eb5366a-ee2d-4da6-a2ae-23f66988fbf8 container env3cont: <nil>
STEP: delete the pod
Mar 13 02:07:54.485: INFO: Waiting for pod client-envvars-5eb5366a-ee2d-4da6-a2ae-23f66988fbf8 to disappear
Mar 13 02:07:54.495: INFO: Pod client-envvars-5eb5366a-ee2d-4da6-a2ae-23f66988fbf8 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:07:54.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2537" for this suite.
Mar 13 02:08:22.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:08:22.689: INFO: namespace pods-2537 deletion completed in 28.18526826s

• [SLOW TEST:36.462 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:08:22.691: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 13 02:08:22.769: INFO: Waiting up to 5m0s for pod "pod-77adc064-55cc-49dd-b72b-cbd9eec17138" in namespace "emptydir-6814" to be "success or failure"
Mar 13 02:08:22.774: INFO: Pod "pod-77adc064-55cc-49dd-b72b-cbd9eec17138": Phase="Pending", Reason="", readiness=false. Elapsed: 4.896583ms
Mar 13 02:08:24.781: INFO: Pod "pod-77adc064-55cc-49dd-b72b-cbd9eec17138": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011703708s
Mar 13 02:08:26.788: INFO: Pod "pod-77adc064-55cc-49dd-b72b-cbd9eec17138": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018611578s
STEP: Saw pod success
Mar 13 02:08:26.788: INFO: Pod "pod-77adc064-55cc-49dd-b72b-cbd9eec17138" satisfied condition "success or failure"
Mar 13 02:08:26.795: INFO: Trying to get logs from node silbory-nirmata0 pod pod-77adc064-55cc-49dd-b72b-cbd9eec17138 container test-container: <nil>
STEP: delete the pod
Mar 13 02:08:26.835: INFO: Waiting for pod pod-77adc064-55cc-49dd-b72b-cbd9eec17138 to disappear
Mar 13 02:08:26.842: INFO: Pod pod-77adc064-55cc-49dd-b72b-cbd9eec17138 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:08:26.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6814" for this suite.
Mar 13 02:08:32.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:08:33.037: INFO: namespace emptydir-6814 deletion completed in 6.174945754s

• [SLOW TEST:10.346 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:08:33.042: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:08:37.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4792" for this suite.
Mar 13 02:09:21.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:09:21.340: INFO: namespace kubelet-test-4792 deletion completed in 44.176711411s

• [SLOW TEST:48.298 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:09:21.340: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-1f1a463c-2355-49b3-b69d-2ec0663584ae
STEP: Creating a pod to test consume secrets
Mar 13 02:09:21.434: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e4b03c92-2efc-4c13-adf6-b00fd95fc987" in namespace "projected-6140" to be "success or failure"
Mar 13 02:09:21.441: INFO: Pod "pod-projected-secrets-e4b03c92-2efc-4c13-adf6-b00fd95fc987": Phase="Pending", Reason="", readiness=false. Elapsed: 6.177343ms
Mar 13 02:09:23.453: INFO: Pod "pod-projected-secrets-e4b03c92-2efc-4c13-adf6-b00fd95fc987": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018211976s
Mar 13 02:09:25.460: INFO: Pod "pod-projected-secrets-e4b03c92-2efc-4c13-adf6-b00fd95fc987": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025273159s
STEP: Saw pod success
Mar 13 02:09:25.460: INFO: Pod "pod-projected-secrets-e4b03c92-2efc-4c13-adf6-b00fd95fc987" satisfied condition "success or failure"
Mar 13 02:09:25.465: INFO: Trying to get logs from node silbory-nirmata0 pod pod-projected-secrets-e4b03c92-2efc-4c13-adf6-b00fd95fc987 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 13 02:09:25.504: INFO: Waiting for pod pod-projected-secrets-e4b03c92-2efc-4c13-adf6-b00fd95fc987 to disappear
Mar 13 02:09:25.510: INFO: Pod pod-projected-secrets-e4b03c92-2efc-4c13-adf6-b00fd95fc987 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:09:25.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6140" for this suite.
Mar 13 02:09:31.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:09:31.707: INFO: namespace projected-6140 deletion completed in 6.187876053s

• [SLOW TEST:10.367 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:09:31.708: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:09:39.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1913" for this suite.
Mar 13 02:09:45.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:09:45.969: INFO: namespace job-1913 deletion completed in 6.172981762s

• [SLOW TEST:14.261 seconds]
[sig-apps] Job
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:09:45.970: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 13 02:09:46.044: INFO: Waiting up to 5m0s for pod "downward-api-54d03b9c-8410-43f3-a3bb-47bab2a71bc4" in namespace "downward-api-4419" to be "success or failure"
Mar 13 02:09:46.050: INFO: Pod "downward-api-54d03b9c-8410-43f3-a3bb-47bab2a71bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.690258ms
Mar 13 02:09:48.058: INFO: Pod "downward-api-54d03b9c-8410-43f3-a3bb-47bab2a71bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013300223s
Mar 13 02:09:50.065: INFO: Pod "downward-api-54d03b9c-8410-43f3-a3bb-47bab2a71bc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021034342s
STEP: Saw pod success
Mar 13 02:09:50.065: INFO: Pod "downward-api-54d03b9c-8410-43f3-a3bb-47bab2a71bc4" satisfied condition "success or failure"
Mar 13 02:09:50.070: INFO: Trying to get logs from node silbory-nirmata0 pod downward-api-54d03b9c-8410-43f3-a3bb-47bab2a71bc4 container dapi-container: <nil>
STEP: delete the pod
Mar 13 02:09:50.122: INFO: Waiting for pod downward-api-54d03b9c-8410-43f3-a3bb-47bab2a71bc4 to disappear
Mar 13 02:09:50.129: INFO: Pod downward-api-54d03b9c-8410-43f3-a3bb-47bab2a71bc4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:09:50.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4419" for this suite.
Mar 13 02:09:56.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:09:56.340: INFO: namespace downward-api-4419 deletion completed in 6.197372103s

• [SLOW TEST:10.370 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:09:56.340: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar 13 02:09:56.411: INFO: PodSpec: initContainers in spec.initContainers
Mar 13 02:10:47.164: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-cad354c3-b477-4d60-9759-b6c183bd37ef", GenerateName:"", Namespace:"init-container-7136", SelfLink:"/api/v1/namespaces/init-container-7136/pods/pod-init-cad354c3-b477-4d60-9759-b6c183bd37ef", UID:"bd32d8b9-1540-4fe2-8270-1fa0393b3354", ResourceVersion:"17999", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63719662196, loc:(*time.Location)(0x84be2c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"411135144"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-bp2c5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc005b62400), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bp2c5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bp2c5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bp2c5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0025cdd28), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"silbory-nirmata0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000f18a20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0025cddb0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0025cddd0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0025cddd8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0025cdddc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662196, loc:(*time.Location)(0x84be2c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662196, loc:(*time.Location)(0x84be2c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662196, loc:(*time.Location)(0x84be2c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662196, loc:(*time.Location)(0x84be2c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.1.79", PodIP:"10.244.2.22", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.2.22"}}, StartTime:(*v1.Time)(0xc00198c960), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006b33b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://02b19955a4460af3ea7c59f71726b3835908e2e0c516cca8c893cb11c438d8b4", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00198c9a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00198c980), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc0025cde6f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:10:47.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7136" for this suite.
Mar 13 02:11:15.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:11:15.345: INFO: namespace init-container-7136 deletion completed in 28.168608322s

• [SLOW TEST:79.005 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:11:15.346: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 02:11:16.256: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 02:11:18.277: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662276, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662276, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662276, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662276, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 02:11:21.300: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:11:21.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2414" for this suite.
Mar 13 02:11:27.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:11:27.791: INFO: namespace webhook-2414 deletion completed in 6.190584756s
STEP: Destroying namespace "webhook-2414-markers" for this suite.
Mar 13 02:11:33.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:11:33.971: INFO: namespace webhook-2414-markers deletion completed in 6.179932082s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.653 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:11:34.002: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 02:11:34.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 version'
Mar 13 02:11:34.254: INFO: stderr: ""
Mar 13 02:11:34.255: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.0\", GitCommit:\"2bd9643cee5b3b3a5ecbd3af49d09018f0773c77\", GitTreeState:\"clean\", BuildDate:\"2019-09-18T14:36:53Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.0\", GitCommit:\"2bd9643cee5b3b3a5ecbd3af49d09018f0773c77\", GitTreeState:\"clean\", BuildDate:\"2019-09-18T14:27:17Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:11:34.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1044" for this suite.
Mar 13 02:11:40.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:11:40.463: INFO: namespace kubectl-1044 deletion completed in 6.200040959s

• [SLOW TEST:6.461 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:11:40.471: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar 13 02:11:45.145: INFO: Successfully updated pod "annotationupdate263406c6-79d5-4802-adcb-869062324b0d"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:11:47.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2871" for this suite.
Mar 13 02:12:15.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:12:15.375: INFO: namespace projected-2871 deletion completed in 28.186221635s

• [SLOW TEST:34.905 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:12:15.376: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 13 02:12:15.444: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 13 02:12:15.468: INFO: Waiting for terminating namespaces to be deleted...
Mar 13 02:12:15.473: INFO: 
Logging pods the kubelet thinks is on node sam-node2 before test
Mar 13 02:12:15.499: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-w6d2v from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 02:12:15.499: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 13 02:12:15.499: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 13 02:12:15.499: INFO: kube-dns-869c556677-bfkcm from kube-system started at 2020-03-13 00:13:29 +0000 UTC (3 container statuses recorded)
Mar 13 02:12:15.499: INFO: 	Container dnsmasq ready: true, restart count 0
Mar 13 02:12:15.499: INFO: 	Container kubedns ready: true, restart count 0
Mar 13 02:12:15.500: INFO: 	Container sidecar ready: true, restart count 0
Mar 13 02:12:15.500: INFO: kube-flannel-ds-d9nq5 from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 02:12:15.500: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 02:12:15.500: INFO: nirmata-cni-installer-nkzv7 from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 02:12:15.500: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 02:12:15.500: INFO: 
Logging pods the kubelet thinks is on node sam-node3 before test
Mar 13 02:12:15.522: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-f9nz4 from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 02:12:15.522: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 13 02:12:15.522: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 13 02:12:15.523: INFO: kube-flannel-ds-2rr2t from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 02:12:15.523: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 02:12:15.523: INFO: nirmata-kube-controller-666cf5cf5f-j4mmx from nirmata started at 2020-03-13 00:13:27 +0000 UTC (1 container statuses recorded)
Mar 13 02:12:15.523: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
Mar 13 02:12:15.523: INFO: nirmata-cni-installer-bf5qr from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 02:12:15.523: INFO: 	Container install-cni ready: true, restart count 0
Mar 13 02:12:15.523: INFO: 
Logging pods the kubelet thinks is on node silbory-nirmata0 before test
Mar 13 02:12:15.538: INFO: sonobuoy-systemd-logs-daemon-set-5206d17636204e77-qf4zs from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 02:12:15.538: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 13 02:12:15.538: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 13 02:12:15.538: INFO: haproxy-ingress-8688f746b4-2xbx5 from ingress-haproxy started at 2020-03-13 00:14:00 +0000 UTC (1 container statuses recorded)
Mar 13 02:12:15.538: INFO: 	Container haproxy-ingress ready: true, restart count 0
Mar 13 02:12:15.538: INFO: sonobuoy from sonobuoy started at 2020-03-13 00:21:12 +0000 UTC (1 container statuses recorded)
Mar 13 02:12:15.538: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 13 02:12:15.538: INFO: metrics-server-56c7b465d6-xr59t from kube-system started at 2020-03-13 00:13:52 +0000 UTC (1 container statuses recorded)
Mar 13 02:12:15.538: INFO: 	Container metrics-server ready: true, restart count 0
Mar 13 02:12:15.538: INFO: sonobuoy-e2e-job-fce51d5a97b943f8 from sonobuoy started at 2020-03-13 00:21:22 +0000 UTC (2 container statuses recorded)
Mar 13 02:12:15.538: INFO: 	Container e2e ready: true, restart count 0
Mar 13 02:12:15.538: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 02:12:15.538: INFO: kube-flannel-ds-959x5 from kube-system started at 2020-03-13 00:13:09 +0000 UTC (1 container statuses recorded)
Mar 13 02:12:15.538: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 02:12:15.538: INFO: ingress-default-backend-cf675c575-nttt9 from ingress-haproxy started at 2020-03-13 00:14:00 +0000 UTC (1 container statuses recorded)
Mar 13 02:12:15.538: INFO: 	Container ingress-default-backend ready: true, restart count 0
Mar 13 02:12:15.538: INFO: nirmata-cni-installer-lnhv6 from nirmata started at 2020-03-13 00:13:08 +0000 UTC (1 container statuses recorded)
Mar 13 02:12:15.538: INFO: 	Container install-cni ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node sam-node2
STEP: verifying the node has the label node sam-node3
STEP: verifying the node has the label node silbory-nirmata0
Mar 13 02:12:15.635: INFO: Pod haproxy-ingress-8688f746b4-2xbx5 requesting resource cpu=0m on Node silbory-nirmata0
Mar 13 02:12:15.635: INFO: Pod ingress-default-backend-cf675c575-nttt9 requesting resource cpu=0m on Node silbory-nirmata0
Mar 13 02:12:15.635: INFO: Pod kube-dns-869c556677-bfkcm requesting resource cpu=260m on Node sam-node2
Mar 13 02:12:15.635: INFO: Pod kube-flannel-ds-2rr2t requesting resource cpu=0m on Node sam-node3
Mar 13 02:12:15.636: INFO: Pod kube-flannel-ds-959x5 requesting resource cpu=0m on Node silbory-nirmata0
Mar 13 02:12:15.636: INFO: Pod kube-flannel-ds-d9nq5 requesting resource cpu=0m on Node sam-node2
Mar 13 02:12:15.636: INFO: Pod metrics-server-56c7b465d6-xr59t requesting resource cpu=0m on Node silbory-nirmata0
Mar 13 02:12:15.636: INFO: Pod nirmata-cni-installer-bf5qr requesting resource cpu=0m on Node sam-node3
Mar 13 02:12:15.636: INFO: Pod nirmata-cni-installer-lnhv6 requesting resource cpu=0m on Node silbory-nirmata0
Mar 13 02:12:15.636: INFO: Pod nirmata-cni-installer-nkzv7 requesting resource cpu=0m on Node sam-node2
Mar 13 02:12:15.636: INFO: Pod nirmata-kube-controller-666cf5cf5f-j4mmx requesting resource cpu=0m on Node sam-node3
Mar 13 02:12:15.636: INFO: Pod sonobuoy requesting resource cpu=0m on Node silbory-nirmata0
Mar 13 02:12:15.636: INFO: Pod sonobuoy-e2e-job-fce51d5a97b943f8 requesting resource cpu=0m on Node silbory-nirmata0
Mar 13 02:12:15.636: INFO: Pod sonobuoy-systemd-logs-daemon-set-5206d17636204e77-f9nz4 requesting resource cpu=0m on Node sam-node3
Mar 13 02:12:15.636: INFO: Pod sonobuoy-systemd-logs-daemon-set-5206d17636204e77-qf4zs requesting resource cpu=0m on Node silbory-nirmata0
Mar 13 02:12:15.636: INFO: Pod sonobuoy-systemd-logs-daemon-set-5206d17636204e77-w6d2v requesting resource cpu=0m on Node sam-node2
STEP: Starting Pods to consume most of the cluster CPU.
Mar 13 02:12:15.636: INFO: Creating a pod which consumes cpu=1218m on Node sam-node2
Mar 13 02:12:15.656: INFO: Creating a pod which consumes cpu=1400m on Node sam-node3
Mar 13 02:12:15.676: INFO: Creating a pod which consumes cpu=4200m on Node silbory-nirmata0
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-15d31597-0b85-4b9d-a484-280cd1d532d3.15fbbb3642ee1ef7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7075/filler-pod-15d31597-0b85-4b9d-a484-280cd1d532d3 to sam-node2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-15d31597-0b85-4b9d-a484-280cd1d532d3.15fbbb36ba3fb039], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-15d31597-0b85-4b9d-a484-280cd1d532d3.15fbbb36bfc6a39b], Reason = [Created], Message = [Created container filler-pod-15d31597-0b85-4b9d-a484-280cd1d532d3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-15d31597-0b85-4b9d-a484-280cd1d532d3.15fbbb36cff3698f], Reason = [Started], Message = [Started container filler-pod-15d31597-0b85-4b9d-a484-280cd1d532d3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-796b5e8e-674f-4d8a-9850-e83cfdca99f6.15fbbb3644555d4d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7075/filler-pod-796b5e8e-674f-4d8a-9850-e83cfdca99f6 to sam-node3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-796b5e8e-674f-4d8a-9850-e83cfdca99f6.15fbbb36da204668], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-796b5e8e-674f-4d8a-9850-e83cfdca99f6.15fbbb36e8c4ee21], Reason = [Created], Message = [Created container filler-pod-796b5e8e-674f-4d8a-9850-e83cfdca99f6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-796b5e8e-674f-4d8a-9850-e83cfdca99f6.15fbbb36ff40cee2], Reason = [Started], Message = [Started container filler-pod-796b5e8e-674f-4d8a-9850-e83cfdca99f6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cd6ae875-04c4-4a0c-a3c7-d3fb79dfb3b0.15fbbb3645121729], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7075/filler-pod-cd6ae875-04c4-4a0c-a3c7-d3fb79dfb3b0 to silbory-nirmata0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cd6ae875-04c4-4a0c-a3c7-d3fb79dfb3b0.15fbbb3692245f7f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cd6ae875-04c4-4a0c-a3c7-d3fb79dfb3b0.15fbbb36996090e4], Reason = [Created], Message = [Created container filler-pod-cd6ae875-04c4-4a0c-a3c7-d3fb79dfb3b0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cd6ae875-04c4-4a0c-a3c7-d3fb79dfb3b0.15fbbb36b1548b25], Reason = [Started], Message = [Started container filler-pod-cd6ae875-04c4-4a0c-a3c7-d3fb79dfb3b0]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15fbbb3734e40381], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node sam-node2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node sam-node3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node silbory-nirmata0
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:12:20.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7075" for this suite.
Mar 13 02:12:26.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:12:27.021: INFO: namespace sched-pred-7075 deletion completed in 6.195526066s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:11.646 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:12:27.022: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Mar 13 02:12:27.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 api-versions'
Mar 13 02:12:27.349: INFO: stderr: ""
Mar 13 02:12:27.349: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:12:27.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5307" for this suite.
Mar 13 02:12:33.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:12:33.562: INFO: namespace kubectl-5307 deletion completed in 6.203458749s

• [SLOW TEST:6.539 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:12:33.562: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-295b8183-5e0a-4e59-b450-a053cb6968a4
STEP: Creating a pod to test consume configMaps
Mar 13 02:12:33.652: INFO: Waiting up to 5m0s for pod "pod-configmaps-5fbc945f-f510-4368-9be6-6487af82531e" in namespace "configmap-247" to be "success or failure"
Mar 13 02:12:33.661: INFO: Pod "pod-configmaps-5fbc945f-f510-4368-9be6-6487af82531e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.465683ms
Mar 13 02:12:35.668: INFO: Pod "pod-configmaps-5fbc945f-f510-4368-9be6-6487af82531e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015528652s
Mar 13 02:12:37.675: INFO: Pod "pod-configmaps-5fbc945f-f510-4368-9be6-6487af82531e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022692031s
STEP: Saw pod success
Mar 13 02:12:37.675: INFO: Pod "pod-configmaps-5fbc945f-f510-4368-9be6-6487af82531e" satisfied condition "success or failure"
Mar 13 02:12:37.681: INFO: Trying to get logs from node silbory-nirmata0 pod pod-configmaps-5fbc945f-f510-4368-9be6-6487af82531e container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 02:12:37.718: INFO: Waiting for pod pod-configmaps-5fbc945f-f510-4368-9be6-6487af82531e to disappear
Mar 13 02:12:37.727: INFO: Pod pod-configmaps-5fbc945f-f510-4368-9be6-6487af82531e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:12:37.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-247" for this suite.
Mar 13 02:12:43.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:12:43.925: INFO: namespace configmap-247 deletion completed in 6.190522751s

• [SLOW TEST:10.363 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:12:43.928: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:12:44.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8720" for this suite.
Mar 13 02:12:50.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:12:50.251: INFO: namespace kubelet-test-8720 deletion completed in 6.176161189s

• [SLOW TEST:6.323 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:12:50.253: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:12:57.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6694" for this suite.
Mar 13 02:13:03.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:13:03.545: INFO: namespace resourcequota-6694 deletion completed in 6.187512463s

• [SLOW TEST:13.293 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:13:03.547: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-e9c637fd-93ad-478c-b651-579113c96115
STEP: Creating a pod to test consume configMaps
Mar 13 02:13:03.644: INFO: Waiting up to 5m0s for pod "pod-configmaps-71c28ba4-5fcb-49c9-99ec-a13fc5ac93b6" in namespace "configmap-5947" to be "success or failure"
Mar 13 02:13:03.659: INFO: Pod "pod-configmaps-71c28ba4-5fcb-49c9-99ec-a13fc5ac93b6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.666452ms
Mar 13 02:13:05.666: INFO: Pod "pod-configmaps-71c28ba4-5fcb-49c9-99ec-a13fc5ac93b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021567872s
Mar 13 02:13:07.674: INFO: Pod "pod-configmaps-71c28ba4-5fcb-49c9-99ec-a13fc5ac93b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029730808s
STEP: Saw pod success
Mar 13 02:13:07.674: INFO: Pod "pod-configmaps-71c28ba4-5fcb-49c9-99ec-a13fc5ac93b6" satisfied condition "success or failure"
Mar 13 02:13:07.679: INFO: Trying to get logs from node silbory-nirmata0 pod pod-configmaps-71c28ba4-5fcb-49c9-99ec-a13fc5ac93b6 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 02:13:07.719: INFO: Waiting for pod pod-configmaps-71c28ba4-5fcb-49c9-99ec-a13fc5ac93b6 to disappear
Mar 13 02:13:07.725: INFO: Pod pod-configmaps-71c28ba4-5fcb-49c9-99ec-a13fc5ac93b6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:13:07.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5947" for this suite.
Mar 13 02:13:13.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:13:13.913: INFO: namespace configmap-5947 deletion completed in 6.177698311s

• [SLOW TEST:10.366 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:13:13.914: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 13 02:13:14.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f26eb99-039d-4365-9e64-0ea4a466f41a" in namespace "projected-7262" to be "success or failure"
Mar 13 02:13:14.013: INFO: Pod "downwardapi-volume-2f26eb99-039d-4365-9e64-0ea4a466f41a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.97633ms
Mar 13 02:13:16.019: INFO: Pod "downwardapi-volume-2f26eb99-039d-4365-9e64-0ea4a466f41a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015703693s
Mar 13 02:13:18.026: INFO: Pod "downwardapi-volume-2f26eb99-039d-4365-9e64-0ea4a466f41a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022832609s
STEP: Saw pod success
Mar 13 02:13:18.026: INFO: Pod "downwardapi-volume-2f26eb99-039d-4365-9e64-0ea4a466f41a" satisfied condition "success or failure"
Mar 13 02:13:18.031: INFO: Trying to get logs from node silbory-nirmata0 pod downwardapi-volume-2f26eb99-039d-4365-9e64-0ea4a466f41a container client-container: <nil>
STEP: delete the pod
Mar 13 02:13:18.074: INFO: Waiting for pod downwardapi-volume-2f26eb99-039d-4365-9e64-0ea4a466f41a to disappear
Mar 13 02:13:18.080: INFO: Pod downwardapi-volume-2f26eb99-039d-4365-9e64-0ea4a466f41a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:13:18.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7262" for this suite.
Mar 13 02:13:24.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:13:24.274: INFO: namespace projected-7262 deletion completed in 6.176957657s

• [SLOW TEST:10.360 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:13:24.275: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 02:13:25.129: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 13 02:13:27.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662405, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662405, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662405, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662405, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 02:13:30.171: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:13:30.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1511" for this suite.
Mar 13 02:13:36.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:13:36.421: INFO: namespace webhook-1511 deletion completed in 6.225037452s
STEP: Destroying namespace "webhook-1511-markers" for this suite.
Mar 13 02:13:42.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:13:42.626: INFO: namespace webhook-1511-markers deletion completed in 6.205318588s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.380 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:13:42.656: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar 13 02:13:42.719: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:13:47.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-954" for this suite.
Mar 13 02:13:53.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:13:53.970: INFO: namespace init-container-954 deletion completed in 6.19690175s

• [SLOW TEST:11.314 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:13:53.972: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0313 02:14:04.116395      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 13 02:14:04.116: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:14:04.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7860" for this suite.
Mar 13 02:14:10.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:14:10.335: INFO: namespace gc-7860 deletion completed in 6.20625058s

• [SLOW TEST:16.363 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:14:10.338: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Mar 13 02:14:10.422: INFO: Waiting up to 5m0s for pod "var-expansion-5e8b3c07-605e-442a-a501-da75848ecedd" in namespace "var-expansion-5056" to be "success or failure"
Mar 13 02:14:10.428: INFO: Pod "var-expansion-5e8b3c07-605e-442a-a501-da75848ecedd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.722588ms
Mar 13 02:14:12.435: INFO: Pod "var-expansion-5e8b3c07-605e-442a-a501-da75848ecedd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012969578s
Mar 13 02:14:14.441: INFO: Pod "var-expansion-5e8b3c07-605e-442a-a501-da75848ecedd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019264184s
STEP: Saw pod success
Mar 13 02:14:14.442: INFO: Pod "var-expansion-5e8b3c07-605e-442a-a501-da75848ecedd" satisfied condition "success or failure"
Mar 13 02:14:14.447: INFO: Trying to get logs from node silbory-nirmata0 pod var-expansion-5e8b3c07-605e-442a-a501-da75848ecedd container dapi-container: <nil>
STEP: delete the pod
Mar 13 02:14:14.480: INFO: Waiting for pod var-expansion-5e8b3c07-605e-442a-a501-da75848ecedd to disappear
Mar 13 02:14:14.486: INFO: Pod var-expansion-5e8b3c07-605e-442a-a501-da75848ecedd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:14:14.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5056" for this suite.
Mar 13 02:14:20.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:14:20.671: INFO: namespace var-expansion-5056 deletion completed in 6.175664186s

• [SLOW TEST:10.333 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:14:20.671: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-ddc17422-bc16-4b6f-9d9a-6ac6ace8ed5c
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-ddc17422-bc16-4b6f-9d9a-6ac6ace8ed5c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:14:26.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4552" for this suite.
Mar 13 02:14:38.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:14:39.036: INFO: namespace configmap-4552 deletion completed in 12.186814739s

• [SLOW TEST:18.365 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:14:39.039: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 13 02:14:47.178: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 02:14:47.184: INFO: Pod pod-with-prestop-http-hook still exists
Mar 13 02:14:49.184: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 02:14:49.192: INFO: Pod pod-with-prestop-http-hook still exists
Mar 13 02:14:51.184: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 02:14:51.192: INFO: Pod pod-with-prestop-http-hook still exists
Mar 13 02:14:53.184: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 02:14:53.191: INFO: Pod pod-with-prestop-http-hook still exists
Mar 13 02:14:55.184: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 02:14:55.191: INFO: Pod pod-with-prestop-http-hook still exists
Mar 13 02:14:57.184: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 02:14:57.191: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:14:57.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9017" for this suite.
Mar 13 02:15:25.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:15:25.393: INFO: namespace container-lifecycle-hook-9017 deletion completed in 28.172922749s

• [SLOW TEST:46.354 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:15:25.394: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:15:41.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7211" for this suite.
Mar 13 02:15:47.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:15:47.812: INFO: namespace resourcequota-7211 deletion completed in 6.174975875s

• [SLOW TEST:22.418 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:15:47.812: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:15:52.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6914" for this suite.
Mar 13 02:15:58.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:15:58.907: INFO: namespace watch-6914 deletion completed in 6.276081346s

• [SLOW TEST:11.094 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:15:58.908: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-402104b7-b3bd-4c1c-b9b9-126b2223e259 in namespace container-probe-3097
Mar 13 02:16:02.993: INFO: Started pod liveness-402104b7-b3bd-4c1c-b9b9-126b2223e259 in namespace container-probe-3097
STEP: checking the pod's current state and verifying that restartCount is present
Mar 13 02:16:02.999: INFO: Initial restart count of pod liveness-402104b7-b3bd-4c1c-b9b9-126b2223e259 is 0
Mar 13 02:16:19.057: INFO: Restart count of pod container-probe-3097/liveness-402104b7-b3bd-4c1c-b9b9-126b2223e259 is now 1 (16.058319127s elapsed)
Mar 13 02:16:39.130: INFO: Restart count of pod container-probe-3097/liveness-402104b7-b3bd-4c1c-b9b9-126b2223e259 is now 2 (36.131547073s elapsed)
Mar 13 02:16:59.200: INFO: Restart count of pod container-probe-3097/liveness-402104b7-b3bd-4c1c-b9b9-126b2223e259 is now 3 (56.201121375s elapsed)
Mar 13 02:17:19.268: INFO: Restart count of pod container-probe-3097/liveness-402104b7-b3bd-4c1c-b9b9-126b2223e259 is now 4 (1m16.268852325s elapsed)
Mar 13 02:18:21.481: INFO: Restart count of pod container-probe-3097/liveness-402104b7-b3bd-4c1c-b9b9-126b2223e259 is now 5 (2m18.482258976s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:18:21.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3097" for this suite.
Mar 13 02:18:27.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:18:27.702: INFO: namespace container-probe-3097 deletion completed in 6.183366217s

• [SLOW TEST:148.794 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:18:27.703: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 13 02:18:27.834: INFO: Number of nodes with available pods: 0
Mar 13 02:18:27.835: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:18:28.851: INFO: Number of nodes with available pods: 0
Mar 13 02:18:28.851: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:18:29.850: INFO: Number of nodes with available pods: 1
Mar 13 02:18:29.850: INFO: Node sam-node3 is running more than one daemon pod
Mar 13 02:18:30.851: INFO: Number of nodes with available pods: 3
Mar 13 02:18:30.851: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 13 02:18:30.890: INFO: Number of nodes with available pods: 2
Mar 13 02:18:30.890: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:18:31.907: INFO: Number of nodes with available pods: 2
Mar 13 02:18:31.907: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:18:32.905: INFO: Number of nodes with available pods: 2
Mar 13 02:18:32.905: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:18:33.909: INFO: Number of nodes with available pods: 2
Mar 13 02:18:33.910: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:18:34.905: INFO: Number of nodes with available pods: 2
Mar 13 02:18:34.905: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:18:35.908: INFO: Number of nodes with available pods: 2
Mar 13 02:18:35.908: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:18:36.905: INFO: Number of nodes with available pods: 3
Mar 13 02:18:36.906: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9377, will wait for the garbage collector to delete the pods
Mar 13 02:18:36.984: INFO: Deleting DaemonSet.extensions daemon-set took: 13.393244ms
Mar 13 02:18:37.385: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.719406ms
Mar 13 02:18:46.192: INFO: Number of nodes with available pods: 0
Mar 13 02:18:46.192: INFO: Number of running nodes: 0, number of available pods: 0
Mar 13 02:18:46.199: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9377/daemonsets","resourceVersion":"19149"},"items":null}

Mar 13 02:18:46.205: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9377/pods","resourceVersion":"19149"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:18:46.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9377" for this suite.
Mar 13 02:18:52.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:18:52.431: INFO: namespace daemonsets-9377 deletion completed in 6.196275059s

• [SLOW TEST:24.728 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:18:52.433: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 02:18:52.528: INFO: (0) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 21.963161ms)
Mar 13 02:18:52.538: INFO: (1) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.359334ms)
Mar 13 02:18:52.546: INFO: (2) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.835883ms)
Mar 13 02:18:52.555: INFO: (3) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.521591ms)
Mar 13 02:18:52.562: INFO: (4) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.301972ms)
Mar 13 02:18:52.571: INFO: (5) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.578852ms)
Mar 13 02:18:52.579: INFO: (6) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.679401ms)
Mar 13 02:18:52.586: INFO: (7) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.519909ms)
Mar 13 02:18:52.594: INFO: (8) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.134302ms)
Mar 13 02:18:52.603: INFO: (9) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.869774ms)
Mar 13 02:18:52.610: INFO: (10) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.760237ms)
Mar 13 02:18:52.618: INFO: (11) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.852919ms)
Mar 13 02:18:52.627: INFO: (12) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.945245ms)
Mar 13 02:18:52.638: INFO: (13) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.386806ms)
Mar 13 02:18:52.646: INFO: (14) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.56085ms)
Mar 13 02:18:52.652: INFO: (15) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.06387ms)
Mar 13 02:18:52.658: INFO: (16) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.231171ms)
Mar 13 02:18:52.664: INFO: (17) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.09597ms)
Mar 13 02:18:52.672: INFO: (18) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.23766ms)
Mar 13 02:18:52.679: INFO: (19) /api/v1/nodes/sam-node2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.85255ms)
[AfterEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:18:52.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5398" for this suite.
Mar 13 02:18:58.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:18:58.858: INFO: namespace proxy-5398 deletion completed in 6.171310932s

• [SLOW TEST:6.426 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:18:58.859: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 13 02:18:58.939: INFO: Waiting up to 5m0s for pod "pod-0cdae257-ea4a-4c7b-9dae-2d0b04d88ded" in namespace "emptydir-9240" to be "success or failure"
Mar 13 02:18:58.945: INFO: Pod "pod-0cdae257-ea4a-4c7b-9dae-2d0b04d88ded": Phase="Pending", Reason="", readiness=false. Elapsed: 5.681799ms
Mar 13 02:19:00.951: INFO: Pod "pod-0cdae257-ea4a-4c7b-9dae-2d0b04d88ded": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011774718s
Mar 13 02:19:02.959: INFO: Pod "pod-0cdae257-ea4a-4c7b-9dae-2d0b04d88ded": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018988171s
STEP: Saw pod success
Mar 13 02:19:02.959: INFO: Pod "pod-0cdae257-ea4a-4c7b-9dae-2d0b04d88ded" satisfied condition "success or failure"
Mar 13 02:19:02.965: INFO: Trying to get logs from node silbory-nirmata0 pod pod-0cdae257-ea4a-4c7b-9dae-2d0b04d88ded container test-container: <nil>
STEP: delete the pod
Mar 13 02:19:03.022: INFO: Waiting for pod pod-0cdae257-ea4a-4c7b-9dae-2d0b04d88ded to disappear
Mar 13 02:19:03.029: INFO: Pod pod-0cdae257-ea4a-4c7b-9dae-2d0b04d88ded no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:19:03.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9240" for this suite.
Mar 13 02:19:09.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:19:09.209: INFO: namespace emptydir-9240 deletion completed in 6.165663291s

• [SLOW TEST:10.350 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:19:09.209: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 13 02:19:10.669: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 13 02:19:12.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662750, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662750, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662750, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719662750, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 13 02:19:15.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 13 02:19:15.751: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:19:15.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9319" for this suite.
Mar 13 02:19:21.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:19:21.980: INFO: namespace webhook-9319 deletion completed in 6.188326066s
STEP: Destroying namespace "webhook-9319-markers" for this suite.
Mar 13 02:19:28.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:19:28.176: INFO: namespace webhook-9319-markers deletion completed in 6.196350023s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.995 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:19:28.205: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:19:33.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5761" for this suite.
Mar 13 02:20:01.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:20:01.526: INFO: namespace replication-controller-5761 deletion completed in 28.198656938s

• [SLOW TEST:33.321 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:20:01.527: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0313 02:20:32.159420      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 13 02:20:32.159: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:20:32.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9444" for this suite.
Mar 13 02:20:38.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:20:38.383: INFO: namespace gc-9444 deletion completed in 6.215006609s

• [SLOW TEST:36.855 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:20:38.384: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-9355
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9355
STEP: Deleting pre-stop pod
Mar 13 02:20:51.607: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:20:51.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9355" for this suite.
Mar 13 02:21:35.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:21:35.850: INFO: namespace prestop-9355 deletion completed in 44.216626148s

• [SLOW TEST:57.466 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:21:35.850: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 13 02:21:35.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5141'
Mar 13 02:21:36.327: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 13 02:21:36.327: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Mar 13 02:21:36.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 delete deployment e2e-test-httpd-deployment --namespace=kubectl-5141'
Mar 13 02:21:36.587: INFO: stderr: ""
Mar 13 02:21:36.587: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:21:36.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5141" for this suite.
Mar 13 02:21:48.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:21:48.774: INFO: namespace kubectl-5141 deletion completed in 12.179608937s

• [SLOW TEST:12.924 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:21:48.775: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Mar 13 02:21:48.867: INFO: Waiting up to 5m0s for pod "var-expansion-18bbb480-04de-467f-b62d-9f19a380ba5f" in namespace "var-expansion-8241" to be "success or failure"
Mar 13 02:21:48.881: INFO: Pod "var-expansion-18bbb480-04de-467f-b62d-9f19a380ba5f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.913881ms
Mar 13 02:21:50.896: INFO: Pod "var-expansion-18bbb480-04de-467f-b62d-9f19a380ba5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028800007s
Mar 13 02:21:52.903: INFO: Pod "var-expansion-18bbb480-04de-467f-b62d-9f19a380ba5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035220424s
STEP: Saw pod success
Mar 13 02:21:52.903: INFO: Pod "var-expansion-18bbb480-04de-467f-b62d-9f19a380ba5f" satisfied condition "success or failure"
Mar 13 02:21:52.908: INFO: Trying to get logs from node silbory-nirmata0 pod var-expansion-18bbb480-04de-467f-b62d-9f19a380ba5f container dapi-container: <nil>
STEP: delete the pod
Mar 13 02:21:52.961: INFO: Waiting for pod var-expansion-18bbb480-04de-467f-b62d-9f19a380ba5f to disappear
Mar 13 02:21:52.966: INFO: Pod var-expansion-18bbb480-04de-467f-b62d-9f19a380ba5f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:21:52.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8241" for this suite.
Mar 13 02:21:59.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:21:59.171: INFO: namespace var-expansion-8241 deletion completed in 6.197347932s

• [SLOW TEST:10.397 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:21:59.173: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-83c5d4a1-74c4-400c-94f0-0bf458be074c
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:21:59.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9228" for this suite.
Mar 13 02:22:05.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:22:05.416: INFO: namespace configmap-9228 deletion completed in 6.167967057s

• [SLOW TEST:6.243 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:22:05.416: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Mar 13 02:22:05.482: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-940252648 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:22:05.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8443" for this suite.
Mar 13 02:22:11.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:22:11.876: INFO: namespace kubectl-8443 deletion completed in 6.17940221s

• [SLOW TEST:6.460 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:22:11.878: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 02:22:11.998: INFO: Create a RollingUpdate DaemonSet
Mar 13 02:22:12.007: INFO: Check that daemon pods launch on every node of the cluster
Mar 13 02:22:12.023: INFO: Number of nodes with available pods: 0
Mar 13 02:22:12.023: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:22:13.042: INFO: Number of nodes with available pods: 0
Mar 13 02:22:13.042: INFO: Node sam-node2 is running more than one daemon pod
Mar 13 02:22:14.038: INFO: Number of nodes with available pods: 1
Mar 13 02:22:14.038: INFO: Node sam-node3 is running more than one daemon pod
Mar 13 02:22:15.042: INFO: Number of nodes with available pods: 2
Mar 13 02:22:15.042: INFO: Node sam-node3 is running more than one daemon pod
Mar 13 02:22:16.038: INFO: Number of nodes with available pods: 3
Mar 13 02:22:16.038: INFO: Number of running nodes: 3, number of available pods: 3
Mar 13 02:22:16.038: INFO: Update the DaemonSet to trigger a rollout
Mar 13 02:22:16.054: INFO: Updating DaemonSet daemon-set
Mar 13 02:22:22.099: INFO: Roll back the DaemonSet before rollout is complete
Mar 13 02:22:22.122: INFO: Updating DaemonSet daemon-set
Mar 13 02:22:22.123: INFO: Make sure DaemonSet rollback is complete
Mar 13 02:22:22.143: INFO: Wrong image for pod: daemon-set-mbhhb. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 13 02:22:22.143: INFO: Pod daemon-set-mbhhb is not available
Mar 13 02:22:23.168: INFO: Wrong image for pod: daemon-set-mbhhb. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 13 02:22:23.168: INFO: Pod daemon-set-mbhhb is not available
Mar 13 02:22:25.169: INFO: Pod daemon-set-5vg6g is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6558, will wait for the garbage collector to delete the pods
Mar 13 02:22:25.257: INFO: Deleting DaemonSet.extensions daemon-set took: 10.367364ms
Mar 13 02:22:25.558: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.593852ms
Mar 13 02:22:36.164: INFO: Number of nodes with available pods: 0
Mar 13 02:22:36.165: INFO: Number of running nodes: 0, number of available pods: 0
Mar 13 02:22:36.171: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6558/daemonsets","resourceVersion":"19749"},"items":null}

Mar 13 02:22:36.175: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6558/pods","resourceVersion":"19749"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:22:36.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6558" for this suite.
Mar 13 02:22:42.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:22:42.392: INFO: namespace daemonsets-6558 deletion completed in 6.178636459s

• [SLOW TEST:30.514 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:22:42.393: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 13 02:22:52.575: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0313 02:22:52.575815      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:22:52.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2570" for this suite.
Mar 13 02:23:00.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:23:00.758: INFO: namespace gc-2570 deletion completed in 8.176807008s

• [SLOW TEST:18.366 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:23:00.759: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7950
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7950
I0313 02:23:00.876188      25 runners.go:184] Created replication controller with name: externalname-service, namespace: services-7950, replica count: 2
I0313 02:23:03.927121      25 runners.go:184] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 13 02:23:06.927: INFO: Creating new exec pod
I0313 02:23:06.927541      25 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 13 02:23:11.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=services-7950 execpodztpsw -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 13 02:23:12.617: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 13 02:23:12.618: INFO: stdout: ""
Mar 13 02:23:12.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 exec --namespace=services-7950 execpodztpsw -- /bin/sh -x -c nc -zv -t -w 2 10.10.52.66 80'
Mar 13 02:23:13.205: INFO: stderr: "+ nc -zv -t -w 2 10.10.52.66 80\nConnection to 10.10.52.66 80 port [tcp/http] succeeded!\n"
Mar 13 02:23:13.205: INFO: stdout: ""
Mar 13 02:23:13.205: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:23:13.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7950" for this suite.
Mar 13 02:23:19.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:23:19.458: INFO: namespace services-7950 deletion completed in 6.201152102s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:18.700 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:23:19.459: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Mar 13 02:23:19.557: INFO: Waiting up to 5m0s for pod "var-expansion-949bec11-651a-4c16-93aa-6120788407ee" in namespace "var-expansion-7502" to be "success or failure"
Mar 13 02:23:19.582: INFO: Pod "var-expansion-949bec11-651a-4c16-93aa-6120788407ee": Phase="Pending", Reason="", readiness=false. Elapsed: 24.421061ms
Mar 13 02:23:21.589: INFO: Pod "var-expansion-949bec11-651a-4c16-93aa-6120788407ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031648553s
Mar 13 02:23:23.596: INFO: Pod "var-expansion-949bec11-651a-4c16-93aa-6120788407ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038388524s
STEP: Saw pod success
Mar 13 02:23:23.596: INFO: Pod "var-expansion-949bec11-651a-4c16-93aa-6120788407ee" satisfied condition "success or failure"
Mar 13 02:23:23.602: INFO: Trying to get logs from node silbory-nirmata0 pod var-expansion-949bec11-651a-4c16-93aa-6120788407ee container dapi-container: <nil>
STEP: delete the pod
Mar 13 02:23:23.655: INFO: Waiting for pod var-expansion-949bec11-651a-4c16-93aa-6120788407ee to disappear
Mar 13 02:23:23.661: INFO: Pod var-expansion-949bec11-651a-4c16-93aa-6120788407ee no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:23:23.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7502" for this suite.
Mar 13 02:23:29.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:23:29.847: INFO: namespace var-expansion-7502 deletion completed in 6.178028109s

• [SLOW TEST:10.388 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:23:29.849: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 13 02:23:29.919: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
Mar 13 02:23:35.698: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:23:59.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9863" for this suite.
Mar 13 02:24:05.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:24:05.753: INFO: namespace crd-publish-openapi-9863 deletion completed in 6.18137578s

• [SLOW TEST:35.905 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:24:05.754: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-2b161b39-39f8-4a82-9ecb-f26bf091190c
STEP: Creating secret with name s-test-opt-upd-79733932-18b2-4af2-a3b6-ee39e14f0391
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-2b161b39-39f8-4a82-9ecb-f26bf091190c
STEP: Updating secret s-test-opt-upd-79733932-18b2-4af2-a3b6-ee39e14f0391
STEP: Creating secret with name s-test-opt-create-2058925f-d663-46b9-908c-33543deeeaf4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:25:40.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9509" for this suite.
Mar 13 02:25:58.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:25:59.176: INFO: namespace projected-9509 deletion completed in 18.200460039s

• [SLOW TEST:113.422 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:25:59.176: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-69f4c3fb-2984-4c8f-ba3b-00c8ceccf015
STEP: Creating a pod to test consume secrets
Mar 13 02:25:59.267: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-63445ae8-7bbe-4561-b5f7-79bf4efc1fb4" in namespace "projected-7010" to be "success or failure"
Mar 13 02:25:59.273: INFO: Pod "pod-projected-secrets-63445ae8-7bbe-4561-b5f7-79bf4efc1fb4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.890531ms
Mar 13 02:26:01.280: INFO: Pod "pod-projected-secrets-63445ae8-7bbe-4561-b5f7-79bf4efc1fb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012411361s
Mar 13 02:26:03.287: INFO: Pod "pod-projected-secrets-63445ae8-7bbe-4561-b5f7-79bf4efc1fb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020041828s
STEP: Saw pod success
Mar 13 02:26:03.287: INFO: Pod "pod-projected-secrets-63445ae8-7bbe-4561-b5f7-79bf4efc1fb4" satisfied condition "success or failure"
Mar 13 02:26:03.294: INFO: Trying to get logs from node silbory-nirmata0 pod pod-projected-secrets-63445ae8-7bbe-4561-b5f7-79bf4efc1fb4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 13 02:26:03.332: INFO: Waiting for pod pod-projected-secrets-63445ae8-7bbe-4561-b5f7-79bf4efc1fb4 to disappear
Mar 13 02:26:03.339: INFO: Pod pod-projected-secrets-63445ae8-7bbe-4561-b5f7-79bf4efc1fb4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:26:03.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7010" for this suite.
Mar 13 02:26:09.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:26:09.530: INFO: namespace projected-7010 deletion completed in 6.183819493s

• [SLOW TEST:10.353 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:26:09.530: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 02:26:09.594: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar 13 02:26:15.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-4437 create -f -'
Mar 13 02:26:16.371: INFO: stderr: ""
Mar 13 02:26:16.371: INFO: stdout: "e2e-test-crd-publish-openapi-3857-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 13 02:26:16.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-4437 delete e2e-test-crd-publish-openapi-3857-crds test-foo'
Mar 13 02:26:16.595: INFO: stderr: ""
Mar 13 02:26:16.595: INFO: stdout: "e2e-test-crd-publish-openapi-3857-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 13 02:26:16.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-4437 apply -f -'
Mar 13 02:26:17.003: INFO: stderr: ""
Mar 13 02:26:17.003: INFO: stdout: "e2e-test-crd-publish-openapi-3857-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 13 02:26:17.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-4437 delete e2e-test-crd-publish-openapi-3857-crds test-foo'
Mar 13 02:26:17.239: INFO: stderr: ""
Mar 13 02:26:17.239: INFO: stdout: "e2e-test-crd-publish-openapi-3857-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 13 02:26:17.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-4437 create -f -'
Mar 13 02:26:17.602: INFO: rc: 1
Mar 13 02:26:17.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-4437 apply -f -'
Mar 13 02:26:17.951: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar 13 02:26:17.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-4437 create -f -'
Mar 13 02:26:18.318: INFO: rc: 1
Mar 13 02:26:18.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 --namespace=crd-publish-openapi-4437 apply -f -'
Mar 13 02:26:18.694: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 13 02:26:18.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 explain e2e-test-crd-publish-openapi-3857-crds'
Mar 13 02:26:19.110: INFO: stderr: ""
Mar 13 02:26:19.110: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3857-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 13 02:26:19.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 explain e2e-test-crd-publish-openapi-3857-crds.metadata'
Mar 13 02:26:19.480: INFO: stderr: ""
Mar 13 02:26:19.480: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3857-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 13 02:26:19.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 explain e2e-test-crd-publish-openapi-3857-crds.spec'
Mar 13 02:26:19.895: INFO: stderr: ""
Mar 13 02:26:19.895: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3857-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 13 02:26:19.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 explain e2e-test-crd-publish-openapi-3857-crds.spec.bars'
Mar 13 02:26:20.335: INFO: stderr: ""
Mar 13 02:26:20.335: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3857-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 13 02:26:20.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940252648 explain e2e-test-crd-publish-openapi-3857-crds.spec.bars2'
Mar 13 02:26:20.752: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:26:26.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4437" for this suite.
Mar 13 02:26:32.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:26:32.525: INFO: namespace crd-publish-openapi-4437 deletion completed in 6.183117958s

• [SLOW TEST:22.995 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:26:32.525: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 13 02:26:32.601: INFO: Waiting up to 5m0s for pod "downward-api-26399c47-6997-48a7-866b-da52df963991" in namespace "downward-api-3916" to be "success or failure"
Mar 13 02:26:32.607: INFO: Pod "downward-api-26399c47-6997-48a7-866b-da52df963991": Phase="Pending", Reason="", readiness=false. Elapsed: 5.64473ms
Mar 13 02:26:34.613: INFO: Pod "downward-api-26399c47-6997-48a7-866b-da52df963991": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011994126s
Mar 13 02:26:36.621: INFO: Pod "downward-api-26399c47-6997-48a7-866b-da52df963991": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019466973s
STEP: Saw pod success
Mar 13 02:26:36.621: INFO: Pod "downward-api-26399c47-6997-48a7-866b-da52df963991" satisfied condition "success or failure"
Mar 13 02:26:36.627: INFO: Trying to get logs from node silbory-nirmata0 pod downward-api-26399c47-6997-48a7-866b-da52df963991 container dapi-container: <nil>
STEP: delete the pod
Mar 13 02:26:36.664: INFO: Waiting for pod downward-api-26399c47-6997-48a7-866b-da52df963991 to disappear
Mar 13 02:26:36.670: INFO: Pod downward-api-26399c47-6997-48a7-866b-da52df963991 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:26:36.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3916" for this suite.
Mar 13 02:26:42.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:26:42.839: INFO: namespace downward-api-3916 deletion completed in 6.164367015s

• [SLOW TEST:10.314 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:26:42.841: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 13 02:26:42.926: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 13 02:26:47.935: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 13 02:26:47.936: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 13 02:26:49.943: INFO: Creating deployment "test-rollover-deployment"
Mar 13 02:26:49.954: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 13 02:26:51.968: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 13 02:26:51.980: INFO: Ensure that both replica sets have 1 created replica
Mar 13 02:26:51.993: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 13 02:26:52.006: INFO: Updating deployment test-rollover-deployment
Mar 13 02:26:52.006: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 13 02:26:54.022: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 13 02:26:54.034: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 13 02:26:54.045: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 02:26:54.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663210, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663210, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663212, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663209, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 02:26:56.059: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 02:26:56.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663210, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663210, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663214, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663209, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 02:26:58.059: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 02:26:58.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663210, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663210, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663214, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663209, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 02:27:00.061: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 02:27:00.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663210, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663210, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663214, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663209, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 02:27:02.057: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 02:27:02.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663210, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663210, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663214, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663209, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 02:27:04.060: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 02:27:04.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663210, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663210, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663214, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719663209, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 02:27:06.069: INFO: 
Mar 13 02:27:06.069: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 13 02:27:06.084: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4412 /apis/apps/v1/namespaces/deployment-4412/deployments/test-rollover-deployment 60030bb3-da92-4ee9-aee4-938ad0e934f1 20486 2 2020-03-13 02:26:49 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003e51aa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-13 02:26:50 +0000 UTC,LastTransitionTime:2020-03-13 02:26:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-03-13 02:27:04 +0000 UTC,LastTransitionTime:2020-03-13 02:26:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 13 02:27:06.092: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-4412 /apis/apps/v1/namespaces/deployment-4412/replicasets/test-rollover-deployment-7d7dc6548c 549a42a5-39a3-415b-880a-80e0f081503a 20475 2 2020-03-13 02:26:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 60030bb3-da92-4ee9-aee4-938ad0e934f1 0xc0040aa057 0xc0040aa058}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0040aa0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 13 02:27:06.092: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 13 02:27:06.092: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4412 /apis/apps/v1/namespaces/deployment-4412/replicasets/test-rollover-controller 49fbff35-465a-4089-acf9-464da42ce2e0 20485 2 2020-03-13 02:26:42 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 60030bb3-da92-4ee9-aee4-938ad0e934f1 0xc003e51f87 0xc003e51f88}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003e51fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 13 02:27:06.093: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-4412 /apis/apps/v1/namespaces/deployment-4412/replicasets/test-rollover-deployment-f6c94f66c 392e33a5-b2bf-4e28-8697-04f2887c2039 20454 2 2020-03-13 02:26:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 60030bb3-da92-4ee9-aee4-938ad0e934f1 0xc0040aa120 0xc0040aa121}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0040aa198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 13 02:27:06.102: INFO: Pod "test-rollover-deployment-7d7dc6548c-nthdz" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-nthdz test-rollover-deployment-7d7dc6548c- deployment-4412 /api/v1/namespaces/deployment-4412/pods/test-rollover-deployment-7d7dc6548c-nthdz 1f343126-2429-4432-8abc-8bb8a4d66856 20467 0 2020-03-13 02:26:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 549a42a5-39a3-415b-880a-80e0f081503a 0xc0035ddb87 0xc0035ddb88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rc54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rc54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rc54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:silbory-nirmata0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 02:26:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 02:26:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 02:26:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-13 02:26:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.79,PodIP:10.244.2.61,StartTime:2020-03-13 02:26:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-13 02:26:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://eda84f13b472cbd3b7085dd1477435e61d6277b7871fd4d692b82bbabb8c66d1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:27:06.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4412" for this suite.
Mar 13 02:27:12.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:27:12.309: INFO: namespace deployment-4412 deletion completed in 6.198280146s

• [SLOW TEST:29.469 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:27:12.312: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 13 02:27:12.422: INFO: Waiting up to 5m0s for pod "downward-api-24abe89a-481e-4d6c-9a6d-b11974d3743d" in namespace "downward-api-6064" to be "success or failure"
Mar 13 02:27:12.430: INFO: Pod "downward-api-24abe89a-481e-4d6c-9a6d-b11974d3743d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.277034ms
Mar 13 02:27:14.438: INFO: Pod "downward-api-24abe89a-481e-4d6c-9a6d-b11974d3743d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015642535s
Mar 13 02:27:16.445: INFO: Pod "downward-api-24abe89a-481e-4d6c-9a6d-b11974d3743d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022106686s
STEP: Saw pod success
Mar 13 02:27:16.445: INFO: Pod "downward-api-24abe89a-481e-4d6c-9a6d-b11974d3743d" satisfied condition "success or failure"
Mar 13 02:27:16.451: INFO: Trying to get logs from node silbory-nirmata0 pod downward-api-24abe89a-481e-4d6c-9a6d-b11974d3743d container dapi-container: <nil>
STEP: delete the pod
Mar 13 02:27:16.499: INFO: Waiting for pod downward-api-24abe89a-481e-4d6c-9a6d-b11974d3743d to disappear
Mar 13 02:27:16.506: INFO: Pod downward-api-24abe89a-481e-4d6c-9a6d-b11974d3743d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:27:16.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6064" for this suite.
Mar 13 02:27:22.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:27:22.692: INFO: namespace downward-api-6064 deletion completed in 6.17771541s

• [SLOW TEST:10.380 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:27:22.693: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-tdjs
STEP: Creating a pod to test atomic-volume-subpath
Mar 13 02:27:22.855: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tdjs" in namespace "subpath-681" to be "success or failure"
Mar 13 02:27:22.861: INFO: Pod "pod-subpath-test-configmap-tdjs": Phase="Pending", Reason="", readiness=false. Elapsed: 5.428747ms
Mar 13 02:27:24.868: INFO: Pod "pod-subpath-test-configmap-tdjs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012983689s
Mar 13 02:27:26.876: INFO: Pod "pod-subpath-test-configmap-tdjs": Phase="Running", Reason="", readiness=true. Elapsed: 4.020729385s
Mar 13 02:27:28.885: INFO: Pod "pod-subpath-test-configmap-tdjs": Phase="Running", Reason="", readiness=true. Elapsed: 6.029719197s
Mar 13 02:27:30.892: INFO: Pod "pod-subpath-test-configmap-tdjs": Phase="Running", Reason="", readiness=true. Elapsed: 8.036603525s
Mar 13 02:27:32.898: INFO: Pod "pod-subpath-test-configmap-tdjs": Phase="Running", Reason="", readiness=true. Elapsed: 10.043086763s
Mar 13 02:27:34.905: INFO: Pod "pod-subpath-test-configmap-tdjs": Phase="Running", Reason="", readiness=true. Elapsed: 12.049365331s
Mar 13 02:27:36.911: INFO: Pod "pod-subpath-test-configmap-tdjs": Phase="Running", Reason="", readiness=true. Elapsed: 14.05602053s
Mar 13 02:27:38.918: INFO: Pod "pod-subpath-test-configmap-tdjs": Phase="Running", Reason="", readiness=true. Elapsed: 16.063018219s
Mar 13 02:27:40.928: INFO: Pod "pod-subpath-test-configmap-tdjs": Phase="Running", Reason="", readiness=true. Elapsed: 18.072831622s
Mar 13 02:27:42.935: INFO: Pod "pod-subpath-test-configmap-tdjs": Phase="Running", Reason="", readiness=true. Elapsed: 20.080125944s
Mar 13 02:27:44.942: INFO: Pod "pod-subpath-test-configmap-tdjs": Phase="Running", Reason="", readiness=true. Elapsed: 22.086496647s
Mar 13 02:27:46.947: INFO: Pod "pod-subpath-test-configmap-tdjs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.091665266s
STEP: Saw pod success
Mar 13 02:27:46.947: INFO: Pod "pod-subpath-test-configmap-tdjs" satisfied condition "success or failure"
Mar 13 02:27:46.951: INFO: Trying to get logs from node silbory-nirmata0 pod pod-subpath-test-configmap-tdjs container test-container-subpath-configmap-tdjs: <nil>
STEP: delete the pod
Mar 13 02:27:46.988: INFO: Waiting for pod pod-subpath-test-configmap-tdjs to disappear
Mar 13 02:27:46.996: INFO: Pod pod-subpath-test-configmap-tdjs no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tdjs
Mar 13 02:27:46.996: INFO: Deleting pod "pod-subpath-test-configmap-tdjs" in namespace "subpath-681"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:27:47.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-681" for this suite.
Mar 13 02:27:53.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:27:53.203: INFO: namespace subpath-681 deletion completed in 6.189333094s

• [SLOW TEST:30.510 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 13 02:27:53.204: INFO: >>> kubeConfig: /tmp/kubeconfig-940252648
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 13 02:27:57.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3973" for this suite.
Mar 13 02:28:41.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 02:28:41.526: INFO: namespace kubelet-test-3973 deletion completed in 44.196080364s

• [SLOW TEST:48.322 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSMar 13 02:28:41.527: INFO: Running AfterSuite actions on all nodes
Mar 13 02:28:41.527: INFO: Running AfterSuite actions on node 1
Mar 13 02:28:41.527: INFO: Skipping dumping logs from cluster

Ran 274 of 4897 Specs in 7599.201 seconds
SUCCESS! -- 274 Passed | 0 Failed | 0 Pending | 4623 Skipped
PASS

Ginkgo ran 1 suite in 2h6m43.176607025s
Test Suite Passed
