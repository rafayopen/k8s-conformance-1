I0103 12:58:38.713237      23 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-937082782
I0103 12:58:38.713414      23 e2e.go:92] Starting e2e run "44774371-286b-4fa5-bb21-ee06b15f775f" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1578056317 - Will randomize all specs
Will run 276 of 4897 specs

Jan  3 12:58:38.731: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 12:58:38.734: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan  3 12:58:38.750: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan  3 12:58:38.773: INFO: The status of Pod ceph-pools-audit-1578055500-klsmd is Succeeded, skipping waiting
Jan  3 12:58:38.773: INFO: The status of Pod ceph-pools-audit-1578055800-68tpt is Succeeded, skipping waiting
Jan  3 12:58:38.773: INFO: The status of Pod ceph-pools-audit-1578056100-6lh95 is Succeeded, skipping waiting
Jan  3 12:58:38.773: INFO: The status of Pod storage-init-rbd-provisioner-mt758 is Succeeded, skipping waiting
Jan  3 12:58:38.773: INFO: 24 / 28 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan  3 12:58:38.773: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Jan  3 12:58:38.773: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan  3 12:58:38.779: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan  3 12:58:38.779: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'ingress' (0 seconds elapsed)
Jan  3 12:58:38.779: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds-amd64' (0 seconds elapsed)
Jan  3 12:58:38.779: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan  3 12:58:38.779: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-sriov-cni-ds-amd64' (0 seconds elapsed)
Jan  3 12:58:38.779: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-sriov-device-plugin-amd64' (0 seconds elapsed)
Jan  3 12:58:38.779: INFO: e2e test version: v1.16.2
Jan  3 12:58:38.779: INFO: kube-apiserver version: v1.16.2
Jan  3 12:58:38.779: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 12:58:38.782: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 12:58:38.783: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename job
Jan  3 12:58:38.801: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9686, will wait for the garbage collector to delete the pods
Jan  3 12:58:56.859: INFO: Deleting Job.batch foo took: 1.959623ms
Jan  3 12:58:57.259: INFO: Terminating Job.batch foo pods took: 400.168048ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 12:59:36.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9686" for this suite.
Jan  3 12:59:42.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 12:59:43.227: INFO: namespace job-9686 deletion completed in 6.262939009s

• [SLOW TEST:64.444 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 12:59:43.228: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan  3 12:59:43.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5623'
Jan  3 12:59:43.447: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan  3 12:59:43.447: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Jan  3 12:59:43.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete jobs e2e-test-httpd-job --namespace=kubectl-5623'
Jan  3 12:59:43.547: INFO: stderr: ""
Jan  3 12:59:43.547: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 12:59:43.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5623" for this suite.
Jan  3 12:59:49.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 12:59:49.655: INFO: namespace kubectl-5623 deletion completed in 6.106105472s

• [SLOW TEST:6.428 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 12:59:49.655: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jan  3 12:59:49.674: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Jan  3 12:59:50.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:59:52.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:59:54.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:59:56.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:59:58.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:00:00.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:00:02.155: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:00:04.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:00:06.151: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:00:08.155: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:00:10.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653190, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:00:13.122: INFO: Waited 962.305605ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:00:13.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-83" for this suite.
Jan  3 13:00:19.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:00:19.960: INFO: namespace aggregator-83 deletion completed in 6.190669429s

• [SLOW TEST:30.305 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:00:19.961: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-49455c26-e29a-4a64-b193-6dc3c2a0cfa5
STEP: Creating a pod to test consume secrets
Jan  3 13:00:19.985: INFO: Waiting up to 5m0s for pod "pod-secrets-f703ecdc-c7ec-48b8-b09d-b147ddb21b07" in namespace "secrets-9840" to be "success or failure"
Jan  3 13:00:19.988: INFO: Pod "pod-secrets-f703ecdc-c7ec-48b8-b09d-b147ddb21b07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131826ms
Jan  3 13:00:21.997: INFO: Pod "pod-secrets-f703ecdc-c7ec-48b8-b09d-b147ddb21b07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01202003s
Jan  3 13:00:24.002: INFO: Pod "pod-secrets-f703ecdc-c7ec-48b8-b09d-b147ddb21b07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016668103s
Jan  3 13:00:26.005: INFO: Pod "pod-secrets-f703ecdc-c7ec-48b8-b09d-b147ddb21b07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019223716s
Jan  3 13:00:28.006: INFO: Pod "pod-secrets-f703ecdc-c7ec-48b8-b09d-b147ddb21b07": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02057967s
Jan  3 13:00:30.008: INFO: Pod "pod-secrets-f703ecdc-c7ec-48b8-b09d-b147ddb21b07": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02221733s
Jan  3 13:00:32.009: INFO: Pod "pod-secrets-f703ecdc-c7ec-48b8-b09d-b147ddb21b07": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02370176s
Jan  3 13:00:34.011: INFO: Pod "pod-secrets-f703ecdc-c7ec-48b8-b09d-b147ddb21b07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.025175972s
STEP: Saw pod success
Jan  3 13:00:34.011: INFO: Pod "pod-secrets-f703ecdc-c7ec-48b8-b09d-b147ddb21b07" satisfied condition "success or failure"
Jan  3 13:00:34.012: INFO: Trying to get logs from node controller-0 pod pod-secrets-f703ecdc-c7ec-48b8-b09d-b147ddb21b07 container secret-volume-test: <nil>
STEP: delete the pod
Jan  3 13:00:34.029: INFO: Waiting for pod pod-secrets-f703ecdc-c7ec-48b8-b09d-b147ddb21b07 to disappear
Jan  3 13:00:34.030: INFO: Pod pod-secrets-f703ecdc-c7ec-48b8-b09d-b147ddb21b07 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:00:34.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9840" for this suite.
Jan  3 13:00:40.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:00:40.169: INFO: namespace secrets-9840 deletion completed in 6.124292727s

• [SLOW TEST:20.208 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:00:40.173: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:00:56.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1619" for this suite.
Jan  3 13:01:02.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:01:02.508: INFO: namespace resourcequota-1619 deletion completed in 6.035153156s

• [SLOW TEST:22.335 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:01:02.508: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-5dce8cd5-1249-4a14-8da6-1b6b5980ffc9
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:01:02.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7797" for this suite.
Jan  3 13:01:08.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:01:08.559: INFO: namespace secrets-7797 deletion completed in 6.036357474s

• [SLOW TEST:6.051 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:01:08.560: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3339.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3339.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3339.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3339.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan  3 13:02:14.587: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:14.588: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:14.589: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:14.590: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:14.593: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:14.594: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:14.595: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:14.596: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:14.600: INFO: Lookups using dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3339.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3339.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local jessie_udp@dns-test-service-2.dns-3339.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3339.svc.cluster.local]

Jan  3 13:02:19.604: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:19.606: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:19.608: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:19.610: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:19.615: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:19.616: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:19.617: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:19.618: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:19.621: INFO: Lookups using dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3339.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3339.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local jessie_udp@dns-test-service-2.dns-3339.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3339.svc.cluster.local]

Jan  3 13:02:24.602: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:24.604: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:24.606: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:24.607: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:24.610: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:24.611: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:24.612: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:24.613: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:24.615: INFO: Lookups using dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3339.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3339.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local jessie_udp@dns-test-service-2.dns-3339.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3339.svc.cluster.local]

Jan  3 13:02:29.602: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:29.603: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:29.604: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:29.606: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3339.svc.cluster.local from pod dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd: the server could not find the requested resource (get pods dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd)
Jan  3 13:02:29.614: INFO: Lookups using dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3339.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3339.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3339.svc.cluster.local]

Jan  3 13:02:34.627: INFO: DNS probes using dns-3339/dns-test-bd3f9f97-e66c-446f-82f8-0ef6b77b55fd succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:02:34.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3339" for this suite.
Jan  3 13:02:40.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:02:40.701: INFO: namespace dns-3339 deletion completed in 6.042586148s

• [SLOW TEST:92.141 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:02:40.701: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:02:56.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1821" for this suite.
Jan  3 13:03:02.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:03:02.823: INFO: namespace resourcequota-1821 deletion completed in 6.041523053s

• [SLOW TEST:22.122 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:03:02.823: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 13:03:02.838: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdd02a6d-2bc1-4b14-a8a8-3f952f1d3b58" in namespace "projected-2876" to be "success or failure"
Jan  3 13:03:02.839: INFO: Pod "downwardapi-volume-bdd02a6d-2bc1-4b14-a8a8-3f952f1d3b58": Phase="Pending", Reason="", readiness=false. Elapsed: 984.017µs
Jan  3 13:03:04.840: INFO: Pod "downwardapi-volume-bdd02a6d-2bc1-4b14-a8a8-3f952f1d3b58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002590385s
Jan  3 13:03:06.846: INFO: Pod "downwardapi-volume-bdd02a6d-2bc1-4b14-a8a8-3f952f1d3b58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008352599s
Jan  3 13:03:08.848: INFO: Pod "downwardapi-volume-bdd02a6d-2bc1-4b14-a8a8-3f952f1d3b58": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010050869s
Jan  3 13:03:10.858: INFO: Pod "downwardapi-volume-bdd02a6d-2bc1-4b14-a8a8-3f952f1d3b58": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020284752s
Jan  3 13:03:12.860: INFO: Pod "downwardapi-volume-bdd02a6d-2bc1-4b14-a8a8-3f952f1d3b58": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021968041s
Jan  3 13:03:14.866: INFO: Pod "downwardapi-volume-bdd02a6d-2bc1-4b14-a8a8-3f952f1d3b58": Phase="Pending", Reason="", readiness=false. Elapsed: 12.027983617s
Jan  3 13:03:16.867: INFO: Pod "downwardapi-volume-bdd02a6d-2bc1-4b14-a8a8-3f952f1d3b58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.029782741s
STEP: Saw pod success
Jan  3 13:03:16.867: INFO: Pod "downwardapi-volume-bdd02a6d-2bc1-4b14-a8a8-3f952f1d3b58" satisfied condition "success or failure"
Jan  3 13:03:16.868: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-bdd02a6d-2bc1-4b14-a8a8-3f952f1d3b58 container client-container: <nil>
STEP: delete the pod
Jan  3 13:03:16.911: INFO: Waiting for pod downwardapi-volume-bdd02a6d-2bc1-4b14-a8a8-3f952f1d3b58 to disappear
Jan  3 13:03:16.941: INFO: Pod downwardapi-volume-bdd02a6d-2bc1-4b14-a8a8-3f952f1d3b58 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:03:16.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2876" for this suite.
Jan  3 13:03:22.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:03:22.995: INFO: namespace projected-2876 deletion completed in 6.052741339s

• [SLOW TEST:20.172 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:03:22.995: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan  3 13:03:23.092: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:03:59.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-477" for this suite.
Jan  3 13:04:05.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:04:05.394: INFO: namespace pods-477 deletion completed in 6.034309218s

• [SLOW TEST:42.399 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:04:05.394: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-6b56a764-160a-4e0d-b56e-abe320d34519
STEP: Creating a pod to test consume secrets
Jan  3 13:04:05.416: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5280edef-d010-44fb-b16a-90406f7698f8" in namespace "projected-1641" to be "success or failure"
Jan  3 13:04:05.417: INFO: Pod "pod-projected-secrets-5280edef-d010-44fb-b16a-90406f7698f8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.376686ms
Jan  3 13:04:07.422: INFO: Pod "pod-projected-secrets-5280edef-d010-44fb-b16a-90406f7698f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005852866s
Jan  3 13:04:09.424: INFO: Pod "pod-projected-secrets-5280edef-d010-44fb-b16a-90406f7698f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007706431s
Jan  3 13:04:11.430: INFO: Pod "pod-projected-secrets-5280edef-d010-44fb-b16a-90406f7698f8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013791614s
Jan  3 13:04:13.431: INFO: Pod "pod-projected-secrets-5280edef-d010-44fb-b16a-90406f7698f8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015482297s
Jan  3 13:04:15.433: INFO: Pod "pod-projected-secrets-5280edef-d010-44fb-b16a-90406f7698f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.017173771s
STEP: Saw pod success
Jan  3 13:04:15.433: INFO: Pod "pod-projected-secrets-5280edef-d010-44fb-b16a-90406f7698f8" satisfied condition "success or failure"
Jan  3 13:04:15.434: INFO: Trying to get logs from node controller-0 pod pod-projected-secrets-5280edef-d010-44fb-b16a-90406f7698f8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan  3 13:04:15.445: INFO: Waiting for pod pod-projected-secrets-5280edef-d010-44fb-b16a-90406f7698f8 to disappear
Jan  3 13:04:15.446: INFO: Pod pod-projected-secrets-5280edef-d010-44fb-b16a-90406f7698f8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:04:15.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1641" for this suite.
Jan  3 13:04:21.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:04:21.534: INFO: namespace projected-1641 deletion completed in 6.08584053s

• [SLOW TEST:16.140 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:04:21.537: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-d0a14921-915a-474d-9b0e-795172128b26
STEP: Creating configMap with name cm-test-opt-upd-6e509fb9-a545-464d-813d-5f8829cacbc6
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d0a14921-915a-474d-9b0e-795172128b26
STEP: Updating configmap cm-test-opt-upd-6e509fb9-a545-464d-813d-5f8829cacbc6
STEP: Creating configMap with name cm-test-opt-create-3366a2ba-0978-4cbd-9f49-4b64ba32a50c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:04:31.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2425" for this suite.
Jan  3 13:04:49.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:04:49.719: INFO: namespace projected-2425 deletion completed in 18.044129879s

• [SLOW TEST:28.182 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:04:49.719: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Jan  3 13:04:49.739: INFO: Waiting up to 5m0s for pod "pod-4b704346-d345-47b6-befc-6ccea19e2ebc" in namespace "emptydir-4194" to be "success or failure"
Jan  3 13:04:49.740: INFO: Pod "pod-4b704346-d345-47b6-befc-6ccea19e2ebc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.622348ms
Jan  3 13:04:51.743: INFO: Pod "pod-4b704346-d345-47b6-befc-6ccea19e2ebc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003748966s
Jan  3 13:04:53.747: INFO: Pod "pod-4b704346-d345-47b6-befc-6ccea19e2ebc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008015457s
Jan  3 13:04:55.748: INFO: Pod "pod-4b704346-d345-47b6-befc-6ccea19e2ebc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009467532s
Jan  3 13:04:57.756: INFO: Pod "pod-4b704346-d345-47b6-befc-6ccea19e2ebc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017477375s
Jan  3 13:04:59.759: INFO: Pod "pod-4b704346-d345-47b6-befc-6ccea19e2ebc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.020119088s
STEP: Saw pod success
Jan  3 13:04:59.759: INFO: Pod "pod-4b704346-d345-47b6-befc-6ccea19e2ebc" satisfied condition "success or failure"
Jan  3 13:04:59.762: INFO: Trying to get logs from node controller-0 pod pod-4b704346-d345-47b6-befc-6ccea19e2ebc container test-container: <nil>
STEP: delete the pod
Jan  3 13:04:59.772: INFO: Waiting for pod pod-4b704346-d345-47b6-befc-6ccea19e2ebc to disappear
Jan  3 13:04:59.773: INFO: Pod pod-4b704346-d345-47b6-befc-6ccea19e2ebc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:04:59.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4194" for this suite.
Jan  3 13:05:05.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:05:05.817: INFO: namespace emptydir-4194 deletion completed in 6.04159776s

• [SLOW TEST:16.098 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:05:05.817: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 13:05:05.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aaf1af06-9f3a-46e1-b57b-d9924be370af" in namespace "projected-3516" to be "success or failure"
Jan  3 13:05:05.834: INFO: Pod "downwardapi-volume-aaf1af06-9f3a-46e1-b57b-d9924be370af": Phase="Pending", Reason="", readiness=false. Elapsed: 1.598718ms
Jan  3 13:05:07.842: INFO: Pod "downwardapi-volume-aaf1af06-9f3a-46e1-b57b-d9924be370af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010030498s
Jan  3 13:05:09.853: INFO: Pod "downwardapi-volume-aaf1af06-9f3a-46e1-b57b-d9924be370af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021171957s
Jan  3 13:05:11.860: INFO: Pod "downwardapi-volume-aaf1af06-9f3a-46e1-b57b-d9924be370af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0278637s
Jan  3 13:05:13.861: INFO: Pod "downwardapi-volume-aaf1af06-9f3a-46e1-b57b-d9924be370af": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029394824s
Jan  3 13:05:15.863: INFO: Pod "downwardapi-volume-aaf1af06-9f3a-46e1-b57b-d9924be370af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.030932579s
STEP: Saw pod success
Jan  3 13:05:15.863: INFO: Pod "downwardapi-volume-aaf1af06-9f3a-46e1-b57b-d9924be370af" satisfied condition "success or failure"
Jan  3 13:05:15.864: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-aaf1af06-9f3a-46e1-b57b-d9924be370af container client-container: <nil>
STEP: delete the pod
Jan  3 13:05:15.873: INFO: Waiting for pod downwardapi-volume-aaf1af06-9f3a-46e1-b57b-d9924be370af to disappear
Jan  3 13:05:15.875: INFO: Pod downwardapi-volume-aaf1af06-9f3a-46e1-b57b-d9924be370af no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:05:15.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3516" for this suite.
Jan  3 13:05:21.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:05:22.078: INFO: namespace projected-3516 deletion completed in 6.199765262s

• [SLOW TEST:16.261 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:05:22.078: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:05:22.174: INFO: (0) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 36.358315ms)
Jan  3 13:05:22.182: INFO: (1) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.383607ms)
Jan  3 13:05:22.189: INFO: (2) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 7.432581ms)
Jan  3 13:05:22.200: INFO: (3) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 10.741838ms)
Jan  3 13:05:22.204: INFO: (4) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 4.081613ms)
Jan  3 13:05:22.206: INFO: (5) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.082615ms)
Jan  3 13:05:22.209: INFO: (6) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.427577ms)
Jan  3 13:05:22.225: INFO: (7) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 15.574788ms)
Jan  3 13:05:22.257: INFO: (8) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 32.002562ms)
Jan  3 13:05:22.302: INFO: (9) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 41.917514ms)
Jan  3 13:05:22.305: INFO: (10) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 3.518351ms)
Jan  3 13:05:22.308: INFO: (11) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.737209ms)
Jan  3 13:05:22.321: INFO: (12) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 12.395887ms)
Jan  3 13:05:22.329: INFO: (13) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 5.450235ms)
Jan  3 13:05:22.331: INFO: (14) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.394413ms)
Jan  3 13:05:22.333: INFO: (15) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 1.840895ms)
Jan  3 13:05:22.336: INFO: (16) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.233704ms)
Jan  3 13:05:22.347: INFO: (17) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 11.188297ms)
Jan  3 13:05:22.371: INFO: (18) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 3.275809ms)
Jan  3 13:05:22.374: INFO: (19) /api/v1/nodes/controller-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.398722ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:05:22.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7752" for this suite.
Jan  3 13:05:28.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:05:28.412: INFO: namespace proxy-7752 deletion completed in 6.035955941s

• [SLOW TEST:6.334 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:05:28.412: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:05:58.436: INFO: Container started at 2020-01-03 13:05:41 +0000 UTC, pod became ready at 2020-01-03 13:05:57 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:05:58.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3161" for this suite.
Jan  3 13:06:10.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:06:10.474: INFO: namespace container-probe-3161 deletion completed in 12.036041603s

• [SLOW TEST:42.062 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:06:10.474: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 13:06:10.493: INFO: Waiting up to 5m0s for pod "downwardapi-volume-30af02dd-bfd0-4aff-900c-d4686cb78f63" in namespace "downward-api-9532" to be "success or failure"
Jan  3 13:06:10.494: INFO: Pod "downwardapi-volume-30af02dd-bfd0-4aff-900c-d4686cb78f63": Phase="Pending", Reason="", readiness=false. Elapsed: 1.134606ms
Jan  3 13:06:12.509: INFO: Pod "downwardapi-volume-30af02dd-bfd0-4aff-900c-d4686cb78f63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015289045s
Jan  3 13:06:14.514: INFO: Pod "downwardapi-volume-30af02dd-bfd0-4aff-900c-d4686cb78f63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0211633s
Jan  3 13:06:16.518: INFO: Pod "downwardapi-volume-30af02dd-bfd0-4aff-900c-d4686cb78f63": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024544657s
Jan  3 13:06:18.572: INFO: Pod "downwardapi-volume-30af02dd-bfd0-4aff-900c-d4686cb78f63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.078983638s
STEP: Saw pod success
Jan  3 13:06:18.572: INFO: Pod "downwardapi-volume-30af02dd-bfd0-4aff-900c-d4686cb78f63" satisfied condition "success or failure"
Jan  3 13:06:18.578: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-30af02dd-bfd0-4aff-900c-d4686cb78f63 container client-container: <nil>
STEP: delete the pod
Jan  3 13:06:18.638: INFO: Waiting for pod downwardapi-volume-30af02dd-bfd0-4aff-900c-d4686cb78f63 to disappear
Jan  3 13:06:18.642: INFO: Pod downwardapi-volume-30af02dd-bfd0-4aff-900c-d4686cb78f63 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:06:18.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9532" for this suite.
Jan  3 13:06:24.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:06:24.698: INFO: namespace downward-api-9532 deletion completed in 6.05155077s

• [SLOW TEST:14.224 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:06:24.698: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-04b89b3b-3e1d-4d4c-89b0-fa37414b0412
STEP: Creating a pod to test consume secrets
Jan  3 13:06:24.778: INFO: Waiting up to 5m0s for pod "pod-secrets-a8af8956-ca33-4bf5-9cf5-1276eaedb288" in namespace "secrets-7965" to be "success or failure"
Jan  3 13:06:24.779: INFO: Pod "pod-secrets-a8af8956-ca33-4bf5-9cf5-1276eaedb288": Phase="Pending", Reason="", readiness=false. Elapsed: 1.126805ms
Jan  3 13:06:26.782: INFO: Pod "pod-secrets-a8af8956-ca33-4bf5-9cf5-1276eaedb288": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003631118s
Jan  3 13:06:28.784: INFO: Pod "pod-secrets-a8af8956-ca33-4bf5-9cf5-1276eaedb288": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005661885s
Jan  3 13:06:30.786: INFO: Pod "pod-secrets-a8af8956-ca33-4bf5-9cf5-1276eaedb288": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008034528s
Jan  3 13:06:32.788: INFO: Pod "pod-secrets-a8af8956-ca33-4bf5-9cf5-1276eaedb288": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009852202s
STEP: Saw pod success
Jan  3 13:06:32.788: INFO: Pod "pod-secrets-a8af8956-ca33-4bf5-9cf5-1276eaedb288" satisfied condition "success or failure"
Jan  3 13:06:32.789: INFO: Trying to get logs from node controller-1 pod pod-secrets-a8af8956-ca33-4bf5-9cf5-1276eaedb288 container secret-volume-test: <nil>
STEP: delete the pod
Jan  3 13:06:32.799: INFO: Waiting for pod pod-secrets-a8af8956-ca33-4bf5-9cf5-1276eaedb288 to disappear
Jan  3 13:06:32.804: INFO: Pod pod-secrets-a8af8956-ca33-4bf5-9cf5-1276eaedb288 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:06:32.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7965" for this suite.
Jan  3 13:06:38.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:06:38.859: INFO: namespace secrets-7965 deletion completed in 6.051107877s
STEP: Destroying namespace "secret-namespace-5294" for this suite.
Jan  3 13:06:44.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:06:45.159: INFO: namespace secret-namespace-5294 deletion completed in 6.299951903s

• [SLOW TEST:20.462 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:06:45.160: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Jan  3 13:06:45.175: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:07:02.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-422" for this suite.
Jan  3 13:07:08.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:07:08.102: INFO: namespace crd-publish-openapi-422 deletion completed in 6.052734879s

• [SLOW TEST:22.942 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:07:08.102: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 13:07:08.119: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f96c8713-51e1-4450-816d-b1f90c9cccb7" in namespace "projected-4075" to be "success or failure"
Jan  3 13:07:08.121: INFO: Pod "downwardapi-volume-f96c8713-51e1-4450-816d-b1f90c9cccb7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.695266ms
Jan  3 13:07:10.122: INFO: Pod "downwardapi-volume-f96c8713-51e1-4450-816d-b1f90c9cccb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003396749s
Jan  3 13:07:12.124: INFO: Pod "downwardapi-volume-f96c8713-51e1-4450-816d-b1f90c9cccb7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005326342s
Jan  3 13:07:14.142: INFO: Pod "downwardapi-volume-f96c8713-51e1-4450-816d-b1f90c9cccb7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022543547s
Jan  3 13:07:16.146: INFO: Pod "downwardapi-volume-f96c8713-51e1-4450-816d-b1f90c9cccb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.026790191s
STEP: Saw pod success
Jan  3 13:07:16.146: INFO: Pod "downwardapi-volume-f96c8713-51e1-4450-816d-b1f90c9cccb7" satisfied condition "success or failure"
Jan  3 13:07:16.150: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-f96c8713-51e1-4450-816d-b1f90c9cccb7 container client-container: <nil>
STEP: delete the pod
Jan  3 13:07:16.162: INFO: Waiting for pod downwardapi-volume-f96c8713-51e1-4450-816d-b1f90c9cccb7 to disappear
Jan  3 13:07:16.164: INFO: Pod downwardapi-volume-f96c8713-51e1-4450-816d-b1f90c9cccb7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:07:16.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4075" for this suite.
Jan  3 13:07:22.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:07:22.239: INFO: namespace projected-4075 deletion completed in 6.073554369s

• [SLOW TEST:14.137 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:07:22.239: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6927
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6927
STEP: creating replication controller externalsvc in namespace services-6927
I0103 13:07:22.312348      23 runners.go:184] Created replication controller with name: externalsvc, namespace: services-6927, replica count: 2
I0103 13:07:25.386541      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 13:07:28.386691      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 13:07:31.386848      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 13:07:34.386994      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 13:07:37.387153      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 13:07:40.387431      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 13:07:43.392032      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 13:07:46.392169      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 13:07:49.392896      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 13:07:52.393842      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 13:07:55.394004      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 13:07:58.394155      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan  3 13:07:58.406: INFO: Creating new exec pod
Jan  3 13:08:06.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=services-6927 execpods5jjp -- /bin/sh -x -c nslookup nodeport-service'
Jan  3 13:08:06.965: INFO: stderr: "+ nslookup nodeport-service\n"
Jan  3 13:08:06.965: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-6927.svc.cluster.local\tcanonical name = externalsvc.services-6927.svc.cluster.local.\nName:\texternalsvc.services-6927.svc.cluster.local\nAddress: 10.101.241.17\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6927, will wait for the garbage collector to delete the pods
Jan  3 13:08:07.022: INFO: Deleting ReplicationController externalsvc took: 4.818937ms
Jan  3 13:08:07.422: INFO: Terminating ReplicationController externalsvc pods took: 400.343053ms
Jan  3 13:08:16.946: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:08:16.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6927" for this suite.
Jan  3 13:08:22.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:08:23.008: INFO: namespace services-6927 deletion completed in 6.032696333s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:60.769 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:08:23.009: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:08:30.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7781" for this suite.
Jan  3 13:08:36.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:08:36.079: INFO: namespace resourcequota-7781 deletion completed in 6.051490861s

• [SLOW TEST:13.070 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:08:36.079: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-56c720d8-1264-4755-86a9-378b33e32bd9
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-56c720d8-1264-4755-86a9-378b33e32bd9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:08:46.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-991" for this suite.
Jan  3 13:09:00.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:09:00.181: INFO: namespace configmap-991 deletion completed in 14.032437106s

• [SLOW TEST:24.102 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:09:00.182: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jan  3 13:09:00.195: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:09:09.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2522" for this suite.
Jan  3 13:09:15.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:09:15.926: INFO: namespace init-container-2522 deletion completed in 6.035524377s

• [SLOW TEST:15.744 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:09:15.927: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jan  3 13:09:24.461: INFO: Successfully updated pod "labelsupdateb2aa0495-e3d3-4b13-87b3-b9e68895667d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:09:28.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6810" for this suite.
Jan  3 13:09:40.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:09:40.542: INFO: namespace downward-api-6810 deletion completed in 12.034797669s

• [SLOW TEST:24.616 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:09:40.543: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan  3 13:09:41.080: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan  3 13:09:43.111: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:09:45.220: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:09:47.113: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:09:49.114: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713653781, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 13:09:52.118: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:09:52.119: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:09:52.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7790" for this suite.
Jan  3 13:09:58.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:09:58.989: INFO: namespace crd-webhook-7790 deletion completed in 6.07944366s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:18.452 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:09:58.995: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-188e14be-d937-492e-a08e-0d76b7c10e50
STEP: Creating a pod to test consume configMaps
Jan  3 13:09:59.023: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-beedcf93-08aa-4b94-b356-aa80b8ccfca5" in namespace "projected-6927" to be "success or failure"
Jan  3 13:09:59.026: INFO: Pod "pod-projected-configmaps-beedcf93-08aa-4b94-b356-aa80b8ccfca5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.017179ms
Jan  3 13:10:01.028: INFO: Pod "pod-projected-configmaps-beedcf93-08aa-4b94-b356-aa80b8ccfca5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00428382s
Jan  3 13:10:03.029: INFO: Pod "pod-projected-configmaps-beedcf93-08aa-4b94-b356-aa80b8ccfca5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005751458s
Jan  3 13:10:05.031: INFO: Pod "pod-projected-configmaps-beedcf93-08aa-4b94-b356-aa80b8ccfca5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007375911s
Jan  3 13:10:07.032: INFO: Pod "pod-projected-configmaps-beedcf93-08aa-4b94-b356-aa80b8ccfca5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008891926s
STEP: Saw pod success
Jan  3 13:10:07.032: INFO: Pod "pod-projected-configmaps-beedcf93-08aa-4b94-b356-aa80b8ccfca5" satisfied condition "success or failure"
Jan  3 13:10:07.034: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-beedcf93-08aa-4b94-b356-aa80b8ccfca5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 13:10:07.092: INFO: Waiting for pod pod-projected-configmaps-beedcf93-08aa-4b94-b356-aa80b8ccfca5 to disappear
Jan  3 13:10:07.093: INFO: Pod pod-projected-configmaps-beedcf93-08aa-4b94-b356-aa80b8ccfca5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:10:07.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6927" for this suite.
Jan  3 13:10:13.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:10:13.271: INFO: namespace projected-6927 deletion completed in 6.176499684s

• [SLOW TEST:14.276 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:10:13.272: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:10:13.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5050" for this suite.
Jan  3 13:10:19.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:10:19.426: INFO: namespace custom-resource-definition-5050 deletion completed in 6.047630431s

• [SLOW TEST:6.154 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:10:19.427: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-sd76
STEP: Creating a pod to test atomic-volume-subpath
Jan  3 13:10:19.464: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sd76" in namespace "subpath-7220" to be "success or failure"
Jan  3 13:10:19.465: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Pending", Reason="", readiness=false. Elapsed: 919.158µs
Jan  3 13:10:21.467: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003528145s
Jan  3 13:10:23.469: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005284477s
Jan  3 13:10:25.471: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006916047s
Jan  3 13:10:27.473: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Running", Reason="", readiness=true. Elapsed: 8.009233973s
Jan  3 13:10:29.475: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Running", Reason="", readiness=true. Elapsed: 10.011015654s
Jan  3 13:10:31.477: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Running", Reason="", readiness=true. Elapsed: 12.012854283s
Jan  3 13:10:33.479: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Running", Reason="", readiness=true. Elapsed: 14.014776028s
Jan  3 13:10:35.482: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Running", Reason="", readiness=true. Elapsed: 16.017659861s
Jan  3 13:10:37.483: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Running", Reason="", readiness=true. Elapsed: 18.019155648s
Jan  3 13:10:39.485: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Running", Reason="", readiness=true. Elapsed: 20.020798118s
Jan  3 13:10:41.495: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Running", Reason="", readiness=true. Elapsed: 22.031378364s
Jan  3 13:10:43.497: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Running", Reason="", readiness=true. Elapsed: 24.033209737s
Jan  3 13:10:45.503: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Running", Reason="", readiness=true. Elapsed: 26.038865894s
Jan  3 13:10:47.506: INFO: Pod "pod-subpath-test-secret-sd76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.042305804s
STEP: Saw pod success
Jan  3 13:10:47.506: INFO: Pod "pod-subpath-test-secret-sd76" satisfied condition "success or failure"
Jan  3 13:10:47.507: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-secret-sd76 container test-container-subpath-secret-sd76: <nil>
STEP: delete the pod
Jan  3 13:10:47.520: INFO: Waiting for pod pod-subpath-test-secret-sd76 to disappear
Jan  3 13:10:47.521: INFO: Pod pod-subpath-test-secret-sd76 no longer exists
STEP: Deleting pod pod-subpath-test-secret-sd76
Jan  3 13:10:47.521: INFO: Deleting pod "pod-subpath-test-secret-sd76" in namespace "subpath-7220"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:10:47.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7220" for this suite.
Jan  3 13:10:53.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:10:53.568: INFO: namespace subpath-7220 deletion completed in 6.043600478s

• [SLOW TEST:34.142 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:10:53.569: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-cbcb3052-ed7d-477c-8a49-dd1ac58d6ff5
STEP: Creating a pod to test consume configMaps
Jan  3 13:10:53.594: INFO: Waiting up to 5m0s for pod "pod-configmaps-28621d39-ba52-468e-af7e-2ee68fc6776a" in namespace "configmap-6148" to be "success or failure"
Jan  3 13:10:53.596: INFO: Pod "pod-configmaps-28621d39-ba52-468e-af7e-2ee68fc6776a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.468535ms
Jan  3 13:10:55.597: INFO: Pod "pod-configmaps-28621d39-ba52-468e-af7e-2ee68fc6776a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002977804s
Jan  3 13:10:57.599: INFO: Pod "pod-configmaps-28621d39-ba52-468e-af7e-2ee68fc6776a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004713927s
Jan  3 13:10:59.601: INFO: Pod "pod-configmaps-28621d39-ba52-468e-af7e-2ee68fc6776a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006637395s
Jan  3 13:11:01.606: INFO: Pod "pod-configmaps-28621d39-ba52-468e-af7e-2ee68fc6776a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011329813s
STEP: Saw pod success
Jan  3 13:11:01.606: INFO: Pod "pod-configmaps-28621d39-ba52-468e-af7e-2ee68fc6776a" satisfied condition "success or failure"
Jan  3 13:11:01.608: INFO: Trying to get logs from node controller-0 pod pod-configmaps-28621d39-ba52-468e-af7e-2ee68fc6776a container configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 13:11:01.627: INFO: Waiting for pod pod-configmaps-28621d39-ba52-468e-af7e-2ee68fc6776a to disappear
Jan  3 13:11:01.629: INFO: Pod pod-configmaps-28621d39-ba52-468e-af7e-2ee68fc6776a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:11:01.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6148" for this suite.
Jan  3 13:11:07.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:11:07.706: INFO: namespace configmap-6148 deletion completed in 6.074348154s

• [SLOW TEST:14.137 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:11:07.709: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Jan  3 13:11:07.743: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-937082782 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:11:07.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-526" for this suite.
Jan  3 13:11:13.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:11:13.964: INFO: namespace kubectl-526 deletion completed in 6.03924793s

• [SLOW TEST:6.256 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:11:13.965: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan  3 13:11:13.982: INFO: Waiting up to 5m0s for pod "pod-5aae191a-d314-4f58-af1e-dfd659d661e6" in namespace "emptydir-744" to be "success or failure"
Jan  3 13:11:13.983: INFO: Pod "pod-5aae191a-d314-4f58-af1e-dfd659d661e6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.023865ms
Jan  3 13:11:15.984: INFO: Pod "pod-5aae191a-d314-4f58-af1e-dfd659d661e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002396541s
Jan  3 13:11:17.987: INFO: Pod "pod-5aae191a-d314-4f58-af1e-dfd659d661e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005220603s
Jan  3 13:11:19.989: INFO: Pod "pod-5aae191a-d314-4f58-af1e-dfd659d661e6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00672383s
Jan  3 13:11:21.990: INFO: Pod "pod-5aae191a-d314-4f58-af1e-dfd659d661e6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008615578s
Jan  3 13:11:23.992: INFO: Pod "pod-5aae191a-d314-4f58-af1e-dfd659d661e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.010341253s
STEP: Saw pod success
Jan  3 13:11:23.992: INFO: Pod "pod-5aae191a-d314-4f58-af1e-dfd659d661e6" satisfied condition "success or failure"
Jan  3 13:11:23.993: INFO: Trying to get logs from node controller-1 pod pod-5aae191a-d314-4f58-af1e-dfd659d661e6 container test-container: <nil>
STEP: delete the pod
Jan  3 13:11:24.002: INFO: Waiting for pod pod-5aae191a-d314-4f58-af1e-dfd659d661e6 to disappear
Jan  3 13:11:24.003: INFO: Pod pod-5aae191a-d314-4f58-af1e-dfd659d661e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:11:24.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-744" for this suite.
Jan  3 13:11:30.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:11:30.039: INFO: namespace emptydir-744 deletion completed in 6.034383557s

• [SLOW TEST:16.075 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:11:30.040: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:11:30.052: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:11:30.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8531" for this suite.
Jan  3 13:11:36.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:11:36.120: INFO: namespace custom-resource-definition-8531 deletion completed in 6.039287305s

• [SLOW TEST:6.081 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:11:36.121: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan  3 13:11:36.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8140'
Jan  3 13:11:36.236: INFO: stderr: ""
Jan  3 13:11:36.236: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Jan  3 13:11:36.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete pods e2e-test-httpd-pod --namespace=kubectl-8140'
Jan  3 13:11:46.926: INFO: stderr: ""
Jan  3 13:11:46.926: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:11:46.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8140" for this suite.
Jan  3 13:11:52.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:11:52.971: INFO: namespace kubectl-8140 deletion completed in 6.038413741s

• [SLOW TEST:16.850 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:11:52.972: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-36c32b93-b143-419e-9b39-def91ec521e1
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:12:01.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7471" for this suite.
Jan  3 13:12:29.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:12:29.036: INFO: namespace configmap-7471 deletion completed in 28.034072693s

• [SLOW TEST:36.065 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:12:29.037: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan  3 13:12:37.573: INFO: Successfully updated pod "pod-update-activedeadlineseconds-809f55b3-eda2-4ba5-9eed-a715a37fb232"
Jan  3 13:12:37.573: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-809f55b3-eda2-4ba5-9eed-a715a37fb232" in namespace "pods-5021" to be "terminated due to deadline exceeded"
Jan  3 13:12:37.578: INFO: Pod "pod-update-activedeadlineseconds-809f55b3-eda2-4ba5-9eed-a715a37fb232": Phase="Running", Reason="", readiness=true. Elapsed: 5.005272ms
Jan  3 13:12:39.587: INFO: Pod "pod-update-activedeadlineseconds-809f55b3-eda2-4ba5-9eed-a715a37fb232": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.014125614s
Jan  3 13:12:39.587: INFO: Pod "pod-update-activedeadlineseconds-809f55b3-eda2-4ba5-9eed-a715a37fb232" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:12:39.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5021" for this suite.
Jan  3 13:12:45.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:12:45.624: INFO: namespace pods-5021 deletion completed in 6.034606652s

• [SLOW TEST:16.587 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:12:45.624: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:12:45.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3336" for this suite.
Jan  3 13:12:57.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:12:57.690: INFO: namespace pods-3336 deletion completed in 12.046210416s

• [SLOW TEST:12.065 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:12:57.690: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan  3 13:12:57.708: INFO: Waiting up to 5m0s for pod "pod-0b8323d5-dedd-424a-847c-4613b03827f4" in namespace "emptydir-437" to be "success or failure"
Jan  3 13:12:57.711: INFO: Pod "pod-0b8323d5-dedd-424a-847c-4613b03827f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.777387ms
Jan  3 13:12:59.712: INFO: Pod "pod-0b8323d5-dedd-424a-847c-4613b03827f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004549474s
Jan  3 13:13:01.714: INFO: Pod "pod-0b8323d5-dedd-424a-847c-4613b03827f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006178932s
Jan  3 13:13:03.722: INFO: Pod "pod-0b8323d5-dedd-424a-847c-4613b03827f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01401298s
Jan  3 13:13:05.727: INFO: Pod "pod-0b8323d5-dedd-424a-847c-4613b03827f4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01937931s
Jan  3 13:13:07.729: INFO: Pod "pod-0b8323d5-dedd-424a-847c-4613b03827f4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021646424s
Jan  3 13:13:09.731: INFO: Pod "pod-0b8323d5-dedd-424a-847c-4613b03827f4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023160725s
Jan  3 13:13:11.734: INFO: Pod "pod-0b8323d5-dedd-424a-847c-4613b03827f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.025833647s
STEP: Saw pod success
Jan  3 13:13:11.734: INFO: Pod "pod-0b8323d5-dedd-424a-847c-4613b03827f4" satisfied condition "success or failure"
Jan  3 13:13:11.735: INFO: Trying to get logs from node controller-1 pod pod-0b8323d5-dedd-424a-847c-4613b03827f4 container test-container: <nil>
STEP: delete the pod
Jan  3 13:13:11.759: INFO: Waiting for pod pod-0b8323d5-dedd-424a-847c-4613b03827f4 to disappear
Jan  3 13:13:11.771: INFO: Pod pod-0b8323d5-dedd-424a-847c-4613b03827f4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:13:11.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-437" for this suite.
Jan  3 13:13:17.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:13:17.858: INFO: namespace emptydir-437 deletion completed in 6.076295986s

• [SLOW TEST:20.169 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:13:17.859: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:13:17.887: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan  3 13:13:20.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-4635 create -f -'
Jan  3 13:13:20.988: INFO: stderr: ""
Jan  3 13:13:20.988: INFO: stdout: "e2e-test-crd-publish-openapi-4185-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  3 13:13:20.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-4635 delete e2e-test-crd-publish-openapi-4185-crds test-cr'
Jan  3 13:13:21.061: INFO: stderr: ""
Jan  3 13:13:21.061: INFO: stdout: "e2e-test-crd-publish-openapi-4185-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan  3 13:13:21.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-4635 apply -f -'
Jan  3 13:13:21.194: INFO: stderr: ""
Jan  3 13:13:21.194: INFO: stdout: "e2e-test-crd-publish-openapi-4185-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  3 13:13:21.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-4635 delete e2e-test-crd-publish-openapi-4185-crds test-cr'
Jan  3 13:13:21.263: INFO: stderr: ""
Jan  3 13:13:21.263: INFO: stdout: "e2e-test-crd-publish-openapi-4185-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan  3 13:13:21.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 explain e2e-test-crd-publish-openapi-4185-crds'
Jan  3 13:13:21.386: INFO: stderr: ""
Jan  3 13:13:21.386: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4185-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:13:24.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4635" for this suite.
Jan  3 13:13:30.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:13:30.974: INFO: namespace crd-publish-openapi-4635 deletion completed in 6.053296767s

• [SLOW TEST:13.115 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:13:30.974: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-66b341ec-2f4b-46ce-8b8b-6e4814887ce6
STEP: Creating a pod to test consume configMaps
Jan  3 13:13:30.991: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6bdd870d-54af-49be-bca6-1812c1a74776" in namespace "projected-1781" to be "success or failure"
Jan  3 13:13:30.993: INFO: Pod "pod-projected-configmaps-6bdd870d-54af-49be-bca6-1812c1a74776": Phase="Pending", Reason="", readiness=false. Elapsed: 1.490693ms
Jan  3 13:13:32.995: INFO: Pod "pod-projected-configmaps-6bdd870d-54af-49be-bca6-1812c1a74776": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003495579s
Jan  3 13:13:34.997: INFO: Pod "pod-projected-configmaps-6bdd870d-54af-49be-bca6-1812c1a74776": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005316554s
Jan  3 13:13:36.999: INFO: Pod "pod-projected-configmaps-6bdd870d-54af-49be-bca6-1812c1a74776": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007127108s
Jan  3 13:13:39.002: INFO: Pod "pod-projected-configmaps-6bdd870d-54af-49be-bca6-1812c1a74776": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010301013s
Jan  3 13:13:41.004: INFO: Pod "pod-projected-configmaps-6bdd870d-54af-49be-bca6-1812c1a74776": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.012300392s
STEP: Saw pod success
Jan  3 13:13:41.004: INFO: Pod "pod-projected-configmaps-6bdd870d-54af-49be-bca6-1812c1a74776" satisfied condition "success or failure"
Jan  3 13:13:41.008: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-6bdd870d-54af-49be-bca6-1812c1a74776 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 13:13:41.028: INFO: Waiting for pod pod-projected-configmaps-6bdd870d-54af-49be-bca6-1812c1a74776 to disappear
Jan  3 13:13:41.029: INFO: Pod pod-projected-configmaps-6bdd870d-54af-49be-bca6-1812c1a74776 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:13:41.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1781" for this suite.
Jan  3 13:13:47.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:13:47.085: INFO: namespace projected-1781 deletion completed in 6.054213053s

• [SLOW TEST:16.111 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:13:47.085: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan  3 13:13:55.610: INFO: Successfully updated pod "adopt-release-5zc55"
STEP: Checking that the Job readopts the Pod
Jan  3 13:13:55.610: INFO: Waiting up to 15m0s for pod "adopt-release-5zc55" in namespace "job-3095" to be "adopted"
Jan  3 13:13:55.612: INFO: Pod "adopt-release-5zc55": Phase="Running", Reason="", readiness=true. Elapsed: 2.054488ms
Jan  3 13:13:57.615: INFO: Pod "adopt-release-5zc55": Phase="Running", Reason="", readiness=true. Elapsed: 2.004853969s
Jan  3 13:13:57.615: INFO: Pod "adopt-release-5zc55" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan  3 13:13:58.155: INFO: Successfully updated pod "adopt-release-5zc55"
STEP: Checking that the Job releases the Pod
Jan  3 13:13:58.155: INFO: Waiting up to 15m0s for pod "adopt-release-5zc55" in namespace "job-3095" to be "released"
Jan  3 13:13:58.156: INFO: Pod "adopt-release-5zc55": Phase="Running", Reason="", readiness=true. Elapsed: 1.08014ms
Jan  3 13:14:00.158: INFO: Pod "adopt-release-5zc55": Phase="Running", Reason="", readiness=true. Elapsed: 2.002715864s
Jan  3 13:14:00.158: INFO: Pod "adopt-release-5zc55" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:14:00.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3095" for this suite.
Jan  3 13:14:50.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:14:50.195: INFO: namespace job-3095 deletion completed in 50.035238454s

• [SLOW TEST:63.109 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:14:50.195: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Jan  3 13:14:50.210: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:15:08.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4434" for this suite.
Jan  3 13:15:14.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:15:14.850: INFO: namespace crd-publish-openapi-4434 deletion completed in 6.055219599s

• [SLOW TEST:24.656 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:15:14.853: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan  3 13:16:23.906: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:16:24.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2482" for this suite.
Jan  3 13:16:52.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:16:53.019: INFO: namespace replicaset-2482 deletion completed in 28.106691596s

• [SLOW TEST:98.167 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:16:53.020: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 13:16:53.066: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2463cc00-2933-4238-862d-3c76d4cd0b20" in namespace "downward-api-5590" to be "success or failure"
Jan  3 13:16:53.067: INFO: Pod "downwardapi-volume-2463cc00-2933-4238-862d-3c76d4cd0b20": Phase="Pending", Reason="", readiness=false. Elapsed: 1.269772ms
Jan  3 13:16:55.070: INFO: Pod "downwardapi-volume-2463cc00-2933-4238-862d-3c76d4cd0b20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0039719s
Jan  3 13:16:57.071: INFO: Pod "downwardapi-volume-2463cc00-2933-4238-862d-3c76d4cd0b20": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005342472s
Jan  3 13:16:59.074: INFO: Pod "downwardapi-volume-2463cc00-2933-4238-862d-3c76d4cd0b20": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008192625s
Jan  3 13:17:01.076: INFO: Pod "downwardapi-volume-2463cc00-2933-4238-862d-3c76d4cd0b20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009974205s
STEP: Saw pod success
Jan  3 13:17:01.076: INFO: Pod "downwardapi-volume-2463cc00-2933-4238-862d-3c76d4cd0b20" satisfied condition "success or failure"
Jan  3 13:17:01.077: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-2463cc00-2933-4238-862d-3c76d4cd0b20 container client-container: <nil>
STEP: delete the pod
Jan  3 13:17:01.100: INFO: Waiting for pod downwardapi-volume-2463cc00-2933-4238-862d-3c76d4cd0b20 to disappear
Jan  3 13:17:01.101: INFO: Pod downwardapi-volume-2463cc00-2933-4238-862d-3c76d4cd0b20 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:17:01.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5590" for this suite.
Jan  3 13:17:07.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:17:07.139: INFO: namespace downward-api-5590 deletion completed in 6.03460517s

• [SLOW TEST:14.118 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:17:07.139: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan  3 13:17:07.192: INFO: Waiting up to 5m0s for pod "pod-11d06af0-b311-4af8-9ee1-ae9a4371cc30" in namespace "emptydir-2756" to be "success or failure"
Jan  3 13:17:07.194: INFO: Pod "pod-11d06af0-b311-4af8-9ee1-ae9a4371cc30": Phase="Pending", Reason="", readiness=false. Elapsed: 1.595877ms
Jan  3 13:17:09.195: INFO: Pod "pod-11d06af0-b311-4af8-9ee1-ae9a4371cc30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003212937s
Jan  3 13:17:11.198: INFO: Pod "pod-11d06af0-b311-4af8-9ee1-ae9a4371cc30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006446778s
Jan  3 13:17:13.216: INFO: Pod "pod-11d06af0-b311-4af8-9ee1-ae9a4371cc30": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024267732s
Jan  3 13:17:15.236: INFO: Pod "pod-11d06af0-b311-4af8-9ee1-ae9a4371cc30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.044389235s
STEP: Saw pod success
Jan  3 13:17:15.236: INFO: Pod "pod-11d06af0-b311-4af8-9ee1-ae9a4371cc30" satisfied condition "success or failure"
Jan  3 13:17:15.245: INFO: Trying to get logs from node controller-1 pod pod-11d06af0-b311-4af8-9ee1-ae9a4371cc30 container test-container: <nil>
STEP: delete the pod
Jan  3 13:17:15.332: INFO: Waiting for pod pod-11d06af0-b311-4af8-9ee1-ae9a4371cc30 to disappear
Jan  3 13:17:15.333: INFO: Pod pod-11d06af0-b311-4af8-9ee1-ae9a4371cc30 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:17:15.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2756" for this suite.
Jan  3 13:17:21.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:17:21.387: INFO: namespace emptydir-2756 deletion completed in 6.051528597s

• [SLOW TEST:14.248 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:17:21.388: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Jan  3 13:17:21.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 cluster-info'
Jan  3 13:17:21.470: INFO: stderr: ""
Jan  3 13:17:21.470: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:17:21.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5258" for this suite.
Jan  3 13:17:27.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:17:27.509: INFO: namespace kubectl-5258 deletion completed in 6.037960298s

• [SLOW TEST:6.121 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:17:27.510: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Jan  3 13:17:27.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-738'
Jan  3 13:17:27.706: INFO: stderr: ""
Jan  3 13:17:27.706: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan  3 13:17:27.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jan  3 13:17:27.828: INFO: stderr: ""
Jan  3 13:17:27.828: INFO: stdout: "update-demo-nautilus-5zvw2 update-demo-nautilus-cx2st "
Jan  3 13:17:27.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-5zvw2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:27.964: INFO: stderr: ""
Jan  3 13:17:27.964: INFO: stdout: ""
Jan  3 13:17:27.964: INFO: update-demo-nautilus-5zvw2 is created but not running
Jan  3 13:17:32.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jan  3 13:17:33.128: INFO: stderr: ""
Jan  3 13:17:33.128: INFO: stdout: "update-demo-nautilus-5zvw2 update-demo-nautilus-cx2st "
Jan  3 13:17:33.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-5zvw2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:33.228: INFO: stderr: ""
Jan  3 13:17:33.228: INFO: stdout: ""
Jan  3 13:17:33.228: INFO: update-demo-nautilus-5zvw2 is created but not running
Jan  3 13:17:38.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jan  3 13:17:38.318: INFO: stderr: ""
Jan  3 13:17:38.318: INFO: stdout: "update-demo-nautilus-5zvw2 update-demo-nautilus-cx2st "
Jan  3 13:17:38.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-5zvw2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:38.459: INFO: stderr: ""
Jan  3 13:17:38.459: INFO: stdout: ""
Jan  3 13:17:38.459: INFO: update-demo-nautilus-5zvw2 is created but not running
Jan  3 13:17:43.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jan  3 13:17:43.821: INFO: stderr: ""
Jan  3 13:17:43.821: INFO: stdout: "update-demo-nautilus-5zvw2 update-demo-nautilus-cx2st "
Jan  3 13:17:43.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-5zvw2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:43.889: INFO: stderr: ""
Jan  3 13:17:43.890: INFO: stdout: "true"
Jan  3 13:17:43.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-5zvw2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:43.951: INFO: stderr: ""
Jan  3 13:17:43.951: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan  3 13:17:43.951: INFO: validating pod update-demo-nautilus-5zvw2
Jan  3 13:17:43.953: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 13:17:43.953: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 13:17:43.953: INFO: update-demo-nautilus-5zvw2 is verified up and running
Jan  3 13:17:43.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-cx2st -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:44.011: INFO: stderr: ""
Jan  3 13:17:44.011: INFO: stdout: "true"
Jan  3 13:17:44.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-cx2st -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:44.475: INFO: stderr: ""
Jan  3 13:17:44.475: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan  3 13:17:44.475: INFO: validating pod update-demo-nautilus-cx2st
Jan  3 13:17:44.478: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 13:17:44.479: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 13:17:44.479: INFO: update-demo-nautilus-cx2st is verified up and running
STEP: scaling down the replication controller
Jan  3 13:17:44.480: INFO: scanned /root for discovery docs: <nil>
Jan  3 13:17:44.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-738'
Jan  3 13:17:45.874: INFO: stderr: ""
Jan  3 13:17:45.874: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan  3 13:17:45.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jan  3 13:17:45.958: INFO: stderr: ""
Jan  3 13:17:45.958: INFO: stdout: "update-demo-nautilus-5zvw2 update-demo-nautilus-cx2st "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan  3 13:17:50.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jan  3 13:17:51.017: INFO: stderr: ""
Jan  3 13:17:51.018: INFO: stdout: "update-demo-nautilus-5zvw2 "
Jan  3 13:17:51.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-5zvw2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:51.076: INFO: stderr: ""
Jan  3 13:17:51.076: INFO: stdout: "true"
Jan  3 13:17:51.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-5zvw2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:51.134: INFO: stderr: ""
Jan  3 13:17:51.134: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan  3 13:17:51.134: INFO: validating pod update-demo-nautilus-5zvw2
Jan  3 13:17:51.136: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 13:17:51.136: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 13:17:51.136: INFO: update-demo-nautilus-5zvw2 is verified up and running
STEP: scaling up the replication controller
Jan  3 13:17:51.137: INFO: scanned /root for discovery docs: <nil>
Jan  3 13:17:51.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-738'
Jan  3 13:17:51.221: INFO: stderr: ""
Jan  3 13:17:51.221: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan  3 13:17:51.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jan  3 13:17:51.289: INFO: stderr: ""
Jan  3 13:17:51.289: INFO: stdout: "update-demo-nautilus-5zvw2 update-demo-nautilus-sgt74 "
Jan  3 13:17:51.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-5zvw2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:51.350: INFO: stderr: ""
Jan  3 13:17:51.350: INFO: stdout: "true"
Jan  3 13:17:51.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-5zvw2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:51.408: INFO: stderr: ""
Jan  3 13:17:51.408: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan  3 13:17:51.408: INFO: validating pod update-demo-nautilus-5zvw2
Jan  3 13:17:51.410: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 13:17:51.410: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 13:17:51.410: INFO: update-demo-nautilus-5zvw2 is verified up and running
Jan  3 13:17:51.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-sgt74 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:51.467: INFO: stderr: ""
Jan  3 13:17:51.467: INFO: stdout: ""
Jan  3 13:17:51.467: INFO: update-demo-nautilus-sgt74 is created but not running
Jan  3 13:17:56.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jan  3 13:17:56.524: INFO: stderr: ""
Jan  3 13:17:56.524: INFO: stdout: "update-demo-nautilus-5zvw2 update-demo-nautilus-sgt74 "
Jan  3 13:17:56.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-5zvw2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:56.580: INFO: stderr: ""
Jan  3 13:17:56.580: INFO: stdout: "true"
Jan  3 13:17:56.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-5zvw2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:56.636: INFO: stderr: ""
Jan  3 13:17:56.637: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan  3 13:17:56.637: INFO: validating pod update-demo-nautilus-5zvw2
Jan  3 13:17:56.639: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 13:17:56.639: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 13:17:56.639: INFO: update-demo-nautilus-5zvw2 is verified up and running
Jan  3 13:17:56.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-sgt74 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:17:56.711: INFO: stderr: ""
Jan  3 13:17:56.711: INFO: stdout: ""
Jan  3 13:17:56.711: INFO: update-demo-nautilus-sgt74 is created but not running
Jan  3 13:18:01.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jan  3 13:18:01.772: INFO: stderr: ""
Jan  3 13:18:01.772: INFO: stdout: "update-demo-nautilus-5zvw2 update-demo-nautilus-sgt74 "
Jan  3 13:18:01.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-5zvw2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:18:01.832: INFO: stderr: ""
Jan  3 13:18:01.832: INFO: stdout: "true"
Jan  3 13:18:01.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-5zvw2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:18:01.895: INFO: stderr: ""
Jan  3 13:18:01.896: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan  3 13:18:01.896: INFO: validating pod update-demo-nautilus-5zvw2
Jan  3 13:18:01.897: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 13:18:01.897: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 13:18:01.897: INFO: update-demo-nautilus-5zvw2 is verified up and running
Jan  3 13:18:01.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-sgt74 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:18:01.966: INFO: stderr: ""
Jan  3 13:18:01.966: INFO: stdout: "true"
Jan  3 13:18:01.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-sgt74 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jan  3 13:18:02.043: INFO: stderr: ""
Jan  3 13:18:02.044: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan  3 13:18:02.044: INFO: validating pod update-demo-nautilus-sgt74
Jan  3 13:18:02.046: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 13:18:02.046: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 13:18:02.046: INFO: update-demo-nautilus-sgt74 is verified up and running
STEP: using delete to clean up resources
Jan  3 13:18:02.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete --grace-period=0 --force -f - --namespace=kubectl-738'
Jan  3 13:18:02.100: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 13:18:02.100: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  3 13:18:02.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-738'
Jan  3 13:18:02.172: INFO: stderr: "No resources found in kubectl-738 namespace.\n"
Jan  3 13:18:02.172: INFO: stdout: ""
Jan  3 13:18:02.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -l name=update-demo --namespace=kubectl-738 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  3 13:18:02.238: INFO: stderr: ""
Jan  3 13:18:02.238: INFO: stdout: "update-demo-nautilus-5zvw2\nupdate-demo-nautilus-sgt74\n"
Jan  3 13:18:02.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-738'
Jan  3 13:18:02.872: INFO: stderr: "No resources found in kubectl-738 namespace.\n"
Jan  3 13:18:02.872: INFO: stdout: ""
Jan  3 13:18:02.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -l name=update-demo --namespace=kubectl-738 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  3 13:18:03.047: INFO: stderr: ""
Jan  3 13:18:03.047: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:18:03.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-738" for this suite.
Jan  3 13:18:15.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:18:15.161: INFO: namespace kubectl-738 deletion completed in 12.111985247s

• [SLOW TEST:47.651 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:18:15.165: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:18:23.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2288" for this suite.
Jan  3 13:18:51.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:18:51.301: INFO: namespace replication-controller-2288 deletion completed in 28.064078084s

• [SLOW TEST:36.137 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:18:51.301: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:18:51.319: INFO: Creating ReplicaSet my-hostname-basic-89fb8b7c-955d-4d2d-9b9a-e20c8f130b75
Jan  3 13:18:51.324: INFO: Pod name my-hostname-basic-89fb8b7c-955d-4d2d-9b9a-e20c8f130b75: Found 0 pods out of 1
Jan  3 13:18:56.326: INFO: Pod name my-hostname-basic-89fb8b7c-955d-4d2d-9b9a-e20c8f130b75: Found 1 pods out of 1
Jan  3 13:18:56.326: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-89fb8b7c-955d-4d2d-9b9a-e20c8f130b75" is running
Jan  3 13:19:00.329: INFO: Pod "my-hostname-basic-89fb8b7c-955d-4d2d-9b9a-e20c8f130b75-tng9g" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-03 13:18:51 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-03 13:18:51 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-89fb8b7c-955d-4d2d-9b9a-e20c8f130b75]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-03 13:18:51 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-89fb8b7c-955d-4d2d-9b9a-e20c8f130b75]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-03 13:18:51 +0000 UTC Reason: Message:}])
Jan  3 13:19:00.329: INFO: Trying to dial the pod
Jan  3 13:19:05.336: INFO: Controller my-hostname-basic-89fb8b7c-955d-4d2d-9b9a-e20c8f130b75: Got expected result from replica 1 [my-hostname-basic-89fb8b7c-955d-4d2d-9b9a-e20c8f130b75-tng9g]: "my-hostname-basic-89fb8b7c-955d-4d2d-9b9a-e20c8f130b75-tng9g", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:19:05.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6949" for this suite.
Jan  3 13:19:11.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:19:11.445: INFO: namespace replicaset-6949 deletion completed in 6.105271695s

• [SLOW TEST:20.144 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:19:11.445: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 13:19:12.138: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan  3 13:19:14.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654352, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654352, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654352, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654352, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:19:16.147: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654352, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654352, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654352, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654352, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:19:18.147: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654352, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654352, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654352, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654352, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 13:19:21.152: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:19:33.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2860" for this suite.
Jan  3 13:19:39.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:19:39.333: INFO: namespace webhook-2860 deletion completed in 6.11182455s
STEP: Destroying namespace "webhook-2860-markers" for this suite.
Jan  3 13:19:45.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:19:45.448: INFO: namespace webhook-2860-markers deletion completed in 6.115128542s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:34.014 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:19:45.460: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:19:45.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 version'
Jan  3 13:19:45.587: INFO: stderr: ""
Jan  3 13:19:45.587: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:18:23Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:09:08Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:19:45.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1789" for this suite.
Jan  3 13:19:51.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:19:51.633: INFO: namespace kubectl-1789 deletion completed in 6.043537473s

• [SLOW TEST:6.173 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:19:51.633: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan  3 13:19:51.653: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-642 /api/v1/namespaces/watch-642/configmaps/e2e-watch-test-watch-closed 483ea3ef-b595-46fb-8e40-dda9ad385708 40190 0 2020-01-03 13:19:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan  3 13:19:51.653: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-642 /api/v1/namespaces/watch-642/configmaps/e2e-watch-test-watch-closed 483ea3ef-b595-46fb-8e40-dda9ad385708 40191 0 2020-01-03 13:19:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan  3 13:19:51.659: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-642 /api/v1/namespaces/watch-642/configmaps/e2e-watch-test-watch-closed 483ea3ef-b595-46fb-8e40-dda9ad385708 40192 0 2020-01-03 13:19:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan  3 13:19:51.659: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-642 /api/v1/namespaces/watch-642/configmaps/e2e-watch-test-watch-closed 483ea3ef-b595-46fb-8e40-dda9ad385708 40193 0 2020-01-03 13:19:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:19:51.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-642" for this suite.
Jan  3 13:19:57.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:19:57.698: INFO: namespace watch-642 deletion completed in 6.036662765s

• [SLOW TEST:6.065 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:19:57.698: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 13:19:58.208: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 13:20:00.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654398, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654398, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654398, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654398, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:20:02.214: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654398, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654398, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654398, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654398, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:20:04.214: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654398, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654398, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654398, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654398, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 13:20:07.303: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:20:07.306: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Jan  3 13:20:07.346: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:20:08.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-864" for this suite.
Jan  3 13:20:14.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:20:14.541: INFO: namespace webhook-864 deletion completed in 6.462578278s
STEP: Destroying namespace "webhook-864-markers" for this suite.
Jan  3 13:20:20.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:20:20.594: INFO: namespace webhook-864-markers deletion completed in 6.0524519s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:22.901 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:20:20.601: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan  3 13:20:28.670: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:20:28.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2691" for this suite.
Jan  3 13:20:34.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:20:34.748: INFO: namespace container-runtime-2691 deletion completed in 6.045279862s

• [SLOW TEST:14.147 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:20:34.748: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan  3 13:20:34.817: INFO: Waiting up to 5m0s for pod "pod-fcc2fe4d-2c2e-4026-98a1-462366eca072" in namespace "emptydir-6102" to be "success or failure"
Jan  3 13:20:34.818: INFO: Pod "pod-fcc2fe4d-2c2e-4026-98a1-462366eca072": Phase="Pending", Reason="", readiness=false. Elapsed: 1.315202ms
Jan  3 13:20:36.819: INFO: Pod "pod-fcc2fe4d-2c2e-4026-98a1-462366eca072": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002531478s
Jan  3 13:20:38.822: INFO: Pod "pod-fcc2fe4d-2c2e-4026-98a1-462366eca072": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0049434s
Jan  3 13:20:40.823: INFO: Pod "pod-fcc2fe4d-2c2e-4026-98a1-462366eca072": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006414523s
Jan  3 13:20:42.827: INFO: Pod "pod-fcc2fe4d-2c2e-4026-98a1-462366eca072": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009804994s
STEP: Saw pod success
Jan  3 13:20:42.827: INFO: Pod "pod-fcc2fe4d-2c2e-4026-98a1-462366eca072" satisfied condition "success or failure"
Jan  3 13:20:42.828: INFO: Trying to get logs from node controller-1 pod pod-fcc2fe4d-2c2e-4026-98a1-462366eca072 container test-container: <nil>
STEP: delete the pod
Jan  3 13:20:42.846: INFO: Waiting for pod pod-fcc2fe4d-2c2e-4026-98a1-462366eca072 to disappear
Jan  3 13:20:42.847: INFO: Pod pod-fcc2fe4d-2c2e-4026-98a1-462366eca072 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:20:42.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6102" for this suite.
Jan  3 13:20:48.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:20:48.929: INFO: namespace emptydir-6102 deletion completed in 6.079056763s

• [SLOW TEST:14.181 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:20:48.930: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Jan  3 13:20:56.975: INFO: Pod pod-hostip-33fa8148-71ef-4d6a-9fbf-9c8dd2890e63 has hostIP: 10.10.61.11
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:20:56.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1017" for this suite.
Jan  3 13:21:08.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:21:09.020: INFO: namespace pods-1017 deletion completed in 12.042952724s

• [SLOW TEST:20.090 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:21:09.022: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan  3 13:21:25.061: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 13:21:25.063: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  3 13:21:27.063: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 13:21:27.065: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  3 13:21:29.063: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 13:21:29.065: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  3 13:21:31.063: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 13:21:31.065: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  3 13:21:33.064: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 13:21:33.065: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  3 13:21:35.064: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 13:21:35.065: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  3 13:21:37.063: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 13:21:37.065: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  3 13:21:39.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 13:21:39.072: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  3 13:21:41.063: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 13:21:41.065: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:21:41.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6933" for this suite.
Jan  3 13:21:53.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:21:53.108: INFO: namespace container-lifecycle-hook-6933 deletion completed in 12.04121348s

• [SLOW TEST:44.087 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:21:53.109: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan  3 13:21:53.126: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan  3 13:22:06.023: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 13:22:09.205: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:22:22.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3136" for this suite.
Jan  3 13:22:28.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:22:28.963: INFO: namespace crd-publish-openapi-3136 deletion completed in 6.126474941s

• [SLOW TEST:35.854 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:22:28.964: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-2b52a011-baa6-4dd6-8589-9cd03e869110
STEP: Creating a pod to test consume configMaps
Jan  3 13:22:29.026: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7be1acd9-570d-4191-9807-8bbdddd80b0b" in namespace "projected-4816" to be "success or failure"
Jan  3 13:22:29.035: INFO: Pod "pod-projected-configmaps-7be1acd9-570d-4191-9807-8bbdddd80b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.996633ms
Jan  3 13:22:31.036: INFO: Pod "pod-projected-configmaps-7be1acd9-570d-4191-9807-8bbdddd80b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008327112s
Jan  3 13:22:33.039: INFO: Pod "pod-projected-configmaps-7be1acd9-570d-4191-9807-8bbdddd80b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010329621s
Jan  3 13:22:35.040: INFO: Pod "pod-projected-configmaps-7be1acd9-570d-4191-9807-8bbdddd80b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011892808s
Jan  3 13:22:37.043: INFO: Pod "pod-projected-configmaps-7be1acd9-570d-4191-9807-8bbdddd80b0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014864704s
STEP: Saw pod success
Jan  3 13:22:37.043: INFO: Pod "pod-projected-configmaps-7be1acd9-570d-4191-9807-8bbdddd80b0b" satisfied condition "success or failure"
Jan  3 13:22:37.045: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-7be1acd9-570d-4191-9807-8bbdddd80b0b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 13:22:37.072: INFO: Waiting for pod pod-projected-configmaps-7be1acd9-570d-4191-9807-8bbdddd80b0b to disappear
Jan  3 13:22:37.073: INFO: Pod pod-projected-configmaps-7be1acd9-570d-4191-9807-8bbdddd80b0b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:22:37.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4816" for this suite.
Jan  3 13:22:43.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:22:43.343: INFO: namespace projected-4816 deletion completed in 6.267429365s

• [SLOW TEST:14.379 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:22:43.343: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-8a439119-1b4f-4000-9353-e1c46d2c6a1d in namespace container-probe-7577
Jan  3 13:22:51.421: INFO: Started pod liveness-8a439119-1b4f-4000-9353-e1c46d2c6a1d in namespace container-probe-7577
STEP: checking the pod's current state and verifying that restartCount is present
Jan  3 13:22:51.422: INFO: Initial restart count of pod liveness-8a439119-1b4f-4000-9353-e1c46d2c6a1d is 0
Jan  3 13:23:15.453: INFO: Restart count of pod container-probe-7577/liveness-8a439119-1b4f-4000-9353-e1c46d2c6a1d is now 1 (24.030690059s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:23:15.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7577" for this suite.
Jan  3 13:23:21.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:23:21.496: INFO: namespace container-probe-7577 deletion completed in 6.036194688s

• [SLOW TEST:38.154 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:23:21.498: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jan  3 13:23:30.062: INFO: Successfully updated pod "annotationupdatede72331e-2faf-4884-aa12-5b57bd594f8b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:23:34.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4221" for this suite.
Jan  3 13:23:46.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:23:46.126: INFO: namespace projected-4221 deletion completed in 12.040615877s

• [SLOW TEST:24.628 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:23:46.127: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-11d46a6d-5972-468e-8916-c37f76fd8a6c
STEP: Creating a pod to test consume configMaps
Jan  3 13:23:46.154: INFO: Waiting up to 5m0s for pod "pod-configmaps-385974c2-052b-4318-937f-ebe3c25c018a" in namespace "configmap-7335" to be "success or failure"
Jan  3 13:23:46.156: INFO: Pod "pod-configmaps-385974c2-052b-4318-937f-ebe3c25c018a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.506411ms
Jan  3 13:23:48.158: INFO: Pod "pod-configmaps-385974c2-052b-4318-937f-ebe3c25c018a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00405759s
Jan  3 13:23:50.159: INFO: Pod "pod-configmaps-385974c2-052b-4318-937f-ebe3c25c018a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005392335s
Jan  3 13:23:52.161: INFO: Pod "pod-configmaps-385974c2-052b-4318-937f-ebe3c25c018a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007332276s
Jan  3 13:23:54.173: INFO: Pod "pod-configmaps-385974c2-052b-4318-937f-ebe3c25c018a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019713363s
STEP: Saw pod success
Jan  3 13:23:54.176: INFO: Pod "pod-configmaps-385974c2-052b-4318-937f-ebe3c25c018a" satisfied condition "success or failure"
Jan  3 13:23:54.187: INFO: Trying to get logs from node controller-1 pod pod-configmaps-385974c2-052b-4318-937f-ebe3c25c018a container configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 13:23:54.199: INFO: Waiting for pod pod-configmaps-385974c2-052b-4318-937f-ebe3c25c018a to disappear
Jan  3 13:23:54.199: INFO: Pod pod-configmaps-385974c2-052b-4318-937f-ebe3c25c018a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:23:54.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7335" for this suite.
Jan  3 13:24:00.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:24:00.272: INFO: namespace configmap-7335 deletion completed in 6.070508862s

• [SLOW TEST:14.145 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:24:00.272: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Jan  3 13:24:00.300: INFO: Waiting up to 5m0s for pod "var-expansion-4c3aa758-0c55-4700-948c-569a40a3535b" in namespace "var-expansion-5731" to be "success or failure"
Jan  3 13:24:00.301: INFO: Pod "var-expansion-4c3aa758-0c55-4700-948c-569a40a3535b": Phase="Pending", Reason="", readiness=false. Elapsed: 953.043µs
Jan  3 13:24:02.303: INFO: Pod "var-expansion-4c3aa758-0c55-4700-948c-569a40a3535b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002784187s
Jan  3 13:24:04.311: INFO: Pod "var-expansion-4c3aa758-0c55-4700-948c-569a40a3535b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010760224s
Jan  3 13:24:06.314: INFO: Pod "var-expansion-4c3aa758-0c55-4700-948c-569a40a3535b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013599894s
Jan  3 13:24:08.315: INFO: Pod "var-expansion-4c3aa758-0c55-4700-948c-569a40a3535b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.015507693s
STEP: Saw pod success
Jan  3 13:24:08.315: INFO: Pod "var-expansion-4c3aa758-0c55-4700-948c-569a40a3535b" satisfied condition "success or failure"
Jan  3 13:24:08.318: INFO: Trying to get logs from node controller-0 pod var-expansion-4c3aa758-0c55-4700-948c-569a40a3535b container dapi-container: <nil>
STEP: delete the pod
Jan  3 13:24:08.338: INFO: Waiting for pod var-expansion-4c3aa758-0c55-4700-948c-569a40a3535b to disappear
Jan  3 13:24:08.340: INFO: Pod var-expansion-4c3aa758-0c55-4700-948c-569a40a3535b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:24:08.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5731" for this suite.
Jan  3 13:24:14.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:24:14.546: INFO: namespace var-expansion-5731 deletion completed in 6.200624155s

• [SLOW TEST:14.274 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:24:14.552: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:24:22.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1991" for this suite.
Jan  3 13:24:28.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:24:28.734: INFO: namespace emptydir-wrapper-1991 deletion completed in 6.077174421s

• [SLOW TEST:14.183 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:24:28.735: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0103 13:24:38.832277      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan  3 13:24:38.832: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:24:38.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7178" for this suite.
Jan  3 13:24:44.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:24:45.282: INFO: namespace gc-7178 deletion completed in 6.445597862s

• [SLOW TEST:16.548 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:24:45.283: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Jan  3 13:24:45.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-1529'
Jan  3 13:24:46.049: INFO: stderr: ""
Jan  3 13:24:46.049: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan  3 13:24:47.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:24:47.051: INFO: Found 0 / 1
Jan  3 13:24:48.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:24:48.051: INFO: Found 0 / 1
Jan  3 13:24:49.061: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:24:49.061: INFO: Found 0 / 1
Jan  3 13:24:50.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:24:50.051: INFO: Found 0 / 1
Jan  3 13:24:51.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:24:51.051: INFO: Found 0 / 1
Jan  3 13:24:52.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:24:52.051: INFO: Found 0 / 1
Jan  3 13:24:53.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:24:53.051: INFO: Found 0 / 1
Jan  3 13:24:54.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:24:54.051: INFO: Found 0 / 1
Jan  3 13:24:55.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:24:55.051: INFO: Found 0 / 1
Jan  3 13:24:56.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:24:56.051: INFO: Found 0 / 1
Jan  3 13:24:57.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:24:57.051: INFO: Found 0 / 1
Jan  3 13:24:58.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:24:58.051: INFO: Found 0 / 1
Jan  3 13:24:59.054: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:24:59.054: INFO: Found 0 / 1
Jan  3 13:25:00.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:00.051: INFO: Found 0 / 1
Jan  3 13:25:01.052: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:01.052: INFO: Found 0 / 1
Jan  3 13:25:02.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:02.051: INFO: Found 0 / 1
Jan  3 13:25:03.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:03.051: INFO: Found 0 / 1
Jan  3 13:25:04.109: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:04.109: INFO: Found 0 / 1
Jan  3 13:25:05.050: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:05.050: INFO: Found 0 / 1
Jan  3 13:25:06.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:06.051: INFO: Found 0 / 1
Jan  3 13:25:07.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:07.051: INFO: Found 0 / 1
Jan  3 13:25:08.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:08.051: INFO: Found 0 / 1
Jan  3 13:25:09.053: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:09.053: INFO: Found 0 / 1
Jan  3 13:25:10.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:10.051: INFO: Found 0 / 1
Jan  3 13:25:11.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:11.051: INFO: Found 0 / 1
Jan  3 13:25:12.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:12.051: INFO: Found 0 / 1
Jan  3 13:25:13.058: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:13.058: INFO: Found 0 / 1
Jan  3 13:25:14.081: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:14.091: INFO: Found 0 / 1
Jan  3 13:25:15.052: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:15.052: INFO: Found 0 / 1
Jan  3 13:25:16.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:16.051: INFO: Found 0 / 1
Jan  3 13:25:17.055: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:17.055: INFO: Found 0 / 1
Jan  3 13:25:18.051: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:18.051: INFO: Found 0 / 1
Jan  3 13:25:19.068: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:19.068: INFO: Found 1 / 1
Jan  3 13:25:19.068: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan  3 13:25:19.072: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:19.072: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  3 13:25:19.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 patch pod redis-master-tnhwk --namespace=kubectl-1529 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan  3 13:25:19.399: INFO: stderr: ""
Jan  3 13:25:19.399: INFO: stdout: "pod/redis-master-tnhwk patched\n"
STEP: checking annotations
Jan  3 13:25:19.416: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:25:19.416: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:25:19.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1529" for this suite.
Jan  3 13:25:31.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:25:31.468: INFO: namespace kubectl-1529 deletion completed in 12.047205118s

• [SLOW TEST:46.185 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:25:31.469: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan  3 13:25:31.482: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 13:25:35.275: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:25:50.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8390" for this suite.
Jan  3 13:25:56.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:25:56.914: INFO: namespace crd-publish-openapi-8390 deletion completed in 6.086656051s

• [SLOW TEST:25.447 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:25:56.917: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 13:25:57.442: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 13:25:59.448: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654757, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654757, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654757, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654757, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:26:01.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654757, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654757, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654757, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654757, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:26:03.449: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654757, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654757, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654757, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713654757, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 13:26:06.456: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:26:06.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2194" for this suite.
Jan  3 13:26:12.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:26:12.647: INFO: namespace webhook-2194 deletion completed in 6.045992882s
STEP: Destroying namespace "webhook-2194-markers" for this suite.
Jan  3 13:26:18.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:26:18.706: INFO: namespace webhook-2194-markers deletion completed in 6.05907492s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.800 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:26:18.718: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jan  3 13:26:18.809: INFO: Waiting up to 5m0s for pod "downward-api-3f09bb15-b6c9-4f7f-8a41-379b8abfc9a0" in namespace "downward-api-7008" to be "success or failure"
Jan  3 13:26:18.809: INFO: Pod "downward-api-3f09bb15-b6c9-4f7f-8a41-379b8abfc9a0": Phase="Pending", Reason="", readiness=false. Elapsed: 880.076µs
Jan  3 13:26:20.811: INFO: Pod "downward-api-3f09bb15-b6c9-4f7f-8a41-379b8abfc9a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002051336s
Jan  3 13:26:22.812: INFO: Pod "downward-api-3f09bb15-b6c9-4f7f-8a41-379b8abfc9a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003450867s
Jan  3 13:26:24.814: INFO: Pod "downward-api-3f09bb15-b6c9-4f7f-8a41-379b8abfc9a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005680682s
Jan  3 13:26:26.816: INFO: Pod "downward-api-3f09bb15-b6c9-4f7f-8a41-379b8abfc9a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007660582s
STEP: Saw pod success
Jan  3 13:26:26.816: INFO: Pod "downward-api-3f09bb15-b6c9-4f7f-8a41-379b8abfc9a0" satisfied condition "success or failure"
Jan  3 13:26:26.818: INFO: Trying to get logs from node controller-0 pod downward-api-3f09bb15-b6c9-4f7f-8a41-379b8abfc9a0 container dapi-container: <nil>
STEP: delete the pod
Jan  3 13:26:26.833: INFO: Waiting for pod downward-api-3f09bb15-b6c9-4f7f-8a41-379b8abfc9a0 to disappear
Jan  3 13:26:26.836: INFO: Pod downward-api-3f09bb15-b6c9-4f7f-8a41-379b8abfc9a0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:26:26.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7008" for this suite.
Jan  3 13:26:32.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:26:32.871: INFO: namespace downward-api-7008 deletion completed in 6.033110279s

• [SLOW TEST:14.153 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:26:32.871: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan  3 13:26:33.126: INFO: Pod name wrapped-volume-race-d29e8377-03a8-4580-b82c-937512c7cea2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d29e8377-03a8-4580-b82c-937512c7cea2 in namespace emptydir-wrapper-8669, will wait for the garbage collector to delete the pods
Jan  3 13:26:55.240: INFO: Deleting ReplicationController wrapped-volume-race-d29e8377-03a8-4580-b82c-937512c7cea2 took: 2.695575ms
Jan  3 13:26:55.640: INFO: Terminating ReplicationController wrapped-volume-race-d29e8377-03a8-4580-b82c-937512c7cea2 pods took: 400.134909ms
STEP: Creating RC which spawns configmap-volume pods
Jan  3 13:27:32.150: INFO: Pod name wrapped-volume-race-b4cfe7cf-b479-4f8b-88c6-fc7a7a6604fb: Found 0 pods out of 5
Jan  3 13:27:37.154: INFO: Pod name wrapped-volume-race-b4cfe7cf-b479-4f8b-88c6-fc7a7a6604fb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b4cfe7cf-b479-4f8b-88c6-fc7a7a6604fb in namespace emptydir-wrapper-8669, will wait for the garbage collector to delete the pods
Jan  3 13:27:55.219: INFO: Deleting ReplicationController wrapped-volume-race-b4cfe7cf-b479-4f8b-88c6-fc7a7a6604fb took: 2.07688ms
Jan  3 13:27:55.619: INFO: Terminating ReplicationController wrapped-volume-race-b4cfe7cf-b479-4f8b-88c6-fc7a7a6604fb pods took: 400.147951ms
STEP: Creating RC which spawns configmap-volume pods
Jan  3 13:28:32.333: INFO: Pod name wrapped-volume-race-38d1ea09-eb79-4991-a305-421ad95306ea: Found 0 pods out of 5
Jan  3 13:28:37.342: INFO: Pod name wrapped-volume-race-38d1ea09-eb79-4991-a305-421ad95306ea: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-38d1ea09-eb79-4991-a305-421ad95306ea in namespace emptydir-wrapper-8669, will wait for the garbage collector to delete the pods
Jan  3 13:28:53.410: INFO: Deleting ReplicationController wrapped-volume-race-38d1ea09-eb79-4991-a305-421ad95306ea took: 3.341502ms
Jan  3 13:28:53.510: INFO: Terminating ReplicationController wrapped-volume-race-38d1ea09-eb79-4991-a305-421ad95306ea pods took: 100.117151ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:29:30.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8669" for this suite.
Jan  3 13:29:36.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:29:36.710: INFO: namespace emptydir-wrapper-8669 deletion completed in 6.087321683s

• [SLOW TEST:183.839 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:29:36.710: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Jan  3 13:29:36.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-7683'
Jan  3 13:29:36.929: INFO: stderr: ""
Jan  3 13:29:36.929: INFO: stdout: "pod/pause created\n"
Jan  3 13:29:36.929: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan  3 13:29:36.929: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7683" to be "running and ready"
Jan  3 13:29:36.930: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.198039ms
Jan  3 13:29:38.932: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003028939s
Jan  3 13:29:40.934: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005299832s
Jan  3 13:29:42.942: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013715676s
Jan  3 13:29:44.987: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.058463713s
Jan  3 13:29:46.989: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.060280438s
Jan  3 13:29:48.991: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 12.062096042s
Jan  3 13:29:48.991: INFO: Pod "pause" satisfied condition "running and ready"
Jan  3 13:29:48.991: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Jan  3 13:29:48.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 label pods pause testing-label=testing-label-value --namespace=kubectl-7683'
Jan  3 13:29:49.060: INFO: stderr: ""
Jan  3 13:29:49.060: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan  3 13:29:49.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pod pause -L testing-label --namespace=kubectl-7683'
Jan  3 13:29:49.168: INFO: stderr: ""
Jan  3 13:29:49.168: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          13s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan  3 13:29:49.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 label pods pause testing-label- --namespace=kubectl-7683'
Jan  3 13:29:49.253: INFO: stderr: ""
Jan  3 13:29:49.254: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan  3 13:29:49.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pod pause -L testing-label --namespace=kubectl-7683'
Jan  3 13:29:49.348: INFO: stderr: ""
Jan  3 13:29:49.348: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          13s   \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Jan  3 13:29:49.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete --grace-period=0 --force -f - --namespace=kubectl-7683'
Jan  3 13:29:49.443: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 13:29:49.443: INFO: stdout: "pod \"pause\" force deleted\n"
Jan  3 13:29:49.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get rc,svc -l name=pause --no-headers --namespace=kubectl-7683'
Jan  3 13:29:49.623: INFO: stderr: "No resources found in kubectl-7683 namespace.\n"
Jan  3 13:29:49.623: INFO: stdout: ""
Jan  3 13:29:49.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -l name=pause --namespace=kubectl-7683 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  3 13:29:49.830: INFO: stderr: ""
Jan  3 13:29:49.830: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:29:49.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7683" for this suite.
Jan  3 13:29:55.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:29:55.874: INFO: namespace kubectl-7683 deletion completed in 6.038695727s

• [SLOW TEST:19.164 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:29:55.874: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3919
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3919
STEP: creating replication controller externalsvc in namespace services-3919
I0103 13:29:55.905345      23 runners.go:184] Created replication controller with name: externalsvc, namespace: services-3919, replica count: 2
I0103 13:29:58.962363      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 13:30:01.962522      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 13:30:04.962699      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan  3 13:30:04.970: INFO: Creating new exec pod
Jan  3 13:30:12.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=services-3919 execpod77bgr -- /bin/sh -x -c nslookup clusterip-service'
Jan  3 13:30:13.496: INFO: stderr: "+ nslookup clusterip-service\n"
Jan  3 13:30:13.496: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3919.svc.cluster.local\tcanonical name = externalsvc.services-3919.svc.cluster.local.\nName:\texternalsvc.services-3919.svc.cluster.local\nAddress: 10.102.49.188\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3919, will wait for the garbage collector to delete the pods
Jan  3 13:30:13.568: INFO: Deleting ReplicationController externalsvc took: 5.503413ms
Jan  3 13:30:13.969: INFO: Terminating ReplicationController externalsvc pods took: 400.2344ms
Jan  3 13:30:26.982: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:30:26.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3919" for this suite.
Jan  3 13:30:32.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:30:33.026: INFO: namespace services-3919 deletion completed in 6.038209614s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:37.152 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:30:33.027: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-701b246a-4589-4699-9292-e1d4f962c3bd in namespace container-probe-701
Jan  3 13:30:41.045: INFO: Started pod liveness-701b246a-4589-4699-9292-e1d4f962c3bd in namespace container-probe-701
STEP: checking the pod's current state and verifying that restartCount is present
Jan  3 13:30:41.047: INFO: Initial restart count of pod liveness-701b246a-4589-4699-9292-e1d4f962c3bd is 0
Jan  3 13:30:57.070: INFO: Restart count of pod container-probe-701/liveness-701b246a-4589-4699-9292-e1d4f962c3bd is now 1 (16.023180828s elapsed)
Jan  3 13:31:17.122: INFO: Restart count of pod container-probe-701/liveness-701b246a-4589-4699-9292-e1d4f962c3bd is now 2 (36.07534398s elapsed)
Jan  3 13:31:37.141: INFO: Restart count of pod container-probe-701/liveness-701b246a-4589-4699-9292-e1d4f962c3bd is now 3 (56.094413715s elapsed)
Jan  3 13:31:57.179: INFO: Restart count of pod container-probe-701/liveness-701b246a-4589-4699-9292-e1d4f962c3bd is now 4 (1m16.132634872s elapsed)
Jan  3 13:33:07.405: INFO: Restart count of pod container-probe-701/liveness-701b246a-4589-4699-9292-e1d4f962c3bd is now 5 (2m26.357791224s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:33:07.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-701" for this suite.
Jan  3 13:33:13.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:33:13.472: INFO: namespace container-probe-701 deletion completed in 6.047669962s

• [SLOW TEST:160.445 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:33:13.473: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 13:33:13.492: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72706d6c-c8d8-4b1f-8c96-f93cbbe92687" in namespace "projected-5431" to be "success or failure"
Jan  3 13:33:13.494: INFO: Pod "downwardapi-volume-72706d6c-c8d8-4b1f-8c96-f93cbbe92687": Phase="Pending", Reason="", readiness=false. Elapsed: 1.856883ms
Jan  3 13:33:15.511: INFO: Pod "downwardapi-volume-72706d6c-c8d8-4b1f-8c96-f93cbbe92687": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019072047s
Jan  3 13:33:17.513: INFO: Pod "downwardapi-volume-72706d6c-c8d8-4b1f-8c96-f93cbbe92687": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020881969s
Jan  3 13:33:19.514: INFO: Pod "downwardapi-volume-72706d6c-c8d8-4b1f-8c96-f93cbbe92687": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022256529s
Jan  3 13:33:21.517: INFO: Pod "downwardapi-volume-72706d6c-c8d8-4b1f-8c96-f93cbbe92687": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.025541722s
STEP: Saw pod success
Jan  3 13:33:21.517: INFO: Pod "downwardapi-volume-72706d6c-c8d8-4b1f-8c96-f93cbbe92687" satisfied condition "success or failure"
Jan  3 13:33:21.519: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-72706d6c-c8d8-4b1f-8c96-f93cbbe92687 container client-container: <nil>
STEP: delete the pod
Jan  3 13:33:21.540: INFO: Waiting for pod downwardapi-volume-72706d6c-c8d8-4b1f-8c96-f93cbbe92687 to disappear
Jan  3 13:33:21.544: INFO: Pod downwardapi-volume-72706d6c-c8d8-4b1f-8c96-f93cbbe92687 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:33:21.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5431" for this suite.
Jan  3 13:33:27.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:33:27.584: INFO: namespace projected-5431 deletion completed in 6.038725306s

• [SLOW TEST:14.112 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:33:27.585: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-92d83be8-c6b4-453c-beec-942baf613429
STEP: Creating configMap with name cm-test-opt-upd-fa33c56f-bfbc-461c-87bc-1791dfce9aeb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-92d83be8-c6b4-453c-beec-942baf613429
STEP: Updating configmap cm-test-opt-upd-fa33c56f-bfbc-461c-87bc-1791dfce9aeb
STEP: Creating configMap with name cm-test-opt-create-910608b5-acd7-4f7c-844b-5df60713e702
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:34:58.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4404" for this suite.
Jan  3 13:35:10.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:35:10.101: INFO: namespace configmap-4404 deletion completed in 12.045900749s

• [SLOW TEST:102.516 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:35:10.101: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan  3 13:35:10.123: INFO: Waiting up to 5m0s for pod "pod-837e8135-0124-46ac-9da3-1de706ae5ae0" in namespace "emptydir-275" to be "success or failure"
Jan  3 13:35:10.126: INFO: Pod "pod-837e8135-0124-46ac-9da3-1de706ae5ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.481241ms
Jan  3 13:35:12.128: INFO: Pod "pod-837e8135-0124-46ac-9da3-1de706ae5ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004713601s
Jan  3 13:35:14.148: INFO: Pod "pod-837e8135-0124-46ac-9da3-1de706ae5ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024383073s
Jan  3 13:35:16.153: INFO: Pod "pod-837e8135-0124-46ac-9da3-1de706ae5ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029343038s
Jan  3 13:35:18.154: INFO: Pod "pod-837e8135-0124-46ac-9da3-1de706ae5ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030782708s
Jan  3 13:35:20.192: INFO: Pod "pod-837e8135-0124-46ac-9da3-1de706ae5ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.068494963s
Jan  3 13:35:22.199: INFO: Pod "pod-837e8135-0124-46ac-9da3-1de706ae5ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.075593403s
Jan  3 13:35:24.201: INFO: Pod "pod-837e8135-0124-46ac-9da3-1de706ae5ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.07731093s
Jan  3 13:35:26.203: INFO: Pod "pod-837e8135-0124-46ac-9da3-1de706ae5ae0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.079503267s
STEP: Saw pod success
Jan  3 13:35:26.203: INFO: Pod "pod-837e8135-0124-46ac-9da3-1de706ae5ae0" satisfied condition "success or failure"
Jan  3 13:35:26.204: INFO: Trying to get logs from node controller-0 pod pod-837e8135-0124-46ac-9da3-1de706ae5ae0 container test-container: <nil>
STEP: delete the pod
Jan  3 13:35:26.235: INFO: Waiting for pod pod-837e8135-0124-46ac-9da3-1de706ae5ae0 to disappear
Jan  3 13:35:26.238: INFO: Pod pod-837e8135-0124-46ac-9da3-1de706ae5ae0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:35:26.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-275" for this suite.
Jan  3 13:35:32.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:35:32.288: INFO: namespace emptydir-275 deletion completed in 6.048096905s

• [SLOW TEST:22.187 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:35:32.289: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jan  3 13:35:40.852: INFO: Successfully updated pod "labelsupdatec26025fb-b8ea-4d4b-855f-fa0ba41d360a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:35:45.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2582" for this suite.
Jan  3 13:35:57.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:35:57.260: INFO: namespace projected-2582 deletion completed in 12.238309186s

• [SLOW TEST:24.972 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:35:57.260: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jan  3 13:35:57.286: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  3 13:35:57.291: INFO: Waiting for terminating namespaces to be deleted...
Jan  3 13:35:57.292: INFO: 
Logging pods the kubelet thinks is on node controller-0 before test
Jan  3 13:35:57.325: INFO: keystone-domain-manage-lk44k from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container keystone-domain-manage ready: false, restart count 0
Jan  3 13:35:57.325: INFO: ceph-pools-audit-1578057600-2vqm7 from kube-system started at 2020-01-03 13:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 13:35:57.325: INFO: nova-api-proxy-577495bf7f-nqbkw from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container nova-api-proxy ready: true, restart count 0
Jan  3 13:35:57.325: INFO: nova-scheduler-76b67b674d-bxmlc from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container nova-scheduler ready: true, restart count 0
Jan  3 13:35:57.325: INFO: neutron-db-init-p9bt8 from openstack started at 2020-01-03 11:34:31 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container neutron-db-init-0 ready: false, restart count 0
Jan  3 13:35:57.325: INFO: cinder-volume-usage-audit-1578057900-9xbm8 from openstack started at 2020-01-03 13:25:06 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 13:35:57.325: INFO: kube-sriov-cni-ds-amd64-h4sh4 from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container kube-sriov-cni ready: true, restart count 1
Jan  3 13:35:57.325: INFO: mariadb-server-1 from openstack started at 2020-01-03 11:20:51 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container mariadb ready: true, restart count 0
Jan  3 13:35:57.325: INFO: keystone-db-sync-bc8tl from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container keystone-db-sync ready: false, restart count 0
Jan  3 13:35:57.325: INFO: keystone-rabbit-init-b4rvs from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 13:35:57.325: INFO: glance-api-747954666d-z9hzh from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container glance-api ready: true, restart count 0
Jan  3 13:35:57.325: INFO: heat-engine-54745654c7-kt6dn from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container heat-engine ready: true, restart count 0
Jan  3 13:35:57.325: INFO: fm-ks-user-xsvv5 from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:35:57.325: INFO: horizon-db-init-krgpq from openstack started at 2020-01-03 11:42:50 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container horizon-db-init-0 ready: false, restart count 0
Jan  3 13:35:57.325: INFO: tiller-deploy-d6b59fcb-ksjbc from kube-system started at 2020-01-03 09:59:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container tiller ready: true, restart count 1
Jan  3 13:35:57.325: INFO: kube-controller-manager-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan  3 13:35:57.325: INFO: keystone-api-7b4d98b8c5-hhwqm from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container keystone-api ready: true, restart count 0
Jan  3 13:35:57.325: INFO: libvirt-libvirt-default-nt27c from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container libvirt ready: true, restart count 0
Jan  3 13:35:57.325: INFO: nova-cell-setup-p9vn4 from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container nova-cell-setup ready: false, restart count 0
Jan  3 13:35:57.325: INFO: nova-db-sync-rhcfq from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container nova-db-sync ready: false, restart count 0
Jan  3 13:35:57.325: INFO: nova-service-cleaner-1578056400-xk8zr from openstack started at 2020-01-03 13:00:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container nova-service-cleaner ready: false, restart count 0
Jan  3 13:35:57.325: INFO: ingress-error-pages-5bcb8b5f6c-4dxmf from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 13:35:57.325: INFO: neutron-ks-user-dlm6h from openstack started at 2020-01-03 11:34:32 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:35:57.325: INFO: kube-proxy-5l8fd from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container kube-proxy ready: true, restart count 1
Jan  3 13:35:57.325: INFO: calico-node-rb47m from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container calico-node ready: true, restart count 3
Jan  3 13:35:57.325: INFO: glance-db-sync-6sk2d from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container glance-db-sync ready: false, restart count 0
Jan  3 13:35:57.325: INFO: cinder-rabbit-init-2kjgk from openstack started at 2020-01-03 11:29:43 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 13:35:57.325: INFO: nova-conductor-bb6d86d69-p9v25 from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container nova-conductor ready: true, restart count 0
Jan  3 13:35:57.325: INFO: neutron-l3-agent-controller-0-937646f6-dzqjf from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container neutron-l3-agent ready: true, restart count 0
Jan  3 13:35:57.325: INFO: kube-multus-ds-amd64-kfg66 from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container kube-multus ready: true, restart count 1
Jan  3 13:35:57.325: INFO: nova-rabbit-init-88k4z from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 13:35:57.325: INFO: ceph-pools-audit-1578057900-jtn5h from kube-system started at 2020-01-03 13:25:06 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 13:35:57.325: INFO: cinder-volume-usage-audit-1578058200-v59p6 from openstack started at 2020-01-03 13:30:08 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 13:35:57.325: INFO: calico-kube-controllers-855577b7b5-6c5kq from kube-system started at 2020-01-03 09:59:31 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Jan  3 13:35:57.325: INFO: rbd-provisioner-7484d49cf6-bztgh from kube-system started at 2020-01-03 10:30:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container rbd-provisioner ready: true, restart count 0
Jan  3 13:35:57.325: INFO: storage-init-rbd-provisioner-mt758 from kube-system started at 2020-01-03 10:54:49 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container storage-init-general ready: false, restart count 0
Jan  3 13:35:57.325: INFO: osh-openstack-rabbitmq-rabbitmq-0 from openstack started at 2020-01-03 11:22:34 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container rabbitmq ready: true, restart count 0
Jan  3 13:35:57.325: INFO: horizon-db-sync-sc5ng from openstack started at 2020-01-03 11:42:50 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container horizon-db-sync ready: false, restart count 0
Jan  3 13:35:57.325: INFO: keystone-fernet-rotate-1578052800-fbtn9 from openstack started at 2020-01-03 12:00:07 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container keystone-fernet-rotate ready: false, restart count 0
Jan  3 13:35:57.325: INFO: cinder-scheduler-664bb87785-99gm8 from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container cinder-scheduler ready: true, restart count 0
Jan  3 13:35:57.325: INFO: placement-api-5b65bc5576-fw2vt from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container placement-api ready: true, restart count 0
Jan  3 13:35:57.325: INFO: nova-storage-init-gshl7 from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container nova-storage-init-ephemeral ready: false, restart count 0
Jan  3 13:35:57.325: INFO: heat-trusts-dgjrv from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container heat-trusts ready: false, restart count 0
Jan  3 13:35:57.325: INFO: cinder-volume-usage-audit-1578058500-ptsqr from openstack started at 2020-01-03 13:35:09 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 13:35:57.325: INFO: sonobuoy-systemd-logs-daemon-set-5e158e08c5964d78-s6m8n from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 13:35:57.325: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 13:35:57.325: INFO: kube-apiserver-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container kube-apiserver ready: true, restart count 2
Jan  3 13:35:57.325: INFO: ingress-bc886876f-gcg7c from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container ingress ready: true, restart count 0
Jan  3 13:35:57.325: INFO: cinder-backup-storage-init-b45t5 from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container cinder-backup-storage-init ready: false, restart count 0
Jan  3 13:35:57.325: INFO: cinder-storage-init-vj9nb from openstack started at 2020-01-03 11:29:43 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container cinder-storage-init-ceph-store ready: false, restart count 0
Jan  3 13:35:57.325: INFO: nova-api-osapi-68846d5959-bld9x from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container nova-osapi ready: true, restart count 0
Jan  3 13:35:57.325: INFO: heat-db-sync-qj5gp from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container heat-db-sync ready: false, restart count 0
Jan  3 13:35:57.325: INFO: ceph-pools-audit-1578058200-zwwdc from kube-system started at 2020-01-03 13:30:08 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 13:35:57.325: INFO: heat-engine-cleaner-1578058200-x4t9c from openstack started at 2020-01-03 13:30:08 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 13:35:57.325: INFO: ingress-r62j4 from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container ingress ready: true, restart count 0
Jan  3 13:35:57.325: INFO: ceph-pools-audit-1578058500-4kq4m from kube-system started at 2020-01-03 13:35:09 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container ceph-pools-audit-ceph-store ready: true, restart count 0
Jan  3 13:35:57.325: INFO: glance-rabbit-init-92z6q from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 13:35:57.325: INFO: cinder-bootstrap-95x9f from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 13:35:57.325: INFO: placement-db-sync-2h4wd from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container placement-db-sync ready: false, restart count 0
Jan  3 13:35:57.325: INFO: neutron-sriov-agent-controller-0-937646f6-5sggz from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container neutron-sriov-agent ready: true, restart count 0
Jan  3 13:35:57.325: INFO: heat-engine-cleaner-1578058500-8qhgc from openstack started at 2020-01-03 13:35:09 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 13:35:57.325: INFO: coredns-6bc668cd76-mcj6r from kube-system started at 2020-01-03 09:59:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container coredns ready: true, restart count 1
Jan  3 13:35:57.325: INFO: keystone-fernet-setup-mdqx2 from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container keystone-fernet-setup ready: false, restart count 0
Jan  3 13:35:57.325: INFO: glance-ks-endpoints-pz9wc from openstack started at 2020-01-03 11:25:03 +0000 UTC (3 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container image-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:35:57.325: INFO: 	Container image-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:35:57.325: INFO: 	Container image-ks-endpoints-public ready: false, restart count 0
Jan  3 13:35:57.325: INFO: glance-storage-init-km9lz from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container glance-storage-init ready: false, restart count 0
Jan  3 13:35:57.325: INFO: nova-compute-controller-0-937646f6-nz464 from openstack started at 2020-01-03 11:34:28 +0000 UTC (2 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container nova-compute ready: true, restart count 0
Jan  3 13:35:57.325: INFO: 	Container nova-compute-ssh ready: true, restart count 0
Jan  3 13:35:57.325: INFO: neutron-db-sync-5f8lm from openstack started at 2020-01-03 11:34:31 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container neutron-db-sync ready: false, restart count 0
Jan  3 13:35:57.325: INFO: fm-db-sync-plkwf from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container fm-db-sync ready: false, restart count 0
Jan  3 13:35:57.325: INFO: nova-service-cleaner-1578052800-c8kbg from openstack started at 2020-01-03 12:00:07 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container nova-service-cleaner ready: false, restart count 0
Jan  3 13:35:57.325: INFO: cinder-db-sync-fc48c from openstack started at 2020-01-03 11:29:43 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container cinder-db-sync ready: false, restart count 0
Jan  3 13:35:57.325: INFO: neutron-server-54b46f798-cstmn from openstack started at 2020-01-03 11:34:31 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container neutron-server ready: true, restart count 0
Jan  3 13:35:57.325: INFO: neutron-ks-service-wmlfq from openstack started at 2020-01-03 11:34:31 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container network-ks-service-registration ready: false, restart count 0
Jan  3 13:35:57.325: INFO: heat-cfn-c8f5b9b4b-5g2dk from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container heat-cfn ready: true, restart count 0
Jan  3 13:35:57.325: INFO: heat-api-58bf859968-hpnzj from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container heat-api ready: true, restart count 0
Jan  3 13:35:57.325: INFO: heat-trustee-ks-user-65bc4 from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:35:57.325: INFO: fm-rest-api-b4bc757f4-wrm9c from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container fm-rest-api ready: true, restart count 0
Jan  3 13:35:57.325: INFO: horizon-6865446ff5-rztk2 from openstack started at 2020-01-03 11:42:50 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container horizon ready: true, restart count 0
Jan  3 13:35:57.325: INFO: cinder-backup-fd5f96bf4-s964v from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container cinder-backup ready: true, restart count 0
Jan  3 13:35:57.325: INFO: mariadb-ingress-5bb8b69fc8-2pdr5 from openstack started at 2020-01-03 11:20:48 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container ingress ready: true, restart count 0
Jan  3 13:35:57.325: INFO: nova-novncproxy-6c54c7d98-m2tcl from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container nova-novncproxy ready: true, restart count 0
Jan  3 13:35:57.325: INFO: kube-scheduler-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan  3 13:35:57.325: INFO: osh-openstack-rabbitmq-cluster-wait-xl9t7 from openstack started at 2020-01-03 11:22:33 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container osh-openstack-rabbitmq-rabbitmq-cluster-wait ready: false, restart count 0
Jan  3 13:35:57.325: INFO: keystone-credential-setup-qkqd5 from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container keystone-credential-setup ready: false, restart count 0
Jan  3 13:35:57.325: INFO: keystone-db-init-bz6pm from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container keystone-db-init-0 ready: false, restart count 0
Jan  3 13:35:57.325: INFO: nova-bootstrap-6tpt5 from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 13:35:57.325: INFO: heat-domain-ks-user-lbck5 from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container heat-ks-domain-user ready: false, restart count 0
Jan  3 13:35:57.325: INFO: osh-openstack-memcached-memcached-545668bdbd-p88zm from openstack started at 2020-01-03 11:22:14 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container memcached ready: true, restart count 0
Jan  3 13:35:57.325: INFO: cinder-volume-549d7c447c-h5gcw from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container cinder-volume ready: true, restart count 0
Jan  3 13:35:57.325: INFO: cinder-ks-service-rq6n6 from openstack started at 2020-01-03 11:29:43 +0000 UTC (3 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container volume-ks-service-registration ready: false, restart count 0
Jan  3 13:35:57.325: INFO: 	Container volumev2-ks-service-registration ready: false, restart count 0
Jan  3 13:35:57.325: INFO: 	Container volumev3-ks-service-registration ready: false, restart count 0
Jan  3 13:35:57.325: INFO: placement-ks-endpoints-fh88g from openstack started at 2020-01-03 11:34:26 +0000 UTC (3 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container placement-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:35:57.325: INFO: 	Container placement-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:35:57.325: INFO: 	Container placement-ks-endpoints-public ready: false, restart count 0
Jan  3 13:35:57.325: INFO: nova-api-metadata-7bdf79d754-9ds2d from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container nova-api ready: true, restart count 1
Jan  3 13:35:57.325: INFO: neutron-ovs-agent-controller-0-937646f6-bp6gb from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container neutron-ovs-agent ready: true, restart count 0
Jan  3 13:35:57.325: INFO: neutron-rabbit-init-4f6lx from openstack started at 2020-01-03 11:34:32 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 13:35:57.325: INFO: ingress-error-pages-cf6c65b-lqksh from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 13:35:57.325: INFO: cinder-api-7ff9984869-56lvq from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container cinder-api ready: true, restart count 0
Jan  3 13:35:57.325: INFO: placement-ks-service-wpvrq from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container placement-ks-service-registration ready: false, restart count 0
Jan  3 13:35:57.325: INFO: neutron-dhcp-agent-controller-0-937646f6-qgt4d from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container neutron-dhcp-agent ready: true, restart count 0
Jan  3 13:35:57.325: INFO: neutron-metadata-agent-controller-0-937646f6-nbcf6 from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.325: INFO: 	Container neutron-metadata-agent ready: true, restart count 0
Jan  3 13:35:57.325: INFO: 
Logging pods the kubelet thinks is on node controller-1 before test
Jan  3 13:35:57.374: INFO: kube-scheduler-controller-1 from kube-system started at 2020-01-03 10:52:37 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  3 13:35:57.374: INFO: placement-db-init-4t4cq from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container placement-db-init-0 ready: false, restart count 0
Jan  3 13:35:57.374: INFO: neutron-l3-agent-controller-1-cab72f56-nm27w from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container neutron-l3-agent ready: true, restart count 0
Jan  3 13:35:57.374: INFO: fm-ks-endpoints-rswss from openstack started at 2020-01-03 11:41:38 +0000 UTC (3 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container faultmanagement-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container faultmanagement-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container faultmanagement-ks-endpoints-public ready: false, restart count 0
Jan  3 13:35:57.374: INFO: cinder-ks-user-rhjbx from openstack started at 2020-01-03 11:29:43 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:35:57.374: INFO: nova-ks-service-x57sz from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container compute-ks-service-registration ready: false, restart count 0
Jan  3 13:35:57.374: INFO: heat-engine-cleaner-1578057900-wctm8 from openstack started at 2020-01-03 13:25:06 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 13:35:57.374: INFO: glance-db-init-9hfsz from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container glance-db-init-0 ready: false, restart count 0
Jan  3 13:35:57.374: INFO: cinder-scheduler-664bb87785-67xsw from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container cinder-scheduler ready: true, restart count 0
Jan  3 13:35:57.374: INFO: heat-api-58bf859968-c7cp2 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container heat-api ready: true, restart count 0
Jan  3 13:35:57.374: INFO: heat-db-init-dmbl7 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container heat-db-init-0 ready: false, restart count 0
Jan  3 13:35:57.374: INFO: sonobuoy from sonobuoy started at 2020-01-03 12:54:34 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  3 13:35:57.374: INFO: calico-node-nkz88 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container calico-node ready: true, restart count 1
Jan  3 13:35:57.374: INFO: osh-openstack-rabbitmq-rabbitmq-1 from openstack started at 2020-01-03 11:22:47 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container rabbitmq ready: true, restart count 0
Jan  3 13:35:57.374: INFO: neutron-sriov-agent-controller-1-cab72f56-t65r5 from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container neutron-sriov-agent ready: true, restart count 0
Jan  3 13:35:57.374: INFO: heat-bootstrap-9xrdd from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 13:35:57.374: INFO: kube-sriov-cni-ds-amd64-jnv4d from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Jan  3 13:35:57.374: INFO: keystone-bootstrap-jmtqn from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 13:35:57.374: INFO: glance-api-747954666d-j2ldf from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container glance-api ready: true, restart count 0
Jan  3 13:35:57.374: INFO: fm-ks-service-nwst8 from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container faultmanagement-ks-service-registration ready: false, restart count 0
Jan  3 13:35:57.374: INFO: heat-ks-user-hbk96 from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:35:57.374: INFO: kube-proxy-2fz94 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 13:35:57.374: INFO: ingress-bc886876f-b6vc9 from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container ingress ready: true, restart count 0
Jan  3 13:35:57.374: INFO: mariadb-ingress-5bb8b69fc8-r2qvn from openstack started at 2020-01-03 11:20:48 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container ingress ready: true, restart count 0
Jan  3 13:35:57.374: INFO: nova-ks-endpoints-qv68g from openstack started at 2020-01-03 11:34:26 +0000 UTC (3 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container compute-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container compute-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container compute-ks-endpoints-public ready: false, restart count 0
Jan  3 13:35:57.374: INFO: placement-api-5b65bc5576-f9d4b from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container placement-api ready: true, restart count 0
Jan  3 13:35:57.374: INFO: nova-compute-controller-1-cab72f56-c922s from openstack started at 2020-01-03 11:34:28 +0000 UTC (2 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container nova-compute ready: true, restart count 0
Jan  3 13:35:57.374: INFO: 	Container nova-compute-ssh ready: true, restart count 0
Jan  3 13:35:57.374: INFO: heat-engine-54745654c7-4ln84 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container heat-engine ready: true, restart count 0
Jan  3 13:35:57.374: INFO: fm-rest-api-b4bc757f4-8tmgc from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container fm-rest-api ready: true, restart count 0
Jan  3 13:35:57.374: INFO: kube-multus-ds-amd64-ggv58 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container kube-multus ready: true, restart count 0
Jan  3 13:35:57.374: INFO: neutron-server-54b46f798-4f8d9 from openstack started at 2020-01-03 11:34:31 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container neutron-server ready: true, restart count 0
Jan  3 13:35:57.374: INFO: sonobuoy-systemd-logs-daemon-set-5e158e08c5964d78-2qpxv from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 13:35:57.374: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 13:35:57.374: INFO: cinder-create-internal-tenant-twkzc from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container create-internal-tenant ready: false, restart count 0
Jan  3 13:35:57.374: INFO: cinder-backup-fd5f96bf4-vzc8x from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container cinder-backup ready: true, restart count 0
Jan  3 13:35:57.374: INFO: cinder-ks-endpoints-wvm8w from openstack started at 2020-01-03 11:29:43 +0000 UTC (9 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container volume-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container volume-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container volume-ks-endpoints-public ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container volumev2-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container volumev2-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container volumev2-ks-endpoints-public ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container volumev3-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container volumev3-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container volumev3-ks-endpoints-public ready: false, restart count 0
Jan  3 13:35:57.374: INFO: nova-api-metadata-7bdf79d754-kr2gk from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container nova-api ready: true, restart count 1
Jan  3 13:35:57.374: INFO: nova-db-init-86cfh from openstack started at 2020-01-03 11:34:29 +0000 UTC (3 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container nova-db-init-0 ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container nova-db-init-1 ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container nova-db-init-2 ready: false, restart count 0
Jan  3 13:35:57.374: INFO: fm-db-init-2t975 from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container fm-db-init-0 ready: false, restart count 0
Jan  3 13:35:57.374: INFO: sonobuoy-e2e-job-08a7a8b2e6d84bc6 from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container e2e ready: true, restart count 0
Jan  3 13:35:57.374: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 13:35:57.374: INFO: heat-cfn-c8f5b9b4b-wd8h4 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container heat-cfn ready: true, restart count 0
Jan  3 13:35:57.374: INFO: ingress-5fgmr from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container ingress ready: true, restart count 0
Jan  3 13:35:57.374: INFO: ingress-error-pages-5bcb8b5f6c-8rf2d from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 13:35:57.374: INFO: mariadb-ingress-error-pages-847467b5d5-d57l2 from openstack started at 2020-01-03 11:20:48 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 13:35:57.374: INFO: libvirt-libvirt-default-np4lk from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container libvirt ready: true, restart count 0
Jan  3 13:35:57.374: INFO: nova-api-proxy-577495bf7f-b5t9q from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container nova-api-proxy ready: true, restart count 0
Jan  3 13:35:57.374: INFO: nova-conductor-bb6d86d69-5tkcn from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container nova-conductor ready: true, restart count 0
Jan  3 13:35:57.374: INFO: neutron-ovs-agent-controller-1-cab72f56-mntzv from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container neutron-ovs-agent ready: true, restart count 0
Jan  3 13:35:57.374: INFO: heat-ks-endpoints-jjzrq from openstack started at 2020-01-03 11:40:01 +0000 UTC (6 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container cloudformation-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container cloudformation-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container cloudformation-ks-endpoints-public ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container orchestration-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container orchestration-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container orchestration-ks-endpoints-public ready: false, restart count 0
Jan  3 13:35:57.374: INFO: heat-ks-service-b7t2s from openstack started at 2020-01-03 11:40:01 +0000 UTC (2 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container cloudformation-ks-service-registration ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container orchestration-ks-service-registration ready: false, restart count 0
Jan  3 13:35:57.374: INFO: nova-ks-user-gtl4m from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:35:57.374: INFO: neutron-metadata-agent-controller-1-cab72f56-xwlz5 from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container neutron-metadata-agent ready: true, restart count 0
Jan  3 13:35:57.374: INFO: ingress-error-pages-cf6c65b-zwhqk from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 13:35:57.374: INFO: keystone-api-7b4d98b8c5-8wjmf from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container keystone-api ready: true, restart count 0
Jan  3 13:35:57.374: INFO: neutron-dhcp-agent-controller-1-cab72f56-t5s8d from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container neutron-dhcp-agent ready: true, restart count 0
Jan  3 13:35:57.374: INFO: kube-apiserver-controller-1 from kube-system started at 2020-01-03 10:52:37 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  3 13:35:57.374: INFO: rbd-provisioner-7484d49cf6-2tb85 from kube-system started at 2020-01-03 10:55:54 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container rbd-provisioner ready: true, restart count 0
Jan  3 13:35:57.374: INFO: nova-api-osapi-68846d5959-ggf8r from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container nova-osapi ready: true, restart count 0
Jan  3 13:35:57.374: INFO: nova-scheduler-76b67b674d-4g9bx from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container nova-scheduler ready: true, restart count 0
Jan  3 13:35:57.374: INFO: coredns-6bc668cd76-9nlk5 from kube-system started at 2020-01-03 10:48:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container coredns ready: true, restart count 0
Jan  3 13:35:57.374: INFO: cinder-api-7ff9984869-9g8tp from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container cinder-api ready: true, restart count 0
Jan  3 13:35:57.374: INFO: cinder-volume-549d7c447c-2sjzd from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container cinder-volume ready: true, restart count 0
Jan  3 13:35:57.374: INFO: nova-novncproxy-6c54c7d98-d2pzp from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container nova-novncproxy ready: true, restart count 0
Jan  3 13:35:57.374: INFO: mariadb-server-0 from openstack started at 2020-01-03 11:20:52 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container mariadb ready: true, restart count 0
Jan  3 13:35:57.374: INFO: glance-ks-service-kbjlh from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container image-ks-service-registration ready: false, restart count 0
Jan  3 13:35:57.374: INFO: cinder-db-init-g8w2v from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container cinder-db-init-0 ready: false, restart count 0
Jan  3 13:35:57.374: INFO: kube-controller-manager-controller-1 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  3 13:35:57.374: INFO: glance-ks-user-h6kmn from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:35:57.374: INFO: placement-ks-user-swghx from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:35:57.374: INFO: neutron-ks-endpoints-xrvqm from openstack started at 2020-01-03 11:34:31 +0000 UTC (3 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container network-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container network-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:35:57.374: INFO: 	Container network-ks-endpoints-public ready: false, restart count 0
Jan  3 13:35:57.374: INFO: heat-rabbit-init-mtk7j from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 13:35:57.374: INFO: 	Container rabbit-init ready: false, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15e663e5a6400509], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:35:58.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5785" for this suite.
Jan  3 13:36:04.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:36:04.463: INFO: namespace sched-pred-5785 deletion completed in 6.032747465s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.203 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:36:04.463: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 13:36:04.743: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 13:36:06.746: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:36:08.748: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:36:10.752: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:36:12.749: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713655364, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 13:36:15.764: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:36:15.781: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2102-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:36:16.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9188" for this suite.
Jan  3 13:36:22.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:36:22.439: INFO: namespace webhook-9188 deletion completed in 6.046999552s
STEP: Destroying namespace "webhook-9188-markers" for this suite.
Jan  3 13:36:28.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:36:28.480: INFO: namespace webhook-9188-markers deletion completed in 6.040633277s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.021 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:36:28.486: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 13:36:28.516: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ddcc95b-b846-4819-8258-3aaac8d32b12" in namespace "downward-api-4419" to be "success or failure"
Jan  3 13:36:28.518: INFO: Pod "downwardapi-volume-8ddcc95b-b846-4819-8258-3aaac8d32b12": Phase="Pending", Reason="", readiness=false. Elapsed: 1.767782ms
Jan  3 13:36:30.519: INFO: Pod "downwardapi-volume-8ddcc95b-b846-4819-8258-3aaac8d32b12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003677692s
Jan  3 13:36:32.521: INFO: Pod "downwardapi-volume-8ddcc95b-b846-4819-8258-3aaac8d32b12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005224981s
Jan  3 13:36:34.522: INFO: Pod "downwardapi-volume-8ddcc95b-b846-4819-8258-3aaac8d32b12": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00667621s
Jan  3 13:36:36.524: INFO: Pod "downwardapi-volume-8ddcc95b-b846-4819-8258-3aaac8d32b12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008249966s
STEP: Saw pod success
Jan  3 13:36:36.524: INFO: Pod "downwardapi-volume-8ddcc95b-b846-4819-8258-3aaac8d32b12" satisfied condition "success or failure"
Jan  3 13:36:36.525: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-8ddcc95b-b846-4819-8258-3aaac8d32b12 container client-container: <nil>
STEP: delete the pod
Jan  3 13:36:36.535: INFO: Waiting for pod downwardapi-volume-8ddcc95b-b846-4819-8258-3aaac8d32b12 to disappear
Jan  3 13:36:36.536: INFO: Pod downwardapi-volume-8ddcc95b-b846-4819-8258-3aaac8d32b12 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:36:36.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4419" for this suite.
Jan  3 13:36:42.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:36:42.619: INFO: namespace downward-api-4419 deletion completed in 6.079407606s

• [SLOW TEST:14.133 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:36:42.619: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-n4vn
STEP: Creating a pod to test atomic-volume-subpath
Jan  3 13:36:42.642: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-n4vn" in namespace "subpath-4033" to be "success or failure"
Jan  3 13:36:42.650: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Pending", Reason="", readiness=false. Elapsed: 7.824078ms
Jan  3 13:36:44.652: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01051075s
Jan  3 13:36:46.655: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013173559s
Jan  3 13:36:48.658: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016444621s
Jan  3 13:36:50.660: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Running", Reason="", readiness=true. Elapsed: 8.017986367s
Jan  3 13:36:52.661: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Running", Reason="", readiness=true. Elapsed: 10.019405256s
Jan  3 13:36:54.663: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Running", Reason="", readiness=true. Elapsed: 12.021004146s
Jan  3 13:36:56.665: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Running", Reason="", readiness=true. Elapsed: 14.022773345s
Jan  3 13:36:58.667: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Running", Reason="", readiness=true. Elapsed: 16.025501972s
Jan  3 13:37:00.669: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Running", Reason="", readiness=true. Elapsed: 18.027354161s
Jan  3 13:37:02.671: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Running", Reason="", readiness=true. Elapsed: 20.029012164s
Jan  3 13:37:04.672: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Running", Reason="", readiness=true. Elapsed: 22.030516731s
Jan  3 13:37:06.674: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Running", Reason="", readiness=true. Elapsed: 24.032424547s
Jan  3 13:37:08.676: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Running", Reason="", readiness=true. Elapsed: 26.034663218s
Jan  3 13:37:10.678: INFO: Pod "pod-subpath-test-configmap-n4vn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.036428284s
STEP: Saw pod success
Jan  3 13:37:10.678: INFO: Pod "pod-subpath-test-configmap-n4vn" satisfied condition "success or failure"
Jan  3 13:37:10.680: INFO: Trying to get logs from node controller-0 pod pod-subpath-test-configmap-n4vn container test-container-subpath-configmap-n4vn: <nil>
STEP: delete the pod
Jan  3 13:37:10.691: INFO: Waiting for pod pod-subpath-test-configmap-n4vn to disappear
Jan  3 13:37:10.692: INFO: Pod pod-subpath-test-configmap-n4vn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-n4vn
Jan  3 13:37:10.692: INFO: Deleting pod "pod-subpath-test-configmap-n4vn" in namespace "subpath-4033"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:37:10.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4033" for this suite.
Jan  3 13:37:16.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:37:16.735: INFO: namespace subpath-4033 deletion completed in 6.039761418s

• [SLOW TEST:34.117 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:37:16.735: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Jan  3 13:37:16.753: INFO: Waiting up to 5m0s for pod "client-containers-e3833e5f-9cef-4702-a503-71936980b479" in namespace "containers-5422" to be "success or failure"
Jan  3 13:37:16.754: INFO: Pod "client-containers-e3833e5f-9cef-4702-a503-71936980b479": Phase="Pending", Reason="", readiness=false. Elapsed: 1.015217ms
Jan  3 13:37:18.758: INFO: Pod "client-containers-e3833e5f-9cef-4702-a503-71936980b479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004254003s
Jan  3 13:37:20.759: INFO: Pod "client-containers-e3833e5f-9cef-4702-a503-71936980b479": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006016747s
Jan  3 13:37:22.762: INFO: Pod "client-containers-e3833e5f-9cef-4702-a503-71936980b479": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009151817s
Jan  3 13:37:24.765: INFO: Pod "client-containers-e3833e5f-9cef-4702-a503-71936980b479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01133161s
STEP: Saw pod success
Jan  3 13:37:24.765: INFO: Pod "client-containers-e3833e5f-9cef-4702-a503-71936980b479" satisfied condition "success or failure"
Jan  3 13:37:24.766: INFO: Trying to get logs from node controller-0 pod client-containers-e3833e5f-9cef-4702-a503-71936980b479 container test-container: <nil>
STEP: delete the pod
Jan  3 13:37:24.777: INFO: Waiting for pod client-containers-e3833e5f-9cef-4702-a503-71936980b479 to disappear
Jan  3 13:37:24.778: INFO: Pod client-containers-e3833e5f-9cef-4702-a503-71936980b479 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:37:24.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5422" for this suite.
Jan  3 13:37:30.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:37:30.817: INFO: namespace containers-5422 deletion completed in 6.037425319s

• [SLOW TEST:14.081 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:37:30.817: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-188d2623-d02c-4d4b-afad-764e54e21b97 in namespace container-probe-2169
Jan  3 13:37:40.833: INFO: Started pod busybox-188d2623-d02c-4d4b-afad-764e54e21b97 in namespace container-probe-2169
STEP: checking the pod's current state and verifying that restartCount is present
Jan  3 13:37:40.835: INFO: Initial restart count of pod busybox-188d2623-d02c-4d4b-afad-764e54e21b97 is 0
Jan  3 13:38:32.947: INFO: Restart count of pod container-probe-2169/busybox-188d2623-d02c-4d4b-afad-764e54e21b97 is now 1 (52.11156964s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:38:32.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2169" for this suite.
Jan  3 13:38:38.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:38:38.997: INFO: namespace container-probe-2169 deletion completed in 6.03469308s

• [SLOW TEST:68.180 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:38:38.997: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 13:38:39.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05fcb923-0bf5-468f-acda-8008d4dbc94e" in namespace "downward-api-5910" to be "success or failure"
Jan  3 13:38:39.017: INFO: Pod "downwardapi-volume-05fcb923-0bf5-468f-acda-8008d4dbc94e": Phase="Pending", Reason="", readiness=false. Elapsed: 946.815µs
Jan  3 13:38:41.024: INFO: Pod "downwardapi-volume-05fcb923-0bf5-468f-acda-8008d4dbc94e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008008883s
Jan  3 13:38:43.062: INFO: Pod "downwardapi-volume-05fcb923-0bf5-468f-acda-8008d4dbc94e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046307869s
Jan  3 13:38:45.069: INFO: Pod "downwardapi-volume-05fcb923-0bf5-468f-acda-8008d4dbc94e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053117214s
Jan  3 13:38:47.071: INFO: Pod "downwardapi-volume-05fcb923-0bf5-468f-acda-8008d4dbc94e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.055394131s
STEP: Saw pod success
Jan  3 13:38:47.071: INFO: Pod "downwardapi-volume-05fcb923-0bf5-468f-acda-8008d4dbc94e" satisfied condition "success or failure"
Jan  3 13:38:47.072: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-05fcb923-0bf5-468f-acda-8008d4dbc94e container client-container: <nil>
STEP: delete the pod
Jan  3 13:38:47.110: INFO: Waiting for pod downwardapi-volume-05fcb923-0bf5-468f-acda-8008d4dbc94e to disappear
Jan  3 13:38:47.111: INFO: Pod downwardapi-volume-05fcb923-0bf5-468f-acda-8008d4dbc94e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:38:47.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5910" for this suite.
Jan  3 13:38:53.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:38:53.157: INFO: namespace downward-api-5910 deletion completed in 6.041723673s

• [SLOW TEST:14.160 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:38:53.160: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-9mm2
STEP: Creating a pod to test atomic-volume-subpath
Jan  3 13:38:53.178: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9mm2" in namespace "subpath-7784" to be "success or failure"
Jan  3 13:38:53.181: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.102696ms
Jan  3 13:38:55.182: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004785024s
Jan  3 13:38:57.184: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006622021s
Jan  3 13:38:59.186: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007961293s
Jan  3 13:39:01.187: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Running", Reason="", readiness=true. Elapsed: 8.009378745s
Jan  3 13:39:03.189: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Running", Reason="", readiness=true. Elapsed: 10.01139634s
Jan  3 13:39:05.192: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Running", Reason="", readiness=true. Elapsed: 12.013964887s
Jan  3 13:39:07.200: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Running", Reason="", readiness=true. Elapsed: 14.022745483s
Jan  3 13:39:09.202: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Running", Reason="", readiness=true. Elapsed: 16.024208488s
Jan  3 13:39:11.205: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Running", Reason="", readiness=true. Elapsed: 18.0271282s
Jan  3 13:39:13.206: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Running", Reason="", readiness=true. Elapsed: 20.028480938s
Jan  3 13:39:15.208: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Running", Reason="", readiness=true. Elapsed: 22.030249644s
Jan  3 13:39:17.210: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Running", Reason="", readiness=true. Elapsed: 24.031918252s
Jan  3 13:39:19.211: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Running", Reason="", readiness=true. Elapsed: 26.033096757s
Jan  3 13:39:21.213: INFO: Pod "pod-subpath-test-configmap-9mm2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.034957909s
STEP: Saw pod success
Jan  3 13:39:21.213: INFO: Pod "pod-subpath-test-configmap-9mm2" satisfied condition "success or failure"
Jan  3 13:39:21.214: INFO: Trying to get logs from node controller-0 pod pod-subpath-test-configmap-9mm2 container test-container-subpath-configmap-9mm2: <nil>
STEP: delete the pod
Jan  3 13:39:21.234: INFO: Waiting for pod pod-subpath-test-configmap-9mm2 to disappear
Jan  3 13:39:21.235: INFO: Pod pod-subpath-test-configmap-9mm2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9mm2
Jan  3 13:39:21.235: INFO: Deleting pod "pod-subpath-test-configmap-9mm2" in namespace "subpath-7784"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:39:21.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7784" for this suite.
Jan  3 13:39:27.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:39:27.302: INFO: namespace subpath-7784 deletion completed in 6.064272339s

• [SLOW TEST:34.142 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:39:27.311: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:39:27.356: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan  3 13:39:27.392: INFO: Number of nodes with available pods: 0
Jan  3 13:39:27.399: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:39:28.402: INFO: Number of nodes with available pods: 0
Jan  3 13:39:28.402: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:39:29.402: INFO: Number of nodes with available pods: 0
Jan  3 13:39:29.402: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:39:30.404: INFO: Number of nodes with available pods: 0
Jan  3 13:39:30.404: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:39:31.419: INFO: Number of nodes with available pods: 0
Jan  3 13:39:31.419: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:39:32.402: INFO: Number of nodes with available pods: 0
Jan  3 13:39:32.403: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:39:33.402: INFO: Number of nodes with available pods: 0
Jan  3 13:39:33.402: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:39:34.402: INFO: Number of nodes with available pods: 0
Jan  3 13:39:34.402: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:39:35.408: INFO: Number of nodes with available pods: 2
Jan  3 13:39:35.408: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan  3 13:39:35.426: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:35.426: INFO: Wrong image for pod: daemon-set-kw4fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:36.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:36.430: INFO: Wrong image for pod: daemon-set-kw4fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:37.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:37.430: INFO: Wrong image for pod: daemon-set-kw4fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:38.431: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:38.431: INFO: Wrong image for pod: daemon-set-kw4fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:38.431: INFO: Pod daemon-set-kw4fc is not available
Jan  3 13:39:39.431: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:39.431: INFO: Wrong image for pod: daemon-set-kw4fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:39.431: INFO: Pod daemon-set-kw4fc is not available
Jan  3 13:39:40.433: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:40.433: INFO: Wrong image for pod: daemon-set-kw4fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:40.433: INFO: Pod daemon-set-kw4fc is not available
Jan  3 13:39:41.432: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:41.432: INFO: Wrong image for pod: daemon-set-kw4fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:41.432: INFO: Pod daemon-set-kw4fc is not available
Jan  3 13:39:42.484: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:42.484: INFO: Wrong image for pod: daemon-set-kw4fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:42.484: INFO: Pod daemon-set-kw4fc is not available
Jan  3 13:39:43.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:43.430: INFO: Wrong image for pod: daemon-set-kw4fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:43.430: INFO: Pod daemon-set-kw4fc is not available
Jan  3 13:39:44.462: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:44.462: INFO: Wrong image for pod: daemon-set-kw4fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:44.462: INFO: Pod daemon-set-kw4fc is not available
Jan  3 13:39:45.510: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:45.510: INFO: Wrong image for pod: daemon-set-kw4fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:45.510: INFO: Pod daemon-set-kw4fc is not available
Jan  3 13:39:46.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:46.430: INFO: Wrong image for pod: daemon-set-kw4fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:46.430: INFO: Pod daemon-set-kw4fc is not available
Jan  3 13:39:47.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:39:47.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:48.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:39:48.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:49.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:39:49.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:50.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:39:50.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:51.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:39:51.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:52.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:39:52.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:53.454: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:39:53.454: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:54.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:39:54.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:55.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:39:55.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:56.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:39:56.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:57.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:39:57.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:58.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:39:58.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:39:59.432: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:39:59.432: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:00.432: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:00.432: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:01.440: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:01.440: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:02.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:02.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:03.431: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:03.431: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:04.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:04.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:05.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:05.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:06.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:06.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:07.432: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:07.432: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:08.431: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:08.431: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:09.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:09.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:10.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:10.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:11.431: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:11.431: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:12.433: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:12.433: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:13.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:13.432: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:14.447: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:14.447: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:15.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:15.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:16.431: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:16.431: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:17.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:17.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:18.430: INFO: Pod daemon-set-56v5q is not available
Jan  3 13:40:18.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:19.431: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:20.439: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:21.430: INFO: Wrong image for pod: daemon-set-8plk9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jan  3 13:40:21.430: INFO: Pod daemon-set-8plk9 is not available
Jan  3 13:40:22.430: INFO: Pod daemon-set-zsxpg is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jan  3 13:40:22.435: INFO: Number of nodes with available pods: 1
Jan  3 13:40:22.435: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:40:23.439: INFO: Number of nodes with available pods: 1
Jan  3 13:40:23.439: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:40:24.438: INFO: Number of nodes with available pods: 1
Jan  3 13:40:24.438: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:40:25.439: INFO: Number of nodes with available pods: 1
Jan  3 13:40:25.439: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:40:26.439: INFO: Number of nodes with available pods: 1
Jan  3 13:40:26.439: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:40:27.439: INFO: Number of nodes with available pods: 1
Jan  3 13:40:27.439: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:40:28.439: INFO: Number of nodes with available pods: 1
Jan  3 13:40:28.439: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:40:29.448: INFO: Number of nodes with available pods: 1
Jan  3 13:40:29.448: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:40:30.439: INFO: Number of nodes with available pods: 2
Jan  3 13:40:30.439: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6722, will wait for the garbage collector to delete the pods
Jan  3 13:40:30.505: INFO: Deleting DaemonSet.extensions daemon-set took: 3.589083ms
Jan  3 13:40:30.909: INFO: Terminating DaemonSet.extensions daemon-set pods took: 403.664298ms
Jan  3 13:40:33.812: INFO: Number of nodes with available pods: 0
Jan  3 13:40:33.812: INFO: Number of running nodes: 0, number of available pods: 0
Jan  3 13:40:33.813: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6722/daemonsets","resourceVersion":"47008"},"items":null}

Jan  3 13:40:33.814: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6722/pods","resourceVersion":"47008"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:40:33.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6722" for this suite.
Jan  3 13:40:39.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:40:39.868: INFO: namespace daemonsets-6722 deletion completed in 6.048022473s

• [SLOW TEST:72.557 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:40:39.871: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan  3 13:40:47.954: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:40:47.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3635" for this suite.
Jan  3 13:40:53.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:40:54.008: INFO: namespace container-runtime-3635 deletion completed in 6.040233502s

• [SLOW TEST:14.138 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:40:54.009: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Jan  3 13:40:54.026: INFO: Waiting up to 5m0s for pod "var-expansion-a125f1a3-ace0-4a3d-94c3-4a62516f5567" in namespace "var-expansion-2328" to be "success or failure"
Jan  3 13:40:54.027: INFO: Pod "var-expansion-a125f1a3-ace0-4a3d-94c3-4a62516f5567": Phase="Pending", Reason="", readiness=false. Elapsed: 1.129155ms
Jan  3 13:40:56.029: INFO: Pod "var-expansion-a125f1a3-ace0-4a3d-94c3-4a62516f5567": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002472972s
Jan  3 13:40:58.030: INFO: Pod "var-expansion-a125f1a3-ace0-4a3d-94c3-4a62516f5567": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003977943s
Jan  3 13:41:00.032: INFO: Pod "var-expansion-a125f1a3-ace0-4a3d-94c3-4a62516f5567": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005595349s
Jan  3 13:41:02.035: INFO: Pod "var-expansion-a125f1a3-ace0-4a3d-94c3-4a62516f5567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009205887s
STEP: Saw pod success
Jan  3 13:41:02.035: INFO: Pod "var-expansion-a125f1a3-ace0-4a3d-94c3-4a62516f5567" satisfied condition "success or failure"
Jan  3 13:41:02.037: INFO: Trying to get logs from node controller-0 pod var-expansion-a125f1a3-ace0-4a3d-94c3-4a62516f5567 container dapi-container: <nil>
STEP: delete the pod
Jan  3 13:41:02.052: INFO: Waiting for pod var-expansion-a125f1a3-ace0-4a3d-94c3-4a62516f5567 to disappear
Jan  3 13:41:02.053: INFO: Pod var-expansion-a125f1a3-ace0-4a3d-94c3-4a62516f5567 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:41:02.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2328" for this suite.
Jan  3 13:41:08.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:41:08.086: INFO: namespace var-expansion-2328 deletion completed in 6.032043436s

• [SLOW TEST:14.078 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:41:08.087: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:42:08.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6454" for this suite.
Jan  3 13:42:20.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:42:20.143: INFO: namespace container-probe-6454 deletion completed in 12.037714477s

• [SLOW TEST:72.057 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:42:20.145: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-5178
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5178 to expose endpoints map[]
Jan  3 13:42:20.171: INFO: Get endpoints failed (2.39632ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jan  3 13:42:21.172: INFO: successfully validated that service multi-endpoint-test in namespace services-5178 exposes endpoints map[] (1.003953785s elapsed)
STEP: Creating pod pod1 in namespace services-5178
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5178 to expose endpoints map[pod1:[100]]
Jan  3 13:42:25.206: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.030442297s elapsed, will retry)
Jan  3 13:42:29.218: INFO: successfully validated that service multi-endpoint-test in namespace services-5178 exposes endpoints map[pod1:[100]] (8.042815393s elapsed)
STEP: Creating pod pod2 in namespace services-5178
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5178 to expose endpoints map[pod1:[100] pod2:[101]]
Jan  3 13:42:33.238: INFO: Unexpected endpoints: found map[ec9f46ad-1b2b-40df-a2c9-5aca05c18891:[100]], expected map[pod1:[100] pod2:[101]] (4.017269744s elapsed, will retry)
Jan  3 13:42:37.261: INFO: successfully validated that service multi-endpoint-test in namespace services-5178 exposes endpoints map[pod1:[100] pod2:[101]] (8.040437908s elapsed)
STEP: Deleting pod pod1 in namespace services-5178
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5178 to expose endpoints map[pod2:[101]]
Jan  3 13:42:38.276: INFO: successfully validated that service multi-endpoint-test in namespace services-5178 exposes endpoints map[pod2:[101]] (1.012347321s elapsed)
STEP: Deleting pod pod2 in namespace services-5178
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5178 to expose endpoints map[]
Jan  3 13:42:39.287: INFO: successfully validated that service multi-endpoint-test in namespace services-5178 exposes endpoints map[] (1.004269375s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:42:39.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5178" for this suite.
Jan  3 13:42:51.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:42:51.366: INFO: namespace services-5178 deletion completed in 12.070541876s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:31.221 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:42:51.366: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:42:51.404: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-32ee1a89-a473-45a4-8a66-690e3749c88f" in namespace "security-context-test-9461" to be "success or failure"
Jan  3 13:42:51.405: INFO: Pod "busybox-readonly-false-32ee1a89-a473-45a4-8a66-690e3749c88f": Phase="Pending", Reason="", readiness=false. Elapsed: 931.944µs
Jan  3 13:42:53.406: INFO: Pod "busybox-readonly-false-32ee1a89-a473-45a4-8a66-690e3749c88f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002431611s
Jan  3 13:42:55.411: INFO: Pod "busybox-readonly-false-32ee1a89-a473-45a4-8a66-690e3749c88f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007297992s
Jan  3 13:42:57.413: INFO: Pod "busybox-readonly-false-32ee1a89-a473-45a4-8a66-690e3749c88f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008780982s
Jan  3 13:42:59.414: INFO: Pod "busybox-readonly-false-32ee1a89-a473-45a4-8a66-690e3749c88f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.010559779s
Jan  3 13:42:59.414: INFO: Pod "busybox-readonly-false-32ee1a89-a473-45a4-8a66-690e3749c88f" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:42:59.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9461" for this suite.
Jan  3 13:43:05.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:43:05.480: INFO: namespace security-context-test-9461 deletion completed in 6.063358437s

• [SLOW TEST:14.114 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:43:05.480: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:43:10.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-275" for this suite.
Jan  3 13:43:16.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:43:17.014: INFO: namespace watch-275 deletion completed in 6.12701306s

• [SLOW TEST:11.534 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:43:17.014: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0103 13:43:57.039835      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan  3 13:43:57.039: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:43:57.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9095" for this suite.
Jan  3 13:44:03.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:44:03.287: INFO: namespace gc-9095 deletion completed in 6.246668648s

• [SLOW TEST:46.275 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:44:03.290: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan  3 13:44:03.675: INFO: Number of nodes with available pods: 0
Jan  3 13:44:03.675: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:04.679: INFO: Number of nodes with available pods: 0
Jan  3 13:44:04.679: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:05.679: INFO: Number of nodes with available pods: 0
Jan  3 13:44:05.679: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:06.679: INFO: Number of nodes with available pods: 0
Jan  3 13:44:06.679: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:07.679: INFO: Number of nodes with available pods: 0
Jan  3 13:44:07.679: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:08.679: INFO: Number of nodes with available pods: 0
Jan  3 13:44:08.679: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:09.679: INFO: Number of nodes with available pods: 0
Jan  3 13:44:09.679: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:10.680: INFO: Number of nodes with available pods: 0
Jan  3 13:44:10.680: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:11.691: INFO: Number of nodes with available pods: 2
Jan  3 13:44:11.691: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan  3 13:44:11.709: INFO: Number of nodes with available pods: 1
Jan  3 13:44:11.709: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:12.715: INFO: Number of nodes with available pods: 1
Jan  3 13:44:12.715: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:13.712: INFO: Number of nodes with available pods: 1
Jan  3 13:44:13.712: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:14.713: INFO: Number of nodes with available pods: 1
Jan  3 13:44:14.714: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:15.713: INFO: Number of nodes with available pods: 1
Jan  3 13:44:15.713: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:16.712: INFO: Number of nodes with available pods: 1
Jan  3 13:44:16.712: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:17.712: INFO: Number of nodes with available pods: 1
Jan  3 13:44:17.712: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:18.713: INFO: Number of nodes with available pods: 1
Jan  3 13:44:18.713: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:19.713: INFO: Number of nodes with available pods: 1
Jan  3 13:44:19.713: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:20.712: INFO: Number of nodes with available pods: 1
Jan  3 13:44:20.712: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:21.712: INFO: Number of nodes with available pods: 1
Jan  3 13:44:21.712: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:22.713: INFO: Number of nodes with available pods: 1
Jan  3 13:44:22.713: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:23.712: INFO: Number of nodes with available pods: 1
Jan  3 13:44:23.712: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:24.715: INFO: Number of nodes with available pods: 1
Jan  3 13:44:24.715: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:25.713: INFO: Number of nodes with available pods: 1
Jan  3 13:44:25.713: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:26.712: INFO: Number of nodes with available pods: 1
Jan  3 13:44:26.712: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:44:27.712: INFO: Number of nodes with available pods: 2
Jan  3 13:44:27.712: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3362, will wait for the garbage collector to delete the pods
Jan  3 13:44:27.767: INFO: Deleting DaemonSet.extensions daemon-set took: 2.491861ms
Jan  3 13:44:28.167: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.148634ms
Jan  3 13:44:39.369: INFO: Number of nodes with available pods: 0
Jan  3 13:44:39.369: INFO: Number of running nodes: 0, number of available pods: 0
Jan  3 13:44:39.372: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3362/daemonsets","resourceVersion":"48462"},"items":null}

Jan  3 13:44:39.374: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3362/pods","resourceVersion":"48462"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:44:39.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3362" for this suite.
Jan  3 13:44:45.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:44:45.429: INFO: namespace daemonsets-3362 deletion completed in 6.043334503s

• [SLOW TEST:42.139 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:44:45.431: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan  3 13:44:53.469: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:44:53.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3167" for this suite.
Jan  3 13:44:59.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:44:59.522: INFO: namespace container-runtime-3167 deletion completed in 6.044744524s

• [SLOW TEST:14.092 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:44:59.522: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan  3 13:45:15.570: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  3 13:45:15.576: INFO: Pod pod-with-prestop-http-hook still exists
Jan  3 13:45:17.581: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  3 13:45:17.583: INFO: Pod pod-with-prestop-http-hook still exists
Jan  3 13:45:19.579: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  3 13:45:19.586: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:45:19.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7886" for this suite.
Jan  3 13:45:31.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:45:31.654: INFO: namespace container-lifecycle-hook-7886 deletion completed in 12.057637072s

• [SLOW TEST:32.132 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:45:31.663: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 13:45:31.698: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb8ad933-daa7-4118-afa8-e510e90daff9" in namespace "downward-api-9842" to be "success or failure"
Jan  3 13:45:31.701: INFO: Pod "downwardapi-volume-eb8ad933-daa7-4118-afa8-e510e90daff9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.364368ms
Jan  3 13:45:33.703: INFO: Pod "downwardapi-volume-eb8ad933-daa7-4118-afa8-e510e90daff9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004810069s
Jan  3 13:45:35.705: INFO: Pod "downwardapi-volume-eb8ad933-daa7-4118-afa8-e510e90daff9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006639009s
Jan  3 13:45:37.707: INFO: Pod "downwardapi-volume-eb8ad933-daa7-4118-afa8-e510e90daff9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009165649s
Jan  3 13:45:39.709: INFO: Pod "downwardapi-volume-eb8ad933-daa7-4118-afa8-e510e90daff9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011038846s
STEP: Saw pod success
Jan  3 13:45:39.709: INFO: Pod "downwardapi-volume-eb8ad933-daa7-4118-afa8-e510e90daff9" satisfied condition "success or failure"
Jan  3 13:45:39.713: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-eb8ad933-daa7-4118-afa8-e510e90daff9 container client-container: <nil>
STEP: delete the pod
Jan  3 13:45:39.742: INFO: Waiting for pod downwardapi-volume-eb8ad933-daa7-4118-afa8-e510e90daff9 to disappear
Jan  3 13:45:39.744: INFO: Pod downwardapi-volume-eb8ad933-daa7-4118-afa8-e510e90daff9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:45:39.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9842" for this suite.
Jan  3 13:45:45.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:45:45.811: INFO: namespace downward-api-9842 deletion completed in 6.062578278s

• [SLOW TEST:14.148 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:45:45.812: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-fa4f8c5c-1861-45dc-8c00-bdbfc849b8c9
Jan  3 13:45:45.890: INFO: Pod name my-hostname-basic-fa4f8c5c-1861-45dc-8c00-bdbfc849b8c9: Found 1 pods out of 1
Jan  3 13:45:45.890: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-fa4f8c5c-1861-45dc-8c00-bdbfc849b8c9" are running
Jan  3 13:45:53.895: INFO: Pod "my-hostname-basic-fa4f8c5c-1861-45dc-8c00-bdbfc849b8c9-zkx2p" is running (conditions: [])
Jan  3 13:45:53.895: INFO: Trying to dial the pod
Jan  3 13:45:58.900: INFO: Controller my-hostname-basic-fa4f8c5c-1861-45dc-8c00-bdbfc849b8c9: Got expected result from replica 1 [my-hostname-basic-fa4f8c5c-1861-45dc-8c00-bdbfc849b8c9-zkx2p]: "my-hostname-basic-fa4f8c5c-1861-45dc-8c00-bdbfc849b8c9-zkx2p", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:45:58.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6272" for this suite.
Jan  3 13:46:04.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:46:04.950: INFO: namespace replication-controller-6272 deletion completed in 6.048300901s

• [SLOW TEST:19.138 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:46:04.950: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Jan  3 13:46:04.967: INFO: namespace kubectl-9602
Jan  3 13:46:04.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-9602'
Jan  3 13:46:05.173: INFO: stderr: ""
Jan  3 13:46:05.173: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan  3 13:46:06.197: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:46:06.197: INFO: Found 0 / 1
Jan  3 13:46:07.175: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:46:07.175: INFO: Found 0 / 1
Jan  3 13:46:08.174: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:46:08.174: INFO: Found 0 / 1
Jan  3 13:46:09.174: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:46:09.174: INFO: Found 0 / 1
Jan  3 13:46:10.175: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:46:10.175: INFO: Found 0 / 1
Jan  3 13:46:11.179: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:46:11.179: INFO: Found 0 / 1
Jan  3 13:46:12.175: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:46:12.175: INFO: Found 0 / 1
Jan  3 13:46:13.200: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:46:13.200: INFO: Found 0 / 1
Jan  3 13:46:14.221: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:46:14.221: INFO: Found 1 / 1
Jan  3 13:46:14.221: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  3 13:46:14.222: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 13:46:14.222: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  3 13:46:14.222: INFO: wait on redis-master startup in kubectl-9602 
Jan  3 13:46:14.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 logs redis-master-qkxbj redis-master --namespace=kubectl-9602'
Jan  3 13:46:14.381: INFO: stderr: ""
Jan  3 13:46:14.381: INFO: stdout: "1:C 03 Jan 2020 13:46:12.286 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 03 Jan 2020 13:46:12.286 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 03 Jan 2020 13:46:12.286 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 03 Jan 2020 13:46:12.288 * Running mode=standalone, port=6379.\n1:M 03 Jan 2020 13:46:12.288 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Jan 2020 13:46:12.288 # Server initialized\n1:M 03 Jan 2020 13:46:12.289 * Ready to accept connections\n"
STEP: exposing RC
Jan  3 13:46:14.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9602'
Jan  3 13:46:14.580: INFO: stderr: ""
Jan  3 13:46:14.580: INFO: stdout: "service/rm2 exposed\n"
Jan  3 13:46:14.584: INFO: Service rm2 in namespace kubectl-9602 found.
STEP: exposing service
Jan  3 13:46:16.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9602'
Jan  3 13:46:16.781: INFO: stderr: ""
Jan  3 13:46:16.781: INFO: stdout: "service/rm3 exposed\n"
Jan  3 13:46:16.782: INFO: Service rm3 in namespace kubectl-9602 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:46:18.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9602" for this suite.
Jan  3 13:46:46.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:46:46.824: INFO: namespace kubectl-9602 deletion completed in 28.038073243s

• [SLOW TEST:41.874 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:46:46.825: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:46:46.837: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan  3 13:46:48.872: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:46:49.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5200" for this suite.
Jan  3 13:46:55.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:46:55.917: INFO: namespace replication-controller-5200 deletion completed in 6.040365235s

• [SLOW TEST:9.092 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:46:55.917: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:46:55.939: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-da86a09b-e563-4b62-8397-d5080c141618" in namespace "security-context-test-4618" to be "success or failure"
Jan  3 13:46:55.941: INFO: Pod "busybox-privileged-false-da86a09b-e563-4b62-8397-d5080c141618": Phase="Pending", Reason="", readiness=false. Elapsed: 1.992254ms
Jan  3 13:46:57.943: INFO: Pod "busybox-privileged-false-da86a09b-e563-4b62-8397-d5080c141618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003692981s
Jan  3 13:46:59.944: INFO: Pod "busybox-privileged-false-da86a09b-e563-4b62-8397-d5080c141618": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005310781s
Jan  3 13:47:01.946: INFO: Pod "busybox-privileged-false-da86a09b-e563-4b62-8397-d5080c141618": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007510608s
Jan  3 13:47:03.963: INFO: Pod "busybox-privileged-false-da86a09b-e563-4b62-8397-d5080c141618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024245458s
Jan  3 13:47:03.964: INFO: Pod "busybox-privileged-false-da86a09b-e563-4b62-8397-d5080c141618" satisfied condition "success or failure"
Jan  3 13:47:03.970: INFO: Got logs for pod "busybox-privileged-false-da86a09b-e563-4b62-8397-d5080c141618": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:47:03.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4618" for this suite.
Jan  3 13:47:09.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:47:10.009: INFO: namespace security-context-test-4618 deletion completed in 6.036787382s

• [SLOW TEST:14.092 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:47:10.010: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jan  3 13:47:10.022: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  3 13:47:10.026: INFO: Waiting for terminating namespaces to be deleted...
Jan  3 13:47:10.027: INFO: 
Logging pods the kubelet thinks is on node controller-0 before test
Jan  3 13:47:10.079: INFO: keystone-credential-setup-qkqd5 from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.079: INFO: 	Container keystone-credential-setup ready: false, restart count 0
Jan  3 13:47:10.080: INFO: keystone-db-init-bz6pm from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.080: INFO: 	Container keystone-db-init-0 ready: false, restart count 0
Jan  3 13:47:10.080: INFO: nova-bootstrap-6tpt5 from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.080: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 13:47:10.080: INFO: heat-domain-ks-user-lbck5 from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.080: INFO: 	Container heat-ks-domain-user ready: false, restart count 0
Jan  3 13:47:10.080: INFO: cinder-volume-usage-audit-1578058800-zbk46 from openstack started at 2020-01-03 13:40:00 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.080: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 13:47:10.080: INFO: kube-scheduler-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.081: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan  3 13:47:10.081: INFO: osh-openstack-rabbitmq-cluster-wait-xl9t7 from openstack started at 2020-01-03 11:22:33 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.081: INFO: 	Container osh-openstack-rabbitmq-rabbitmq-cluster-wait ready: false, restart count 0
Jan  3 13:47:10.081: INFO: cinder-volume-549d7c447c-h5gcw from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.081: INFO: 	Container cinder-volume ready: true, restart count 0
Jan  3 13:47:10.081: INFO: cinder-ks-service-rq6n6 from openstack started at 2020-01-03 11:29:43 +0000 UTC (3 container statuses recorded)
Jan  3 13:47:10.081: INFO: 	Container volume-ks-service-registration ready: false, restart count 0
Jan  3 13:47:10.081: INFO: 	Container volumev2-ks-service-registration ready: false, restart count 0
Jan  3 13:47:10.082: INFO: 	Container volumev3-ks-service-registration ready: false, restart count 0
Jan  3 13:47:10.082: INFO: placement-ks-endpoints-fh88g from openstack started at 2020-01-03 11:34:26 +0000 UTC (3 container statuses recorded)
Jan  3 13:47:10.082: INFO: 	Container placement-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:47:10.082: INFO: 	Container placement-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:47:10.082: INFO: 	Container placement-ks-endpoints-public ready: false, restart count 0
Jan  3 13:47:10.082: INFO: nova-api-metadata-7bdf79d754-9ds2d from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.082: INFO: 	Container nova-api ready: true, restart count 1
Jan  3 13:47:10.082: INFO: neutron-ovs-agent-controller-0-937646f6-bp6gb from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.082: INFO: 	Container neutron-ovs-agent ready: true, restart count 0
Jan  3 13:47:10.082: INFO: neutron-rabbit-init-4f6lx from openstack started at 2020-01-03 11:34:32 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 13:47:10.083: INFO: osh-openstack-memcached-memcached-545668bdbd-p88zm from openstack started at 2020-01-03 11:22:14 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container memcached ready: true, restart count 0
Jan  3 13:47:10.083: INFO: placement-ks-service-wpvrq from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container placement-ks-service-registration ready: false, restart count 0
Jan  3 13:47:10.083: INFO: neutron-dhcp-agent-controller-0-937646f6-qgt4d from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container neutron-dhcp-agent ready: true, restart count 0
Jan  3 13:47:10.083: INFO: neutron-metadata-agent-controller-0-937646f6-nbcf6 from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container neutron-metadata-agent ready: true, restart count 0
Jan  3 13:47:10.083: INFO: ingress-error-pages-cf6c65b-lqksh from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 13:47:10.083: INFO: cinder-api-7ff9984869-56lvq from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container cinder-api ready: true, restart count 0
Jan  3 13:47:10.083: INFO: nova-api-proxy-577495bf7f-nqbkw from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container nova-api-proxy ready: true, restart count 0
Jan  3 13:47:10.083: INFO: nova-scheduler-76b67b674d-bxmlc from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container nova-scheduler ready: true, restart count 0
Jan  3 13:47:10.083: INFO: neutron-db-init-p9bt8 from openstack started at 2020-01-03 11:34:31 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container neutron-db-init-0 ready: false, restart count 0
Jan  3 13:47:10.083: INFO: keystone-domain-manage-lk44k from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container keystone-domain-manage ready: false, restart count 0
Jan  3 13:47:10.083: INFO: keystone-db-sync-bc8tl from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container keystone-db-sync ready: false, restart count 0
Jan  3 13:47:10.083: INFO: keystone-rabbit-init-b4rvs from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 13:47:10.083: INFO: glance-api-747954666d-z9hzh from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container glance-api ready: true, restart count 0
Jan  3 13:47:10.083: INFO: heat-engine-54745654c7-kt6dn from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container heat-engine ready: true, restart count 0
Jan  3 13:47:10.083: INFO: fm-ks-user-xsvv5 from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:47:10.083: INFO: horizon-db-init-krgpq from openstack started at 2020-01-03 11:42:50 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.083: INFO: 	Container horizon-db-init-0 ready: false, restart count 0
Jan  3 13:47:10.083: INFO: kube-sriov-cni-ds-amd64-h4sh4 from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container kube-sriov-cni ready: true, restart count 1
Jan  3 13:47:10.084: INFO: mariadb-server-1 from openstack started at 2020-01-03 11:20:51 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container mariadb ready: true, restart count 0
Jan  3 13:47:10.084: INFO: keystone-api-7b4d98b8c5-hhwqm from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container keystone-api ready: true, restart count 0
Jan  3 13:47:10.084: INFO: libvirt-libvirt-default-nt27c from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container libvirt ready: true, restart count 0
Jan  3 13:47:10.084: INFO: nova-cell-setup-p9vn4 from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container nova-cell-setup ready: false, restart count 0
Jan  3 13:47:10.084: INFO: nova-db-sync-rhcfq from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container nova-db-sync ready: false, restart count 0
Jan  3 13:47:10.084: INFO: nova-service-cleaner-1578056400-xk8zr from openstack started at 2020-01-03 13:00:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container nova-service-cleaner ready: false, restart count 0
Jan  3 13:47:10.084: INFO: ceph-pools-audit-1578059100-q9q44 from kube-system started at 2020-01-03 13:45:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container ceph-pools-audit-ceph-store ready: true, restart count 0
Jan  3 13:47:10.084: INFO: tiller-deploy-d6b59fcb-ksjbc from kube-system started at 2020-01-03 09:59:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container tiller ready: true, restart count 1
Jan  3 13:47:10.084: INFO: kube-controller-manager-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan  3 13:47:10.084: INFO: ingress-error-pages-5bcb8b5f6c-4dxmf from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 13:47:10.084: INFO: neutron-ks-user-dlm6h from openstack started at 2020-01-03 11:34:32 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:47:10.084: INFO: glance-db-sync-6sk2d from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container glance-db-sync ready: false, restart count 0
Jan  3 13:47:10.084: INFO: cinder-rabbit-init-2kjgk from openstack started at 2020-01-03 11:29:43 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.084: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 13:47:10.085: INFO: nova-conductor-bb6d86d69-p9v25 from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container nova-conductor ready: true, restart count 0
Jan  3 13:47:10.085: INFO: neutron-l3-agent-controller-0-937646f6-dzqjf from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container neutron-l3-agent ready: true, restart count 0
Jan  3 13:47:10.085: INFO: ceph-pools-audit-1578058800-sgg2l from kube-system started at 2020-01-03 13:40:00 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 13:47:10.085: INFO: kube-proxy-5l8fd from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container kube-proxy ready: true, restart count 1
Jan  3 13:47:10.085: INFO: calico-node-rb47m from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container calico-node ready: true, restart count 3
Jan  3 13:47:10.085: INFO: kube-multus-ds-amd64-kfg66 from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container kube-multus ready: true, restart count 1
Jan  3 13:47:10.085: INFO: nova-rabbit-init-88k4z from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 13:47:10.085: INFO: storage-init-rbd-provisioner-mt758 from kube-system started at 2020-01-03 10:54:49 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container storage-init-general ready: false, restart count 0
Jan  3 13:47:10.085: INFO: osh-openstack-rabbitmq-rabbitmq-0 from openstack started at 2020-01-03 11:22:34 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container rabbitmq ready: true, restart count 0
Jan  3 13:47:10.085: INFO: horizon-db-sync-sc5ng from openstack started at 2020-01-03 11:42:50 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container horizon-db-sync ready: false, restart count 0
Jan  3 13:47:10.085: INFO: keystone-fernet-rotate-1578052800-fbtn9 from openstack started at 2020-01-03 12:00:07 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container keystone-fernet-rotate ready: false, restart count 0
Jan  3 13:47:10.085: INFO: calico-kube-controllers-855577b7b5-6c5kq from kube-system started at 2020-01-03 09:59:31 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Jan  3 13:47:10.085: INFO: rbd-provisioner-7484d49cf6-bztgh from kube-system started at 2020-01-03 10:30:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container rbd-provisioner ready: true, restart count 0
Jan  3 13:47:10.085: INFO: nova-storage-init-gshl7 from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container nova-storage-init-ephemeral ready: false, restart count 0
Jan  3 13:47:10.085: INFO: heat-trusts-dgjrv from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container heat-trusts ready: false, restart count 0
Jan  3 13:47:10.085: INFO: cinder-volume-usage-audit-1578058500-ptsqr from openstack started at 2020-01-03 13:35:09 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.085: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 13:47:10.085: INFO: sonobuoy-systemd-logs-daemon-set-5e158e08c5964d78-s6m8n from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 13:47:10.086: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 13:47:10.086: INFO: heat-engine-cleaner-1578058800-pqvdk from openstack started at 2020-01-03 13:40:00 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 13:47:10.086: INFO: cinder-scheduler-664bb87785-99gm8 from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container cinder-scheduler ready: true, restart count 0
Jan  3 13:47:10.086: INFO: placement-api-5b65bc5576-fw2vt from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container placement-api ready: true, restart count 0
Jan  3 13:47:10.086: INFO: cinder-backup-storage-init-b45t5 from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container cinder-backup-storage-init ready: false, restart count 0
Jan  3 13:47:10.086: INFO: cinder-storage-init-vj9nb from openstack started at 2020-01-03 11:29:43 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container cinder-storage-init-ceph-store ready: false, restart count 0
Jan  3 13:47:10.086: INFO: nova-api-osapi-68846d5959-bld9x from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container nova-osapi ready: true, restart count 0
Jan  3 13:47:10.086: INFO: heat-db-sync-qj5gp from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container heat-db-sync ready: false, restart count 0
Jan  3 13:47:10.086: INFO: ceph-pools-audit-1578058200-zwwdc from kube-system started at 2020-01-03 13:30:08 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 13:47:10.086: INFO: kube-apiserver-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container kube-apiserver ready: true, restart count 2
Jan  3 13:47:10.086: INFO: ingress-bc886876f-gcg7c from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container ingress ready: true, restart count 0
Jan  3 13:47:10.086: INFO: glance-rabbit-init-92z6q from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 13:47:10.086: INFO: cinder-bootstrap-95x9f from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 13:47:10.086: INFO: placement-db-sync-2h4wd from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container placement-db-sync ready: false, restart count 0
Jan  3 13:47:10.086: INFO: neutron-sriov-agent-controller-0-937646f6-5sggz from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container neutron-sriov-agent ready: true, restart count 0
Jan  3 13:47:10.086: INFO: heat-engine-cleaner-1578058500-8qhgc from openstack started at 2020-01-03 13:35:09 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 13:47:10.086: INFO: ingress-r62j4 from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container ingress ready: true, restart count 0
Jan  3 13:47:10.086: INFO: ceph-pools-audit-1578058500-4kq4m from kube-system started at 2020-01-03 13:35:09 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 13:47:10.086: INFO: glance-ks-endpoints-pz9wc from openstack started at 2020-01-03 11:25:03 +0000 UTC (3 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container image-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:47:10.086: INFO: 	Container image-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:47:10.086: INFO: 	Container image-ks-endpoints-public ready: false, restart count 0
Jan  3 13:47:10.086: INFO: glance-storage-init-km9lz from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container glance-storage-init ready: false, restart count 0
Jan  3 13:47:10.086: INFO: nova-compute-controller-0-937646f6-nz464 from openstack started at 2020-01-03 11:34:28 +0000 UTC (2 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container nova-compute ready: true, restart count 0
Jan  3 13:47:10.086: INFO: 	Container nova-compute-ssh ready: true, restart count 0
Jan  3 13:47:10.086: INFO: neutron-db-sync-5f8lm from openstack started at 2020-01-03 11:34:31 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container neutron-db-sync ready: false, restart count 0
Jan  3 13:47:10.086: INFO: fm-db-sync-plkwf from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container fm-db-sync ready: false, restart count 0
Jan  3 13:47:10.086: INFO: coredns-6bc668cd76-mcj6r from kube-system started at 2020-01-03 09:59:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container coredns ready: true, restart count 1
Jan  3 13:47:10.086: INFO: keystone-fernet-setup-mdqx2 from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container keystone-fernet-setup ready: false, restart count 0
Jan  3 13:47:10.086: INFO: nova-service-cleaner-1578052800-c8kbg from openstack started at 2020-01-03 12:00:07 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container nova-service-cleaner ready: false, restart count 0
Jan  3 13:47:10.086: INFO: neutron-ks-service-wmlfq from openstack started at 2020-01-03 11:34:31 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container network-ks-service-registration ready: false, restart count 0
Jan  3 13:47:10.086: INFO: heat-cfn-c8f5b9b4b-5g2dk from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container heat-cfn ready: true, restart count 0
Jan  3 13:47:10.086: INFO: heat-api-58bf859968-hpnzj from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container heat-api ready: true, restart count 0
Jan  3 13:47:10.086: INFO: heat-trustee-ks-user-65bc4 from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:47:10.086: INFO: cinder-db-sync-fc48c from openstack started at 2020-01-03 11:29:43 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container cinder-db-sync ready: false, restart count 0
Jan  3 13:47:10.086: INFO: neutron-server-54b46f798-cstmn from openstack started at 2020-01-03 11:34:31 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container neutron-server ready: true, restart count 0
Jan  3 13:47:10.086: INFO: fm-rest-api-b4bc757f4-wrm9c from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container fm-rest-api ready: true, restart count 0
Jan  3 13:47:10.086: INFO: horizon-6865446ff5-rztk2 from openstack started at 2020-01-03 11:42:50 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container horizon ready: true, restart count 0
Jan  3 13:47:10.086: INFO: nova-novncproxy-6c54c7d98-m2tcl from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container nova-novncproxy ready: true, restart count 0
Jan  3 13:47:10.086: INFO: cinder-backup-fd5f96bf4-s964v from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container cinder-backup ready: true, restart count 0
Jan  3 13:47:10.086: INFO: mariadb-ingress-5bb8b69fc8-2pdr5 from openstack started at 2020-01-03 11:20:48 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.086: INFO: 	Container ingress ready: true, restart count 0
Jan  3 13:47:10.086: INFO: 
Logging pods the kubelet thinks is on node controller-1 before test
Jan  3 13:47:10.106: INFO: heat-db-init-dmbl7 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container heat-db-init-0 ready: false, restart count 0
Jan  3 13:47:10.106: INFO: sonobuoy from sonobuoy started at 2020-01-03 12:54:34 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  3 13:47:10.106: INFO: heat-bootstrap-9xrdd from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 13:47:10.106: INFO: cinder-volume-usage-audit-1578059100-g5wnl from openstack started at 2020-01-03 13:45:02 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 13:47:10.106: INFO: calico-node-nkz88 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container calico-node ready: true, restart count 1
Jan  3 13:47:10.106: INFO: osh-openstack-rabbitmq-rabbitmq-1 from openstack started at 2020-01-03 11:22:47 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container rabbitmq ready: true, restart count 0
Jan  3 13:47:10.106: INFO: neutron-sriov-agent-controller-1-cab72f56-t65r5 from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container neutron-sriov-agent ready: true, restart count 0
Jan  3 13:47:10.106: INFO: fm-ks-service-nwst8 from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container faultmanagement-ks-service-registration ready: false, restart count 0
Jan  3 13:47:10.106: INFO: kube-sriov-cni-ds-amd64-jnv4d from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Jan  3 13:47:10.106: INFO: keystone-bootstrap-jmtqn from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 13:47:10.106: INFO: glance-api-747954666d-j2ldf from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container glance-api ready: true, restart count 0
Jan  3 13:47:10.106: INFO: nova-ks-endpoints-qv68g from openstack started at 2020-01-03 11:34:26 +0000 UTC (3 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container compute-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container compute-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container compute-ks-endpoints-public ready: false, restart count 0
Jan  3 13:47:10.106: INFO: placement-api-5b65bc5576-f9d4b from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container placement-api ready: true, restart count 0
Jan  3 13:47:10.106: INFO: nova-compute-controller-1-cab72f56-c922s from openstack started at 2020-01-03 11:34:28 +0000 UTC (2 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container nova-compute ready: true, restart count 0
Jan  3 13:47:10.106: INFO: 	Container nova-compute-ssh ready: true, restart count 0
Jan  3 13:47:10.106: INFO: heat-engine-54745654c7-4ln84 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container heat-engine ready: true, restart count 0
Jan  3 13:47:10.106: INFO: heat-ks-user-hbk96 from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:47:10.106: INFO: kube-proxy-2fz94 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 13:47:10.106: INFO: ingress-bc886876f-b6vc9 from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container ingress ready: true, restart count 0
Jan  3 13:47:10.106: INFO: mariadb-ingress-5bb8b69fc8-r2qvn from openstack started at 2020-01-03 11:20:48 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container ingress ready: true, restart count 0
Jan  3 13:47:10.106: INFO: fm-rest-api-b4bc757f4-8tmgc from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container fm-rest-api ready: true, restart count 0
Jan  3 13:47:10.106: INFO: kube-multus-ds-amd64-ggv58 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container kube-multus ready: true, restart count 0
Jan  3 13:47:10.106: INFO: neutron-server-54b46f798-4f8d9 from openstack started at 2020-01-03 11:34:31 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container neutron-server ready: true, restart count 0
Jan  3 13:47:10.106: INFO: sonobuoy-systemd-logs-daemon-set-5e158e08c5964d78-2qpxv from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 13:47:10.106: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 13:47:10.106: INFO: nova-api-metadata-7bdf79d754-kr2gk from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container nova-api ready: true, restart count 1
Jan  3 13:47:10.106: INFO: nova-db-init-86cfh from openstack started at 2020-01-03 11:34:29 +0000 UTC (3 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container nova-db-init-0 ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container nova-db-init-1 ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container nova-db-init-2 ready: false, restart count 0
Jan  3 13:47:10.106: INFO: fm-db-init-2t975 from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container fm-db-init-0 ready: false, restart count 0
Jan  3 13:47:10.106: INFO: sonobuoy-e2e-job-08a7a8b2e6d84bc6 from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container e2e ready: true, restart count 0
Jan  3 13:47:10.106: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 13:47:10.106: INFO: cinder-create-internal-tenant-twkzc from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container create-internal-tenant ready: false, restart count 0
Jan  3 13:47:10.106: INFO: cinder-backup-fd5f96bf4-vzc8x from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container cinder-backup ready: true, restart count 0
Jan  3 13:47:10.106: INFO: cinder-ks-endpoints-wvm8w from openstack started at 2020-01-03 11:29:43 +0000 UTC (9 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container volume-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container volume-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container volume-ks-endpoints-public ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container volumev2-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container volumev2-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container volumev2-ks-endpoints-public ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container volumev3-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container volumev3-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container volumev3-ks-endpoints-public ready: false, restart count 0
Jan  3 13:47:10.106: INFO: libvirt-libvirt-default-np4lk from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container libvirt ready: true, restart count 0
Jan  3 13:47:10.106: INFO: nova-api-proxy-577495bf7f-b5t9q from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container nova-api-proxy ready: true, restart count 0
Jan  3 13:47:10.106: INFO: nova-conductor-bb6d86d69-5tkcn from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container nova-conductor ready: true, restart count 0
Jan  3 13:47:10.106: INFO: neutron-ovs-agent-controller-1-cab72f56-mntzv from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container neutron-ovs-agent ready: true, restart count 0
Jan  3 13:47:10.106: INFO: heat-cfn-c8f5b9b4b-wd8h4 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container heat-cfn ready: true, restart count 0
Jan  3 13:47:10.106: INFO: ingress-5fgmr from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container ingress ready: true, restart count 0
Jan  3 13:47:10.106: INFO: ingress-error-pages-5bcb8b5f6c-8rf2d from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 13:47:10.106: INFO: mariadb-ingress-error-pages-847467b5d5-d57l2 from openstack started at 2020-01-03 11:20:48 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 13:47:10.106: INFO: heat-ks-endpoints-jjzrq from openstack started at 2020-01-03 11:40:01 +0000 UTC (6 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container cloudformation-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container cloudformation-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container cloudformation-ks-endpoints-public ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container orchestration-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container orchestration-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container orchestration-ks-endpoints-public ready: false, restart count 0
Jan  3 13:47:10.106: INFO: heat-ks-service-b7t2s from openstack started at 2020-01-03 11:40:01 +0000 UTC (2 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container cloudformation-ks-service-registration ready: false, restart count 0
Jan  3 13:47:10.106: INFO: 	Container orchestration-ks-service-registration ready: false, restart count 0
Jan  3 13:47:10.106: INFO: nova-ks-user-gtl4m from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:47:10.106: INFO: neutron-metadata-agent-controller-1-cab72f56-xwlz5 from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container neutron-metadata-agent ready: true, restart count 0
Jan  3 13:47:10.106: INFO: ingress-error-pages-cf6c65b-zwhqk from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 13:47:10.106: INFO: keystone-api-7b4d98b8c5-8wjmf from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.106: INFO: 	Container keystone-api ready: true, restart count 0
Jan  3 13:47:10.106: INFO: neutron-dhcp-agent-controller-1-cab72f56-t5s8d from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container neutron-dhcp-agent ready: true, restart count 0
Jan  3 13:47:10.107: INFO: nova-scheduler-76b67b674d-4g9bx from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container nova-scheduler ready: true, restart count 0
Jan  3 13:47:10.107: INFO: kube-apiserver-controller-1 from kube-system started at 2020-01-03 10:52:37 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  3 13:47:10.107: INFO: rbd-provisioner-7484d49cf6-2tb85 from kube-system started at 2020-01-03 10:55:54 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container rbd-provisioner ready: true, restart count 0
Jan  3 13:47:10.107: INFO: nova-api-osapi-68846d5959-ggf8r from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container nova-osapi ready: true, restart count 0
Jan  3 13:47:10.107: INFO: nova-novncproxy-6c54c7d98-d2pzp from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container nova-novncproxy ready: true, restart count 0
Jan  3 13:47:10.107: INFO: coredns-6bc668cd76-9nlk5 from kube-system started at 2020-01-03 10:48:30 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container coredns ready: true, restart count 0
Jan  3 13:47:10.107: INFO: cinder-api-7ff9984869-9g8tp from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container cinder-api ready: true, restart count 0
Jan  3 13:47:10.107: INFO: cinder-volume-549d7c447c-2sjzd from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container cinder-volume ready: true, restart count 0
Jan  3 13:47:10.107: INFO: mariadb-server-0 from openstack started at 2020-01-03 11:20:52 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container mariadb ready: true, restart count 0
Jan  3 13:47:10.107: INFO: glance-ks-service-kbjlh from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container image-ks-service-registration ready: false, restart count 0
Jan  3 13:47:10.107: INFO: cinder-db-init-g8w2v from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container cinder-db-init-0 ready: false, restart count 0
Jan  3 13:47:10.107: INFO: neutron-ks-endpoints-xrvqm from openstack started at 2020-01-03 11:34:31 +0000 UTC (3 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container network-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:47:10.107: INFO: 	Container network-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:47:10.107: INFO: 	Container network-ks-endpoints-public ready: false, restart count 0
Jan  3 13:47:10.107: INFO: heat-rabbit-init-mtk7j from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 13:47:10.107: INFO: heat-engine-cleaner-1578059100-zgw5x from openstack started at 2020-01-03 13:45:02 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 13:47:10.107: INFO: kube-controller-manager-controller-1 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  3 13:47:10.107: INFO: glance-ks-user-h6kmn from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:47:10.107: INFO: placement-ks-user-swghx from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:47:10.107: INFO: fm-ks-endpoints-rswss from openstack started at 2020-01-03 11:41:38 +0000 UTC (3 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container faultmanagement-ks-endpoints-admin ready: false, restart count 0
Jan  3 13:47:10.107: INFO: 	Container faultmanagement-ks-endpoints-internal ready: false, restart count 0
Jan  3 13:47:10.107: INFO: 	Container faultmanagement-ks-endpoints-public ready: false, restart count 0
Jan  3 13:47:10.107: INFO: kube-scheduler-controller-1 from kube-system started at 2020-01-03 10:52:37 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  3 13:47:10.107: INFO: placement-db-init-4t4cq from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container placement-db-init-0 ready: false, restart count 0
Jan  3 13:47:10.107: INFO: neutron-l3-agent-controller-1-cab72f56-nm27w from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container neutron-l3-agent ready: true, restart count 0
Jan  3 13:47:10.107: INFO: cinder-ks-user-rhjbx from openstack started at 2020-01-03 11:29:43 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 13:47:10.107: INFO: nova-ks-service-x57sz from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container compute-ks-service-registration ready: false, restart count 0
Jan  3 13:47:10.107: INFO: glance-db-init-9hfsz from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container glance-db-init-0 ready: false, restart count 0
Jan  3 13:47:10.107: INFO: cinder-scheduler-664bb87785-67xsw from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container cinder-scheduler ready: true, restart count 0
Jan  3 13:47:10.107: INFO: heat-api-58bf859968-c7cp2 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 13:47:10.107: INFO: 	Container heat-api ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1fd5f92c-bc63-4e1a-b90b-2f01ce4b1db8 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-1fd5f92c-bc63-4e1a-b90b-2f01ce4b1db8 off the node controller-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1fd5f92c-bc63-4e1a-b90b-2f01ce4b1db8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:47:42.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3616" for this suite.
Jan  3 13:48:00.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:48:00.304: INFO: namespace sched-pred-3616 deletion completed in 18.053742812s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:50.295 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:48:00.305: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-974/secret-test-d97baccd-d9c1-4573-8a8f-94a74a382df6
STEP: Creating a pod to test consume secrets
Jan  3 13:48:00.321: INFO: Waiting up to 5m0s for pod "pod-configmaps-198680e0-bb1c-49e6-902e-d1db1ed5c32b" in namespace "secrets-974" to be "success or failure"
Jan  3 13:48:00.321: INFO: Pod "pod-configmaps-198680e0-bb1c-49e6-902e-d1db1ed5c32b": Phase="Pending", Reason="", readiness=false. Elapsed: 842.419µs
Jan  3 13:48:02.324: INFO: Pod "pod-configmaps-198680e0-bb1c-49e6-902e-d1db1ed5c32b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003580529s
Jan  3 13:48:04.326: INFO: Pod "pod-configmaps-198680e0-bb1c-49e6-902e-d1db1ed5c32b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004849372s
Jan  3 13:48:06.328: INFO: Pod "pod-configmaps-198680e0-bb1c-49e6-902e-d1db1ed5c32b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007420672s
Jan  3 13:48:08.333: INFO: Pod "pod-configmaps-198680e0-bb1c-49e6-902e-d1db1ed5c32b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01200868s
STEP: Saw pod success
Jan  3 13:48:08.333: INFO: Pod "pod-configmaps-198680e0-bb1c-49e6-902e-d1db1ed5c32b" satisfied condition "success or failure"
Jan  3 13:48:08.352: INFO: Trying to get logs from node controller-0 pod pod-configmaps-198680e0-bb1c-49e6-902e-d1db1ed5c32b container env-test: <nil>
STEP: delete the pod
Jan  3 13:48:08.376: INFO: Waiting for pod pod-configmaps-198680e0-bb1c-49e6-902e-d1db1ed5c32b to disappear
Jan  3 13:48:08.377: INFO: Pod pod-configmaps-198680e0-bb1c-49e6-902e-d1db1ed5c32b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:48:08.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-974" for this suite.
Jan  3 13:48:14.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:48:14.445: INFO: namespace secrets-974 deletion completed in 6.066155572s

• [SLOW TEST:14.141 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:48:14.447: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:48:14.478: INFO: Creating deployment "webserver-deployment"
Jan  3 13:48:14.480: INFO: Waiting for observed generation 1
Jan  3 13:48:16.484: INFO: Waiting for all required pods to come up
Jan  3 13:48:16.487: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan  3 13:48:24.495: INFO: Waiting for deployment "webserver-deployment" to complete
Jan  3 13:48:24.500: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan  3 13:48:24.505: INFO: Updating deployment webserver-deployment
Jan  3 13:48:24.505: INFO: Waiting for observed generation 2
Jan  3 13:48:26.528: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan  3 13:48:26.529: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan  3 13:48:26.533: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  3 13:48:26.540: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan  3 13:48:26.540: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan  3 13:48:26.542: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  3 13:48:26.544: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan  3 13:48:26.544: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan  3 13:48:26.549: INFO: Updating deployment webserver-deployment
Jan  3 13:48:26.549: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan  3 13:48:26.564: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan  3 13:48:26.580: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jan  3 13:48:26.597: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1377 /apis/apps/v1/namespaces/deployment-1377/deployments/webserver-deployment 5cf092dc-b5a3-4910-86d2-25c0b2c02381 49986 3 2020-01-03 13:48:14 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0023162c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-01-03 13:48:24 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-01-03 13:48:26 +0000 UTC,LastTransitionTime:2020-01-03 13:48:26 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan  3 13:48:26.611: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-1377 /apis/apps/v1/namespaces/deployment-1377/replicasets/webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 49971 3 2020-01-03 13:48:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 5cf092dc-b5a3-4910-86d2-25c0b2c02381 0xc002316817 0xc002316818}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002316888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 13:48:26.611: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan  3 13:48:26.611: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-1377 /apis/apps/v1/namespaces/deployment-1377/replicasets/webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 49968 3 2020-01-03 13:48:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 5cf092dc-b5a3-4910-86d2-25c0b2c02381 0xc002316747 0xc002316748}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0023167a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan  3 13:48:26.634: INFO: Pod "webserver-deployment-595b5b9587-229b6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-229b6 webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-229b6 6928d880-5faf-4646-97c8-06b0277de2ae 50004 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc002316d87 0xc002316d88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.635: INFO: Pod "webserver-deployment-595b5b9587-5kzv5" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5kzv5 webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-5kzv5 2f66ca67-63df-4bee-b920-4c43dac32119 49995 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc002316e87 0xc002316e88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.635: INFO: Pod "webserver-deployment-595b5b9587-ctbsf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ctbsf webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-ctbsf 422d41c7-f555-4c16-be98-2c721143f06c 50003 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc002316f77 0xc002316f78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.635: INFO: Pod "webserver-deployment-595b5b9587-h5hxj" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h5hxj webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-h5hxj 8b2db5fc-53c5-4c61-85ae-7c7433900921 49901 0 2020-01-03 13:48:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.192.119/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.119"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc002317067 0xc002317068}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.11,PodIP:172.16.192.119,StartTime:2020-01-03 13:48:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-03 13:48:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://750630f29f667e20532fce83787b6d8e5e255a63ad87c75331574bb3e614f392,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.636: INFO: Pod "webserver-deployment-595b5b9587-hvfcm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hvfcm webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-hvfcm f531ebc5-5694-4fc9-9f0f-45708d7b99c2 49980 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc0023171e7 0xc0023171e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.636: INFO: Pod "webserver-deployment-595b5b9587-hww26" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hww26 webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-hww26 861d189a-399c-4fe5-8bac-64e65554119c 49871 0 2020-01-03 13:48:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.166.186/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.186"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc002317300 0xc002317301}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.12,PodIP:172.16.166.186,StartTime:2020-01-03 13:48:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-03 13:48:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://29abe003bdee72dde22dc4cffd0378b79da7b5533a5b088a889e8e5fd86ecc37,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.636: INFO: Pod "webserver-deployment-595b5b9587-l6x8d" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-l6x8d webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-l6x8d f73fb8c9-2b52-43c5-9aed-1bc6dd7aa5dd 49895 0 2020-01-03 13:48:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.166.188/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.188"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc002317477 0xc002317478}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.12,PodIP:172.16.166.188,StartTime:2020-01-03 13:48:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-03 13:48:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e32346c8376dbe14fedf6046d60c256554f36cc1c8cce62cb22d8b168a32ddc8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.636: INFO: Pod "webserver-deployment-595b5b9587-lc9n7" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lc9n7 webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-lc9n7 4f2840a9-4324-46be-9219-1e8cde2d14f4 50007 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc0023175f7 0xc0023175f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.637: INFO: Pod "webserver-deployment-595b5b9587-ljk7h" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ljk7h webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-ljk7h 8d419de9-5922-4086-813c-3b014670b732 49987 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc0023176e7 0xc0023176e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.637: INFO: Pod "webserver-deployment-595b5b9587-p9pzz" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p9pzz webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-p9pzz a4fe43e3-cbce-4056-9e42-aa9278692d68 49892 0 2020-01-03 13:48:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.166.185/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.185"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc0023177d7 0xc0023177d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.12,PodIP:172.16.166.185,StartTime:2020-01-03 13:48:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-03 13:48:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://01287d1cf2fb7ae19f503978c8ec3865b39c0752a559483a648cdca03f4f8c07,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.637: INFO: Pod "webserver-deployment-595b5b9587-pc8zk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pc8zk webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-pc8zk cf4971ef-13aa-4a57-bb4d-5484ff6872cc 50006 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc002317967 0xc002317968}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.637: INFO: Pod "webserver-deployment-595b5b9587-pjc5d" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pjc5d webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-pjc5d 2d3ea1ac-8303-4f77-ae6b-351a477750e4 49878 0 2020-01-03 13:48:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.192.109/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.109"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc002317a57 0xc002317a58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.11,PodIP:172.16.192.109,StartTime:2020-01-03 13:48:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-03 13:48:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://58586d4c797fb14c4f7d45170c50b5dbafb93844690bd7420adfd4ecd7e3b4d4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.638: INFO: Pod "webserver-deployment-595b5b9587-pm6c6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pm6c6 webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-pm6c6 4e67c82d-e6cc-44f7-a006-f8b34d5de6f3 49992 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc002317be7 0xc002317be8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.638: INFO: Pod "webserver-deployment-595b5b9587-qgghl" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qgghl webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-qgghl d2a01916-7629-4734-94e5-2a866c7e1ab8 49991 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc002317cd7 0xc002317cd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.638: INFO: Pod "webserver-deployment-595b5b9587-rh98r" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rh98r webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-rh98r 0f5f4366-2a3a-46be-a8d5-12febc1b5bc9 50005 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc002317dc7 0xc002317dc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.638: INFO: Pod "webserver-deployment-595b5b9587-rvr7w" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rvr7w webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-rvr7w 0c0887ac-1822-46cd-bbfc-3e23c4ebb4c7 49994 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc002317eb7 0xc002317eb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.639: INFO: Pod "webserver-deployment-595b5b9587-swwn8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-swwn8 webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-swwn8 f06ecb54-fc29-4c1a-a18b-da02840321dd 49875 0 2020-01-03 13:48:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.192.111/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.111"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc002317fa7 0xc002317fa8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.11,PodIP:172.16.192.111,StartTime:2020-01-03 13:48:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-03 13:48:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3570ed1b6c87af7af77946a1b71be44145dca4a0845afcbd58bccac7874e279f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.639: INFO: Pod "webserver-deployment-595b5b9587-w6mmb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-w6mmb webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-w6mmb f9b34409-f375-41ea-9801-267b1189f2c7 49886 0 2020-01-03 13:48:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.166.183/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.183"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc0025341a7 0xc0025341a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.12,PodIP:172.16.166.183,StartTime:2020-01-03 13:48:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-03 13:48:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e323b201c09cd8936a5e55a42ad66a60c52ed9a1e59b9d400869dfdc48ffd842,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.639: INFO: Pod "webserver-deployment-595b5b9587-xhjvx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xhjvx webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-xhjvx b580421d-7fb8-4083-9fd0-474ab13e3547 50001 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc0025343e7 0xc0025343e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.12,PodIP:,StartTime:2020-01-03 13:48:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.640: INFO: Pod "webserver-deployment-595b5b9587-zvztg" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zvztg webserver-deployment-595b5b9587- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-595b5b9587-zvztg 062c921e-c364-4fd7-87a8-94366ec75ef6 49864 0 2020-01-03 13:48:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.192.97/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.97"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 bd41b1a7-50c5-44a7-acbf-5ef128d942a0 0xc0025347b7 0xc0025347b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.11,PodIP:172.16.192.97,StartTime:2020-01-03 13:48:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-03 13:48:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://dc0e668468e22e27bafcfdaac8ae19635232cbd14420462c86965d3a108b1a35,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.642: INFO: Pod "webserver-deployment-c7997dcc8-25dv7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-25dv7 webserver-deployment-c7997dcc8- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-c7997dcc8-25dv7 5bc0de22-d794-4640-92ed-d2e418f6c3d3 49993 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 0xc002534af7 0xc002534af8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.642: INFO: Pod "webserver-deployment-c7997dcc8-7h86z" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7h86z webserver-deployment-c7997dcc8- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-c7997dcc8-7h86z 91f95aee-0724-404d-8112-0ef05a96e98c 49997 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 0xc002534d97 0xc002534d98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.643: INFO: Pod "webserver-deployment-c7997dcc8-cbhzn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cbhzn webserver-deployment-c7997dcc8- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-c7997dcc8-cbhzn d08999c4-35b6-447b-9379-cd9be3c309c5 49953 0 2020-01-03 13:48:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 0xc002534fd0 0xc002534fd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.11,PodIP:,StartTime:2020-01-03 13:48:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.643: INFO: Pod "webserver-deployment-c7997dcc8-dtzvb" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-dtzvb webserver-deployment-c7997dcc8- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-c7997dcc8-dtzvb ec8c875e-2b07-42ea-b500-84b83583b6f8 49952 0 2020-01-03 13:48:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 0xc002535150 0xc002535151}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.11,PodIP:,StartTime:2020-01-03 13:48:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.643: INFO: Pod "webserver-deployment-c7997dcc8-f445w" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-f445w webserver-deployment-c7997dcc8- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-c7997dcc8-f445w 9a8d4bb8-946d-4976-8afe-5abc25f25e79 49979 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 0xc0025353b0 0xc0025353b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.654: INFO: Pod "webserver-deployment-c7997dcc8-fjqvb" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-fjqvb webserver-deployment-c7997dcc8- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-c7997dcc8-fjqvb ffe82926-7370-4130-8538-afacfec5aeaf 49985 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 0xc0025354e0 0xc0025354e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.655: INFO: Pod "webserver-deployment-c7997dcc8-g2nwc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-g2nwc webserver-deployment-c7997dcc8- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-c7997dcc8-g2nwc 49fc48f2-c40e-4e5f-a04a-afdaff264a61 49948 0 2020-01-03 13:48:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 0xc0025355d7 0xc0025355d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.11,PodIP:,StartTime:2020-01-03 13:48:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.655: INFO: Pod "webserver-deployment-c7997dcc8-khqjw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-khqjw webserver-deployment-c7997dcc8- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-c7997dcc8-khqjw c3709cff-d31e-40ba-a483-5222f8443373 49951 0 2020-01-03 13:48:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 0xc002535750 0xc002535751}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.12,PodIP:,StartTime:2020-01-03 13:48:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.655: INFO: Pod "webserver-deployment-c7997dcc8-qx76k" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qx76k webserver-deployment-c7997dcc8- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-c7997dcc8-qx76k 9886dc59-6c0e-4d83-85ca-102ce7360a07 49984 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 0xc0025358c0 0xc0025358c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.655: INFO: Pod "webserver-deployment-c7997dcc8-v2s5k" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-v2s5k webserver-deployment-c7997dcc8- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-c7997dcc8-v2s5k ef430c2f-4bff-41d6-ad91-de78d0465fd4 49937 0 2020-01-03 13:48:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 0xc0025359b7 0xc0025359b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.12,PodIP:,StartTime:2020-01-03 13:48:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.655: INFO: Pod "webserver-deployment-c7997dcc8-v4t84" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-v4t84 webserver-deployment-c7997dcc8- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-c7997dcc8-v4t84 fda7ebe0-ed8a-45c0-b4ca-9df5ba608de4 50000 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 0xc002535b30 0xc002535b31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.655: INFO: Pod "webserver-deployment-c7997dcc8-z4qwv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-z4qwv webserver-deployment-c7997dcc8- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-c7997dcc8-z4qwv c2a4bb42-5cad-4883-8a8d-e046e73f9fea 49988 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 0xc002535c50 0xc002535c51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 13:48:26.655: INFO: Pod "webserver-deployment-c7997dcc8-zc2vl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zc2vl webserver-deployment-c7997dcc8- deployment-1377 /api/v1/namespaces/deployment-1377/pods/webserver-deployment-c7997dcc8-zc2vl 9a1082e8-4f4e-450f-93a4-995a7048e2a4 50008 0 2020-01-03 13:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e7dc887c-91f5-4475-9a64-6d073516d414 0xc003ba6047 0xc003ba6048}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bggg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bggg2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bggg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:48:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:48:26.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1377" for this suite.
Jan  3 13:48:34.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:48:34.998: INFO: namespace deployment-1377 deletion completed in 8.318666647s

• [SLOW TEST:20.551 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:48:35.006: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-72da7a53-b75d-459a-977f-c93ea3b75b4b
STEP: Creating a pod to test consume configMaps
Jan  3 13:48:35.117: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6ed6f23b-6dce-46cf-9e52-b835988d6fab" in namespace "projected-9996" to be "success or failure"
Jan  3 13:48:35.129: INFO: Pod "pod-projected-configmaps-6ed6f23b-6dce-46cf-9e52-b835988d6fab": Phase="Pending", Reason="", readiness=false. Elapsed: 12.216121ms
Jan  3 13:48:37.145: INFO: Pod "pod-projected-configmaps-6ed6f23b-6dce-46cf-9e52-b835988d6fab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028202029s
Jan  3 13:48:39.146: INFO: Pod "pod-projected-configmaps-6ed6f23b-6dce-46cf-9e52-b835988d6fab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029515565s
Jan  3 13:48:41.149: INFO: Pod "pod-projected-configmaps-6ed6f23b-6dce-46cf-9e52-b835988d6fab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032039604s
Jan  3 13:48:43.173: INFO: Pod "pod-projected-configmaps-6ed6f23b-6dce-46cf-9e52-b835988d6fab": Phase="Pending", Reason="", readiness=false. Elapsed: 8.055844934s
Jan  3 13:48:45.234: INFO: Pod "pod-projected-configmaps-6ed6f23b-6dce-46cf-9e52-b835988d6fab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.117179777s
STEP: Saw pod success
Jan  3 13:48:45.234: INFO: Pod "pod-projected-configmaps-6ed6f23b-6dce-46cf-9e52-b835988d6fab" satisfied condition "success or failure"
Jan  3 13:48:45.246: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-6ed6f23b-6dce-46cf-9e52-b835988d6fab container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 13:48:45.261: INFO: Waiting for pod pod-projected-configmaps-6ed6f23b-6dce-46cf-9e52-b835988d6fab to disappear
Jan  3 13:48:45.262: INFO: Pod pod-projected-configmaps-6ed6f23b-6dce-46cf-9e52-b835988d6fab no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:48:45.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9996" for this suite.
Jan  3 13:48:51.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:48:51.312: INFO: namespace projected-9996 deletion completed in 6.048174756s

• [SLOW TEST:16.307 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:48:51.314: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Jan  3 13:48:51.333: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 13:49:51.462: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:49:51.464: INFO: Starting informer...
STEP: Starting pods...
Jan  3 13:49:51.711: INFO: Pod1 is running on controller-0. Tainting Node
Jan  3 13:49:59.927: INFO: Pod2 is running on controller-0. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jan  3 13:50:29.033: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan  3 13:50:44.790: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:50:44.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7638" for this suite.
Jan  3 13:50:50.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:50:50.926: INFO: namespace taint-multiple-pods-7638 deletion completed in 6.116204103s

• [SLOW TEST:119.612 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:50:50.928: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:50:51.022: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"22424ebd-7414-4f9f-8a1e-9c99a03513db", Controller:(*bool)(0xc00663bae6), BlockOwnerDeletion:(*bool)(0xc00663bae7)}}
Jan  3 13:50:51.026: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"68f63c9c-c6a8-4982-973a-607b83e396b4", Controller:(*bool)(0xc00663bcb6), BlockOwnerDeletion:(*bool)(0xc00663bcb7)}}
Jan  3 13:50:51.028: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"db3ce9b6-a389-4ec0-8523-2f7bfe8f43bf", Controller:(*bool)(0xc001f82646), BlockOwnerDeletion:(*bool)(0xc001f82647)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:50:56.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4420" for this suite.
Jan  3 13:51:02.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:51:02.086: INFO: namespace gc-4420 deletion completed in 6.045834994s

• [SLOW TEST:11.159 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:51:02.089: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Jan  3 13:51:02.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=kubectl-9180 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jan  3 13:51:10.557: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jan  3 13:51:10.557: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:51:12.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9180" for this suite.
Jan  3 13:51:18.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:51:18.706: INFO: namespace kubectl-9180 deletion completed in 6.127231961s

• [SLOW TEST:16.617 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:51:18.706: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-a134960a-9c0c-4747-a974-e6f4a76576da
STEP: Creating a pod to test consume configMaps
Jan  3 13:51:18.791: INFO: Waiting up to 5m0s for pod "pod-configmaps-d0e9953a-d6f6-4031-99ee-839d15e4ad86" in namespace "configmap-2217" to be "success or failure"
Jan  3 13:51:18.802: INFO: Pod "pod-configmaps-d0e9953a-d6f6-4031-99ee-839d15e4ad86": Phase="Pending", Reason="", readiness=false. Elapsed: 10.920108ms
Jan  3 13:51:20.805: INFO: Pod "pod-configmaps-d0e9953a-d6f6-4031-99ee-839d15e4ad86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013934426s
Jan  3 13:51:22.817: INFO: Pod "pod-configmaps-d0e9953a-d6f6-4031-99ee-839d15e4ad86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025349968s
Jan  3 13:51:24.822: INFO: Pod "pod-configmaps-d0e9953a-d6f6-4031-99ee-839d15e4ad86": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030634789s
Jan  3 13:51:26.897: INFO: Pod "pod-configmaps-d0e9953a-d6f6-4031-99ee-839d15e4ad86": Phase="Pending", Reason="", readiness=false. Elapsed: 8.105806642s
Jan  3 13:51:28.902: INFO: Pod "pod-configmaps-d0e9953a-d6f6-4031-99ee-839d15e4ad86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.110302774s
STEP: Saw pod success
Jan  3 13:51:28.905: INFO: Pod "pod-configmaps-d0e9953a-d6f6-4031-99ee-839d15e4ad86" satisfied condition "success or failure"
Jan  3 13:51:28.907: INFO: Trying to get logs from node controller-1 pod pod-configmaps-d0e9953a-d6f6-4031-99ee-839d15e4ad86 container configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 13:51:28.943: INFO: Waiting for pod pod-configmaps-d0e9953a-d6f6-4031-99ee-839d15e4ad86 to disappear
Jan  3 13:51:28.945: INFO: Pod pod-configmaps-d0e9953a-d6f6-4031-99ee-839d15e4ad86 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:51:28.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2217" for this suite.
Jan  3 13:51:34.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:51:35.252: INFO: namespace configmap-2217 deletion completed in 6.304320337s

• [SLOW TEST:16.546 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:51:35.254: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:51:52.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-946" for this suite.
Jan  3 13:51:58.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:51:58.404: INFO: namespace resourcequota-946 deletion completed in 6.067004651s

• [SLOW TEST:23.150 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:51:58.405: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan  3 13:51:58.430: INFO: Waiting up to 5m0s for pod "pod-22d2d8da-832c-4501-9ede-911dc947c007" in namespace "emptydir-8688" to be "success or failure"
Jan  3 13:51:58.431: INFO: Pod "pod-22d2d8da-832c-4501-9ede-911dc947c007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.377277ms
Jan  3 13:52:00.451: INFO: Pod "pod-22d2d8da-832c-4501-9ede-911dc947c007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020854294s
Jan  3 13:52:02.456: INFO: Pod "pod-22d2d8da-832c-4501-9ede-911dc947c007": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026583256s
Jan  3 13:52:04.467: INFO: Pod "pod-22d2d8da-832c-4501-9ede-911dc947c007": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036770741s
Jan  3 13:52:06.474: INFO: Pod "pod-22d2d8da-832c-4501-9ede-911dc947c007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.044215719s
STEP: Saw pod success
Jan  3 13:52:06.475: INFO: Pod "pod-22d2d8da-832c-4501-9ede-911dc947c007" satisfied condition "success or failure"
Jan  3 13:52:06.477: INFO: Trying to get logs from node controller-1 pod pod-22d2d8da-832c-4501-9ede-911dc947c007 container test-container: <nil>
STEP: delete the pod
Jan  3 13:52:06.504: INFO: Waiting for pod pod-22d2d8da-832c-4501-9ede-911dc947c007 to disappear
Jan  3 13:52:06.509: INFO: Pod pod-22d2d8da-832c-4501-9ede-911dc947c007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:52:06.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8688" for this suite.
Jan  3 13:52:12.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:52:12.698: INFO: namespace emptydir-8688 deletion completed in 6.186006613s

• [SLOW TEST:14.293 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:52:12.699: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-f41e9bb8-592c-4580-922e-c7e2310e4199
STEP: Creating a pod to test consume secrets
Jan  3 13:52:12.757: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-812698b8-ebae-4dc1-9674-f734c1db0b07" in namespace "projected-9202" to be "success or failure"
Jan  3 13:52:12.763: INFO: Pod "pod-projected-secrets-812698b8-ebae-4dc1-9674-f734c1db0b07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.238381ms
Jan  3 13:52:14.765: INFO: Pod "pod-projected-secrets-812698b8-ebae-4dc1-9674-f734c1db0b07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008134768s
Jan  3 13:52:16.768: INFO: Pod "pod-projected-secrets-812698b8-ebae-4dc1-9674-f734c1db0b07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011047698s
Jan  3 13:52:18.771: INFO: Pod "pod-projected-secrets-812698b8-ebae-4dc1-9674-f734c1db0b07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014142983s
Jan  3 13:52:20.773: INFO: Pod "pod-projected-secrets-812698b8-ebae-4dc1-9674-f734c1db0b07": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016258876s
Jan  3 13:52:22.778: INFO: Pod "pod-projected-secrets-812698b8-ebae-4dc1-9674-f734c1db0b07": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020533984s
Jan  3 13:52:24.785: INFO: Pod "pod-projected-secrets-812698b8-ebae-4dc1-9674-f734c1db0b07": Phase="Pending", Reason="", readiness=false. Elapsed: 12.028264471s
Jan  3 13:52:26.789: INFO: Pod "pod-projected-secrets-812698b8-ebae-4dc1-9674-f734c1db0b07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.031774873s
STEP: Saw pod success
Jan  3 13:52:26.789: INFO: Pod "pod-projected-secrets-812698b8-ebae-4dc1-9674-f734c1db0b07" satisfied condition "success or failure"
Jan  3 13:52:26.791: INFO: Trying to get logs from node controller-0 pod pod-projected-secrets-812698b8-ebae-4dc1-9674-f734c1db0b07 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan  3 13:52:27.205: INFO: Waiting for pod pod-projected-secrets-812698b8-ebae-4dc1-9674-f734c1db0b07 to disappear
Jan  3 13:52:27.218: INFO: Pod pod-projected-secrets-812698b8-ebae-4dc1-9674-f734c1db0b07 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:52:27.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9202" for this suite.
Jan  3 13:52:33.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:52:33.301: INFO: namespace projected-9202 deletion completed in 6.076919084s

• [SLOW TEST:20.602 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:52:33.302: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-e0d49d4c-13eb-44af-a9ed-925b12237fbb
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:52:33.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9856" for this suite.
Jan  3 13:52:39.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:52:39.513: INFO: namespace configmap-9856 deletion completed in 6.155312503s

• [SLOW TEST:6.211 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:52:39.515: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-1768f939-6205-4651-b181-ef663f27f419
STEP: Creating a pod to test consume secrets
Jan  3 13:52:39.560: INFO: Waiting up to 5m0s for pod "pod-secrets-2a80a317-4b2b-4b9f-a812-d75a42ca6994" in namespace "secrets-6391" to be "success or failure"
Jan  3 13:52:39.565: INFO: Pod "pod-secrets-2a80a317-4b2b-4b9f-a812-d75a42ca6994": Phase="Pending", Reason="", readiness=false. Elapsed: 4.801899ms
Jan  3 13:52:41.568: INFO: Pod "pod-secrets-2a80a317-4b2b-4b9f-a812-d75a42ca6994": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007388042s
Jan  3 13:52:43.588: INFO: Pod "pod-secrets-2a80a317-4b2b-4b9f-a812-d75a42ca6994": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028001742s
Jan  3 13:52:45.592: INFO: Pod "pod-secrets-2a80a317-4b2b-4b9f-a812-d75a42ca6994": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031457864s
Jan  3 13:52:47.597: INFO: Pod "pod-secrets-2a80a317-4b2b-4b9f-a812-d75a42ca6994": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.036848427s
STEP: Saw pod success
Jan  3 13:52:47.597: INFO: Pod "pod-secrets-2a80a317-4b2b-4b9f-a812-d75a42ca6994" satisfied condition "success or failure"
Jan  3 13:52:47.635: INFO: Trying to get logs from node controller-0 pod pod-secrets-2a80a317-4b2b-4b9f-a812-d75a42ca6994 container secret-volume-test: <nil>
STEP: delete the pod
Jan  3 13:52:47.657: INFO: Waiting for pod pod-secrets-2a80a317-4b2b-4b9f-a812-d75a42ca6994 to disappear
Jan  3 13:52:47.669: INFO: Pod pod-secrets-2a80a317-4b2b-4b9f-a812-d75a42ca6994 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:52:47.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6391" for this suite.
Jan  3 13:52:53.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:52:53.796: INFO: namespace secrets-6391 deletion completed in 6.114765521s

• [SLOW TEST:14.281 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:52:53.796: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 13:52:53.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edabe5d9-0178-4821-b400-a6699d9070e4" in namespace "projected-3645" to be "success or failure"
Jan  3 13:52:53.840: INFO: Pod "downwardapi-volume-edabe5d9-0178-4821-b400-a6699d9070e4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.785772ms
Jan  3 13:52:55.843: INFO: Pod "downwardapi-volume-edabe5d9-0178-4821-b400-a6699d9070e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010670436s
Jan  3 13:52:57.874: INFO: Pod "downwardapi-volume-edabe5d9-0178-4821-b400-a6699d9070e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042300087s
Jan  3 13:52:59.876: INFO: Pod "downwardapi-volume-edabe5d9-0178-4821-b400-a6699d9070e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043902954s
Jan  3 13:53:01.880: INFO: Pod "downwardapi-volume-edabe5d9-0178-4821-b400-a6699d9070e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.047622797s
STEP: Saw pod success
Jan  3 13:53:01.880: INFO: Pod "downwardapi-volume-edabe5d9-0178-4821-b400-a6699d9070e4" satisfied condition "success or failure"
Jan  3 13:53:01.881: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-edabe5d9-0178-4821-b400-a6699d9070e4 container client-container: <nil>
STEP: delete the pod
Jan  3 13:53:01.900: INFO: Waiting for pod downwardapi-volume-edabe5d9-0178-4821-b400-a6699d9070e4 to disappear
Jan  3 13:53:01.911: INFO: Pod downwardapi-volume-edabe5d9-0178-4821-b400-a6699d9070e4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:53:01.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3645" for this suite.
Jan  3 13:53:07.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:53:07.961: INFO: namespace projected-3645 deletion completed in 6.04846447s

• [SLOW TEST:14.165 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:53:07.962: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:53:07.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2458" for this suite.
Jan  3 13:53:13.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:53:14.009: INFO: namespace tables-2458 deletion completed in 6.030490261s

• [SLOW TEST:6.047 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:53:14.010: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-eb858313-8c31-4ae3-9cc1-79b721326693
STEP: Creating a pod to test consume configMaps
Jan  3 13:53:14.044: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-de5220e0-c0a5-4c14-8b6c-b256b085a321" in namespace "projected-7589" to be "success or failure"
Jan  3 13:53:14.046: INFO: Pod "pod-projected-configmaps-de5220e0-c0a5-4c14-8b6c-b256b085a321": Phase="Pending", Reason="", readiness=false. Elapsed: 1.949805ms
Jan  3 13:53:16.050: INFO: Pod "pod-projected-configmaps-de5220e0-c0a5-4c14-8b6c-b256b085a321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005973078s
Jan  3 13:53:18.052: INFO: Pod "pod-projected-configmaps-de5220e0-c0a5-4c14-8b6c-b256b085a321": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007751669s
Jan  3 13:53:20.054: INFO: Pod "pod-projected-configmaps-de5220e0-c0a5-4c14-8b6c-b256b085a321": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009231073s
Jan  3 13:53:22.060: INFO: Pod "pod-projected-configmaps-de5220e0-c0a5-4c14-8b6c-b256b085a321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01563003s
STEP: Saw pod success
Jan  3 13:53:22.060: INFO: Pod "pod-projected-configmaps-de5220e0-c0a5-4c14-8b6c-b256b085a321" satisfied condition "success or failure"
Jan  3 13:53:22.061: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-de5220e0-c0a5-4c14-8b6c-b256b085a321 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 13:53:22.075: INFO: Waiting for pod pod-projected-configmaps-de5220e0-c0a5-4c14-8b6c-b256b085a321 to disappear
Jan  3 13:53:22.077: INFO: Pod pod-projected-configmaps-de5220e0-c0a5-4c14-8b6c-b256b085a321 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:53:22.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7589" for this suite.
Jan  3 13:53:28.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:53:28.152: INFO: namespace projected-7589 deletion completed in 6.06925548s

• [SLOW TEST:14.142 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:53:28.153: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan  3 13:53:28.176: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8691 /api/v1/namespaces/watch-8691/configmaps/e2e-watch-test-label-changed 9fa0fd2b-ac85-4f57-9e20-2ca051bc6107 53634 0 2020-01-03 13:53:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan  3 13:53:28.176: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8691 /api/v1/namespaces/watch-8691/configmaps/e2e-watch-test-label-changed 9fa0fd2b-ac85-4f57-9e20-2ca051bc6107 53635 0 2020-01-03 13:53:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan  3 13:53:28.176: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8691 /api/v1/namespaces/watch-8691/configmaps/e2e-watch-test-label-changed 9fa0fd2b-ac85-4f57-9e20-2ca051bc6107 53636 0 2020-01-03 13:53:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan  3 13:53:38.192: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8691 /api/v1/namespaces/watch-8691/configmaps/e2e-watch-test-label-changed 9fa0fd2b-ac85-4f57-9e20-2ca051bc6107 53672 0 2020-01-03 13:53:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan  3 13:53:38.192: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8691 /api/v1/namespaces/watch-8691/configmaps/e2e-watch-test-label-changed 9fa0fd2b-ac85-4f57-9e20-2ca051bc6107 53673 0 2020-01-03 13:53:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jan  3 13:53:38.193: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8691 /api/v1/namespaces/watch-8691/configmaps/e2e-watch-test-label-changed 9fa0fd2b-ac85-4f57-9e20-2ca051bc6107 53674 0 2020-01-03 13:53:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:53:38.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8691" for this suite.
Jan  3 13:53:44.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:53:44.544: INFO: namespace watch-8691 deletion completed in 6.348699757s

• [SLOW TEST:16.391 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:53:44.558: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:53:44.623: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan  3 13:53:49.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-1140 create -f -'
Jan  3 13:53:49.444: INFO: stderr: ""
Jan  3 13:53:49.444: INFO: stdout: "e2e-test-crd-publish-openapi-5333-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  3 13:53:49.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-1140 delete e2e-test-crd-publish-openapi-5333-crds test-cr'
Jan  3 13:53:49.562: INFO: stderr: ""
Jan  3 13:53:49.562: INFO: stdout: "e2e-test-crd-publish-openapi-5333-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan  3 13:53:49.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-1140 apply -f -'
Jan  3 13:53:49.847: INFO: stderr: ""
Jan  3 13:53:49.847: INFO: stdout: "e2e-test-crd-publish-openapi-5333-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  3 13:53:49.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-1140 delete e2e-test-crd-publish-openapi-5333-crds test-cr'
Jan  3 13:53:50.039: INFO: stderr: ""
Jan  3 13:53:50.039: INFO: stdout: "e2e-test-crd-publish-openapi-5333-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan  3 13:53:50.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 explain e2e-test-crd-publish-openapi-5333-crds'
Jan  3 13:53:50.222: INFO: stderr: ""
Jan  3 13:53:50.222: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5333-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:53:54.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1140" for this suite.
Jan  3 13:54:00.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:54:00.201: INFO: namespace crd-publish-openapi-1140 deletion completed in 6.032389872s

• [SLOW TEST:15.642 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:54:00.201: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-d40d2c95-f3bc-43d2-88c6-aacb55313b2e
STEP: Creating a pod to test consume secrets
Jan  3 13:54:00.286: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9cd95619-68e6-431e-9873-3dcb1fcd36c4" in namespace "projected-9117" to be "success or failure"
Jan  3 13:54:00.288: INFO: Pod "pod-projected-secrets-9cd95619-68e6-431e-9873-3dcb1fcd36c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109757ms
Jan  3 13:54:02.290: INFO: Pod "pod-projected-secrets-9cd95619-68e6-431e-9873-3dcb1fcd36c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00369104s
Jan  3 13:54:04.291: INFO: Pod "pod-projected-secrets-9cd95619-68e6-431e-9873-3dcb1fcd36c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005263102s
Jan  3 13:54:06.293: INFO: Pod "pod-projected-secrets-9cd95619-68e6-431e-9873-3dcb1fcd36c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006829648s
Jan  3 13:54:08.294: INFO: Pod "pod-projected-secrets-9cd95619-68e6-431e-9873-3dcb1fcd36c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008271287s
STEP: Saw pod success
Jan  3 13:54:08.294: INFO: Pod "pod-projected-secrets-9cd95619-68e6-431e-9873-3dcb1fcd36c4" satisfied condition "success or failure"
Jan  3 13:54:08.295: INFO: Trying to get logs from node controller-0 pod pod-projected-secrets-9cd95619-68e6-431e-9873-3dcb1fcd36c4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan  3 13:54:08.307: INFO: Waiting for pod pod-projected-secrets-9cd95619-68e6-431e-9873-3dcb1fcd36c4 to disappear
Jan  3 13:54:08.309: INFO: Pod pod-projected-secrets-9cd95619-68e6-431e-9873-3dcb1fcd36c4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:54:08.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9117" for this suite.
Jan  3 13:54:14.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:54:14.441: INFO: namespace projected-9117 deletion completed in 6.130328767s

• [SLOW TEST:14.240 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:54:14.448: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:54:22.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1673" for this suite.
Jan  3 13:55:06.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:55:06.557: INFO: namespace kubelet-test-1673 deletion completed in 44.045520147s

• [SLOW TEST:52.109 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:55:06.557: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:55:14.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6111" for this suite.
Jan  3 13:56:00.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:56:00.628: INFO: namespace kubelet-test-6111 deletion completed in 46.043240554s

• [SLOW TEST:54.071 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:56:00.629: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1825.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1825.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1825.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1825.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1825.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1825.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan  3 13:56:10.669: INFO: DNS probes using dns-1825/dns-test-98a4fa38-ee30-4922-a0f7-0622131bb62b succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:56:10.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1825" for this suite.
Jan  3 13:56:16.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:56:16.729: INFO: namespace dns-1825 deletion completed in 6.033837041s

• [SLOW TEST:16.101 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:56:16.730: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jan  3 13:56:16.803: INFO: Waiting up to 5m0s for pod "downward-api-7bbbe578-68ea-492b-93bd-fb795121359a" in namespace "downward-api-9941" to be "success or failure"
Jan  3 13:56:16.804: INFO: Pod "downward-api-7bbbe578-68ea-492b-93bd-fb795121359a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.217475ms
Jan  3 13:56:18.813: INFO: Pod "downward-api-7bbbe578-68ea-492b-93bd-fb795121359a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010039946s
Jan  3 13:56:20.814: INFO: Pod "downward-api-7bbbe578-68ea-492b-93bd-fb795121359a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011584454s
Jan  3 13:56:22.822: INFO: Pod "downward-api-7bbbe578-68ea-492b-93bd-fb795121359a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019153937s
Jan  3 13:56:24.824: INFO: Pod "downward-api-7bbbe578-68ea-492b-93bd-fb795121359a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02157915s
STEP: Saw pod success
Jan  3 13:56:24.827: INFO: Pod "downward-api-7bbbe578-68ea-492b-93bd-fb795121359a" satisfied condition "success or failure"
Jan  3 13:56:24.830: INFO: Trying to get logs from node controller-0 pod downward-api-7bbbe578-68ea-492b-93bd-fb795121359a container dapi-container: <nil>
STEP: delete the pod
Jan  3 13:56:24.845: INFO: Waiting for pod downward-api-7bbbe578-68ea-492b-93bd-fb795121359a to disappear
Jan  3 13:56:24.846: INFO: Pod downward-api-7bbbe578-68ea-492b-93bd-fb795121359a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:56:24.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9941" for this suite.
Jan  3 13:56:30.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:56:30.881: INFO: namespace downward-api-9941 deletion completed in 6.033001821s

• [SLOW TEST:14.151 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:56:30.881: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:56:30.904: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan  3 13:56:30.907: INFO: Number of nodes with available pods: 0
Jan  3 13:56:30.908: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jan  3 13:56:30.923: INFO: Number of nodes with available pods: 0
Jan  3 13:56:30.923: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:31.925: INFO: Number of nodes with available pods: 0
Jan  3 13:56:31.925: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:32.933: INFO: Number of nodes with available pods: 0
Jan  3 13:56:32.933: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:33.929: INFO: Number of nodes with available pods: 0
Jan  3 13:56:33.929: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:34.925: INFO: Number of nodes with available pods: 0
Jan  3 13:56:34.926: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:35.924: INFO: Number of nodes with available pods: 0
Jan  3 13:56:35.925: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:36.926: INFO: Number of nodes with available pods: 0
Jan  3 13:56:36.926: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:37.927: INFO: Number of nodes with available pods: 0
Jan  3 13:56:37.927: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:38.925: INFO: Number of nodes with available pods: 1
Jan  3 13:56:38.925: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan  3 13:56:38.940: INFO: Number of nodes with available pods: 1
Jan  3 13:56:38.940: INFO: Number of running nodes: 0, number of available pods: 1
Jan  3 13:56:39.945: INFO: Number of nodes with available pods: 0
Jan  3 13:56:39.945: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan  3 13:56:39.949: INFO: Number of nodes with available pods: 0
Jan  3 13:56:39.949: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:40.968: INFO: Number of nodes with available pods: 0
Jan  3 13:56:40.968: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:41.959: INFO: Number of nodes with available pods: 0
Jan  3 13:56:41.959: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:42.954: INFO: Number of nodes with available pods: 0
Jan  3 13:56:42.954: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:43.951: INFO: Number of nodes with available pods: 0
Jan  3 13:56:43.951: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:44.952: INFO: Number of nodes with available pods: 0
Jan  3 13:56:44.952: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:45.951: INFO: Number of nodes with available pods: 0
Jan  3 13:56:45.951: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:46.951: INFO: Number of nodes with available pods: 0
Jan  3 13:56:46.951: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:47.951: INFO: Number of nodes with available pods: 0
Jan  3 13:56:47.951: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:48.951: INFO: Number of nodes with available pods: 0
Jan  3 13:56:48.951: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:49.961: INFO: Number of nodes with available pods: 0
Jan  3 13:56:49.961: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:50.953: INFO: Number of nodes with available pods: 0
Jan  3 13:56:50.953: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:51.951: INFO: Number of nodes with available pods: 0
Jan  3 13:56:51.951: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:52.955: INFO: Number of nodes with available pods: 0
Jan  3 13:56:52.955: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:53.951: INFO: Number of nodes with available pods: 0
Jan  3 13:56:53.951: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:54.951: INFO: Number of nodes with available pods: 0
Jan  3 13:56:54.951: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:55.952: INFO: Number of nodes with available pods: 0
Jan  3 13:56:55.952: INFO: Node controller-0 is running more than one daemon pod
Jan  3 13:56:56.952: INFO: Number of nodes with available pods: 1
Jan  3 13:56:56.952: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1805, will wait for the garbage collector to delete the pods
Jan  3 13:56:57.008: INFO: Deleting DaemonSet.extensions daemon-set took: 2.105749ms
Jan  3 13:56:57.408: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.251553ms
Jan  3 13:57:09.410: INFO: Number of nodes with available pods: 0
Jan  3 13:57:09.410: INFO: Number of running nodes: 0, number of available pods: 0
Jan  3 13:57:09.411: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1805/daemonsets","resourceVersion":"54709"},"items":null}

Jan  3 13:57:09.413: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1805/pods","resourceVersion":"54709"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:57:09.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1805" for this suite.
Jan  3 13:57:15.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:57:15.486: INFO: namespace daemonsets-1805 deletion completed in 6.045288483s

• [SLOW TEST:44.606 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:57:15.487: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4466.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4466.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4466.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4466.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan  3 13:57:23.513: INFO: DNS probes using dns-test-35a1c896-98ba-4d61-99f3-cf34e19cefa0 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4466.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4466.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4466.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4466.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan  3 13:58:29.541: INFO: DNS probes using dns-test-de906374-08df-4731-95dd-a80f2e45d63d succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4466.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4466.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4466.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4466.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan  3 13:58:39.571: INFO: DNS probes using dns-test-e237d34d-35c6-4369-b15b-b1191ad698ce succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:58:39.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4466" for this suite.
Jan  3 13:58:45.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:58:45.882: INFO: namespace dns-4466 deletion completed in 6.229177704s

• [SLOW TEST:90.396 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:58:45.885: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 13:58:45.925: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan  3 13:58:45.933: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  3 13:58:50.935: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan  3 13:58:54.938: INFO: Creating deployment "test-rolling-update-deployment"
Jan  3 13:58:54.939: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan  3 13:58:54.942: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan  3 13:58:56.946: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan  3 13:58:56.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713656734, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713656734, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713656734, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713656734, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:58:58.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713656734, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713656734, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713656734, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713656734, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:59:00.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713656734, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713656734, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713656734, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713656734, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:59:02.968: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jan  3 13:59:02.990: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4029 /apis/apps/v1/namespaces/deployment-4029/deployments/test-rolling-update-deployment abcaab05-27c8-4bba-979c-6f962d81a07d 55284 1 2020-01-03 13:58:54 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ffaf98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-01-03 13:58:54 +0000 UTC,LastTransitionTime:2020-01-03 13:58:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-01-03 13:59:02 +0000 UTC,LastTransitionTime:2020-01-03 13:58:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  3 13:59:02.994: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-4029 /apis/apps/v1/namespaces/deployment-4029/replicasets/test-rolling-update-deployment-55d946486 0a4b938d-7fa8-4929-90af-fac9b9f85cde 55275 1 2020-01-03 13:58:54 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment abcaab05-27c8-4bba-979c-6f962d81a07d 0xc003ffb480 0xc003ffb481}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ffb4f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  3 13:59:02.994: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan  3 13:59:02.995: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4029 /apis/apps/v1/namespaces/deployment-4029/replicasets/test-rolling-update-controller 4bf6be40-2a4e-4282-8f27-03e2513e42d2 55283 2 2020-01-03 13:58:45 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment abcaab05-27c8-4bba-979c-6f962d81a07d 0xc003ffb3b7 0xc003ffb3b8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003ffb418 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 13:59:02.999: INFO: Pod "test-rolling-update-deployment-55d946486-6svgq" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-6svgq test-rolling-update-deployment-55d946486- deployment-4029 /api/v1/namespaces/deployment-4029/pods/test-rolling-update-deployment-55d946486-6svgq 64f37fca-c85f-4ebe-97c7-4d50849ae298 55274 0 2020-01-03 13:58:54 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:172.16.192.87/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.87"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 0a4b938d-7fa8-4929-90af-fac9b9f85cde 0xc006e36df0 0xc006e36df1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xhpnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xhpnc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xhpnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 13:58:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.11,PodIP:172.16.192.87,StartTime:2020-01-03 13:58:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-03 13:59:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://8354b74d33262b44bf704bf4809921df15113ac4f9b55aacd122744e132aa7f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:59:02.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4029" for this suite.
Jan  3 13:59:09.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:59:09.045: INFO: namespace deployment-4029 deletion completed in 6.04327027s

• [SLOW TEST:23.160 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:59:09.045: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8132.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8132.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8132.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8132.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8132.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8132.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8132.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8132.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8132.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8132.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8132.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 34.171.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.171.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.171.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.171.34_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8132.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8132.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8132.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8132.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8132.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8132.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8132.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8132.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8132.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8132.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8132.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 34.171.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.171.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.171.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.171.34_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan  3 13:59:19.074: INFO: Unable to read wheezy_udp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:19.076: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:19.077: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:19.078: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:19.087: INFO: Unable to read jessie_udp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:19.088: INFO: Unable to read jessie_tcp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:19.089: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:19.090: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:19.096: INFO: Lookups using dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864 failed for: [wheezy_udp@dns-test-service.dns-8132.svc.cluster.local wheezy_tcp@dns-test-service.dns-8132.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local jessie_udp@dns-test-service.dns-8132.svc.cluster.local jessie_tcp@dns-test-service.dns-8132.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local]

Jan  3 13:59:24.100: INFO: Unable to read wheezy_udp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:24.102: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:24.106: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:24.107: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:24.116: INFO: Unable to read jessie_udp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:24.118: INFO: Unable to read jessie_tcp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:24.121: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:24.123: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:24.129: INFO: Lookups using dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864 failed for: [wheezy_udp@dns-test-service.dns-8132.svc.cluster.local wheezy_tcp@dns-test-service.dns-8132.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local jessie_udp@dns-test-service.dns-8132.svc.cluster.local jessie_tcp@dns-test-service.dns-8132.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local]

Jan  3 13:59:29.098: INFO: Unable to read wheezy_udp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:29.100: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:29.101: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:29.103: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:29.112: INFO: Unable to read jessie_udp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:29.113: INFO: Unable to read jessie_tcp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:29.114: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:29.119: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:29.129: INFO: Lookups using dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864 failed for: [wheezy_udp@dns-test-service.dns-8132.svc.cluster.local wheezy_tcp@dns-test-service.dns-8132.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local jessie_udp@dns-test-service.dns-8132.svc.cluster.local jessie_tcp@dns-test-service.dns-8132.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local]

Jan  3 13:59:34.129: INFO: Unable to read wheezy_udp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:34.131: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:34.133: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:34.134: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:34.155: INFO: Unable to read jessie_udp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:34.161: INFO: Unable to read jessie_tcp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:34.162: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:34.164: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:34.178: INFO: Lookups using dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864 failed for: [wheezy_udp@dns-test-service.dns-8132.svc.cluster.local wheezy_tcp@dns-test-service.dns-8132.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local jessie_udp@dns-test-service.dns-8132.svc.cluster.local jessie_tcp@dns-test-service.dns-8132.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local]

Jan  3 13:59:39.099: INFO: Unable to read wheezy_udp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:39.101: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:39.106: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:39.108: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:39.123: INFO: Unable to read jessie_udp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:39.125: INFO: Unable to read jessie_tcp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:39.127: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:39.129: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:39.142: INFO: Lookups using dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864 failed for: [wheezy_udp@dns-test-service.dns-8132.svc.cluster.local wheezy_tcp@dns-test-service.dns-8132.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local jessie_udp@dns-test-service.dns-8132.svc.cluster.local jessie_tcp@dns-test-service.dns-8132.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local]

Jan  3 13:59:44.098: INFO: Unable to read wheezy_udp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:44.099: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:44.101: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:44.102: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:44.148: INFO: Unable to read jessie_udp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:44.150: INFO: Unable to read jessie_tcp@dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:44.152: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:44.154: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local from pod dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864: the server could not find the requested resource (get pods dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864)
Jan  3 13:59:44.167: INFO: Lookups using dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864 failed for: [wheezy_udp@dns-test-service.dns-8132.svc.cluster.local wheezy_tcp@dns-test-service.dns-8132.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local jessie_udp@dns-test-service.dns-8132.svc.cluster.local jessie_tcp@dns-test-service.dns-8132.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8132.svc.cluster.local]

Jan  3 13:59:49.119: INFO: DNS probes using dns-8132/dns-test-588a985d-6051-4248-b25c-a4ad3ccf8864 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 13:59:49.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8132" for this suite.
Jan  3 13:59:55.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 13:59:55.235: INFO: namespace dns-8132 deletion completed in 6.038266067s

• [SLOW TEST:46.190 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 13:59:55.235: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan  3 13:59:55.252: INFO: Waiting up to 5m0s for pod "pod-eb383234-3e4f-4b09-820b-ab20e74119df" in namespace "emptydir-2321" to be "success or failure"
Jan  3 13:59:55.253: INFO: Pod "pod-eb383234-3e4f-4b09-820b-ab20e74119df": Phase="Pending", Reason="", readiness=false. Elapsed: 890.365µs
Jan  3 13:59:57.255: INFO: Pod "pod-eb383234-3e4f-4b09-820b-ab20e74119df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002958554s
Jan  3 13:59:59.256: INFO: Pod "pod-eb383234-3e4f-4b09-820b-ab20e74119df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004442186s
Jan  3 14:00:01.258: INFO: Pod "pod-eb383234-3e4f-4b09-820b-ab20e74119df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006077036s
Jan  3 14:00:03.260: INFO: Pod "pod-eb383234-3e4f-4b09-820b-ab20e74119df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008320951s
STEP: Saw pod success
Jan  3 14:00:03.260: INFO: Pod "pod-eb383234-3e4f-4b09-820b-ab20e74119df" satisfied condition "success or failure"
Jan  3 14:00:03.262: INFO: Trying to get logs from node controller-0 pod pod-eb383234-3e4f-4b09-820b-ab20e74119df container test-container: <nil>
STEP: delete the pod
Jan  3 14:00:03.275: INFO: Waiting for pod pod-eb383234-3e4f-4b09-820b-ab20e74119df to disappear
Jan  3 14:00:03.277: INFO: Pod pod-eb383234-3e4f-4b09-820b-ab20e74119df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:00:03.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2321" for this suite.
Jan  3 14:00:09.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:00:09.328: INFO: namespace emptydir-2321 deletion completed in 6.049629954s

• [SLOW TEST:14.093 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:00:09.328: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:00:09.347: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:00:17.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7699" for this suite.
Jan  3 14:01:01.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:01:01.439: INFO: namespace pods-7699 deletion completed in 44.059185431s

• [SLOW TEST:52.110 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:01:01.439: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-089147c0-bb7f-46dc-ac12-dec8972e3c8f
STEP: Creating a pod to test consume secrets
Jan  3 14:01:01.488: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4d3b0102-762d-4a4c-bd88-c1a45b7f4afe" in namespace "projected-955" to be "success or failure"
Jan  3 14:01:01.494: INFO: Pod "pod-projected-secrets-4d3b0102-762d-4a4c-bd88-c1a45b7f4afe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.808446ms
Jan  3 14:01:03.496: INFO: Pod "pod-projected-secrets-4d3b0102-762d-4a4c-bd88-c1a45b7f4afe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007533972s
Jan  3 14:01:05.503: INFO: Pod "pod-projected-secrets-4d3b0102-762d-4a4c-bd88-c1a45b7f4afe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014144447s
Jan  3 14:01:07.504: INFO: Pod "pod-projected-secrets-4d3b0102-762d-4a4c-bd88-c1a45b7f4afe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015760549s
Jan  3 14:01:09.507: INFO: Pod "pod-projected-secrets-4d3b0102-762d-4a4c-bd88-c1a45b7f4afe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018359374s
STEP: Saw pod success
Jan  3 14:01:09.507: INFO: Pod "pod-projected-secrets-4d3b0102-762d-4a4c-bd88-c1a45b7f4afe" satisfied condition "success or failure"
Jan  3 14:01:09.508: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-4d3b0102-762d-4a4c-bd88-c1a45b7f4afe container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan  3 14:01:09.524: INFO: Waiting for pod pod-projected-secrets-4d3b0102-762d-4a4c-bd88-c1a45b7f4afe to disappear
Jan  3 14:01:09.526: INFO: Pod pod-projected-secrets-4d3b0102-762d-4a4c-bd88-c1a45b7f4afe no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:01:09.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-955" for this suite.
Jan  3 14:01:15.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:01:15.612: INFO: namespace projected-955 deletion completed in 6.083810505s

• [SLOW TEST:14.173 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:01:15.612: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-a97dcb3d-c97c-4d99-ac9c-e551b4d31303
STEP: Creating a pod to test consume configMaps
Jan  3 14:01:15.637: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-74b74906-c7a3-462b-8d35-7c6544ce456e" in namespace "projected-225" to be "success or failure"
Jan  3 14:01:15.638: INFO: Pod "pod-projected-configmaps-74b74906-c7a3-462b-8d35-7c6544ce456e": Phase="Pending", Reason="", readiness=false. Elapsed: 996.712µs
Jan  3 14:01:17.640: INFO: Pod "pod-projected-configmaps-74b74906-c7a3-462b-8d35-7c6544ce456e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00344044s
Jan  3 14:01:19.642: INFO: Pod "pod-projected-configmaps-74b74906-c7a3-462b-8d35-7c6544ce456e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005373049s
Jan  3 14:01:21.644: INFO: Pod "pod-projected-configmaps-74b74906-c7a3-462b-8d35-7c6544ce456e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00705389s
Jan  3 14:01:23.645: INFO: Pod "pod-projected-configmaps-74b74906-c7a3-462b-8d35-7c6544ce456e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008793518s
STEP: Saw pod success
Jan  3 14:01:23.645: INFO: Pod "pod-projected-configmaps-74b74906-c7a3-462b-8d35-7c6544ce456e" satisfied condition "success or failure"
Jan  3 14:01:23.646: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-74b74906-c7a3-462b-8d35-7c6544ce456e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 14:01:23.661: INFO: Waiting for pod pod-projected-configmaps-74b74906-c7a3-462b-8d35-7c6544ce456e to disappear
Jan  3 14:01:23.662: INFO: Pod pod-projected-configmaps-74b74906-c7a3-462b-8d35-7c6544ce456e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:01:23.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-225" for this suite.
Jan  3 14:01:29.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:01:29.705: INFO: namespace projected-225 deletion completed in 6.039155382s

• [SLOW TEST:14.093 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:01:29.705: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-tn2rv in namespace proxy-2792
I0103 14:01:29.743618      23 runners.go:184] Created replication controller with name: proxy-service-tn2rv, namespace: proxy-2792, replica count: 1
I0103 14:01:30.795317      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:01:31.795452      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:01:32.795583      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:01:33.795729      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:01:34.795864      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:01:35.796308      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:01:36.796564      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0103 14:01:37.798771      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0103 14:01:38.799522      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0103 14:01:39.800423      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0103 14:01:40.800827      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0103 14:01:41.801015      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0103 14:01:42.801487      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0103 14:01:43.805502      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0103 14:01:44.805659      23 runners.go:184] proxy-service-tn2rv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 14:01:44.807: INFO: setup took 15.073553039s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan  3 14:01:44.815: INFO: (0) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 6.706629ms)
Jan  3 14:01:44.815: INFO: (0) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 7.197575ms)
Jan  3 14:01:44.815: INFO: (0) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 7.007207ms)
Jan  3 14:01:44.815: INFO: (0) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 7.208884ms)
Jan  3 14:01:44.815: INFO: (0) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 6.936441ms)
Jan  3 14:01:44.815: INFO: (0) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 7.923501ms)
Jan  3 14:01:44.815: INFO: (0) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 7.701828ms)
Jan  3 14:01:44.815: INFO: (0) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 7.90537ms)
Jan  3 14:01:44.820: INFO: (0) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 12.144657ms)
Jan  3 14:01:44.821: INFO: (0) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 13.305451ms)
Jan  3 14:01:44.822: INFO: (0) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 14.263889ms)
Jan  3 14:01:44.827: INFO: (0) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 19.375953ms)
Jan  3 14:01:44.827: INFO: (0) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 20.597091ms)
Jan  3 14:01:44.847: INFO: (0) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 39.115623ms)
Jan  3 14:01:44.847: INFO: (0) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 39.088459ms)
Jan  3 14:01:44.847: INFO: (0) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 39.689855ms)
Jan  3 14:01:44.851: INFO: (1) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 3.103378ms)
Jan  3 14:01:44.851: INFO: (1) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 3.592975ms)
Jan  3 14:01:44.851: INFO: (1) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 3.677519ms)
Jan  3 14:01:44.852: INFO: (1) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 4.995948ms)
Jan  3 14:01:44.853: INFO: (1) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 5.189558ms)
Jan  3 14:01:44.853: INFO: (1) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 5.566768ms)
Jan  3 14:01:44.853: INFO: (1) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 5.10817ms)
Jan  3 14:01:44.853: INFO: (1) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 5.021869ms)
Jan  3 14:01:44.853: INFO: (1) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 5.072721ms)
Jan  3 14:01:44.853: INFO: (1) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 5.417542ms)
Jan  3 14:01:44.853: INFO: (1) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 6.013219ms)
Jan  3 14:01:44.854: INFO: (1) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 6.351497ms)
Jan  3 14:01:44.854: INFO: (1) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 6.409015ms)
Jan  3 14:01:44.855: INFO: (1) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 6.851689ms)
Jan  3 14:01:44.855: INFO: (1) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 6.772977ms)
Jan  3 14:01:44.856: INFO: (1) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 7.832936ms)
Jan  3 14:01:44.862: INFO: (2) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 6.217234ms)
Jan  3 14:01:44.862: INFO: (2) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 6.184434ms)
Jan  3 14:01:44.866: INFO: (2) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 9.902064ms)
Jan  3 14:01:44.866: INFO: (2) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 9.74767ms)
Jan  3 14:01:44.866: INFO: (2) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 9.889128ms)
Jan  3 14:01:44.866: INFO: (2) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 10.285968ms)
Jan  3 14:01:44.866: INFO: (2) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 10.329715ms)
Jan  3 14:01:44.867: INFO: (2) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 10.581186ms)
Jan  3 14:01:44.867: INFO: (2) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 10.366208ms)
Jan  3 14:01:44.867: INFO: (2) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 10.899945ms)
Jan  3 14:01:44.867: INFO: (2) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 10.787282ms)
Jan  3 14:01:44.867: INFO: (2) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 10.780291ms)
Jan  3 14:01:44.867: INFO: (2) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 11.086921ms)
Jan  3 14:01:44.867: INFO: (2) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 11.145652ms)
Jan  3 14:01:44.867: INFO: (2) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 11.304201ms)
Jan  3 14:01:44.867: INFO: (2) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 11.21257ms)
Jan  3 14:01:44.878: INFO: (3) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 4.491569ms)
Jan  3 14:01:44.879: INFO: (3) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 5.001175ms)
Jan  3 14:01:44.879: INFO: (3) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 5.889531ms)
Jan  3 14:01:44.879: INFO: (3) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 6.181345ms)
Jan  3 14:01:44.879: INFO: (3) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 5.184447ms)
Jan  3 14:01:44.880: INFO: (3) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 5.902581ms)
Jan  3 14:01:44.880: INFO: (3) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 6.175212ms)
Jan  3 14:01:44.880: INFO: (3) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 7.395619ms)
Jan  3 14:01:44.882: INFO: (3) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 8.284353ms)
Jan  3 14:01:44.882: INFO: (3) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 8.169623ms)
Jan  3 14:01:44.882: INFO: (3) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 7.387431ms)
Jan  3 14:01:44.882: INFO: (3) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 9.241912ms)
Jan  3 14:01:44.882: INFO: (3) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 9.643402ms)
Jan  3 14:01:44.882: INFO: (3) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 9.759964ms)
Jan  3 14:01:44.882: INFO: (3) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 9.529275ms)
Jan  3 14:01:44.886: INFO: (3) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 12.416672ms)
Jan  3 14:01:44.894: INFO: (4) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 7.629168ms)
Jan  3 14:01:44.894: INFO: (4) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 7.379764ms)
Jan  3 14:01:44.894: INFO: (4) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 7.325895ms)
Jan  3 14:01:44.894: INFO: (4) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 7.48322ms)
Jan  3 14:01:44.894: INFO: (4) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 7.535929ms)
Jan  3 14:01:44.894: INFO: (4) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 7.875571ms)
Jan  3 14:01:44.894: INFO: (4) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 7.450567ms)
Jan  3 14:01:44.894: INFO: (4) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 7.649222ms)
Jan  3 14:01:44.895: INFO: (4) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 7.741062ms)
Jan  3 14:01:44.895: INFO: (4) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 8.018161ms)
Jan  3 14:01:44.895: INFO: (4) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 7.490829ms)
Jan  3 14:01:44.895: INFO: (4) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 7.570019ms)
Jan  3 14:01:44.895: INFO: (4) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 7.815524ms)
Jan  3 14:01:44.895: INFO: (4) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 8.293505ms)
Jan  3 14:01:44.897: INFO: (4) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 10.801595ms)
Jan  3 14:01:44.897: INFO: (4) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 10.513886ms)
Jan  3 14:01:44.907: INFO: (5) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 9.847911ms)
Jan  3 14:01:44.907: INFO: (5) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 9.396391ms)
Jan  3 14:01:44.918: INFO: (5) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 20.176753ms)
Jan  3 14:01:44.919: INFO: (5) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 21.06283ms)
Jan  3 14:01:44.919: INFO: (5) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 21.675156ms)
Jan  3 14:01:44.919: INFO: (5) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 21.199822ms)
Jan  3 14:01:44.919: INFO: (5) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 21.519442ms)
Jan  3 14:01:44.919: INFO: (5) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 21.604666ms)
Jan  3 14:01:44.919: INFO: (5) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 21.291116ms)
Jan  3 14:01:44.919: INFO: (5) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 21.827713ms)
Jan  3 14:01:44.919: INFO: (5) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 21.919325ms)
Jan  3 14:01:44.919: INFO: (5) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 21.547861ms)
Jan  3 14:01:44.919: INFO: (5) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 21.499339ms)
Jan  3 14:01:44.919: INFO: (5) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 21.210257ms)
Jan  3 14:01:44.919: INFO: (5) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 21.472666ms)
Jan  3 14:01:44.919: INFO: (5) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 21.207637ms)
Jan  3 14:01:44.922: INFO: (6) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 1.916363ms)
Jan  3 14:01:44.927: INFO: (6) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 7.489648ms)
Jan  3 14:01:44.927: INFO: (6) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 5.371993ms)
Jan  3 14:01:44.927: INFO: (6) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 5.711207ms)
Jan  3 14:01:44.927: INFO: (6) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 5.931887ms)
Jan  3 14:01:44.927: INFO: (6) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 5.743019ms)
Jan  3 14:01:44.927: INFO: (6) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 5.583236ms)
Jan  3 14:01:44.927: INFO: (6) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 5.922897ms)
Jan  3 14:01:44.927: INFO: (6) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 5.804048ms)
Jan  3 14:01:44.927: INFO: (6) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 5.745148ms)
Jan  3 14:01:44.927: INFO: (6) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 7.574901ms)
Jan  3 14:01:44.927: INFO: (6) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 5.750648ms)
Jan  3 14:01:44.932: INFO: (6) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 10.208053ms)
Jan  3 14:01:44.932: INFO: (6) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 11.803077ms)
Jan  3 14:01:44.932: INFO: (6) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 10.594203ms)
Jan  3 14:01:44.932: INFO: (6) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 10.490493ms)
Jan  3 14:01:44.951: INFO: (7) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 18.922753ms)
Jan  3 14:01:44.951: INFO: (7) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 18.821722ms)
Jan  3 14:01:44.953: INFO: (7) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 20.587973ms)
Jan  3 14:01:44.953: INFO: (7) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 21.369205ms)
Jan  3 14:01:44.953: INFO: (7) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 21.235898ms)
Jan  3 14:01:44.953: INFO: (7) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 20.973519ms)
Jan  3 14:01:44.953: INFO: (7) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 21.128739ms)
Jan  3 14:01:44.953: INFO: (7) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 21.127144ms)
Jan  3 14:01:44.953: INFO: (7) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 21.533804ms)
Jan  3 14:01:44.953: INFO: (7) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 21.073751ms)
Jan  3 14:01:44.953: INFO: (7) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 21.163152ms)
Jan  3 14:01:44.953: INFO: (7) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 21.127301ms)
Jan  3 14:01:44.953: INFO: (7) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 21.19203ms)
Jan  3 14:01:44.954: INFO: (7) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 21.575141ms)
Jan  3 14:01:44.954: INFO: (7) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 21.789411ms)
Jan  3 14:01:44.954: INFO: (7) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 21.788074ms)
Jan  3 14:01:44.958: INFO: (8) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 3.616437ms)
Jan  3 14:01:44.958: INFO: (8) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 3.895069ms)
Jan  3 14:01:44.959: INFO: (8) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 3.987716ms)
Jan  3 14:01:44.959: INFO: (8) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 4.672636ms)
Jan  3 14:01:44.959: INFO: (8) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 4.782457ms)
Jan  3 14:01:44.959: INFO: (8) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 4.60578ms)
Jan  3 14:01:44.959: INFO: (8) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 5.194614ms)
Jan  3 14:01:44.959: INFO: (8) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 5.394865ms)
Jan  3 14:01:44.960: INFO: (8) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 5.571323ms)
Jan  3 14:01:44.960: INFO: (8) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 5.501535ms)
Jan  3 14:01:44.960: INFO: (8) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 5.201899ms)
Jan  3 14:01:44.960: INFO: (8) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 6.044476ms)
Jan  3 14:01:44.960: INFO: (8) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 5.782558ms)
Jan  3 14:01:44.960: INFO: (8) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 5.397061ms)
Jan  3 14:01:44.960: INFO: (8) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 5.902616ms)
Jan  3 14:01:44.960: INFO: (8) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 5.824872ms)
Jan  3 14:01:44.964: INFO: (9) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 3.536028ms)
Jan  3 14:01:44.964: INFO: (9) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 3.250043ms)
Jan  3 14:01:44.964: INFO: (9) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 3.555279ms)
Jan  3 14:01:44.965: INFO: (9) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 3.780226ms)
Jan  3 14:01:44.965: INFO: (9) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 4.066326ms)
Jan  3 14:01:44.965: INFO: (9) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 4.307084ms)
Jan  3 14:01:44.965: INFO: (9) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 3.449061ms)
Jan  3 14:01:44.965: INFO: (9) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 3.901145ms)
Jan  3 14:01:44.965: INFO: (9) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 4.167159ms)
Jan  3 14:01:44.965: INFO: (9) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 3.647906ms)
Jan  3 14:01:44.965: INFO: (9) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 3.774385ms)
Jan  3 14:01:44.965: INFO: (9) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 3.895177ms)
Jan  3 14:01:44.965: INFO: (9) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 3.648563ms)
Jan  3 14:01:44.965: INFO: (9) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 4.131277ms)
Jan  3 14:01:44.965: INFO: (9) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 3.865286ms)
Jan  3 14:01:44.965: INFO: (9) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 3.736182ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 9.154555ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 8.72582ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 9.072589ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 9.153988ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 8.8902ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 8.967363ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 8.785338ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 9.109584ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 8.90261ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 8.791763ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 9.656802ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 9.575406ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 9.423782ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 9.494815ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 9.154571ms)
Jan  3 14:01:44.976: INFO: (10) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 9.569169ms)
Jan  3 14:01:44.978: INFO: (11) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 1.27338ms)
Jan  3 14:01:44.978: INFO: (11) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 1.235951ms)
Jan  3 14:01:44.981: INFO: (11) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 3.410356ms)
Jan  3 14:01:44.981: INFO: (11) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 3.606568ms)
Jan  3 14:01:44.982: INFO: (11) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 4.076946ms)
Jan  3 14:01:44.983: INFO: (11) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 6.349636ms)
Jan  3 14:01:44.983: INFO: (11) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 5.905663ms)
Jan  3 14:01:44.983: INFO: (11) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 6.068633ms)
Jan  3 14:01:44.983: INFO: (11) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 6.184816ms)
Jan  3 14:01:44.983: INFO: (11) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 5.094689ms)
Jan  3 14:01:44.983: INFO: (11) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 6.034645ms)
Jan  3 14:01:44.983: INFO: (11) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 5.374059ms)
Jan  3 14:01:44.983: INFO: (11) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 5.201124ms)
Jan  3 14:01:44.983: INFO: (11) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 5.275193ms)
Jan  3 14:01:44.983: INFO: (11) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 6.001153ms)
Jan  3 14:01:44.983: INFO: (11) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 5.356019ms)
Jan  3 14:01:44.989: INFO: (12) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 4.359551ms)
Jan  3 14:01:44.989: INFO: (12) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 4.441313ms)
Jan  3 14:01:44.989: INFO: (12) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 4.542779ms)
Jan  3 14:01:44.989: INFO: (12) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 4.349666ms)
Jan  3 14:01:44.990: INFO: (12) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 4.508686ms)
Jan  3 14:01:44.990: INFO: (12) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 4.899485ms)
Jan  3 14:01:44.990: INFO: (12) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 4.99332ms)
Jan  3 14:01:44.990: INFO: (12) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 4.867058ms)
Jan  3 14:01:44.990: INFO: (12) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 5.093561ms)
Jan  3 14:01:44.990: INFO: (12) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 4.698141ms)
Jan  3 14:01:44.990: INFO: (12) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 5.210148ms)
Jan  3 14:01:44.990: INFO: (12) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 5.601696ms)
Jan  3 14:01:44.990: INFO: (12) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 5.518061ms)
Jan  3 14:01:44.990: INFO: (12) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 4.72513ms)
Jan  3 14:01:44.990: INFO: (12) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 4.868293ms)
Jan  3 14:01:44.993: INFO: (12) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 8.49183ms)
Jan  3 14:01:44.997: INFO: (13) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 3.124084ms)
Jan  3 14:01:44.997: INFO: (13) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 3.323496ms)
Jan  3 14:01:44.998: INFO: (13) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 3.584ms)
Jan  3 14:01:44.999: INFO: (13) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 5.433376ms)
Jan  3 14:01:44.999: INFO: (13) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 5.117337ms)
Jan  3 14:01:44.999: INFO: (13) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 5.380938ms)
Jan  3 14:01:44.999: INFO: (13) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 5.655542ms)
Jan  3 14:01:44.999: INFO: (13) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 5.512466ms)
Jan  3 14:01:45.000: INFO: (13) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 5.7722ms)
Jan  3 14:01:45.000: INFO: (13) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 5.548957ms)
Jan  3 14:01:45.000: INFO: (13) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 5.682737ms)
Jan  3 14:01:45.000: INFO: (13) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 6.596511ms)
Jan  3 14:01:45.000: INFO: (13) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 5.88534ms)
Jan  3 14:01:45.001: INFO: (13) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 5.906001ms)
Jan  3 14:01:45.001: INFO: (13) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 6.6319ms)
Jan  3 14:01:45.001: INFO: (13) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 6.728097ms)
Jan  3 14:01:45.006: INFO: (14) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 1.236379ms)
Jan  3 14:01:45.006: INFO: (14) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 1.529856ms)
Jan  3 14:01:45.009: INFO: (14) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 3.845316ms)
Jan  3 14:01:45.009: INFO: (14) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 4.128813ms)
Jan  3 14:01:45.009: INFO: (14) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 4.700432ms)
Jan  3 14:01:45.009: INFO: (14) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 4.08415ms)
Jan  3 14:01:45.009: INFO: (14) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 4.315373ms)
Jan  3 14:01:45.010: INFO: (14) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 4.562282ms)
Jan  3 14:01:45.010: INFO: (14) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 4.803927ms)
Jan  3 14:01:45.010: INFO: (14) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 4.764476ms)
Jan  3 14:01:45.010: INFO: (14) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 4.860876ms)
Jan  3 14:01:45.010: INFO: (14) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 5.111759ms)
Jan  3 14:01:45.010: INFO: (14) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 5.084948ms)
Jan  3 14:01:45.010: INFO: (14) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 5.167747ms)
Jan  3 14:01:45.010: INFO: (14) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 4.853304ms)
Jan  3 14:01:45.010: INFO: (14) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 5.098479ms)
Jan  3 14:01:45.015: INFO: (15) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 3.703961ms)
Jan  3 14:01:45.015: INFO: (15) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 4.275194ms)
Jan  3 14:01:45.015: INFO: (15) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 4.117302ms)
Jan  3 14:01:45.015: INFO: (15) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 4.687804ms)
Jan  3 14:01:45.015: INFO: (15) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 4.042757ms)
Jan  3 14:01:45.015: INFO: (15) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 4.784102ms)
Jan  3 14:01:45.015: INFO: (15) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 4.678205ms)
Jan  3 14:01:45.015: INFO: (15) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 4.939759ms)
Jan  3 14:01:45.015: INFO: (15) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 4.768ms)
Jan  3 14:01:45.015: INFO: (15) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 4.598651ms)
Jan  3 14:01:45.015: INFO: (15) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 4.269865ms)
Jan  3 14:01:45.015: INFO: (15) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 4.43325ms)
Jan  3 14:01:45.015: INFO: (15) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 4.105897ms)
Jan  3 14:01:45.016: INFO: (15) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 5.205303ms)
Jan  3 14:01:45.016: INFO: (15) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 5.535324ms)
Jan  3 14:01:45.016: INFO: (15) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 5.060923ms)
Jan  3 14:01:45.019: INFO: (16) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 3.015783ms)
Jan  3 14:01:45.019: INFO: (16) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 2.92073ms)
Jan  3 14:01:45.019: INFO: (16) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 3.40334ms)
Jan  3 14:01:45.019: INFO: (16) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 3.229103ms)
Jan  3 14:01:45.019: INFO: (16) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 3.60174ms)
Jan  3 14:01:45.020: INFO: (16) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 3.582274ms)
Jan  3 14:01:45.020: INFO: (16) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 3.638336ms)
Jan  3 14:01:45.020: INFO: (16) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 3.920944ms)
Jan  3 14:01:45.020: INFO: (16) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 4.037287ms)
Jan  3 14:01:45.020: INFO: (16) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 3.93243ms)
Jan  3 14:01:45.020: INFO: (16) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 4.15924ms)
Jan  3 14:01:45.020: INFO: (16) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 4.064272ms)
Jan  3 14:01:45.020: INFO: (16) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 4.085597ms)
Jan  3 14:01:45.020: INFO: (16) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 4.180879ms)
Jan  3 14:01:45.021: INFO: (16) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 4.443124ms)
Jan  3 14:01:45.021: INFO: (16) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 4.346175ms)
Jan  3 14:01:45.023: INFO: (17) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 2.460969ms)
Jan  3 14:01:45.024: INFO: (17) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 2.547264ms)
Jan  3 14:01:45.028: INFO: (17) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 6.809529ms)
Jan  3 14:01:45.028: INFO: (17) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 6.803089ms)
Jan  3 14:01:45.028: INFO: (17) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 7.129227ms)
Jan  3 14:01:45.028: INFO: (17) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 6.974415ms)
Jan  3 14:01:45.028: INFO: (17) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 7.340195ms)
Jan  3 14:01:45.028: INFO: (17) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 7.214734ms)
Jan  3 14:01:45.029: INFO: (17) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 7.795164ms)
Jan  3 14:01:45.059: INFO: (17) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 37.876839ms)
Jan  3 14:01:45.059: INFO: (17) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 37.671191ms)
Jan  3 14:01:45.059: INFO: (17) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 37.735763ms)
Jan  3 14:01:45.059: INFO: (17) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 37.417781ms)
Jan  3 14:01:45.059: INFO: (17) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 37.37357ms)
Jan  3 14:01:45.059: INFO: (17) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 37.532824ms)
Jan  3 14:01:45.059: INFO: (17) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 37.642979ms)
Jan  3 14:01:45.066: INFO: (18) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 5.565865ms)
Jan  3 14:01:45.066: INFO: (18) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 5.873955ms)
Jan  3 14:01:45.066: INFO: (18) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 5.445893ms)
Jan  3 14:01:45.066: INFO: (18) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 5.492551ms)
Jan  3 14:01:45.066: INFO: (18) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 5.677163ms)
Jan  3 14:01:45.066: INFO: (18) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 5.877877ms)
Jan  3 14:01:45.066: INFO: (18) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 6.196396ms)
Jan  3 14:01:45.066: INFO: (18) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 6.170048ms)
Jan  3 14:01:45.066: INFO: (18) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 6.070818ms)
Jan  3 14:01:45.066: INFO: (18) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 5.776042ms)
Jan  3 14:01:45.073: INFO: (18) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 13.382757ms)
Jan  3 14:01:45.073: INFO: (18) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 13.17604ms)
Jan  3 14:01:45.073: INFO: (18) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 13.10037ms)
Jan  3 14:01:45.073: INFO: (18) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 13.483337ms)
Jan  3 14:01:45.074: INFO: (18) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 13.376945ms)
Jan  3 14:01:45.083: INFO: (18) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 23.182867ms)
Jan  3 14:01:45.088: INFO: (19) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 4.333099ms)
Jan  3 14:01:45.088: INFO: (19) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:462/proxy/: tls qux (200; 4.568369ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:443/proxy/tlsrewritem... (200; 4.685896ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 5.067828ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname2/proxy/: bar (200; 4.955184ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx/proxy/rewriteme">test</a> (200; 5.306538ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname1/proxy/: tls baz (200; 5.653087ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">test<... (200; 5.418289ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/pods/https:proxy-service-tn2rv-mq9bx:460/proxy/: tls baz (200; 5.527864ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:162/proxy/: bar (200; 5.820333ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname1/proxy/: foo (200; 5.757103ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:160/proxy/: foo (200; 5.664757ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/services/proxy-service-tn2rv:portname2/proxy/: bar (200; 5.587723ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/services/http:proxy-service-tn2rv:portname1/proxy/: foo (200; 5.494896ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/services/https:proxy-service-tn2rv:tlsportname2/proxy/: tls qux (200; 5.407672ms)
Jan  3 14:01:45.089: INFO: (19) /api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2792/pods/http:proxy-service-tn2rv-mq9bx:1080/proxy/rewriteme">... (200; 5.559855ms)
STEP: deleting ReplicationController proxy-service-tn2rv in namespace proxy-2792, will wait for the garbage collector to delete the pods
Jan  3 14:01:45.206: INFO: Deleting ReplicationController proxy-service-tn2rv took: 20.335998ms
Jan  3 14:01:45.587: INFO: Terminating ReplicationController proxy-service-tn2rv pods took: 380.823285ms
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:01:59.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2792" for this suite.
Jan  3 14:02:05.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:02:05.451: INFO: namespace proxy-2792 deletion completed in 6.061945699s

• [SLOW TEST:35.746 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:02:05.459: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:02:13.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5536" for this suite.
Jan  3 14:02:57.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:02:57.574: INFO: namespace kubelet-test-5536 deletion completed in 44.047539695s

• [SLOW TEST:52.115 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:02:57.574: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan  3 14:02:57.608: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2734 /api/v1/namespaces/watch-2734/configmaps/e2e-watch-test-resource-version 2a336064-3bf1-497e-83be-a02f31a0947d 56443 0 2020-01-03 14:02:57 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan  3 14:02:57.608: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2734 /api/v1/namespaces/watch-2734/configmaps/e2e-watch-test-resource-version 2a336064-3bf1-497e-83be-a02f31a0947d 56444 0 2020-01-03 14:02:57 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:02:57.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2734" for this suite.
Jan  3 14:03:03.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:03:03.671: INFO: namespace watch-2734 deletion completed in 6.06077924s

• [SLOW TEST:6.097 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:03:03.674: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan  3 14:03:12.207: INFO: Successfully updated pod "pod-update-0f7db031-d1f5-4170-a60f-db1c4ffe8c22"
STEP: verifying the updated pod is in kubernetes
Jan  3 14:03:12.219: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:03:12.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1841" for this suite.
Jan  3 14:03:40.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:03:40.276: INFO: namespace pods-1841 deletion completed in 28.050271508s

• [SLOW TEST:36.602 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:03:40.277: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-303fac02-2ef2-4c01-a299-3e7b8de8db26
STEP: Creating a pod to test consume secrets
Jan  3 14:03:40.303: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cd5b641d-b8bb-4359-89d4-e5315527c794" in namespace "projected-1708" to be "success or failure"
Jan  3 14:03:40.304: INFO: Pod "pod-projected-secrets-cd5b641d-b8bb-4359-89d4-e5315527c794": Phase="Pending", Reason="", readiness=false. Elapsed: 901.854µs
Jan  3 14:03:42.309: INFO: Pod "pod-projected-secrets-cd5b641d-b8bb-4359-89d4-e5315527c794": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005352514s
Jan  3 14:03:44.311: INFO: Pod "pod-projected-secrets-cd5b641d-b8bb-4359-89d4-e5315527c794": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007727911s
Jan  3 14:03:46.312: INFO: Pod "pod-projected-secrets-cd5b641d-b8bb-4359-89d4-e5315527c794": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00902029s
Jan  3 14:03:48.314: INFO: Pod "pod-projected-secrets-cd5b641d-b8bb-4359-89d4-e5315527c794": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.010808047s
STEP: Saw pod success
Jan  3 14:03:48.314: INFO: Pod "pod-projected-secrets-cd5b641d-b8bb-4359-89d4-e5315527c794" satisfied condition "success or failure"
Jan  3 14:03:48.319: INFO: Trying to get logs from node controller-0 pod pod-projected-secrets-cd5b641d-b8bb-4359-89d4-e5315527c794 container secret-volume-test: <nil>
STEP: delete the pod
Jan  3 14:03:48.365: INFO: Waiting for pod pod-projected-secrets-cd5b641d-b8bb-4359-89d4-e5315527c794 to disappear
Jan  3 14:03:48.366: INFO: Pod pod-projected-secrets-cd5b641d-b8bb-4359-89d4-e5315527c794 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:03:48.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1708" for this suite.
Jan  3 14:03:54.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:03:54.417: INFO: namespace projected-1708 deletion completed in 6.04820394s

• [SLOW TEST:14.140 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:03:54.417: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0103 14:04:24.964446      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan  3 14:04:24.964: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:04:24.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-372" for this suite.
Jan  3 14:04:30.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:04:31.021: INFO: namespace gc-372 deletion completed in 6.051331783s

• [SLOW TEST:36.603 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:04:31.021: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1124
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-1124
Jan  3 14:04:31.042: INFO: Found 0 stateful pods, waiting for 1
Jan  3 14:04:41.044: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jan  3 14:04:41.051: INFO: Deleting all statefulset in ns statefulset-1124
Jan  3 14:04:41.052: INFO: Scaling statefulset ss to 0
Jan  3 14:04:51.063: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 14:04:51.064: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:04:51.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1124" for this suite.
Jan  3 14:04:57.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:04:57.116: INFO: namespace statefulset-1124 deletion completed in 6.046193515s

• [SLOW TEST:26.096 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:04:57.117: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Jan  3 14:04:57.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-6461 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan  3 14:04:57.270: INFO: stderr: ""
Jan  3 14:04:57.270: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Jan  3 14:04:57.270: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan  3 14:04:57.270: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6461" to be "running and ready, or succeeded"
Jan  3 14:04:57.271: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.517498ms
Jan  3 14:04:59.274: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004551232s
Jan  3 14:05:01.276: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006506998s
Jan  3 14:05:03.278: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008231864s
Jan  3 14:05:05.283: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 8.013436646s
Jan  3 14:05:05.283: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan  3 14:05:05.283: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan  3 14:05:05.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 logs logs-generator logs-generator --namespace=kubectl-6461'
Jan  3 14:05:05.660: INFO: stderr: ""
Jan  3 14:05:05.660: INFO: stdout: "I0103 14:05:04.445087       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/xjzk 420\nI0103 14:05:04.648997       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/9l2 513\nI0103 14:05:04.848340       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/4bft 240\nI0103 14:05:05.045174       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/49jk 402\nI0103 14:05:05.249411       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/gvg5 329\nI0103 14:05:05.445540       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/n7g5 481\nI0103 14:05:05.645219       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/88f 424\n"
STEP: limiting log lines
Jan  3 14:05:05.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 logs logs-generator logs-generator --namespace=kubectl-6461 --tail=1'
Jan  3 14:05:05.839: INFO: stderr: ""
Jan  3 14:05:05.839: INFO: stdout: "I0103 14:05:05.645219       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/88f 424\n"
STEP: limiting log bytes
Jan  3 14:05:05.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 logs logs-generator logs-generator --namespace=kubectl-6461 --limit-bytes=1'
Jan  3 14:05:05.954: INFO: stderr: ""
Jan  3 14:05:05.954: INFO: stdout: "I"
STEP: exposing timestamps
Jan  3 14:05:05.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 logs logs-generator logs-generator --namespace=kubectl-6461 --tail=1 --timestamps'
Jan  3 14:05:06.069: INFO: stderr: ""
Jan  3 14:05:06.069: INFO: stdout: "2020-01-03T14:05:06.045490092Z I0103 14:05:06.045358       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/k4pk 389\n"
STEP: restricting to a time range
Jan  3 14:05:08.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 logs logs-generator logs-generator --namespace=kubectl-6461 --since=1s'
Jan  3 14:05:08.802: INFO: stderr: ""
Jan  3 14:05:08.803: INFO: stdout: "I0103 14:05:07.850038       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/wbbh 494\nI0103 14:05:08.052581       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/2g7d 295\nI0103 14:05:08.275563       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/877x 488\nI0103 14:05:08.445666       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/jcg 460\nI0103 14:05:08.705676       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/4q6 241\n"
Jan  3 14:05:08.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 logs logs-generator logs-generator --namespace=kubectl-6461 --since=24h'
Jan  3 14:05:08.933: INFO: stderr: ""
Jan  3 14:05:08.933: INFO: stdout: "I0103 14:05:04.445087       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/xjzk 420\nI0103 14:05:04.648997       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/9l2 513\nI0103 14:05:04.848340       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/4bft 240\nI0103 14:05:05.045174       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/49jk 402\nI0103 14:05:05.249411       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/gvg5 329\nI0103 14:05:05.445540       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/n7g5 481\nI0103 14:05:05.645219       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/88f 424\nI0103 14:05:05.845173       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/62v4 472\nI0103 14:05:06.045358       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/k4pk 389\nI0103 14:05:06.245170       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/46s 344\nI0103 14:05:06.445225       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/8lz 376\nI0103 14:05:06.645174       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/9bj4 222\nI0103 14:05:06.846048       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/skbn 271\nI0103 14:05:07.045178       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/77c 259\nI0103 14:05:07.245187       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/z6nx 360\nI0103 14:05:07.445198       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/x9ql 338\nI0103 14:05:07.645189       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/4p8g 244\nI0103 14:05:07.850038       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/wbbh 494\nI0103 14:05:08.052581       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/2g7d 295\nI0103 14:05:08.275563       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/877x 488\nI0103 14:05:08.445666       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/jcg 460\nI0103 14:05:08.705676       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/4q6 241\nI0103 14:05:08.846478       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/q4z 485\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Jan  3 14:05:08.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete pod logs-generator --namespace=kubectl-6461'
Jan  3 14:05:16.920: INFO: stderr: ""
Jan  3 14:05:16.920: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:05:16.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6461" for this suite.
Jan  3 14:05:22.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:05:22.970: INFO: namespace kubectl-6461 deletion completed in 6.04841867s

• [SLOW TEST:25.853 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:05:22.971: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan  3 14:05:22.991: INFO: Waiting up to 5m0s for pod "pod-9738595a-bce1-49c8-8c4c-acd4dfe23ac3" in namespace "emptydir-5349" to be "success or failure"
Jan  3 14:05:23.020: INFO: Pod "pod-9738595a-bce1-49c8-8c4c-acd4dfe23ac3": Phase="Pending", Reason="", readiness=false. Elapsed: 28.405699ms
Jan  3 14:05:25.021: INFO: Pod "pod-9738595a-bce1-49c8-8c4c-acd4dfe23ac3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029935641s
Jan  3 14:05:27.022: INFO: Pod "pod-9738595a-bce1-49c8-8c4c-acd4dfe23ac3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031376326s
Jan  3 14:05:29.025: INFO: Pod "pod-9738595a-bce1-49c8-8c4c-acd4dfe23ac3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033947433s
Jan  3 14:05:31.027: INFO: Pod "pod-9738595a-bce1-49c8-8c4c-acd4dfe23ac3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.035503932s
Jan  3 14:05:33.028: INFO: Pod "pod-9738595a-bce1-49c8-8c4c-acd4dfe23ac3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.0372705s
STEP: Saw pod success
Jan  3 14:05:33.028: INFO: Pod "pod-9738595a-bce1-49c8-8c4c-acd4dfe23ac3" satisfied condition "success or failure"
Jan  3 14:05:33.029: INFO: Trying to get logs from node controller-1 pod pod-9738595a-bce1-49c8-8c4c-acd4dfe23ac3 container test-container: <nil>
STEP: delete the pod
Jan  3 14:05:33.037: INFO: Waiting for pod pod-9738595a-bce1-49c8-8c4c-acd4dfe23ac3 to disappear
Jan  3 14:05:33.038: INFO: Pod pod-9738595a-bce1-49c8-8c4c-acd4dfe23ac3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:05:33.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5349" for this suite.
Jan  3 14:05:39.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:05:39.185: INFO: namespace emptydir-5349 deletion completed in 6.143772846s

• [SLOW TEST:16.214 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:05:39.185: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:05:39.203: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan  3 14:05:44.220: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan  3 14:05:48.252: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan  3 14:05:50.253: INFO: Creating deployment "test-rollover-deployment"
Jan  3 14:05:50.256: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan  3 14:05:52.268: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan  3 14:05:52.272: INFO: Ensure that both replica sets have 1 created replica
Jan  3 14:05:52.274: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan  3 14:05:52.277: INFO: Updating deployment test-rollover-deployment
Jan  3 14:05:52.277: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan  3 14:05:54.281: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan  3 14:05:54.283: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan  3 14:05:54.285: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 14:05:54.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657152, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:05:56.289: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 14:05:56.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657152, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:05:58.293: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 14:05:58.293: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657152, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:06:00.289: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 14:06:00.289: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657159, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:06:02.289: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 14:06:02.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657159, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:06:04.288: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 14:06:04.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657159, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:06:06.289: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 14:06:06.289: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657159, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:06:08.289: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 14:06:08.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657159, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657150, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:06:10.288: INFO: 
Jan  3 14:06:10.288: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jan  3 14:06:10.292: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-219 /apis/apps/v1/namespaces/deployment-219/deployments/test-rollover-deployment e195fd25-5b36-49f5-966f-0f2098535ce1 57484 2 2020-01-03 14:05:50 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0054b8c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-01-03 14:05:50 +0000 UTC,LastTransitionTime:2020-01-03 14:05:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-01-03 14:06:09 +0000 UTC,LastTransitionTime:2020-01-03 14:05:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  3 14:06:10.293: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-219 /apis/apps/v1/namespaces/deployment-219/replicasets/test-rollover-deployment-7d7dc6548c 1f13049d-d763-4439-a889-1a47ddf8baf6 57475 2 2020-01-03 14:05:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e195fd25-5b36-49f5-966f-0f2098535ce1 0xc0026b7417 0xc0026b7418}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0026b7478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  3 14:06:10.293: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan  3 14:06:10.293: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-219 /apis/apps/v1/namespaces/deployment-219/replicasets/test-rollover-controller 73fcf641-76b8-45a5-b4e5-ccbb3f88d6a8 57483 2 2020-01-03 14:05:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e195fd25-5b36-49f5-966f-0f2098535ce1 0xc0026b7347 0xc0026b7348}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0026b73a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 14:06:10.293: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-219 /apis/apps/v1/namespaces/deployment-219/replicasets/test-rollover-deployment-f6c94f66c f047f34d-00c4-49ed-bf93-d165dc736575 57400 2 2020-01-03 14:05:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e195fd25-5b36-49f5-966f-0f2098535ce1 0xc0026b74e0 0xc0026b74e1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0026b7558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 14:06:10.295: INFO: Pod "test-rollover-deployment-7d7dc6548c-q5wsz" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-q5wsz test-rollover-deployment-7d7dc6548c- deployment-219 /api/v1/namespaces/deployment-219/pods/test-rollover-deployment-7d7dc6548c-q5wsz ec37dca5-8167-4729-922c-a70637094cba 57442 0 2020-01-03 14:05:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:172.16.166.184/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.184"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 1f13049d-d763-4439-a889-1a47ddf8baf6 0xc0026b7aa7 0xc0026b7aa8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4f5mj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4f5mj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4f5mj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 14:05:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 14:05:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 14:05:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 14:05:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.12,PodIP:172.16.166.184,StartTime:2020-01-03 14:05:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-03 14:05:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://ba41b4f89cb4d0a37fb700e449ccd48d9f5d7d01ba1cace1153d9d504263ae3e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:06:10.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-219" for this suite.
Jan  3 14:06:16.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:06:16.344: INFO: namespace deployment-219 deletion completed in 6.047044656s

• [SLOW TEST:37.159 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:06:16.344: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-1643d216-bbc6-40e3-b187-391e9e88895a in namespace container-probe-1494
Jan  3 14:06:24.362: INFO: Started pod busybox-1643d216-bbc6-40e3-b187-391e9e88895a in namespace container-probe-1494
STEP: checking the pod's current state and verifying that restartCount is present
Jan  3 14:06:24.363: INFO: Initial restart count of pod busybox-1643d216-bbc6-40e3-b187-391e9e88895a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:10:25.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1494" for this suite.
Jan  3 14:10:31.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:10:31.328: INFO: namespace container-probe-1494 deletion completed in 6.119953286s

• [SLOW TEST:254.984 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:10:31.329: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Jan  3 14:10:31.372: INFO: Waiting up to 5m0s for pod "var-expansion-30aad9b4-94e2-4734-b1ea-36bf8b59a5c0" in namespace "var-expansion-2888" to be "success or failure"
Jan  3 14:10:31.376: INFO: Pod "var-expansion-30aad9b4-94e2-4734-b1ea-36bf8b59a5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028335ms
Jan  3 14:10:33.377: INFO: Pod "var-expansion-30aad9b4-94e2-4734-b1ea-36bf8b59a5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005459514s
Jan  3 14:10:35.379: INFO: Pod "var-expansion-30aad9b4-94e2-4734-b1ea-36bf8b59a5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006808998s
Jan  3 14:10:37.385: INFO: Pod "var-expansion-30aad9b4-94e2-4734-b1ea-36bf8b59a5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012612959s
Jan  3 14:10:39.407: INFO: Pod "var-expansion-30aad9b4-94e2-4734-b1ea-36bf8b59a5c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.034928557s
STEP: Saw pod success
Jan  3 14:10:39.411: INFO: Pod "var-expansion-30aad9b4-94e2-4734-b1ea-36bf8b59a5c0" satisfied condition "success or failure"
Jan  3 14:10:39.413: INFO: Trying to get logs from node controller-0 pod var-expansion-30aad9b4-94e2-4734-b1ea-36bf8b59a5c0 container dapi-container: <nil>
STEP: delete the pod
Jan  3 14:10:39.456: INFO: Waiting for pod var-expansion-30aad9b4-94e2-4734-b1ea-36bf8b59a5c0 to disappear
Jan  3 14:10:39.472: INFO: Pod var-expansion-30aad9b4-94e2-4734-b1ea-36bf8b59a5c0 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:10:39.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2888" for this suite.
Jan  3 14:10:45.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:10:45.600: INFO: namespace var-expansion-2888 deletion completed in 6.125273148s

• [SLOW TEST:14.271 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:10:45.601: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Jan  3 14:10:45.636: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-937082782 proxy --unix-socket=/tmp/kubectl-proxy-unix029872701/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:10:45.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1246" for this suite.
Jan  3 14:10:51.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:10:51.784: INFO: namespace kubectl-1246 deletion completed in 6.063029591s

• [SLOW TEST:6.184 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:10:51.791: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 14:10:52.211: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 14:10:54.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:10:56.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:10:58.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:11:00.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657452, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 14:11:03.230: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jan  3 14:11:03.241: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:11:03.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6431" for this suite.
Jan  3 14:11:09.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:11:09.309: INFO: namespace webhook-6431 deletion completed in 6.060825915s
STEP: Destroying namespace "webhook-6431-markers" for this suite.
Jan  3 14:11:15.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:11:15.424: INFO: namespace webhook-6431-markers deletion completed in 6.114404127s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.640 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:11:15.431: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan  3 14:11:15.473: INFO: Waiting up to 5m0s for pod "pod-43586956-da69-4f32-8ffb-24b6863061a4" in namespace "emptydir-5411" to be "success or failure"
Jan  3 14:11:15.479: INFO: Pod "pod-43586956-da69-4f32-8ffb-24b6863061a4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.335715ms
Jan  3 14:11:17.482: INFO: Pod "pod-43586956-da69-4f32-8ffb-24b6863061a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003885313s
Jan  3 14:11:19.484: INFO: Pod "pod-43586956-da69-4f32-8ffb-24b6863061a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005921443s
Jan  3 14:11:21.494: INFO: Pod "pod-43586956-da69-4f32-8ffb-24b6863061a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016012334s
Jan  3 14:11:23.495: INFO: Pod "pod-43586956-da69-4f32-8ffb-24b6863061a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.017457306s
STEP: Saw pod success
Jan  3 14:11:23.496: INFO: Pod "pod-43586956-da69-4f32-8ffb-24b6863061a4" satisfied condition "success or failure"
Jan  3 14:11:23.497: INFO: Trying to get logs from node controller-1 pod pod-43586956-da69-4f32-8ffb-24b6863061a4 container test-container: <nil>
STEP: delete the pod
Jan  3 14:11:23.573: INFO: Waiting for pod pod-43586956-da69-4f32-8ffb-24b6863061a4 to disappear
Jan  3 14:11:23.575: INFO: Pod pod-43586956-da69-4f32-8ffb-24b6863061a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:11:23.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5411" for this suite.
Jan  3 14:11:29.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:11:29.677: INFO: namespace emptydir-5411 deletion completed in 6.097598449s

• [SLOW TEST:14.246 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:11:29.695: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 14:11:29.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8b1fd5d7-dc68-4a63-9571-f312184f2594" in namespace "projected-3285" to be "success or failure"
Jan  3 14:11:29.727: INFO: Pod "downwardapi-volume-8b1fd5d7-dc68-4a63-9571-f312184f2594": Phase="Pending", Reason="", readiness=false. Elapsed: 1.170648ms
Jan  3 14:11:31.729: INFO: Pod "downwardapi-volume-8b1fd5d7-dc68-4a63-9571-f312184f2594": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003562193s
Jan  3 14:11:33.731: INFO: Pod "downwardapi-volume-8b1fd5d7-dc68-4a63-9571-f312184f2594": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005348519s
Jan  3 14:11:35.733: INFO: Pod "downwardapi-volume-8b1fd5d7-dc68-4a63-9571-f312184f2594": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007024736s
Jan  3 14:11:37.734: INFO: Pod "downwardapi-volume-8b1fd5d7-dc68-4a63-9571-f312184f2594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008371946s
STEP: Saw pod success
Jan  3 14:11:37.734: INFO: Pod "downwardapi-volume-8b1fd5d7-dc68-4a63-9571-f312184f2594" satisfied condition "success or failure"
Jan  3 14:11:37.735: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-8b1fd5d7-dc68-4a63-9571-f312184f2594 container client-container: <nil>
STEP: delete the pod
Jan  3 14:11:37.743: INFO: Waiting for pod downwardapi-volume-8b1fd5d7-dc68-4a63-9571-f312184f2594 to disappear
Jan  3 14:11:37.744: INFO: Pod downwardapi-volume-8b1fd5d7-dc68-4a63-9571-f312184f2594 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:11:37.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3285" for this suite.
Jan  3 14:11:43.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:11:43.816: INFO: namespace projected-3285 deletion completed in 6.069928922s

• [SLOW TEST:14.122 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:11:43.828: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 14:11:45.343: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan  3 14:11:47.348: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657505, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657505, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657505, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657505, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:11:49.361: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657505, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657505, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657505, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657505, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:11:51.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657505, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657505, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657505, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657505, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 14:11:54.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:11:54.358: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5523-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:11:55.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8212" for this suite.
Jan  3 14:12:01.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:12:01.529: INFO: namespace webhook-8212 deletion completed in 6.083253778s
STEP: Destroying namespace "webhook-8212-markers" for this suite.
Jan  3 14:12:07.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:12:07.569: INFO: namespace webhook-8212-markers deletion completed in 6.040035165s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.745 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:12:07.574: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6577.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6577.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan  3 14:12:17.610: INFO: DNS probes using dns-6577/dns-test-6063de26-bb9d-4800-be76-9ecef80289a0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:12:17.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6577" for this suite.
Jan  3 14:12:23.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:12:23.664: INFO: namespace dns-6577 deletion completed in 6.037912028s

• [SLOW TEST:16.090 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:12:23.665: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3384
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3384
STEP: Creating statefulset with conflicting port in namespace statefulset-3384
STEP: Waiting until pod test-pod will start running in namespace statefulset-3384
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3384
Jan  3 14:12:31.692: INFO: Observed stateful pod in namespace: statefulset-3384, name: ss-0, uid: b5505f6f-f64a-43d3-9b57-3e4c2dedf5c1, status phase: Pending. Waiting for statefulset controller to delete.
Jan  3 14:12:32.132: INFO: Observed stateful pod in namespace: statefulset-3384, name: ss-0, uid: b5505f6f-f64a-43d3-9b57-3e4c2dedf5c1, status phase: Failed. Waiting for statefulset controller to delete.
Jan  3 14:12:32.135: INFO: Observed stateful pod in namespace: statefulset-3384, name: ss-0, uid: b5505f6f-f64a-43d3-9b57-3e4c2dedf5c1, status phase: Failed. Waiting for statefulset controller to delete.
Jan  3 14:12:32.137: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3384
STEP: Removing pod with conflicting port in namespace statefulset-3384
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3384 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jan  3 14:12:42.173: INFO: Deleting all statefulset in ns statefulset-3384
Jan  3 14:12:42.175: INFO: Scaling statefulset ss to 0
Jan  3 14:12:52.181: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 14:12:52.182: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:12:52.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3384" for this suite.
Jan  3 14:12:58.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:12:58.240: INFO: namespace statefulset-3384 deletion completed in 6.050627825s

• [SLOW TEST:34.575 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:12:58.240: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:13:06.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6756" for this suite.
Jan  3 14:13:16.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:13:16.349: INFO: namespace containers-6756 deletion completed in 10.05226093s

• [SLOW TEST:18.109 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:13:16.349: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:13:35.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-436" for this suite.
Jan  3 14:13:41.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:13:41.569: INFO: namespace namespaces-436 deletion completed in 6.103667507s
STEP: Destroying namespace "nsdeletetest-7323" for this suite.
Jan  3 14:13:41.570: INFO: Namespace nsdeletetest-7323 was already deleted
STEP: Destroying namespace "nsdeletetest-9615" for this suite.
Jan  3 14:13:47.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:13:47.602: INFO: namespace nsdeletetest-9615 deletion completed in 6.032163526s

• [SLOW TEST:31.253 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:13:47.604: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan  3 14:13:47.618: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:13:50.707: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:14:03.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9334" for this suite.
Jan  3 14:14:09.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:14:09.320: INFO: namespace crd-publish-openapi-9334 deletion completed in 6.078490023s

• [SLOW TEST:21.717 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:14:09.321: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-8151/configmap-test-0196c898-9387-4845-b43e-7de1893a412a
STEP: Creating a pod to test consume configMaps
Jan  3 14:14:09.352: INFO: Waiting up to 5m0s for pod "pod-configmaps-d3ae7c32-b9dd-414f-8f51-ef8b5cc65c1f" in namespace "configmap-8151" to be "success or failure"
Jan  3 14:14:09.359: INFO: Pod "pod-configmaps-d3ae7c32-b9dd-414f-8f51-ef8b5cc65c1f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.725814ms
Jan  3 14:14:11.362: INFO: Pod "pod-configmaps-d3ae7c32-b9dd-414f-8f51-ef8b5cc65c1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009398295s
Jan  3 14:14:13.376: INFO: Pod "pod-configmaps-d3ae7c32-b9dd-414f-8f51-ef8b5cc65c1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023811576s
Jan  3 14:14:15.407: INFO: Pod "pod-configmaps-d3ae7c32-b9dd-414f-8f51-ef8b5cc65c1f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054569727s
Jan  3 14:14:17.409: INFO: Pod "pod-configmaps-d3ae7c32-b9dd-414f-8f51-ef8b5cc65c1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.05642659s
STEP: Saw pod success
Jan  3 14:14:17.409: INFO: Pod "pod-configmaps-d3ae7c32-b9dd-414f-8f51-ef8b5cc65c1f" satisfied condition "success or failure"
Jan  3 14:14:17.410: INFO: Trying to get logs from node controller-0 pod pod-configmaps-d3ae7c32-b9dd-414f-8f51-ef8b5cc65c1f container env-test: <nil>
STEP: delete the pod
Jan  3 14:14:17.421: INFO: Waiting for pod pod-configmaps-d3ae7c32-b9dd-414f-8f51-ef8b5cc65c1f to disappear
Jan  3 14:14:17.422: INFO: Pod pod-configmaps-d3ae7c32-b9dd-414f-8f51-ef8b5cc65c1f no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:14:17.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8151" for this suite.
Jan  3 14:14:23.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:14:23.458: INFO: namespace configmap-8151 deletion completed in 6.034660164s

• [SLOW TEST:14.138 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:14:23.459: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan  3 14:14:23.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9188'
Jan  3 14:14:23.538: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan  3 14:14:23.538: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Jan  3 14:14:23.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete deployment e2e-test-httpd-deployment --namespace=kubectl-9188'
Jan  3 14:14:23.613: INFO: stderr: ""
Jan  3 14:14:23.613: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:14:23.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9188" for this suite.
Jan  3 14:14:29.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:14:29.754: INFO: namespace kubectl-9188 deletion completed in 6.13838648s

• [SLOW TEST:6.295 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:14:29.754: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:14:29.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5832" for this suite.
Jan  3 14:14:35.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:14:35.872: INFO: namespace kubelet-test-5832 deletion completed in 6.037785405s

• [SLOW TEST:6.118 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:14:35.873: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jan  3 14:14:35.888: INFO: Waiting up to 5m0s for pod "downward-api-578b5cff-bdd0-4b55-a0f8-f2c813e6ff41" in namespace "downward-api-137" to be "success or failure"
Jan  3 14:14:35.890: INFO: Pod "downward-api-578b5cff-bdd0-4b55-a0f8-f2c813e6ff41": Phase="Pending", Reason="", readiness=false. Elapsed: 1.514407ms
Jan  3 14:14:37.892: INFO: Pod "downward-api-578b5cff-bdd0-4b55-a0f8-f2c813e6ff41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003466726s
Jan  3 14:14:39.897: INFO: Pod "downward-api-578b5cff-bdd0-4b55-a0f8-f2c813e6ff41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008387574s
Jan  3 14:14:41.900: INFO: Pod "downward-api-578b5cff-bdd0-4b55-a0f8-f2c813e6ff41": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011367166s
Jan  3 14:14:43.949: INFO: Pod "downward-api-578b5cff-bdd0-4b55-a0f8-f2c813e6ff41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.061082927s
STEP: Saw pod success
Jan  3 14:14:43.949: INFO: Pod "downward-api-578b5cff-bdd0-4b55-a0f8-f2c813e6ff41" satisfied condition "success or failure"
Jan  3 14:14:43.950: INFO: Trying to get logs from node controller-1 pod downward-api-578b5cff-bdd0-4b55-a0f8-f2c813e6ff41 container dapi-container: <nil>
STEP: delete the pod
Jan  3 14:14:43.988: INFO: Waiting for pod downward-api-578b5cff-bdd0-4b55-a0f8-f2c813e6ff41 to disappear
Jan  3 14:14:43.991: INFO: Pod downward-api-578b5cff-bdd0-4b55-a0f8-f2c813e6ff41 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:14:43.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-137" for this suite.
Jan  3 14:14:50.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:14:50.243: INFO: namespace downward-api-137 deletion completed in 6.044104422s

• [SLOW TEST:14.370 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:14:50.243: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jan  3 14:14:50.258: INFO: Waiting up to 5m0s for pod "downward-api-370a8078-c5e3-4451-b66d-d9cd375ebc88" in namespace "downward-api-3761" to be "success or failure"
Jan  3 14:14:50.260: INFO: Pod "downward-api-370a8078-c5e3-4451-b66d-d9cd375ebc88": Phase="Pending", Reason="", readiness=false. Elapsed: 1.83883ms
Jan  3 14:14:52.262: INFO: Pod "downward-api-370a8078-c5e3-4451-b66d-d9cd375ebc88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003457173s
Jan  3 14:14:54.265: INFO: Pod "downward-api-370a8078-c5e3-4451-b66d-d9cd375ebc88": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006760185s
Jan  3 14:14:56.267: INFO: Pod "downward-api-370a8078-c5e3-4451-b66d-d9cd375ebc88": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008327361s
Jan  3 14:14:58.269: INFO: Pod "downward-api-370a8078-c5e3-4451-b66d-d9cd375ebc88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.010548073s
STEP: Saw pod success
Jan  3 14:14:58.269: INFO: Pod "downward-api-370a8078-c5e3-4451-b66d-d9cd375ebc88" satisfied condition "success or failure"
Jan  3 14:14:58.270: INFO: Trying to get logs from node controller-0 pod downward-api-370a8078-c5e3-4451-b66d-d9cd375ebc88 container dapi-container: <nil>
STEP: delete the pod
Jan  3 14:14:58.278: INFO: Waiting for pod downward-api-370a8078-c5e3-4451-b66d-d9cd375ebc88 to disappear
Jan  3 14:14:58.279: INFO: Pod downward-api-370a8078-c5e3-4451-b66d-d9cd375ebc88 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:14:58.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3761" for this suite.
Jan  3 14:15:04.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:15:04.324: INFO: namespace downward-api-3761 deletion completed in 6.042469679s

• [SLOW TEST:14.081 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:15:04.324: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:15:15.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4943" for this suite.
Jan  3 14:15:21.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:15:21.439: INFO: namespace resourcequota-4943 deletion completed in 6.066739369s

• [SLOW TEST:17.115 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:15:21.439: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan  3 14:15:21.815: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan  3 14:15:23.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657721, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657721, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657721, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657721, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:15:25.822: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657721, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657721, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657721, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657721, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:15:27.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657721, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657721, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657721, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657721, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 14:15:30.830: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:15:30.831: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:15:32.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7417" for this suite.
Jan  3 14:15:38.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:15:38.210: INFO: namespace crd-webhook-7417 deletion completed in 6.04617866s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:16.777 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:15:38.216: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-9d693dfe-4165-45a6-9811-f36f34970814
STEP: Creating a pod to test consume configMaps
Jan  3 14:15:38.233: INFO: Waiting up to 5m0s for pod "pod-configmaps-167ef7d5-611d-4298-b710-394007a155c2" in namespace "configmap-1265" to be "success or failure"
Jan  3 14:15:38.234: INFO: Pod "pod-configmaps-167ef7d5-611d-4298-b710-394007a155c2": Phase="Pending", Reason="", readiness=false. Elapsed: 997.911µs
Jan  3 14:15:40.252: INFO: Pod "pod-configmaps-167ef7d5-611d-4298-b710-394007a155c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018933283s
Jan  3 14:15:42.277: INFO: Pod "pod-configmaps-167ef7d5-611d-4298-b710-394007a155c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043569681s
Jan  3 14:15:44.286: INFO: Pod "pod-configmaps-167ef7d5-611d-4298-b710-394007a155c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052391962s
Jan  3 14:15:46.288: INFO: Pod "pod-configmaps-167ef7d5-611d-4298-b710-394007a155c2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.054974803s
Jan  3 14:15:48.290: INFO: Pod "pod-configmaps-167ef7d5-611d-4298-b710-394007a155c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.056764416s
STEP: Saw pod success
Jan  3 14:15:48.290: INFO: Pod "pod-configmaps-167ef7d5-611d-4298-b710-394007a155c2" satisfied condition "success or failure"
Jan  3 14:15:48.291: INFO: Trying to get logs from node controller-1 pod pod-configmaps-167ef7d5-611d-4298-b710-394007a155c2 container configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 14:15:48.300: INFO: Waiting for pod pod-configmaps-167ef7d5-611d-4298-b710-394007a155c2 to disappear
Jan  3 14:15:48.306: INFO: Pod pod-configmaps-167ef7d5-611d-4298-b710-394007a155c2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:15:48.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1265" for this suite.
Jan  3 14:15:54.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:15:54.342: INFO: namespace configmap-1265 deletion completed in 6.034229692s

• [SLOW TEST:16.126 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:15:54.342: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-69201128-8c09-404b-8677-f288f849f94a
STEP: Creating a pod to test consume secrets
Jan  3 14:15:54.361: INFO: Waiting up to 5m0s for pod "pod-secrets-01e99188-d61e-468b-84cd-e199146c29f7" in namespace "secrets-6859" to be "success or failure"
Jan  3 14:15:54.371: INFO: Pod "pod-secrets-01e99188-d61e-468b-84cd-e199146c29f7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004933ms
Jan  3 14:15:56.372: INFO: Pod "pod-secrets-01e99188-d61e-468b-84cd-e199146c29f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011361222s
Jan  3 14:15:58.374: INFO: Pod "pod-secrets-01e99188-d61e-468b-84cd-e199146c29f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013339427s
Jan  3 14:16:00.376: INFO: Pod "pod-secrets-01e99188-d61e-468b-84cd-e199146c29f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015235957s
Jan  3 14:16:02.378: INFO: Pod "pod-secrets-01e99188-d61e-468b-84cd-e199146c29f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.017422261s
STEP: Saw pod success
Jan  3 14:16:02.378: INFO: Pod "pod-secrets-01e99188-d61e-468b-84cd-e199146c29f7" satisfied condition "success or failure"
Jan  3 14:16:02.379: INFO: Trying to get logs from node controller-1 pod pod-secrets-01e99188-d61e-468b-84cd-e199146c29f7 container secret-volume-test: <nil>
STEP: delete the pod
Jan  3 14:16:02.388: INFO: Waiting for pod pod-secrets-01e99188-d61e-468b-84cd-e199146c29f7 to disappear
Jan  3 14:16:02.390: INFO: Pod pod-secrets-01e99188-d61e-468b-84cd-e199146c29f7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:16:02.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6859" for this suite.
Jan  3 14:16:08.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:16:08.445: INFO: namespace secrets-6859 deletion completed in 6.052139975s

• [SLOW TEST:14.103 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:16:08.446: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Jan  3 14:16:08.461: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8119" to be "success or failure"
Jan  3 14:16:08.462: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 1.053848ms
Jan  3 14:16:10.463: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002510171s
Jan  3 14:16:12.466: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005049088s
Jan  3 14:16:14.471: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009908704s
Jan  3 14:16:16.472: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011581344s
Jan  3 14:16:18.474: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.013261106s
STEP: Saw pod success
Jan  3 14:16:18.474: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jan  3 14:16:18.482: INFO: Trying to get logs from node controller-1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jan  3 14:16:18.492: INFO: Waiting for pod pod-host-path-test to disappear
Jan  3 14:16:18.493: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:16:18.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-8119" for this suite.
Jan  3 14:16:24.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:16:24.547: INFO: namespace hostpath-8119 deletion completed in 6.051246518s

• [SLOW TEST:16.101 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:16:24.548: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:16:24.569: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0044d8c3-03e0-4f75-b0be-a6e540501351" in namespace "security-context-test-2951" to be "success or failure"
Jan  3 14:16:24.571: INFO: Pod "alpine-nnp-false-0044d8c3-03e0-4f75-b0be-a6e540501351": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070332ms
Jan  3 14:16:26.573: INFO: Pod "alpine-nnp-false-0044d8c3-03e0-4f75-b0be-a6e540501351": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003823616s
Jan  3 14:16:28.575: INFO: Pod "alpine-nnp-false-0044d8c3-03e0-4f75-b0be-a6e540501351": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005270941s
Jan  3 14:16:30.576: INFO: Pod "alpine-nnp-false-0044d8c3-03e0-4f75-b0be-a6e540501351": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006746275s
Jan  3 14:16:32.577: INFO: Pod "alpine-nnp-false-0044d8c3-03e0-4f75-b0be-a6e540501351": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008211997s
Jan  3 14:16:34.579: INFO: Pod "alpine-nnp-false-0044d8c3-03e0-4f75-b0be-a6e540501351": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009997336s
Jan  3 14:16:36.582: INFO: Pod "alpine-nnp-false-0044d8c3-03e0-4f75-b0be-a6e540501351": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013068724s
Jan  3 14:16:38.584: INFO: Pod "alpine-nnp-false-0044d8c3-03e0-4f75-b0be-a6e540501351": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01466007s
Jan  3 14:16:40.586: INFO: Pod "alpine-nnp-false-0044d8c3-03e0-4f75-b0be-a6e540501351": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.016731031s
Jan  3 14:16:40.586: INFO: Pod "alpine-nnp-false-0044d8c3-03e0-4f75-b0be-a6e540501351" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:16:40.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2951" for this suite.
Jan  3 14:16:46.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:16:46.626: INFO: namespace security-context-test-2951 deletion completed in 6.033632247s

• [SLOW TEST:22.079 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:16:46.627: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan  3 14:17:02.674: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  3 14:17:02.677: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  3 14:17:04.677: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  3 14:17:04.680: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  3 14:17:06.677: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  3 14:17:06.679: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  3 14:17:08.677: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  3 14:17:08.679: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  3 14:17:10.677: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  3 14:17:10.678: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:17:10.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2125" for this suite.
Jan  3 14:17:38.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:17:38.796: INFO: namespace container-lifecycle-hook-2125 deletion completed in 28.111944893s

• [SLOW TEST:52.169 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:17:38.796: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 14:17:39.472: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 14:17:41.502: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:17:43.507: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:17:45.525: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:17:47.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657859, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 14:17:50.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:17:50.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1845" for this suite.
Jan  3 14:17:56.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:17:56.593: INFO: namespace webhook-1845 deletion completed in 6.035608688s
STEP: Destroying namespace "webhook-1845-markers" for this suite.
Jan  3 14:18:02.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:18:02.625: INFO: namespace webhook-1845-markers deletion completed in 6.03147898s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.833 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:18:02.630: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan  3 14:18:02.645: INFO: Waiting up to 5m0s for pod "pod-9e5a123c-9d5b-49e1-abab-a5d4632cc5b3" in namespace "emptydir-3635" to be "success or failure"
Jan  3 14:18:02.647: INFO: Pod "pod-9e5a123c-9d5b-49e1-abab-a5d4632cc5b3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.820025ms
Jan  3 14:18:04.651: INFO: Pod "pod-9e5a123c-9d5b-49e1-abab-a5d4632cc5b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005388673s
Jan  3 14:18:06.653: INFO: Pod "pod-9e5a123c-9d5b-49e1-abab-a5d4632cc5b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007159056s
Jan  3 14:18:08.655: INFO: Pod "pod-9e5a123c-9d5b-49e1-abab-a5d4632cc5b3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009266675s
Jan  3 14:18:10.658: INFO: Pod "pod-9e5a123c-9d5b-49e1-abab-a5d4632cc5b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.012609233s
STEP: Saw pod success
Jan  3 14:18:10.658: INFO: Pod "pod-9e5a123c-9d5b-49e1-abab-a5d4632cc5b3" satisfied condition "success or failure"
Jan  3 14:18:10.659: INFO: Trying to get logs from node controller-1 pod pod-9e5a123c-9d5b-49e1-abab-a5d4632cc5b3 container test-container: <nil>
STEP: delete the pod
Jan  3 14:18:10.674: INFO: Waiting for pod pod-9e5a123c-9d5b-49e1-abab-a5d4632cc5b3 to disappear
Jan  3 14:18:10.675: INFO: Pod pod-9e5a123c-9d5b-49e1-abab-a5d4632cc5b3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:18:10.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3635" for this suite.
Jan  3 14:18:16.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:18:16.718: INFO: namespace emptydir-3635 deletion completed in 6.041726225s

• [SLOW TEST:14.089 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:18:16.718: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 14:18:17.195: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 14:18:19.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657897, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657897, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:18:21.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657897, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657897, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:18:23.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657897, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657897, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 14:18:26.214: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:18:26.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3286" for this suite.
Jan  3 14:18:32.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:18:32.273: INFO: namespace webhook-3286 deletion completed in 6.037911278s
STEP: Destroying namespace "webhook-3286-markers" for this suite.
Jan  3 14:18:38.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:18:38.308: INFO: namespace webhook-3286-markers deletion completed in 6.035618801s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.595 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:18:38.314: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 14:18:38.769: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 14:18:40.773: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657918, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657918, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657918, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657918, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:18:42.775: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657918, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657918, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657918, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657918, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:18:44.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657918, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657918, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657918, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713657918, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 14:18:47.779: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
Jan  3 14:18:47.786: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:18:58.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3112" for this suite.
Jan  3 14:19:04.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:19:04.199: INFO: namespace webhook-3112 deletion completed in 6.048517892s
STEP: Destroying namespace "webhook-3112-markers" for this suite.
Jan  3 14:19:10.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:19:10.253: INFO: namespace webhook-3112-markers deletion completed in 6.053614477s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:31.948 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:19:10.262: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan  3 14:19:10.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-3506'
Jan  3 14:19:10.525: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan  3 14:19:10.525: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Jan  3 14:19:10.534: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jan  3 14:19:10.541: INFO: scanned /root for discovery docs: <nil>
Jan  3 14:19:10.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-3506'
Jan  3 14:19:30.315: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan  3 14:19:30.315: INFO: stdout: "Created e2e-test-httpd-rc-cf6d7ea6d09be2e5df9391e657f9a6f2\nScaling up e2e-test-httpd-rc-cf6d7ea6d09be2e5df9391e657f9a6f2 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-cf6d7ea6d09be2e5df9391e657f9a6f2 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-cf6d7ea6d09be2e5df9391e657f9a6f2 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jan  3 14:19:30.315: INFO: stdout: "Created e2e-test-httpd-rc-cf6d7ea6d09be2e5df9391e657f9a6f2\nScaling up e2e-test-httpd-rc-cf6d7ea6d09be2e5df9391e657f9a6f2 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-cf6d7ea6d09be2e5df9391e657f9a6f2 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-cf6d7ea6d09be2e5df9391e657f9a6f2 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jan  3 14:19:30.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-3506'
Jan  3 14:19:30.555: INFO: stderr: ""
Jan  3 14:19:30.555: INFO: stdout: "e2e-test-httpd-rc-cf6d7ea6d09be2e5df9391e657f9a6f2-bsfqx "
Jan  3 14:19:30.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods e2e-test-httpd-rc-cf6d7ea6d09be2e5df9391e657f9a6f2-bsfqx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3506'
Jan  3 14:19:30.649: INFO: stderr: ""
Jan  3 14:19:30.649: INFO: stdout: "true"
Jan  3 14:19:30.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods e2e-test-httpd-rc-cf6d7ea6d09be2e5df9391e657f9a6f2-bsfqx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3506'
Jan  3 14:19:30.714: INFO: stderr: ""
Jan  3 14:19:30.714: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jan  3 14:19:30.714: INFO: e2e-test-httpd-rc-cf6d7ea6d09be2e5df9391e657f9a6f2-bsfqx is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Jan  3 14:19:30.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete rc e2e-test-httpd-rc --namespace=kubectl-3506'
Jan  3 14:19:30.775: INFO: stderr: ""
Jan  3 14:19:30.775: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:19:30.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3506" for this suite.
Jan  3 14:19:36.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:19:36.827: INFO: namespace kubectl-3506 deletion completed in 6.050326586s

• [SLOW TEST:26.565 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:19:36.827: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Jan  3 14:19:45.665: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9673 pod-service-account-823eb943-bee8-4306-a9ab-895a1906ca62 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan  3 14:19:46.382: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9673 pod-service-account-823eb943-bee8-4306-a9ab-895a1906ca62 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan  3 14:19:46.637: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9673 pod-service-account-823eb943-bee8-4306-a9ab-895a1906ca62 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:19:46.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9673" for this suite.
Jan  3 14:19:53.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:19:53.069: INFO: namespace svcaccounts-9673 deletion completed in 6.074291732s

• [SLOW TEST:16.242 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:19:53.070: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2839
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-2839
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2839
Jan  3 14:19:53.102: INFO: Found 0 stateful pods, waiting for 1
Jan  3 14:20:03.104: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan  3 14:20:03.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-2839 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 14:20:03.314: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 14:20:03.314: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 14:20:03.314: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 14:20:03.316: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  3 14:20:13.323: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 14:20:13.323: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 14:20:13.344: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan  3 14:20:13.344: INFO: ss-0  controller-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:19:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:19:53 +0000 UTC  }]
Jan  3 14:20:13.344: INFO: ss-1                Pending         []
Jan  3 14:20:13.345: INFO: 
Jan  3 14:20:13.345: INFO: StatefulSet ss has not reached scale 3, at 2
Jan  3 14:20:14.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.983762579s
Jan  3 14:20:15.349: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981791732s
Jan  3 14:20:16.354: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976444082s
Jan  3 14:20:17.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.973128228s
Jan  3 14:20:18.362: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968130849s
Jan  3 14:20:19.365: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966255808s
Jan  3 14:20:20.381: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963670804s
Jan  3 14:20:21.388: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.947647471s
Jan  3 14:20:22.390: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.466759ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2839
Jan  3 14:20:23.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-2839 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 14:20:23.590: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 14:20:23.590: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 14:20:23.590: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 14:20:23.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-2839 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 14:20:23.809: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  3 14:20:23.810: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 14:20:23.810: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 14:20:23.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-2839 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 14:20:24.081: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  3 14:20:24.081: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 14:20:24.081: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 14:20:24.089: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan  3 14:20:34.092: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:20:34.092: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:20:34.092: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan  3 14:20:34.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-2839 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 14:20:34.342: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 14:20:34.342: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 14:20:34.342: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 14:20:34.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-2839 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 14:20:34.555: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 14:20:34.555: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 14:20:34.555: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 14:20:34.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-2839 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 14:20:34.809: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 14:20:34.809: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 14:20:34.809: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 14:20:34.809: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 14:20:34.810: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan  3 14:20:44.851: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 14:20:44.851: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 14:20:44.851: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 14:20:44.857: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan  3 14:20:44.857: INFO: ss-0  controller-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:19:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:19:53 +0000 UTC  }]
Jan  3 14:20:44.857: INFO: ss-1  controller-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:13 +0000 UTC  }]
Jan  3 14:20:44.857: INFO: ss-2  controller-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:13 +0000 UTC  }]
Jan  3 14:20:44.857: INFO: 
Jan  3 14:20:44.857: INFO: StatefulSet ss has not reached scale 0, at 3
Jan  3 14:20:45.883: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan  3 14:20:45.883: INFO: ss-0  controller-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:19:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:19:53 +0000 UTC  }]
Jan  3 14:20:45.883: INFO: ss-1  controller-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:13 +0000 UTC  }]
Jan  3 14:20:45.883: INFO: ss-2  controller-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:13 +0000 UTC  }]
Jan  3 14:20:45.883: INFO: 
Jan  3 14:20:45.883: INFO: StatefulSet ss has not reached scale 0, at 3
Jan  3 14:20:46.890: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan  3 14:20:46.890: INFO: ss-0  controller-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:19:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:19:53 +0000 UTC  }]
Jan  3 14:20:46.890: INFO: ss-1  controller-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:13 +0000 UTC  }]
Jan  3 14:20:46.890: INFO: ss-2  controller-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-03 14:20:13 +0000 UTC  }]
Jan  3 14:20:46.890: INFO: 
Jan  3 14:20:46.890: INFO: StatefulSet ss has not reached scale 0, at 3
Jan  3 14:20:47.892: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.965787446s
Jan  3 14:20:48.893: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.964138614s
Jan  3 14:20:49.900: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.958851885s
Jan  3 14:20:50.957: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.950605324s
Jan  3 14:20:51.958: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.899133591s
Jan  3 14:20:52.960: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.897608761s
Jan  3 14:20:53.961: INFO: Verifying statefulset ss doesn't scale past 0 for another 895.951473ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2839
Jan  3 14:20:54.963: INFO: Scaling statefulset ss to 0
Jan  3 14:20:54.972: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jan  3 14:20:54.973: INFO: Deleting all statefulset in ns statefulset-2839
Jan  3 14:20:54.973: INFO: Scaling statefulset ss to 0
Jan  3 14:20:54.978: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 14:20:54.979: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:20:54.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2839" for this suite.
Jan  3 14:21:00.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:21:01.071: INFO: namespace statefulset-2839 deletion completed in 6.083594926s

• [SLOW TEST:68.002 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:21:01.071: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0103 14:21:11.130356      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan  3 14:21:11.130: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:21:11.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9205" for this suite.
Jan  3 14:21:17.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:21:17.191: INFO: namespace gc-9205 deletion completed in 6.058642771s

• [SLOW TEST:16.120 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:21:17.192: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-cc61f863-6059-469e-afb2-b70c2a2c92ae
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-cc61f863-6059-469e-afb2-b70c2a2c92ae
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:21:27.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4459" for this suite.
Jan  3 14:21:39.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:21:39.307: INFO: namespace projected-4459 deletion completed in 12.044522035s

• [SLOW TEST:22.115 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:21:39.309: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-cee61807-633c-447a-bfd8-d4762487ca6c
STEP: Creating a pod to test consume secrets
Jan  3 14:21:39.355: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-98ec1cb0-88db-48c7-a16e-4a009237ef58" in namespace "projected-8566" to be "success or failure"
Jan  3 14:21:39.366: INFO: Pod "pod-projected-secrets-98ec1cb0-88db-48c7-a16e-4a009237ef58": Phase="Pending", Reason="", readiness=false. Elapsed: 11.513077ms
Jan  3 14:21:41.370: INFO: Pod "pod-projected-secrets-98ec1cb0-88db-48c7-a16e-4a009237ef58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015496322s
Jan  3 14:21:43.388: INFO: Pod "pod-projected-secrets-98ec1cb0-88db-48c7-a16e-4a009237ef58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033138473s
Jan  3 14:21:45.434: INFO: Pod "pod-projected-secrets-98ec1cb0-88db-48c7-a16e-4a009237ef58": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07924375s
Jan  3 14:21:47.443: INFO: Pod "pod-projected-secrets-98ec1cb0-88db-48c7-a16e-4a009237ef58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.088664908s
STEP: Saw pod success
Jan  3 14:21:47.443: INFO: Pod "pod-projected-secrets-98ec1cb0-88db-48c7-a16e-4a009237ef58" satisfied condition "success or failure"
Jan  3 14:21:47.448: INFO: Trying to get logs from node controller-0 pod pod-projected-secrets-98ec1cb0-88db-48c7-a16e-4a009237ef58 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan  3 14:21:47.473: INFO: Waiting for pod pod-projected-secrets-98ec1cb0-88db-48c7-a16e-4a009237ef58 to disappear
Jan  3 14:21:47.474: INFO: Pod pod-projected-secrets-98ec1cb0-88db-48c7-a16e-4a009237ef58 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:21:47.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8566" for this suite.
Jan  3 14:21:53.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:21:53.512: INFO: namespace projected-8566 deletion completed in 6.036059166s

• [SLOW TEST:14.204 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:21:53.513: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan  3 14:22:05.542: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1354 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:22:05.542: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:22:05.734: INFO: Exec stderr: ""
Jan  3 14:22:05.734: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1354 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:22:05.734: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:22:06.002: INFO: Exec stderr: ""
Jan  3 14:22:06.002: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1354 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:22:06.002: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:22:06.201: INFO: Exec stderr: ""
Jan  3 14:22:06.201: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1354 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:22:06.201: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:22:06.364: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan  3 14:22:06.364: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1354 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:22:06.364: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:22:06.528: INFO: Exec stderr: ""
Jan  3 14:22:06.528: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1354 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:22:06.528: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:22:06.714: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan  3 14:22:06.714: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1354 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:22:06.715: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:22:06.889: INFO: Exec stderr: ""
Jan  3 14:22:06.889: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1354 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:22:06.889: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:22:07.107: INFO: Exec stderr: ""
Jan  3 14:22:07.107: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1354 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:22:07.107: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:22:07.382: INFO: Exec stderr: ""
Jan  3 14:22:07.382: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1354 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:22:07.382: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:22:07.609: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:22:07.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1354" for this suite.
Jan  3 14:22:51.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:22:51.657: INFO: namespace e2e-kubelet-etc-hosts-1354 deletion completed in 44.045293614s

• [SLOW TEST:58.145 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:22:51.658: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jan  3 14:22:59.689: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-937082782 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jan  3 14:23:09.854: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:23:09.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2290" for this suite.
Jan  3 14:23:15.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:23:15.912: INFO: namespace pods-2290 deletion completed in 6.054411843s

• [SLOW TEST:24.254 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:23:15.912: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-8273/configmap-test-12a3fcba-9a81-4b4b-96cf-268673417ba2
STEP: Creating a pod to test consume configMaps
Jan  3 14:23:15.929: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d310775-fc03-41d8-8b75-3bc17dfc4a51" in namespace "configmap-8273" to be "success or failure"
Jan  3 14:23:15.931: INFO: Pod "pod-configmaps-9d310775-fc03-41d8-8b75-3bc17dfc4a51": Phase="Pending", Reason="", readiness=false. Elapsed: 1.694433ms
Jan  3 14:23:17.933: INFO: Pod "pod-configmaps-9d310775-fc03-41d8-8b75-3bc17dfc4a51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003792076s
Jan  3 14:23:19.935: INFO: Pod "pod-configmaps-9d310775-fc03-41d8-8b75-3bc17dfc4a51": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005750199s
Jan  3 14:23:21.941: INFO: Pod "pod-configmaps-9d310775-fc03-41d8-8b75-3bc17dfc4a51": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01169289s
Jan  3 14:23:23.943: INFO: Pod "pod-configmaps-9d310775-fc03-41d8-8b75-3bc17dfc4a51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013478669s
STEP: Saw pod success
Jan  3 14:23:23.943: INFO: Pod "pod-configmaps-9d310775-fc03-41d8-8b75-3bc17dfc4a51" satisfied condition "success or failure"
Jan  3 14:23:23.944: INFO: Trying to get logs from node controller-0 pod pod-configmaps-9d310775-fc03-41d8-8b75-3bc17dfc4a51 container env-test: <nil>
STEP: delete the pod
Jan  3 14:23:23.952: INFO: Waiting for pod pod-configmaps-9d310775-fc03-41d8-8b75-3bc17dfc4a51 to disappear
Jan  3 14:23:23.954: INFO: Pod pod-configmaps-9d310775-fc03-41d8-8b75-3bc17dfc4a51 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:23:23.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8273" for this suite.
Jan  3 14:23:29.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:23:29.999: INFO: namespace configmap-8273 deletion completed in 6.042834451s

• [SLOW TEST:14.086 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:23:29.999: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan  3 14:23:30.066: INFO: Waiting up to 5m0s for pod "pod-48a21610-a38b-4141-8279-51e1302e4ecf" in namespace "emptydir-3400" to be "success or failure"
Jan  3 14:23:30.067: INFO: Pod "pod-48a21610-a38b-4141-8279-51e1302e4ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.011243ms
Jan  3 14:23:32.069: INFO: Pod "pod-48a21610-a38b-4141-8279-51e1302e4ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002491076s
Jan  3 14:23:34.070: INFO: Pod "pod-48a21610-a38b-4141-8279-51e1302e4ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004028972s
Jan  3 14:23:36.072: INFO: Pod "pod-48a21610-a38b-4141-8279-51e1302e4ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005904692s
Jan  3 14:23:38.074: INFO: Pod "pod-48a21610-a38b-4141-8279-51e1302e4ecf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008124115s
STEP: Saw pod success
Jan  3 14:23:38.074: INFO: Pod "pod-48a21610-a38b-4141-8279-51e1302e4ecf" satisfied condition "success or failure"
Jan  3 14:23:38.077: INFO: Trying to get logs from node controller-1 pod pod-48a21610-a38b-4141-8279-51e1302e4ecf container test-container: <nil>
STEP: delete the pod
Jan  3 14:23:38.096: INFO: Waiting for pod pod-48a21610-a38b-4141-8279-51e1302e4ecf to disappear
Jan  3 14:23:38.102: INFO: Pod pod-48a21610-a38b-4141-8279-51e1302e4ecf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:23:38.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3400" for this suite.
Jan  3 14:23:44.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:23:44.309: INFO: namespace emptydir-3400 deletion completed in 6.205286971s

• [SLOW TEST:14.310 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:23:44.310: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-45208649-5270-494a-8d8f-9c1175fd7979
STEP: Creating a pod to test consume secrets
Jan  3 14:23:44.353: INFO: Waiting up to 5m0s for pod "pod-secrets-9c9969a8-32af-4880-a6ec-f9e51aae4f97" in namespace "secrets-6157" to be "success or failure"
Jan  3 14:23:44.364: INFO: Pod "pod-secrets-9c9969a8-32af-4880-a6ec-f9e51aae4f97": Phase="Pending", Reason="", readiness=false. Elapsed: 11.398191ms
Jan  3 14:23:46.367: INFO: Pod "pod-secrets-9c9969a8-32af-4880-a6ec-f9e51aae4f97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014221821s
Jan  3 14:23:48.370: INFO: Pod "pod-secrets-9c9969a8-32af-4880-a6ec-f9e51aae4f97": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016848629s
Jan  3 14:23:50.371: INFO: Pod "pod-secrets-9c9969a8-32af-4880-a6ec-f9e51aae4f97": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018396552s
Jan  3 14:23:52.373: INFO: Pod "pod-secrets-9c9969a8-32af-4880-a6ec-f9e51aae4f97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020545828s
STEP: Saw pod success
Jan  3 14:23:52.374: INFO: Pod "pod-secrets-9c9969a8-32af-4880-a6ec-f9e51aae4f97" satisfied condition "success or failure"
Jan  3 14:23:52.375: INFO: Trying to get logs from node controller-1 pod pod-secrets-9c9969a8-32af-4880-a6ec-f9e51aae4f97 container secret-volume-test: <nil>
STEP: delete the pod
Jan  3 14:23:52.389: INFO: Waiting for pod pod-secrets-9c9969a8-32af-4880-a6ec-f9e51aae4f97 to disappear
Jan  3 14:23:52.390: INFO: Pod pod-secrets-9c9969a8-32af-4880-a6ec-f9e51aae4f97 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:23:52.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6157" for this suite.
Jan  3 14:23:58.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:23:58.452: INFO: namespace secrets-6157 deletion completed in 6.057812683s

• [SLOW TEST:14.142 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:23:58.452: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:23:58.472: INFO: Waiting up to 5m0s for pod "busybox-user-65534-92000059-a628-462e-b483-87418bbcb1b9" in namespace "security-context-test-3561" to be "success or failure"
Jan  3 14:23:58.474: INFO: Pod "busybox-user-65534-92000059-a628-462e-b483-87418bbcb1b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.94947ms
Jan  3 14:24:00.477: INFO: Pod "busybox-user-65534-92000059-a628-462e-b483-87418bbcb1b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004462826s
Jan  3 14:24:02.478: INFO: Pod "busybox-user-65534-92000059-a628-462e-b483-87418bbcb1b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00622196s
Jan  3 14:24:04.480: INFO: Pod "busybox-user-65534-92000059-a628-462e-b483-87418bbcb1b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00778845s
Jan  3 14:24:06.481: INFO: Pod "busybox-user-65534-92000059-a628-462e-b483-87418bbcb1b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009385726s
Jan  3 14:24:06.482: INFO: Pod "busybox-user-65534-92000059-a628-462e-b483-87418bbcb1b9" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:24:06.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3561" for this suite.
Jan  3 14:24:12.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:24:12.578: INFO: namespace security-context-test-3561 deletion completed in 6.094591253s

• [SLOW TEST:14.126 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:24:12.579: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-f989d686-9716-4842-9d57-823b06389bf4
STEP: Creating secret with name s-test-opt-upd-67f675ec-250f-4184-84bd-54bf93c858ce
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f989d686-9716-4842-9d57-823b06389bf4
STEP: Updating secret s-test-opt-upd-67f675ec-250f-4184-84bd-54bf93c858ce
STEP: Creating secret with name s-test-opt-create-37f5b5c8-6459-403f-9524-1bdae148017d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:25:28.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8249" for this suite.
Jan  3 14:25:50.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:25:50.996: INFO: namespace projected-8249 deletion completed in 22.142380769s

• [SLOW TEST:98.418 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:25:50.997: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:25:51.019: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jan  3 14:25:53.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-6945 create -f -'
Jan  3 14:25:54.211: INFO: stderr: ""
Jan  3 14:25:54.211: INFO: stdout: "e2e-test-crd-publish-openapi-3025-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  3 14:25:54.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-6945 delete e2e-test-crd-publish-openapi-3025-crds test-foo'
Jan  3 14:25:54.285: INFO: stderr: ""
Jan  3 14:25:54.285: INFO: stdout: "e2e-test-crd-publish-openapi-3025-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan  3 14:25:54.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-6945 apply -f -'
Jan  3 14:25:54.416: INFO: stderr: ""
Jan  3 14:25:54.416: INFO: stdout: "e2e-test-crd-publish-openapi-3025-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  3 14:25:54.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-6945 delete e2e-test-crd-publish-openapi-3025-crds test-foo'
Jan  3 14:25:54.479: INFO: stderr: ""
Jan  3 14:25:54.479: INFO: stdout: "e2e-test-crd-publish-openapi-3025-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan  3 14:25:54.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-6945 create -f -'
Jan  3 14:25:54.600: INFO: rc: 1
Jan  3 14:25:54.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-6945 apply -f -'
Jan  3 14:25:54.727: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jan  3 14:25:54.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-6945 create -f -'
Jan  3 14:25:54.853: INFO: rc: 1
Jan  3 14:25:54.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-6945 apply -f -'
Jan  3 14:25:55.023: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan  3 14:25:55.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 explain e2e-test-crd-publish-openapi-3025-crds'
Jan  3 14:25:55.169: INFO: stderr: ""
Jan  3 14:25:55.169: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3025-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan  3 14:25:55.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 explain e2e-test-crd-publish-openapi-3025-crds.metadata'
Jan  3 14:25:55.304: INFO: stderr: ""
Jan  3 14:25:55.304: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3025-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan  3 14:25:55.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 explain e2e-test-crd-publish-openapi-3025-crds.spec'
Jan  3 14:25:55.432: INFO: stderr: ""
Jan  3 14:25:55.432: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3025-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan  3 14:25:55.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 explain e2e-test-crd-publish-openapi-3025-crds.spec.bars'
Jan  3 14:25:55.598: INFO: stderr: ""
Jan  3 14:25:55.598: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3025-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan  3 14:25:55.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 explain e2e-test-crd-publish-openapi-3025-crds.spec.bars2'
Jan  3 14:25:55.802: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:25:59.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6945" for this suite.
Jan  3 14:26:05.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:26:05.791: INFO: namespace crd-publish-openapi-6945 deletion completed in 6.097581595s

• [SLOW TEST:14.794 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:26:05.791: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 14:26:06.353: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 14:26:08.357: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658366, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658366, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658366, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658366, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:26:10.359: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658366, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658366, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658366, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658366, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:26:12.359: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658366, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658366, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658366, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658366, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 14:26:15.367: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:26:15.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-886" for this suite.
Jan  3 14:26:27.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:26:27.649: INFO: namespace webhook-886 deletion completed in 12.042459173s
STEP: Destroying namespace "webhook-886-markers" for this suite.
Jan  3 14:26:33.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:26:33.693: INFO: namespace webhook-886-markers deletion completed in 6.043165752s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.914 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:26:33.706: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:26:33.725: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:26:34.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2913" for this suite.
Jan  3 14:26:40.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:26:40.786: INFO: namespace custom-resource-definition-2913 deletion completed in 6.043317179s

• [SLOW TEST:7.081 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:26:40.788: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:26:40.803: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:26:48.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3096" for this suite.
Jan  3 14:27:32.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:27:32.996: INFO: namespace pods-3096 deletion completed in 44.039206816s

• [SLOW TEST:52.208 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:27:32.996: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:27:46.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9775" for this suite.
Jan  3 14:27:52.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:27:52.117: INFO: namespace resourcequota-9775 deletion completed in 6.04037201s

• [SLOW TEST:19.121 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:27:52.117: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 14:27:52.630: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 14:27:54.634: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658472, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658472, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658472, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658472, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:27:56.636: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658472, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658472, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658472, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658472, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:27:58.636: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658472, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658472, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658472, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658472, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 14:28:01.642: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:28:01.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-962" for this suite.
Jan  3 14:28:07.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:28:07.817: INFO: namespace webhook-962 deletion completed in 6.047501874s
STEP: Destroying namespace "webhook-962-markers" for this suite.
Jan  3 14:28:13.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:28:13.852: INFO: namespace webhook-962-markers deletion completed in 6.035310166s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.739 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:28:13.857: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 14:28:14.736: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 14:28:16.741: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658494, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658494, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658494, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658494, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:28:18.748: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658494, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658494, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658494, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658494, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:28:20.745: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658494, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658494, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658494, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658494, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 14:28:23.748: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:28:23.750: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7117-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:28:25.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6803" for this suite.
Jan  3 14:28:31.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:28:31.171: INFO: namespace webhook-6803 deletion completed in 6.07185291s
STEP: Destroying namespace "webhook-6803-markers" for this suite.
Jan  3 14:28:37.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:28:37.219: INFO: namespace webhook-6803-markers deletion completed in 6.048092828s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.368 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:28:37.225: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 14:28:37.594: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 14:28:39.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:28:41.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:28:43.604: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:28:45.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713658517, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 14:28:48.607: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan  3 14:28:56.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 attach --namespace=webhook-971 to-be-attached-pod -i -c=container1'
Jan  3 14:28:56.706: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:28:56.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-971" for this suite.
Jan  3 14:29:08.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:29:08.753: INFO: namespace webhook-971 deletion completed in 12.043125428s
STEP: Destroying namespace "webhook-971-markers" for this suite.
Jan  3 14:29:14.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:29:14.793: INFO: namespace webhook-971-markers deletion completed in 6.039368105s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:37.578 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:29:14.803: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jan  3 14:29:14.850: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:29:24.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5179" for this suite.
Jan  3 14:29:36.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:29:36.511: INFO: namespace init-container-5179 deletion completed in 12.062071497s

• [SLOW TEST:21.708 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:29:36.511: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Jan  3 14:29:36.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 api-versions'
Jan  3 14:29:36.737: INFO: stderr: ""
Jan  3 14:29:36.737: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\narmada.process/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nk8s.cni.cncf.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:29:36.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9055" for this suite.
Jan  3 14:29:42.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:29:42.874: INFO: namespace kubectl-9055 deletion completed in 6.134915881s

• [SLOW TEST:6.363 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:29:42.875: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0103 14:29:43.624548      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan  3 14:29:43.624: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:29:43.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2533" for this suite.
Jan  3 14:29:49.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:29:49.699: INFO: namespace gc-2533 deletion completed in 6.068123399s

• [SLOW TEST:6.825 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:29:49.700: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan  3 14:29:49.721: INFO: Waiting up to 5m0s for pod "pod-93a61ffd-8317-4bc2-b1ca-ba200b8b8d1e" in namespace "emptydir-4776" to be "success or failure"
Jan  3 14:29:49.729: INFO: Pod "pod-93a61ffd-8317-4bc2-b1ca-ba200b8b8d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.71851ms
Jan  3 14:29:51.733: INFO: Pod "pod-93a61ffd-8317-4bc2-b1ca-ba200b8b8d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011911047s
Jan  3 14:29:53.734: INFO: Pod "pod-93a61ffd-8317-4bc2-b1ca-ba200b8b8d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013327632s
Jan  3 14:29:55.736: INFO: Pod "pod-93a61ffd-8317-4bc2-b1ca-ba200b8b8d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01458887s
Jan  3 14:29:57.737: INFO: Pod "pod-93a61ffd-8317-4bc2-b1ca-ba200b8b8d1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016140492s
STEP: Saw pod success
Jan  3 14:29:57.737: INFO: Pod "pod-93a61ffd-8317-4bc2-b1ca-ba200b8b8d1e" satisfied condition "success or failure"
Jan  3 14:29:57.738: INFO: Trying to get logs from node controller-1 pod pod-93a61ffd-8317-4bc2-b1ca-ba200b8b8d1e container test-container: <nil>
STEP: delete the pod
Jan  3 14:29:57.750: INFO: Waiting for pod pod-93a61ffd-8317-4bc2-b1ca-ba200b8b8d1e to disappear
Jan  3 14:29:57.751: INFO: Pod pod-93a61ffd-8317-4bc2-b1ca-ba200b8b8d1e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:29:57.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4776" for this suite.
Jan  3 14:30:03.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:30:03.818: INFO: namespace emptydir-4776 deletion completed in 6.064976752s

• [SLOW TEST:14.118 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:30:03.818: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:30:03.859: INFO: (0) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 12.169916ms)
Jan  3 14:30:03.867: INFO: (1) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 7.074857ms)
Jan  3 14:30:03.899: INFO: (2) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 32.446958ms)
Jan  3 14:30:03.903: INFO: (3) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 4.19253ms)
Jan  3 14:30:03.906: INFO: (4) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.693313ms)
Jan  3 14:30:03.912: INFO: (5) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 6.029361ms)
Jan  3 14:30:03.914: INFO: (6) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.426674ms)
Jan  3 14:30:03.916: INFO: (7) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.025382ms)
Jan  3 14:30:03.921: INFO: (8) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 4.976924ms)
Jan  3 14:30:03.924: INFO: (9) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.222247ms)
Jan  3 14:30:03.929: INFO: (10) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 5.740262ms)
Jan  3 14:30:03.933: INFO: (11) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 3.491372ms)
Jan  3 14:30:03.936: INFO: (12) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.505719ms)
Jan  3 14:30:03.938: INFO: (13) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.127189ms)
Jan  3 14:30:03.940: INFO: (14) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.458766ms)
Jan  3 14:30:03.945: INFO: (15) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 4.96654ms)
Jan  3 14:30:03.948: INFO: (16) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.749577ms)
Jan  3 14:30:03.950: INFO: (17) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.371496ms)
Jan  3 14:30:03.954: INFO: (18) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 3.255129ms)
Jan  3 14:30:03.957: INFO: (19) /api/v1/nodes/controller-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="armada/">armada/</a>
<a href="audit/">audit/</a>... (200; 2.853239ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:30:03.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4346" for this suite.
Jan  3 14:30:09.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:30:10.033: INFO: namespace proxy-4346 deletion completed in 6.075006112s

• [SLOW TEST:6.215 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:30:10.034: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-d2fd442c-848e-42e3-bdda-7ad628f801b6
STEP: Creating secret with name secret-projected-all-test-volume-3467df11-1372-4103-8ee3-92200e9c74c9
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan  3 14:30:10.069: INFO: Waiting up to 5m0s for pod "projected-volume-65e325df-b509-48bb-b505-2826607dc289" in namespace "projected-8554" to be "success or failure"
Jan  3 14:30:10.070: INFO: Pod "projected-volume-65e325df-b509-48bb-b505-2826607dc289": Phase="Pending", Reason="", readiness=false. Elapsed: 851.776µs
Jan  3 14:30:12.071: INFO: Pod "projected-volume-65e325df-b509-48bb-b505-2826607dc289": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002206855s
Jan  3 14:30:14.079: INFO: Pod "projected-volume-65e325df-b509-48bb-b505-2826607dc289": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010537004s
Jan  3 14:30:16.085: INFO: Pod "projected-volume-65e325df-b509-48bb-b505-2826607dc289": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015820738s
Jan  3 14:30:18.087: INFO: Pod "projected-volume-65e325df-b509-48bb-b505-2826607dc289": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018076264s
Jan  3 14:30:20.091: INFO: Pod "projected-volume-65e325df-b509-48bb-b505-2826607dc289": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.021960474s
STEP: Saw pod success
Jan  3 14:30:20.091: INFO: Pod "projected-volume-65e325df-b509-48bb-b505-2826607dc289" satisfied condition "success or failure"
Jan  3 14:30:20.092: INFO: Trying to get logs from node controller-0 pod projected-volume-65e325df-b509-48bb-b505-2826607dc289 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan  3 14:30:20.104: INFO: Waiting for pod projected-volume-65e325df-b509-48bb-b505-2826607dc289 to disappear
Jan  3 14:30:20.106: INFO: Pod projected-volume-65e325df-b509-48bb-b505-2826607dc289 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:30:20.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8554" for this suite.
Jan  3 14:30:26.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:30:26.156: INFO: namespace projected-8554 deletion completed in 6.048810059s

• [SLOW TEST:16.122 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:30:26.156: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8319
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan  3 14:30:26.177: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan  3 14:30:54.213: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.192.122 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8319 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:30:54.213: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:30:55.350: INFO: Found all expected endpoints: [netserver-0]
Jan  3 14:30:55.352: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.166.188 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8319 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:30:55.352: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:30:56.552: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:30:56.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8319" for this suite.
Jan  3 14:31:08.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:31:08.603: INFO: namespace pod-network-test-8319 deletion completed in 12.042722427s

• [SLOW TEST:42.446 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:31:08.603: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:31:16.646: INFO: Waiting up to 5m0s for pod "client-envvars-74875689-8ae4-4455-b43c-05ca87fe3c1d" in namespace "pods-8306" to be "success or failure"
Jan  3 14:31:16.673: INFO: Pod "client-envvars-74875689-8ae4-4455-b43c-05ca87fe3c1d": Phase="Pending", Reason="", readiness=false. Elapsed: 26.157204ms
Jan  3 14:31:18.678: INFO: Pod "client-envvars-74875689-8ae4-4455-b43c-05ca87fe3c1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031628025s
Jan  3 14:31:20.680: INFO: Pod "client-envvars-74875689-8ae4-4455-b43c-05ca87fe3c1d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033150899s
Jan  3 14:31:22.681: INFO: Pod "client-envvars-74875689-8ae4-4455-b43c-05ca87fe3c1d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034753272s
Jan  3 14:31:24.683: INFO: Pod "client-envvars-74875689-8ae4-4455-b43c-05ca87fe3c1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.036174029s
STEP: Saw pod success
Jan  3 14:31:24.683: INFO: Pod "client-envvars-74875689-8ae4-4455-b43c-05ca87fe3c1d" satisfied condition "success or failure"
Jan  3 14:31:24.684: INFO: Trying to get logs from node controller-1 pod client-envvars-74875689-8ae4-4455-b43c-05ca87fe3c1d container env3cont: <nil>
STEP: delete the pod
Jan  3 14:31:24.693: INFO: Waiting for pod client-envvars-74875689-8ae4-4455-b43c-05ca87fe3c1d to disappear
Jan  3 14:31:24.694: INFO: Pod client-envvars-74875689-8ae4-4455-b43c-05ca87fe3c1d no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:31:24.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8306" for this suite.
Jan  3 14:31:52.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:31:52.802: INFO: namespace pods-8306 deletion completed in 28.100701624s

• [SLOW TEST:44.199 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:31:52.803: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-6281
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6281 to expose endpoints map[]
Jan  3 14:31:52.821: INFO: Get endpoints failed (1.386974ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jan  3 14:31:53.822: INFO: successfully validated that service endpoint-test2 in namespace services-6281 exposes endpoints map[] (1.002882151s elapsed)
STEP: Creating pod pod1 in namespace services-6281
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6281 to expose endpoints map[pod1:[80]]
Jan  3 14:31:57.839: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.013750736s elapsed, will retry)
Jan  3 14:32:00.847: INFO: successfully validated that service endpoint-test2 in namespace services-6281 exposes endpoints map[pod1:[80]] (7.02122865s elapsed)
STEP: Creating pod pod2 in namespace services-6281
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6281 to expose endpoints map[pod1:[80] pod2:[80]]
Jan  3 14:32:04.889: INFO: Unexpected endpoints: found map[5c7751ee-4b2d-4310-9743-4eadff4c7ba6:[80]], expected map[pod1:[80] pod2:[80]] (4.040012884s elapsed, will retry)
Jan  3 14:32:08.939: INFO: successfully validated that service endpoint-test2 in namespace services-6281 exposes endpoints map[pod1:[80] pod2:[80]] (8.090870684s elapsed)
STEP: Deleting pod pod1 in namespace services-6281
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6281 to expose endpoints map[pod2:[80]]
Jan  3 14:32:08.952: INFO: successfully validated that service endpoint-test2 in namespace services-6281 exposes endpoints map[pod2:[80]] (11.517776ms elapsed)
STEP: Deleting pod pod2 in namespace services-6281
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6281 to expose endpoints map[]
Jan  3 14:32:08.958: INFO: successfully validated that service endpoint-test2 in namespace services-6281 exposes endpoints map[] (1.674481ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:32:08.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6281" for this suite.
Jan  3 14:32:36.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:32:37.081: INFO: namespace services-6281 deletion completed in 28.107201689s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:44.278 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:32:37.081: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5427.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5427.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5427.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5427.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5427.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5427.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan  3 14:32:47.132: INFO: DNS probes using dns-5427/dns-test-affc5602-7a5c-4a14-9f7e-277c0f8557c3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:32:47.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5427" for this suite.
Jan  3 14:32:53.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:32:53.200: INFO: namespace dns-5427 deletion completed in 6.043092056s

• [SLOW TEST:16.119 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:32:53.201: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Jan  3 14:32:53.789: INFO: created pod pod-service-account-defaultsa
Jan  3 14:32:53.789: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan  3 14:32:53.792: INFO: created pod pod-service-account-mountsa
Jan  3 14:32:53.792: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan  3 14:32:53.796: INFO: created pod pod-service-account-nomountsa
Jan  3 14:32:53.796: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan  3 14:32:53.797: INFO: created pod pod-service-account-defaultsa-mountspec
Jan  3 14:32:53.797: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan  3 14:32:53.799: INFO: created pod pod-service-account-mountsa-mountspec
Jan  3 14:32:53.799: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan  3 14:32:53.800: INFO: created pod pod-service-account-nomountsa-mountspec
Jan  3 14:32:53.801: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan  3 14:32:53.802: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan  3 14:32:53.803: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan  3 14:32:53.806: INFO: created pod pod-service-account-mountsa-nomountspec
Jan  3 14:32:53.806: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan  3 14:32:53.843: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan  3 14:32:53.843: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:32:53.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3366" for this suite.
Jan  3 14:33:21.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:33:21.939: INFO: namespace svcaccounts-3366 deletion completed in 28.092106241s

• [SLOW TEST:28.738 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:33:21.940: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-734
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Jan  3 14:33:21.984: INFO: Found 0 stateful pods, waiting for 3
Jan  3 14:33:31.992: INFO: Found 2 stateful pods, waiting for 3
Jan  3 14:33:41.987: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:33:41.987: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:33:41.987: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan  3 14:33:51.986: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:33:51.986: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:33:51.986: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jan  3 14:33:52.006: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan  3 14:34:02.030: INFO: Updating stateful set ss2
Jan  3 14:34:02.033: INFO: Waiting for Pod statefulset-734/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jan  3 14:34:12.207: INFO: Found 2 stateful pods, waiting for 3
Jan  3 14:34:22.209: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:34:22.209: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:34:22.209: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan  3 14:34:32.209: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:34:32.209: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:34:32.209: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan  3 14:34:42.215: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:34:42.215: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:34:42.215: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan  3 14:34:52.209: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:34:52.209: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:34:52.209: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan  3 14:35:02.211: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:35:02.211: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:35:02.211: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan  3 14:35:12.211: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:35:12.211: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:35:12.211: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan  3 14:35:22.219: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:35:22.229: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:35:22.232: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan  3 14:35:32.219: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:35:32.219: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:35:32.219: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan  3 14:35:32.244: INFO: Updating stateful set ss2
Jan  3 14:35:32.248: INFO: Waiting for Pod statefulset-734/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan  3 14:35:42.296: INFO: Updating stateful set ss2
Jan  3 14:35:42.311: INFO: Waiting for StatefulSet statefulset-734/ss2 to complete update
Jan  3 14:35:42.311: INFO: Waiting for Pod statefulset-734/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan  3 14:35:52.317: INFO: Waiting for StatefulSet statefulset-734/ss2 to complete update
Jan  3 14:35:52.317: INFO: Waiting for Pod statefulset-734/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan  3 14:36:02.327: INFO: Waiting for StatefulSet statefulset-734/ss2 to complete update
Jan  3 14:36:02.327: INFO: Waiting for Pod statefulset-734/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan  3 14:36:12.315: INFO: Waiting for StatefulSet statefulset-734/ss2 to complete update
Jan  3 14:36:12.315: INFO: Waiting for Pod statefulset-734/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan  3 14:36:22.316: INFO: Waiting for StatefulSet statefulset-734/ss2 to complete update
Jan  3 14:36:22.316: INFO: Waiting for Pod statefulset-734/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan  3 14:36:32.317: INFO: Waiting for StatefulSet statefulset-734/ss2 to complete update
Jan  3 14:36:32.317: INFO: Waiting for Pod statefulset-734/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan  3 14:36:42.317: INFO: Waiting for StatefulSet statefulset-734/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jan  3 14:36:52.317: INFO: Deleting all statefulset in ns statefulset-734
Jan  3 14:36:52.318: INFO: Scaling statefulset ss2 to 0
Jan  3 14:37:02.326: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 14:37:02.327: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:37:02.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-734" for this suite.
Jan  3 14:37:08.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:37:08.371: INFO: namespace statefulset-734 deletion completed in 6.034669031s

• [SLOW TEST:226.431 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:37:08.371: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jan  3 14:37:08.388: INFO: Waiting up to 5m0s for pod "downward-api-dedf59f9-4ec5-4ff6-b98c-0accb05b229f" in namespace "downward-api-8485" to be "success or failure"
Jan  3 14:37:08.389: INFO: Pod "downward-api-dedf59f9-4ec5-4ff6-b98c-0accb05b229f": Phase="Pending", Reason="", readiness=false. Elapsed: 976.502µs
Jan  3 14:37:10.390: INFO: Pod "downward-api-dedf59f9-4ec5-4ff6-b98c-0accb05b229f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002607296s
Jan  3 14:37:12.424: INFO: Pod "downward-api-dedf59f9-4ec5-4ff6-b98c-0accb05b229f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036586994s
Jan  3 14:37:14.430: INFO: Pod "downward-api-dedf59f9-4ec5-4ff6-b98c-0accb05b229f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041825995s
Jan  3 14:37:16.439: INFO: Pod "downward-api-dedf59f9-4ec5-4ff6-b98c-0accb05b229f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.051453334s
STEP: Saw pod success
Jan  3 14:37:16.439: INFO: Pod "downward-api-dedf59f9-4ec5-4ff6-b98c-0accb05b229f" satisfied condition "success or failure"
Jan  3 14:37:16.441: INFO: Trying to get logs from node controller-1 pod downward-api-dedf59f9-4ec5-4ff6-b98c-0accb05b229f container dapi-container: <nil>
STEP: delete the pod
Jan  3 14:37:16.461: INFO: Waiting for pod downward-api-dedf59f9-4ec5-4ff6-b98c-0accb05b229f to disappear
Jan  3 14:37:16.462: INFO: Pod downward-api-dedf59f9-4ec5-4ff6-b98c-0accb05b229f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:37:16.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8485" for this suite.
Jan  3 14:37:22.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:37:22.526: INFO: namespace downward-api-8485 deletion completed in 6.0625053s

• [SLOW TEST:14.155 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:37:22.526: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4199
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4199
I0103 14:37:22.572606      23 runners.go:184] Created replication controller with name: externalname-service, namespace: services-4199, replica count: 2
I0103 14:37:25.622866      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:37:28.623230      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:37:31.623391      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 14:37:31.623: INFO: Creating new exec pod
Jan  3 14:37:40.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=services-4199 execpodb7frq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jan  3 14:37:40.933: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  3 14:37:40.933: INFO: stdout: ""
Jan  3 14:37:40.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=services-4199 execpodb7frq -- /bin/sh -x -c nc -zv -t -w 2 10.108.218.171 80'
Jan  3 14:37:41.377: INFO: stderr: "+ nc -zv -t -w 2 10.108.218.171 80\nConnection to 10.108.218.171 80 port [tcp/http] succeeded!\n"
Jan  3 14:37:41.377: INFO: stdout: ""
Jan  3 14:37:41.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=services-4199 execpodb7frq -- /bin/sh -x -c nc -zv -t -w 2 10.10.61.11 31206'
Jan  3 14:37:41.757: INFO: stderr: "+ nc -zv -t -w 2 10.10.61.11 31206\nConnection to 10.10.61.11 31206 port [tcp/31206] succeeded!\n"
Jan  3 14:37:41.758: INFO: stdout: ""
Jan  3 14:37:41.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=services-4199 execpodb7frq -- /bin/sh -x -c nc -zv -t -w 2 10.10.61.12 31206'
Jan  3 14:37:42.588: INFO: stderr: "+ nc -zv -t -w 2 10.10.61.12 31206\nConnection to 10.10.61.12 31206 port [tcp/31206] succeeded!\n"
Jan  3 14:37:42.588: INFO: stdout: ""
Jan  3 14:37:42.588: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:37:42.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4199" for this suite.
Jan  3 14:37:48.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:37:48.662: INFO: namespace services-4199 deletion completed in 6.038890826s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:26.136 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:37:48.662: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 14:37:48.686: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8bd6fdb-639f-41d6-b252-1997e74f0058" in namespace "projected-433" to be "success or failure"
Jan  3 14:37:48.693: INFO: Pod "downwardapi-volume-f8bd6fdb-639f-41d6-b252-1997e74f0058": Phase="Pending", Reason="", readiness=false. Elapsed: 7.365765ms
Jan  3 14:37:50.695: INFO: Pod "downwardapi-volume-f8bd6fdb-639f-41d6-b252-1997e74f0058": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008892931s
Jan  3 14:37:52.696: INFO: Pod "downwardapi-volume-f8bd6fdb-639f-41d6-b252-1997e74f0058": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010521499s
Jan  3 14:37:54.698: INFO: Pod "downwardapi-volume-f8bd6fdb-639f-41d6-b252-1997e74f0058": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012112898s
Jan  3 14:37:56.700: INFO: Pod "downwardapi-volume-f8bd6fdb-639f-41d6-b252-1997e74f0058": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013932197s
STEP: Saw pod success
Jan  3 14:37:56.700: INFO: Pod "downwardapi-volume-f8bd6fdb-639f-41d6-b252-1997e74f0058" satisfied condition "success or failure"
Jan  3 14:37:56.701: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-f8bd6fdb-639f-41d6-b252-1997e74f0058 container client-container: <nil>
STEP: delete the pod
Jan  3 14:37:56.713: INFO: Waiting for pod downwardapi-volume-f8bd6fdb-639f-41d6-b252-1997e74f0058 to disappear
Jan  3 14:37:56.714: INFO: Pod downwardapi-volume-f8bd6fdb-639f-41d6-b252-1997e74f0058 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:37:56.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-433" for this suite.
Jan  3 14:38:02.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:38:02.767: INFO: namespace projected-433 deletion completed in 6.051105035s

• [SLOW TEST:14.105 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:38:02.768: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9342
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan  3 14:38:02.782: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan  3 14:38:32.806: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.192.103:8080/dial?request=hostName&protocol=http&host=172.16.192.113&port=8080&tries=1'] Namespace:pod-network-test-9342 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:38:32.806: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:38:33.025: INFO: Waiting for endpoints: map[]
Jan  3 14:38:33.026: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.192.103:8080/dial?request=hostName&protocol=http&host=172.16.166.167&port=8080&tries=1'] Namespace:pod-network-test-9342 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:38:33.026: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:38:33.253: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:38:33.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9342" for this suite.
Jan  3 14:38:45.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:38:45.342: INFO: namespace pod-network-test-9342 deletion completed in 12.080885622s

• [SLOW TEST:42.574 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:38:45.342: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-663
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-663
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-663
Jan  3 14:38:45.437: INFO: Found 0 stateful pods, waiting for 1
Jan  3 14:38:55.439: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan  3 14:38:55.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-663 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 14:38:55.686: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 14:38:55.686: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 14:38:55.686: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 14:38:55.691: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  3 14:39:05.693: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 14:39:05.693: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 14:39:05.700: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999764s
Jan  3 14:39:06.702: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997697486s
Jan  3 14:39:07.704: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99537213s
Jan  3 14:39:08.706: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.993525538s
Jan  3 14:39:09.710: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989207907s
Jan  3 14:39:10.712: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.987335317s
Jan  3 14:39:11.714: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.985266419s
Jan  3 14:39:12.718: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.982650956s
Jan  3 14:39:13.720: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.979168733s
Jan  3 14:39:14.723: INFO: Verifying statefulset ss doesn't scale past 1 for another 977.144722ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-663
Jan  3 14:39:15.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-663 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 14:39:15.924: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 14:39:15.924: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 14:39:15.924: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 14:39:15.926: INFO: Found 1 stateful pods, waiting for 3
Jan  3 14:39:25.928: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:39:25.928: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:39:25.928: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan  3 14:39:35.928: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:39:35.928: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 14:39:35.928: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan  3 14:39:35.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-663 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 14:39:36.148: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 14:39:36.148: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 14:39:36.148: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 14:39:36.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-663 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 14:39:36.426: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 14:39:36.426: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 14:39:36.426: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 14:39:36.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-663 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 14:39:36.701: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 14:39:36.701: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 14:39:36.701: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 14:39:36.701: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 14:39:36.703: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan  3 14:39:46.706: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 14:39:46.706: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 14:39:46.706: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 14:39:46.711: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999836s
Jan  3 14:39:47.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997485853s
Jan  3 14:39:48.715: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995626136s
Jan  3 14:39:49.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.993423153s
Jan  3 14:39:50.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.991364318s
Jan  3 14:39:51.721: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.989067241s
Jan  3 14:39:52.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982922555s
Jan  3 14:39:53.730: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.979717695s
Jan  3 14:39:54.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.978109526s
Jan  3 14:39:55.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 976.257319ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-663
Jan  3 14:39:56.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-663 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 14:39:56.952: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 14:39:56.952: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 14:39:56.952: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 14:39:56.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-663 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 14:39:57.173: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 14:39:57.173: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 14:39:57.173: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 14:39:57.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-663 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 14:39:57.658: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 14:39:57.658: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 14:39:57.658: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 14:39:57.658: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jan  3 14:40:17.679: INFO: Deleting all statefulset in ns statefulset-663
Jan  3 14:40:17.687: INFO: Scaling statefulset ss to 0
Jan  3 14:40:17.692: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 14:40:17.696: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:40:17.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-663" for this suite.
Jan  3 14:40:23.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:40:23.745: INFO: namespace statefulset-663 deletion completed in 6.033745705s

• [SLOW TEST:98.402 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:40:23.745: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:40:23.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-376" for this suite.
Jan  3 14:40:29.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:40:29.831: INFO: namespace resourcequota-376 deletion completed in 6.063389472s

• [SLOW TEST:6.087 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:40:29.832: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan  3 14:40:29.848: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1393 /api/v1/namespaces/watch-1393/configmaps/e2e-watch-test-configmap-a 108c662e-26bf-4d8a-b767-3dbb5ffc0c68 68317 0 2020-01-03 14:40:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan  3 14:40:29.848: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1393 /api/v1/namespaces/watch-1393/configmaps/e2e-watch-test-configmap-a 108c662e-26bf-4d8a-b767-3dbb5ffc0c68 68317 0 2020-01-03 14:40:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan  3 14:40:39.852: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1393 /api/v1/namespaces/watch-1393/configmaps/e2e-watch-test-configmap-a 108c662e-26bf-4d8a-b767-3dbb5ffc0c68 68349 0 2020-01-03 14:40:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan  3 14:40:39.852: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1393 /api/v1/namespaces/watch-1393/configmaps/e2e-watch-test-configmap-a 108c662e-26bf-4d8a-b767-3dbb5ffc0c68 68349 0 2020-01-03 14:40:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan  3 14:40:49.857: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1393 /api/v1/namespaces/watch-1393/configmaps/e2e-watch-test-configmap-a 108c662e-26bf-4d8a-b767-3dbb5ffc0c68 68380 0 2020-01-03 14:40:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan  3 14:40:49.857: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1393 /api/v1/namespaces/watch-1393/configmaps/e2e-watch-test-configmap-a 108c662e-26bf-4d8a-b767-3dbb5ffc0c68 68380 0 2020-01-03 14:40:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan  3 14:40:59.860: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1393 /api/v1/namespaces/watch-1393/configmaps/e2e-watch-test-configmap-a 108c662e-26bf-4d8a-b767-3dbb5ffc0c68 68413 0 2020-01-03 14:40:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan  3 14:40:59.860: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1393 /api/v1/namespaces/watch-1393/configmaps/e2e-watch-test-configmap-a 108c662e-26bf-4d8a-b767-3dbb5ffc0c68 68413 0 2020-01-03 14:40:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan  3 14:41:09.863: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1393 /api/v1/namespaces/watch-1393/configmaps/e2e-watch-test-configmap-b 12b5d1b4-714d-4cc0-a369-fcd125d1cd8d 68443 0 2020-01-03 14:41:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan  3 14:41:09.864: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1393 /api/v1/namespaces/watch-1393/configmaps/e2e-watch-test-configmap-b 12b5d1b4-714d-4cc0-a369-fcd125d1cd8d 68443 0 2020-01-03 14:41:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan  3 14:41:19.866: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1393 /api/v1/namespaces/watch-1393/configmaps/e2e-watch-test-configmap-b 12b5d1b4-714d-4cc0-a369-fcd125d1cd8d 68475 0 2020-01-03 14:41:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan  3 14:41:19.866: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1393 /api/v1/namespaces/watch-1393/configmaps/e2e-watch-test-configmap-b 12b5d1b4-714d-4cc0-a369-fcd125d1cd8d 68475 0 2020-01-03 14:41:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:41:29.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1393" for this suite.
Jan  3 14:41:35.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:41:35.919: INFO: namespace watch-1393 deletion completed in 6.044206126s

• [SLOW TEST:66.087 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:41:35.919: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-1192c65d-ac35-4e34-8f6d-e70984adf469 in namespace container-probe-7087
Jan  3 14:41:44.324: INFO: Started pod test-webserver-1192c65d-ac35-4e34-8f6d-e70984adf469 in namespace container-probe-7087
STEP: checking the pod's current state and verifying that restartCount is present
Jan  3 14:41:44.337: INFO: Initial restart count of pod test-webserver-1192c65d-ac35-4e34-8f6d-e70984adf469 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:45:44.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7087" for this suite.
Jan  3 14:45:51.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:45:51.046: INFO: namespace container-probe-7087 deletion completed in 6.042494306s

• [SLOW TEST:255.127 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:45:51.046: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-5077
STEP: creating replication controller nodeport-test in namespace services-5077
I0103 14:45:51.070741      23 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-5077, replica count: 2
I0103 14:45:54.123114      23 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:45:57.123998      23 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 14:46:00.124: INFO: Creating new exec pod
I0103 14:46:00.124128      23 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 14:46:09.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=services-5077 execpod6b857 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jan  3 14:46:09.358: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan  3 14:46:09.358: INFO: stdout: ""
Jan  3 14:46:09.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=services-5077 execpod6b857 -- /bin/sh -x -c nc -zv -t -w 2 10.101.114.255 80'
Jan  3 14:46:09.583: INFO: stderr: "+ nc -zv -t -w 2 10.101.114.255 80\nConnection to 10.101.114.255 80 port [tcp/http] succeeded!\n"
Jan  3 14:46:09.583: INFO: stdout: ""
Jan  3 14:46:09.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=services-5077 execpod6b857 -- /bin/sh -x -c nc -zv -t -w 2 10.10.61.11 31203'
Jan  3 14:46:09.796: INFO: stderr: "+ nc -zv -t -w 2 10.10.61.11 31203\nConnection to 10.10.61.11 31203 port [tcp/31203] succeeded!\n"
Jan  3 14:46:09.796: INFO: stdout: ""
Jan  3 14:46:09.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=services-5077 execpod6b857 -- /bin/sh -x -c nc -zv -t -w 2 10.10.61.12 31203'
Jan  3 14:46:10.028: INFO: stderr: "+ nc -zv -t -w 2 10.10.61.12 31203\nConnection to 10.10.61.12 31203 port [tcp/31203] succeeded!\n"
Jan  3 14:46:10.028: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:46:10.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5077" for this suite.
Jan  3 14:46:16.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:46:16.082: INFO: namespace services-5077 deletion completed in 6.051137323s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:25.035 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:46:16.082: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jan  3 14:46:24.118: INFO: &Pod{ObjectMeta:{send-events-a24e01a2-f3c5-4adf-9e45-22adc10397fe  events-497 /api/v1/namespaces/events-497/pods/send-events-a24e01a2-f3c5-4adf-9e45-22adc10397fe c8c3d3dc-7694-4595-b7a8-c205dbc6b8d2 69626 0 2020-01-03 14:46:16 +0000 UTC <nil> <nil> map[name:foo time:96624555] map[cni.projectcalico.org/podIP:172.16.192.77/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.77"
    ],
    "default": true,
    "dns": {}
}]] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9ltr5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9ltr5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9ltr5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 14:46:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 14:46:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 14:46:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 14:46:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.11,PodIP:172.16.192.77,StartTime:2020-01-03 14:46:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-03 14:46:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://61e276e222128a5d727f85fdbbbf9c20974ada4d23471feef41fff4d19bf4b7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jan  3 14:46:26.120: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jan  3 14:46:28.122: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:46:28.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-497" for this suite.
Jan  3 14:47:12.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:47:12.171: INFO: namespace events-497 deletion completed in 44.037733283s

• [SLOW TEST:56.089 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:47:12.171: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:47:12.193: INFO: Create a RollingUpdate DaemonSet
Jan  3 14:47:12.194: INFO: Check that daemon pods launch on every node of the cluster
Jan  3 14:47:12.197: INFO: Number of nodes with available pods: 0
Jan  3 14:47:12.197: INFO: Node controller-0 is running more than one daemon pod
Jan  3 14:47:13.368: INFO: Number of nodes with available pods: 0
Jan  3 14:47:13.368: INFO: Node controller-0 is running more than one daemon pod
Jan  3 14:47:14.233: INFO: Number of nodes with available pods: 0
Jan  3 14:47:14.233: INFO: Node controller-0 is running more than one daemon pod
Jan  3 14:47:15.206: INFO: Number of nodes with available pods: 0
Jan  3 14:47:15.206: INFO: Node controller-0 is running more than one daemon pod
Jan  3 14:47:16.201: INFO: Number of nodes with available pods: 0
Jan  3 14:47:16.201: INFO: Node controller-0 is running more than one daemon pod
Jan  3 14:47:17.201: INFO: Number of nodes with available pods: 0
Jan  3 14:47:17.201: INFO: Node controller-0 is running more than one daemon pod
Jan  3 14:47:18.214: INFO: Number of nodes with available pods: 0
Jan  3 14:47:18.214: INFO: Node controller-0 is running more than one daemon pod
Jan  3 14:47:19.201: INFO: Number of nodes with available pods: 0
Jan  3 14:47:19.201: INFO: Node controller-0 is running more than one daemon pod
Jan  3 14:47:20.201: INFO: Number of nodes with available pods: 1
Jan  3 14:47:20.201: INFO: Node controller-1 is running more than one daemon pod
Jan  3 14:47:21.202: INFO: Number of nodes with available pods: 2
Jan  3 14:47:21.202: INFO: Number of running nodes: 2, number of available pods: 2
Jan  3 14:47:21.202: INFO: Update the DaemonSet to trigger a rollout
Jan  3 14:47:21.205: INFO: Updating DaemonSet daemon-set
Jan  3 14:47:27.219: INFO: Roll back the DaemonSet before rollout is complete
Jan  3 14:47:27.228: INFO: Updating DaemonSet daemon-set
Jan  3 14:47:27.228: INFO: Make sure DaemonSet rollback is complete
Jan  3 14:47:27.234: INFO: Wrong image for pod: daemon-set-9xfqk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan  3 14:47:27.234: INFO: Pod daemon-set-9xfqk is not available
Jan  3 14:47:28.241: INFO: Wrong image for pod: daemon-set-9xfqk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan  3 14:47:28.241: INFO: Pod daemon-set-9xfqk is not available
Jan  3 14:47:29.243: INFO: Wrong image for pod: daemon-set-9xfqk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan  3 14:47:29.243: INFO: Pod daemon-set-9xfqk is not available
Jan  3 14:47:30.242: INFO: Pod daemon-set-8nwcq is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3750, will wait for the garbage collector to delete the pods
Jan  3 14:47:30.302: INFO: Deleting DaemonSet.extensions daemon-set took: 3.246606ms
Jan  3 14:47:30.702: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.211518ms
Jan  3 14:47:39.405: INFO: Number of nodes with available pods: 0
Jan  3 14:47:39.405: INFO: Number of running nodes: 0, number of available pods: 0
Jan  3 14:47:39.407: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3750/daemonsets","resourceVersion":"69965"},"items":null}

Jan  3 14:47:39.411: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3750/pods","resourceVersion":"69965"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:47:39.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3750" for this suite.
Jan  3 14:47:45.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:47:46.095: INFO: namespace daemonsets-3750 deletion completed in 6.673238692s

• [SLOW TEST:33.924 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:47:46.095: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-04919313-d05b-4916-87c9-b1a4ce81261b
STEP: Creating a pod to test consume secrets
Jan  3 14:47:46.203: INFO: Waiting up to 5m0s for pod "pod-secrets-6cab696d-6dc2-4f2e-a1d9-02778bb9e7d9" in namespace "secrets-4134" to be "success or failure"
Jan  3 14:47:46.214: INFO: Pod "pod-secrets-6cab696d-6dc2-4f2e-a1d9-02778bb9e7d9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.100059ms
Jan  3 14:47:48.221: INFO: Pod "pod-secrets-6cab696d-6dc2-4f2e-a1d9-02778bb9e7d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018258548s
Jan  3 14:47:50.223: INFO: Pod "pod-secrets-6cab696d-6dc2-4f2e-a1d9-02778bb9e7d9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01989341s
Jan  3 14:47:52.224: INFO: Pod "pod-secrets-6cab696d-6dc2-4f2e-a1d9-02778bb9e7d9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021675305s
Jan  3 14:47:54.226: INFO: Pod "pod-secrets-6cab696d-6dc2-4f2e-a1d9-02778bb9e7d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023039975s
STEP: Saw pod success
Jan  3 14:47:54.226: INFO: Pod "pod-secrets-6cab696d-6dc2-4f2e-a1d9-02778bb9e7d9" satisfied condition "success or failure"
Jan  3 14:47:54.227: INFO: Trying to get logs from node controller-0 pod pod-secrets-6cab696d-6dc2-4f2e-a1d9-02778bb9e7d9 container secret-volume-test: <nil>
STEP: delete the pod
Jan  3 14:47:54.240: INFO: Waiting for pod pod-secrets-6cab696d-6dc2-4f2e-a1d9-02778bb9e7d9 to disappear
Jan  3 14:47:54.241: INFO: Pod pod-secrets-6cab696d-6dc2-4f2e-a1d9-02778bb9e7d9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:47:54.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4134" for this suite.
Jan  3 14:48:00.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:48:00.277: INFO: namespace secrets-4134 deletion completed in 6.034168384s

• [SLOW TEST:14.182 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:48:00.277: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-8190
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8190
STEP: Deleting pre-stop pod
Jan  3 14:48:23.318: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:48:23.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8190" for this suite.
Jan  3 14:49:07.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:49:07.370: INFO: namespace prestop-8190 deletion completed in 44.044940252s

• [SLOW TEST:67.092 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:49:07.370: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 14:49:07.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e58f1182-d8f7-4283-a46b-fcb3a452f97b" in namespace "downward-api-4411" to be "success or failure"
Jan  3 14:49:07.400: INFO: Pod "downwardapi-volume-e58f1182-d8f7-4283-a46b-fcb3a452f97b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.931309ms
Jan  3 14:49:09.401: INFO: Pod "downwardapi-volume-e58f1182-d8f7-4283-a46b-fcb3a452f97b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003395827s
Jan  3 14:49:11.409: INFO: Pod "downwardapi-volume-e58f1182-d8f7-4283-a46b-fcb3a452f97b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011169721s
Jan  3 14:49:13.458: INFO: Pod "downwardapi-volume-e58f1182-d8f7-4283-a46b-fcb3a452f97b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060132654s
Jan  3 14:49:15.494: INFO: Pod "downwardapi-volume-e58f1182-d8f7-4283-a46b-fcb3a452f97b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.09680574s
STEP: Saw pod success
Jan  3 14:49:15.495: INFO: Pod "downwardapi-volume-e58f1182-d8f7-4283-a46b-fcb3a452f97b" satisfied condition "success or failure"
Jan  3 14:49:15.504: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-e58f1182-d8f7-4283-a46b-fcb3a452f97b container client-container: <nil>
STEP: delete the pod
Jan  3 14:49:15.544: INFO: Waiting for pod downwardapi-volume-e58f1182-d8f7-4283-a46b-fcb3a452f97b to disappear
Jan  3 14:49:15.547: INFO: Pod downwardapi-volume-e58f1182-d8f7-4283-a46b-fcb3a452f97b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:49:15.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4411" for this suite.
Jan  3 14:49:21.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:49:21.729: INFO: namespace downward-api-4411 deletion completed in 6.128955266s

• [SLOW TEST:14.359 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:49:21.729: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan  3 14:49:37.779: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  3 14:49:37.784: INFO: Pod pod-with-poststart-http-hook still exists
Jan  3 14:49:39.784: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  3 14:49:39.786: INFO: Pod pod-with-poststart-http-hook still exists
Jan  3 14:49:41.784: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  3 14:49:41.788: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:49:41.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8520" for this suite.
Jan  3 14:50:09.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:50:09.861: INFO: namespace container-lifecycle-hook-8520 deletion completed in 28.070511848s

• [SLOW TEST:48.132 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:50:09.862: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jan  3 14:50:17.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec pod-sharedvolume-54fc4004-c2eb-4068-a33a-236310cc4cfb -c busybox-main-container --namespace=emptydir-7409 -- cat /usr/share/volumeshare/shareddata.txt'
Jan  3 14:50:18.212: INFO: stderr: ""
Jan  3 14:50:18.212: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:50:18.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7409" for this suite.
Jan  3 14:50:24.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:50:24.253: INFO: namespace emptydir-7409 deletion completed in 6.038717556s

• [SLOW TEST:14.391 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:50:24.254: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jan  3 14:50:24.268: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:50:34.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6293" for this suite.
Jan  3 14:50:40.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:50:40.388: INFO: namespace init-container-6293 deletion completed in 6.055882942s

• [SLOW TEST:16.134 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:50:40.389: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0103 14:50:46.450615      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan  3 14:50:46.450: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:50:46.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2175" for this suite.
Jan  3 14:50:52.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:50:52.492: INFO: namespace gc-2175 deletion completed in 6.037823221s

• [SLOW TEST:12.103 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:50:52.493: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:51:33.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6948" for this suite.
Jan  3 14:51:39.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:51:39.819: INFO: namespace container-runtime-6948 deletion completed in 6.064498153s

• [SLOW TEST:47.326 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:51:39.819: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-0e84a1cb-e32b-4d8c-b2eb-407942305d93
STEP: Creating a pod to test consume configMaps
Jan  3 14:51:39.846: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3c2677ce-6011-4b08-a4b8-22e351254b9a" in namespace "projected-6860" to be "success or failure"
Jan  3 14:51:39.848: INFO: Pod "pod-projected-configmaps-3c2677ce-6011-4b08-a4b8-22e351254b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.083929ms
Jan  3 14:51:41.853: INFO: Pod "pod-projected-configmaps-3c2677ce-6011-4b08-a4b8-22e351254b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006620691s
Jan  3 14:51:43.863: INFO: Pod "pod-projected-configmaps-3c2677ce-6011-4b08-a4b8-22e351254b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016283967s
Jan  3 14:51:45.947: INFO: Pod "pod-projected-configmaps-3c2677ce-6011-4b08-a4b8-22e351254b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.099750887s
Jan  3 14:51:47.948: INFO: Pod "pod-projected-configmaps-3c2677ce-6011-4b08-a4b8-22e351254b9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.101262317s
STEP: Saw pod success
Jan  3 14:51:47.948: INFO: Pod "pod-projected-configmaps-3c2677ce-6011-4b08-a4b8-22e351254b9a" satisfied condition "success or failure"
Jan  3 14:51:47.949: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-3c2677ce-6011-4b08-a4b8-22e351254b9a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 14:51:47.964: INFO: Waiting for pod pod-projected-configmaps-3c2677ce-6011-4b08-a4b8-22e351254b9a to disappear
Jan  3 14:51:47.965: INFO: Pod pod-projected-configmaps-3c2677ce-6011-4b08-a4b8-22e351254b9a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:51:47.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6860" for this suite.
Jan  3 14:51:53.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:51:54.067: INFO: namespace projected-6860 deletion completed in 6.099471481s

• [SLOW TEST:14.248 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:51:54.072: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:51:54.150: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Creating first CR 
Jan  3 14:51:54.223: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-03T14:51:54Z generation:1 name:name1 resourceVersion:71370 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ef953d96-6bc7-41eb-b92d-7eb35610df31] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan  3 14:52:04.225: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-03T14:52:04Z generation:1 name:name2 resourceVersion:71401 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:a06691fa-85f3-4ec7-95bf-cba56e481d59] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan  3 14:52:14.229: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-03T14:51:54Z generation:2 name:name1 resourceVersion:71432 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ef953d96-6bc7-41eb-b92d-7eb35610df31] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan  3 14:52:24.234: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-03T14:52:04Z generation:2 name:name2 resourceVersion:71463 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:a06691fa-85f3-4ec7-95bf-cba56e481d59] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan  3 14:52:34.292: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-03T14:51:54Z generation:2 name:name1 resourceVersion:71495 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ef953d96-6bc7-41eb-b92d-7eb35610df31] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan  3 14:52:44.373: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-03T14:52:04Z generation:2 name:name2 resourceVersion:71524 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:a06691fa-85f3-4ec7-95bf-cba56e481d59] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:52:54.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7303" for this suite.
Jan  3 14:53:00.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:53:00.980: INFO: namespace crd-watch-7303 deletion completed in 6.096434544s

• [SLOW TEST:66.908 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:53:00.981: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan  3 14:53:01.005: INFO: Pod name pod-release: Found 0 pods out of 1
Jan  3 14:53:06.007: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:53:06.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6935" for this suite.
Jan  3 14:53:12.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:53:12.068: INFO: namespace replication-controller-6935 deletion completed in 6.036281705s

• [SLOW TEST:11.087 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:53:12.068: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jan  3 14:53:12.083: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  3 14:53:12.087: INFO: Waiting for terminating namespaces to be deleted...
Jan  3 14:53:12.088: INFO: 
Logging pods the kubelet thinks is on node controller-0 before test
Jan  3 14:53:12.113: INFO: kube-apiserver-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container kube-apiserver ready: true, restart count 2
Jan  3 14:53:12.113: INFO: kube-multus-ds-amd64-6zbwh from kube-system started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container kube-multus ready: true, restart count 0
Jan  3 14:53:12.113: INFO: nova-scheduler-76b67b674d-lwrc2 from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container nova-scheduler ready: true, restart count 0
Jan  3 14:53:12.113: INFO: cinder-volume-549d7c447c-tr8gw from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container cinder-volume ready: true, restart count 0
Jan  3 14:53:12.113: INFO: kube-sriov-cni-ds-amd64-ddmnd from kube-system started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Jan  3 14:53:12.113: INFO: nova-conductor-bb6d86d69-tgxnd from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container nova-conductor ready: true, restart count 0
Jan  3 14:53:12.113: INFO: nova-api-osapi-68846d5959-w7jrd from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container nova-osapi ready: true, restart count 1
Jan  3 14:53:12.113: INFO: kube-scheduler-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan  3 14:53:12.113: INFO: rbd-provisioner-7484d49cf6-zf49b from kube-system started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container rbd-provisioner ready: true, restart count 0
Jan  3 14:53:12.113: INFO: mariadb-ingress-5bb8b69fc8-xppj8 from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container ingress ready: true, restart count 0
Jan  3 14:53:12.113: INFO: ingress-bc886876f-487zs from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container ingress ready: true, restart count 0
Jan  3 14:53:12.113: INFO: heat-engine-cleaner-1578062400-dw5ck from openstack started at 2020-01-03 14:40:05 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 14:53:12.113: INFO: neutron-ovs-agent-controller-0-937646f6-n2m6q from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container neutron-ovs-agent ready: true, restart count 0
Jan  3 14:53:12.113: INFO: heat-api-58bf859968-7rz2w from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container heat-api ready: true, restart count 0
Jan  3 14:53:12.113: INFO: neutron-server-54b46f798-bbhdw from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container neutron-server ready: true, restart count 0
Jan  3 14:53:12.113: INFO: heat-engine-cleaner-1578062700-ddtrp from openstack started at 2020-01-03 14:45:06 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 14:53:12.113: INFO: kube-controller-manager-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan  3 14:53:12.113: INFO: heat-cfn-c8f5b9b4b-dfrgc from openstack started at 2020-01-03 13:51:06 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container heat-cfn ready: true, restart count 0
Jan  3 14:53:12.113: INFO: ceph-pools-audit-1578063000-76vlr from kube-system started at 2020-01-03 14:50:07 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 14:53:12.113: INFO: calico-node-rb47m from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container calico-node ready: true, restart count 3
Jan  3 14:53:12.113: INFO: ingress-p6rb9 from kube-system started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container ingress ready: true, restart count 0
Jan  3 14:53:12.113: INFO: nova-api-proxy-577495bf7f-k7tnr from openstack started at 2020-01-03 13:51:11 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container nova-api-proxy ready: true, restart count 0
Jan  3 14:53:12.113: INFO: nova-service-cleaner-1578060000-zbvnr from openstack started at 2020-01-03 14:00:06 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container nova-service-cleaner ready: false, restart count 0
Jan  3 14:53:12.113: INFO: cinder-volume-usage-audit-1578063000-2kljs from openstack started at 2020-01-03 14:50:07 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 14:53:12.113: INFO: mariadb-server-1 from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container mariadb ready: true, restart count 0
Jan  3 14:53:12.113: INFO: nova-novncproxy-6c54c7d98-ptb6t from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container nova-novncproxy ready: true, restart count 0
Jan  3 14:53:12.113: INFO: fm-rest-api-b4bc757f4-4fzhx from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container fm-rest-api ready: true, restart count 0
Jan  3 14:53:12.113: INFO: cinder-api-7ff9984869-kgvv6 from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container cinder-api ready: true, restart count 0
Jan  3 14:53:12.113: INFO: neutron-sriov-agent-controller-0-937646f6-dg699 from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container neutron-sriov-agent ready: true, restart count 0
Jan  3 14:53:12.113: INFO: placement-api-5b65bc5576-kh6z7 from openstack started at 2020-01-03 13:50:47 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container placement-api ready: true, restart count 0
Jan  3 14:53:12.113: INFO: nova-compute-controller-0-937646f6-vl8z6 from openstack started at 2020-01-03 13:50:44 +0000 UTC (2 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container nova-compute ready: true, restart count 0
Jan  3 14:53:12.113: INFO: 	Container nova-compute-ssh ready: true, restart count 0
Jan  3 14:53:12.113: INFO: osh-openstack-rabbitmq-rabbitmq-0 from openstack started at 2020-01-03 13:50:53 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container rabbitmq ready: true, restart count 0
Jan  3 14:53:12.113: INFO: neutron-dhcp-agent-controller-0-937646f6-prqt7 from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container neutron-dhcp-agent ready: true, restart count 0
Jan  3 14:53:12.113: INFO: libvirt-libvirt-default-psmq6 from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container libvirt ready: true, restart count 0
Jan  3 14:53:12.113: INFO: coredns-6bc668cd76-nt9h6 from kube-system started at 2020-01-03 13:50:47 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container coredns ready: true, restart count 0
Jan  3 14:53:12.113: INFO: cinder-backup-fd5f96bf4-z4xc2 from openstack started at 2020-01-03 13:50:47 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container cinder-backup ready: true, restart count 0
Jan  3 14:53:12.113: INFO: neutron-metadata-agent-controller-0-937646f6-z2mz9 from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container neutron-metadata-agent ready: true, restart count 0
Jan  3 14:53:12.113: INFO: keystone-api-7b4d98b8c5-2q8z7 from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container keystone-api ready: true, restart count 0
Jan  3 14:53:12.113: INFO: neutron-l3-agent-controller-0-937646f6-ft64l from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container neutron-l3-agent ready: true, restart count 0
Jan  3 14:53:12.113: INFO: cinder-scheduler-664bb87785-p8t9k from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container cinder-scheduler ready: true, restart count 0
Jan  3 14:53:12.113: INFO: ingress-error-pages-5bcb8b5f6c-bn4nq from kube-system started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 14:53:12.113: INFO: kube-proxy-5l8fd from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container kube-proxy ready: true, restart count 1
Jan  3 14:53:12.113: INFO: nova-api-metadata-7bdf79d754-h8r6m from openstack started at 2020-01-03 13:50:47 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container nova-api ready: true, restart count 0
Jan  3 14:53:12.113: INFO: ingress-error-pages-cf6c65b-vmlrg from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 14:53:12.113: INFO: glance-api-747954666d-cmg85 from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container glance-api ready: true, restart count 0
Jan  3 14:53:12.113: INFO: heat-engine-54745654c7-hbkrx from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container heat-engine ready: true, restart count 0
Jan  3 14:53:12.113: INFO: sonobuoy-systemd-logs-daemon-set-5e158e08c5964d78-s6m8n from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 14:53:12.113: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan  3 14:53:12.113: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 14:53:12.113: INFO: 
Logging pods the kubelet thinks is on node controller-1 before test
Jan  3 14:53:12.141: INFO: nova-api-proxy-577495bf7f-b5t9q from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container nova-api-proxy ready: true, restart count 0
Jan  3 14:53:12.141: INFO: nova-conductor-bb6d86d69-5tkcn from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container nova-conductor ready: true, restart count 0
Jan  3 14:53:12.141: INFO: neutron-ovs-agent-controller-1-cab72f56-mntzv from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container neutron-ovs-agent ready: true, restart count 0
Jan  3 14:53:12.141: INFO: heat-cfn-c8f5b9b4b-wd8h4 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container heat-cfn ready: true, restart count 0
Jan  3 14:53:12.141: INFO: ingress-5fgmr from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container ingress ready: true, restart count 0
Jan  3 14:53:12.141: INFO: ingress-error-pages-5bcb8b5f6c-8rf2d from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 14:53:12.141: INFO: mariadb-ingress-error-pages-847467b5d5-d57l2 from openstack started at 2020-01-03 11:20:48 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 14:53:12.141: INFO: libvirt-libvirt-default-np4lk from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container libvirt ready: true, restart count 0
Jan  3 14:53:12.141: INFO: tiller-deploy-d6b59fcb-zpz47 from kube-system started at 2020-01-03 13:50:02 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container tiller ready: true, restart count 0
Jan  3 14:53:12.141: INFO: heat-ks-endpoints-jjzrq from openstack started at 2020-01-03 11:40:01 +0000 UTC (6 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container cloudformation-ks-endpoints-admin ready: false, restart count 0
Jan  3 14:53:12.141: INFO: 	Container cloudformation-ks-endpoints-internal ready: false, restart count 0
Jan  3 14:53:12.141: INFO: 	Container cloudformation-ks-endpoints-public ready: false, restart count 0
Jan  3 14:53:12.141: INFO: 	Container orchestration-ks-endpoints-admin ready: false, restart count 0
Jan  3 14:53:12.141: INFO: 	Container orchestration-ks-endpoints-internal ready: false, restart count 0
Jan  3 14:53:12.141: INFO: 	Container orchestration-ks-endpoints-public ready: false, restart count 0
Jan  3 14:53:12.141: INFO: heat-ks-service-b7t2s from openstack started at 2020-01-03 11:40:01 +0000 UTC (2 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container cloudformation-ks-service-registration ready: false, restart count 0
Jan  3 14:53:12.141: INFO: 	Container orchestration-ks-service-registration ready: false, restart count 0
Jan  3 14:53:12.141: INFO: calico-kube-controllers-855577b7b5-n6tlk from kube-system started at 2020-01-03 13:50:00 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan  3 14:53:12.141: INFO: horizon-6865446ff5-c22fk from openstack started at 2020-01-03 13:50:00 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container horizon ready: true, restart count 0
Jan  3 14:53:12.141: INFO: nova-ks-user-gtl4m from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 14:53:12.141: INFO: neutron-metadata-agent-controller-1-cab72f56-xwlz5 from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container neutron-metadata-agent ready: true, restart count 0
Jan  3 14:53:12.141: INFO: ceph-pools-audit-1578062400-5rsw4 from kube-system started at 2020-01-03 14:40:05 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 14:53:12.141: INFO: cinder-volume-usage-audit-1578062400-46nzm from openstack started at 2020-01-03 14:40:05 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 14:53:12.141: INFO: ingress-error-pages-cf6c65b-zwhqk from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 14:53:12.141: INFO: keystone-api-7b4d98b8c5-8wjmf from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container keystone-api ready: true, restart count 0
Jan  3 14:53:12.141: INFO: neutron-dhcp-agent-controller-1-cab72f56-t5s8d from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container neutron-dhcp-agent ready: true, restart count 0
Jan  3 14:53:12.141: INFO: heat-engine-cleaner-1578063000-vkbvk from openstack started at 2020-01-03 14:50:07 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 14:53:12.141: INFO: kube-apiserver-controller-1 from kube-system started at 2020-01-03 10:52:37 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  3 14:53:12.141: INFO: rbd-provisioner-7484d49cf6-2tb85 from kube-system started at 2020-01-03 10:55:54 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container rbd-provisioner ready: true, restart count 0
Jan  3 14:53:12.141: INFO: nova-api-osapi-68846d5959-ggf8r from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container nova-osapi ready: true, restart count 0
Jan  3 14:53:12.141: INFO: nova-scheduler-76b67b674d-4g9bx from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container nova-scheduler ready: true, restart count 0
Jan  3 14:53:12.141: INFO: coredns-6bc668cd76-9nlk5 from kube-system started at 2020-01-03 10:48:30 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container coredns ready: true, restart count 0
Jan  3 14:53:12.141: INFO: cinder-api-7ff9984869-9g8tp from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container cinder-api ready: true, restart count 0
Jan  3 14:53:12.141: INFO: cinder-volume-549d7c447c-2sjzd from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container cinder-volume ready: true, restart count 0
Jan  3 14:53:12.141: INFO: nova-novncproxy-6c54c7d98-d2pzp from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container nova-novncproxy ready: true, restart count 0
Jan  3 14:53:12.141: INFO: mariadb-server-0 from openstack started at 2020-01-03 11:20:52 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container mariadb ready: true, restart count 0
Jan  3 14:53:12.141: INFO: glance-ks-service-kbjlh from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container image-ks-service-registration ready: false, restart count 0
Jan  3 14:53:12.141: INFO: cinder-db-init-g8w2v from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container cinder-db-init-0 ready: false, restart count 0
Jan  3 14:53:12.141: INFO: osh-openstack-memcached-memcached-545668bdbd-xp7s7 from openstack started at 2020-01-03 13:50:01 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container memcached ready: true, restart count 0
Jan  3 14:53:12.141: INFO: heat-rabbit-init-mtk7j from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 14:53:12.141: INFO: kube-controller-manager-controller-1 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  3 14:53:12.141: INFO: glance-ks-user-h6kmn from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 14:53:12.141: INFO: placement-ks-user-swghx from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 14:53:12.141: INFO: neutron-ks-endpoints-xrvqm from openstack started at 2020-01-03 11:34:31 +0000 UTC (3 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container network-ks-endpoints-admin ready: false, restart count 0
Jan  3 14:53:12.141: INFO: 	Container network-ks-endpoints-internal ready: false, restart count 0
Jan  3 14:53:12.141: INFO: 	Container network-ks-endpoints-public ready: false, restart count 0
Jan  3 14:53:12.141: INFO: kube-scheduler-controller-1 from kube-system started at 2020-01-03 10:52:37 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  3 14:53:12.141: INFO: placement-db-init-4t4cq from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container placement-db-init-0 ready: false, restart count 0
Jan  3 14:53:12.141: INFO: neutron-l3-agent-controller-1-cab72f56-nm27w from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container neutron-l3-agent ready: true, restart count 0
Jan  3 14:53:12.141: INFO: fm-ks-endpoints-rswss from openstack started at 2020-01-03 11:41:38 +0000 UTC (3 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container faultmanagement-ks-endpoints-admin ready: false, restart count 0
Jan  3 14:53:12.141: INFO: 	Container faultmanagement-ks-endpoints-internal ready: false, restart count 0
Jan  3 14:53:12.141: INFO: 	Container faultmanagement-ks-endpoints-public ready: false, restart count 0
Jan  3 14:53:12.141: INFO: cinder-ks-user-rhjbx from openstack started at 2020-01-03 11:29:43 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 14:53:12.141: INFO: nova-ks-service-x57sz from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container compute-ks-service-registration ready: false, restart count 0
Jan  3 14:53:12.141: INFO: glance-db-init-9hfsz from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container glance-db-init-0 ready: false, restart count 0
Jan  3 14:53:12.141: INFO: cinder-scheduler-664bb87785-67xsw from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container cinder-scheduler ready: true, restart count 0
Jan  3 14:53:12.141: INFO: heat-api-58bf859968-c7cp2 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container heat-api ready: true, restart count 0
Jan  3 14:53:12.141: INFO: heat-db-init-dmbl7 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container heat-db-init-0 ready: false, restart count 0
Jan  3 14:53:12.141: INFO: sonobuoy from sonobuoy started at 2020-01-03 12:54:34 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  3 14:53:12.141: INFO: cinder-volume-usage-audit-1578062700-2x2vs from openstack started at 2020-01-03 14:45:06 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 14:53:12.141: INFO: ceph-pools-audit-1578062700-42m4s from kube-system started at 2020-01-03 14:45:06 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 14:53:12.141: INFO: calico-node-nkz88 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container calico-node ready: true, restart count 1
Jan  3 14:53:12.141: INFO: osh-openstack-rabbitmq-rabbitmq-1 from openstack started at 2020-01-03 11:22:47 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container rabbitmq ready: true, restart count 0
Jan  3 14:53:12.141: INFO: neutron-sriov-agent-controller-1-cab72f56-t65r5 from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container neutron-sriov-agent ready: true, restart count 0
Jan  3 14:53:12.141: INFO: heat-bootstrap-9xrdd from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 14:53:12.141: INFO: kube-sriov-cni-ds-amd64-jnv4d from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Jan  3 14:53:12.141: INFO: keystone-bootstrap-jmtqn from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 14:53:12.141: INFO: glance-api-747954666d-j2ldf from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container glance-api ready: true, restart count 0
Jan  3 14:53:12.141: INFO: fm-ks-service-nwst8 from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container faultmanagement-ks-service-registration ready: false, restart count 0
Jan  3 14:53:12.141: INFO: placement-api-5b65bc5576-f9d4b from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container placement-api ready: true, restart count 0
Jan  3 14:53:12.141: INFO: nova-compute-controller-1-cab72f56-c922s from openstack started at 2020-01-03 11:34:28 +0000 UTC (2 container statuses recorded)
Jan  3 14:53:12.141: INFO: 	Container nova-compute ready: true, restart count 0
Jan  3 14:53:12.141: INFO: 	Container nova-compute-ssh ready: true, restart count 0
Jan  3 14:53:12.141: INFO: heat-engine-54745654c7-4ln84 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container heat-engine ready: true, restart count 0
Jan  3 14:53:12.142: INFO: heat-ks-user-hbk96 from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 14:53:12.142: INFO: kube-proxy-2fz94 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 14:53:12.142: INFO: ingress-bc886876f-b6vc9 from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container ingress ready: true, restart count 0
Jan  3 14:53:12.142: INFO: mariadb-ingress-5bb8b69fc8-r2qvn from openstack started at 2020-01-03 11:20:48 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container ingress ready: true, restart count 0
Jan  3 14:53:12.142: INFO: nova-ks-endpoints-qv68g from openstack started at 2020-01-03 11:34:26 +0000 UTC (3 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container compute-ks-endpoints-admin ready: false, restart count 0
Jan  3 14:53:12.142: INFO: 	Container compute-ks-endpoints-internal ready: false, restart count 0
Jan  3 14:53:12.142: INFO: 	Container compute-ks-endpoints-public ready: false, restart count 0
Jan  3 14:53:12.142: INFO: fm-rest-api-b4bc757f4-8tmgc from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container fm-rest-api ready: true, restart count 0
Jan  3 14:53:12.142: INFO: kube-multus-ds-amd64-ggv58 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container kube-multus ready: true, restart count 0
Jan  3 14:53:12.142: INFO: neutron-server-54b46f798-4f8d9 from openstack started at 2020-01-03 11:34:31 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container neutron-server ready: true, restart count 0
Jan  3 14:53:12.142: INFO: sonobuoy-systemd-logs-daemon-set-5e158e08c5964d78-2qpxv from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan  3 14:53:12.142: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 14:53:12.142: INFO: nova-db-init-86cfh from openstack started at 2020-01-03 11:34:29 +0000 UTC (3 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container nova-db-init-0 ready: false, restart count 0
Jan  3 14:53:12.142: INFO: 	Container nova-db-init-1 ready: false, restart count 0
Jan  3 14:53:12.142: INFO: 	Container nova-db-init-2 ready: false, restart count 0
Jan  3 14:53:12.142: INFO: fm-db-init-2t975 from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container fm-db-init-0 ready: false, restart count 0
Jan  3 14:53:12.142: INFO: sonobuoy-e2e-job-08a7a8b2e6d84bc6 from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container e2e ready: true, restart count 0
Jan  3 14:53:12.142: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 14:53:12.142: INFO: cinder-create-internal-tenant-twkzc from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container create-internal-tenant ready: false, restart count 0
Jan  3 14:53:12.142: INFO: cinder-backup-fd5f96bf4-vzc8x from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container cinder-backup ready: true, restart count 0
Jan  3 14:53:12.142: INFO: cinder-ks-endpoints-wvm8w from openstack started at 2020-01-03 11:29:43 +0000 UTC (9 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container volume-ks-endpoints-admin ready: false, restart count 0
Jan  3 14:53:12.142: INFO: 	Container volume-ks-endpoints-internal ready: false, restart count 0
Jan  3 14:53:12.142: INFO: 	Container volume-ks-endpoints-public ready: false, restart count 0
Jan  3 14:53:12.142: INFO: 	Container volumev2-ks-endpoints-admin ready: false, restart count 0
Jan  3 14:53:12.142: INFO: 	Container volumev2-ks-endpoints-internal ready: false, restart count 0
Jan  3 14:53:12.142: INFO: 	Container volumev2-ks-endpoints-public ready: false, restart count 0
Jan  3 14:53:12.142: INFO: 	Container volumev3-ks-endpoints-admin ready: false, restart count 0
Jan  3 14:53:12.142: INFO: 	Container volumev3-ks-endpoints-internal ready: false, restart count 0
Jan  3 14:53:12.142: INFO: 	Container volumev3-ks-endpoints-public ready: false, restart count 0
Jan  3 14:53:12.142: INFO: nova-api-metadata-7bdf79d754-kr2gk from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 14:53:12.142: INFO: 	Container nova-api ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node controller-0
STEP: verifying the node has the label node controller-1
Jan  3 14:53:12.213: INFO: Pod calico-kube-controllers-855577b7b5-n6tlk requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod calico-node-nkz88 requesting resource cpu=250m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod calico-node-rb47m requesting resource cpu=250m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod coredns-6bc668cd76-9nlk5 requesting resource cpu=100m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod coredns-6bc668cd76-nt9h6 requesting resource cpu=100m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod ingress-5fgmr requesting resource cpu=100m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod ingress-error-pages-5bcb8b5f6c-8rf2d requesting resource cpu=100m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod ingress-error-pages-5bcb8b5f6c-bn4nq requesting resource cpu=100m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod ingress-p6rb9 requesting resource cpu=100m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod kube-apiserver-controller-0 requesting resource cpu=250m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod kube-apiserver-controller-1 requesting resource cpu=250m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod kube-controller-manager-controller-0 requesting resource cpu=200m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod kube-controller-manager-controller-1 requesting resource cpu=200m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod kube-multus-ds-amd64-6zbwh requesting resource cpu=100m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod kube-multus-ds-amd64-ggv58 requesting resource cpu=100m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod kube-proxy-2fz94 requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod kube-proxy-5l8fd requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod kube-scheduler-controller-0 requesting resource cpu=100m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod kube-scheduler-controller-1 requesting resource cpu=100m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod kube-sriov-cni-ds-amd64-ddmnd requesting resource cpu=100m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod kube-sriov-cni-ds-amd64-jnv4d requesting resource cpu=100m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod rbd-provisioner-7484d49cf6-2tb85 requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod rbd-provisioner-7484d49cf6-zf49b requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod tiller-deploy-d6b59fcb-zpz47 requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod cinder-api-7ff9984869-9g8tp requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod cinder-api-7ff9984869-kgvv6 requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod cinder-backup-fd5f96bf4-vzc8x requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod cinder-backup-fd5f96bf4-z4xc2 requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod cinder-scheduler-664bb87785-67xsw requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod cinder-scheduler-664bb87785-p8t9k requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod cinder-volume-549d7c447c-2sjzd requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod cinder-volume-549d7c447c-tr8gw requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod fm-rest-api-b4bc757f4-4fzhx requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod fm-rest-api-b4bc757f4-8tmgc requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod glance-api-747954666d-cmg85 requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod glance-api-747954666d-j2ldf requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod heat-api-58bf859968-7rz2w requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod heat-api-58bf859968-c7cp2 requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod heat-cfn-c8f5b9b4b-dfrgc requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod heat-cfn-c8f5b9b4b-wd8h4 requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod heat-engine-54745654c7-4ln84 requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod heat-engine-54745654c7-hbkrx requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod horizon-6865446ff5-c22fk requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod ingress-bc886876f-487zs requesting resource cpu=100m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod ingress-bc886876f-b6vc9 requesting resource cpu=100m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod ingress-error-pages-cf6c65b-vmlrg requesting resource cpu=100m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod ingress-error-pages-cf6c65b-zwhqk requesting resource cpu=100m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod keystone-api-7b4d98b8c5-2q8z7 requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod keystone-api-7b4d98b8c5-8wjmf requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod libvirt-libvirt-default-np4lk requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod libvirt-libvirt-default-psmq6 requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod mariadb-ingress-5bb8b69fc8-r2qvn requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod mariadb-ingress-5bb8b69fc8-xppj8 requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod mariadb-ingress-error-pages-847467b5d5-d57l2 requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod mariadb-server-0 requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod mariadb-server-1 requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod neutron-dhcp-agent-controller-0-937646f6-prqt7 requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod neutron-dhcp-agent-controller-1-cab72f56-t5s8d requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod neutron-l3-agent-controller-0-937646f6-ft64l requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.213: INFO: Pod neutron-l3-agent-controller-1-cab72f56-nm27w requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.213: INFO: Pod neutron-metadata-agent-controller-0-937646f6-z2mz9 requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.214: INFO: Pod neutron-metadata-agent-controller-1-cab72f56-xwlz5 requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod neutron-ovs-agent-controller-0-937646f6-n2m6q requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.214: INFO: Pod neutron-ovs-agent-controller-1-cab72f56-mntzv requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod neutron-server-54b46f798-4f8d9 requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod neutron-server-54b46f798-bbhdw requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.214: INFO: Pod neutron-sriov-agent-controller-0-937646f6-dg699 requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.214: INFO: Pod neutron-sriov-agent-controller-1-cab72f56-t65r5 requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod nova-api-metadata-7bdf79d754-h8r6m requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.214: INFO: Pod nova-api-metadata-7bdf79d754-kr2gk requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod nova-api-osapi-68846d5959-ggf8r requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod nova-api-osapi-68846d5959-w7jrd requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.214: INFO: Pod nova-api-proxy-577495bf7f-b5t9q requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod nova-api-proxy-577495bf7f-k7tnr requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.214: INFO: Pod nova-compute-controller-0-937646f6-vl8z6 requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.214: INFO: Pod nova-compute-controller-1-cab72f56-c922s requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod nova-conductor-bb6d86d69-5tkcn requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod nova-conductor-bb6d86d69-tgxnd requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.214: INFO: Pod nova-novncproxy-6c54c7d98-d2pzp requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod nova-novncproxy-6c54c7d98-ptb6t requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.214: INFO: Pod nova-scheduler-76b67b674d-4g9bx requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod nova-scheduler-76b67b674d-lwrc2 requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.214: INFO: Pod osh-openstack-memcached-memcached-545668bdbd-xp7s7 requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod osh-openstack-rabbitmq-rabbitmq-0 requesting resource cpu=500m on Node controller-0
Jan  3 14:53:12.214: INFO: Pod osh-openstack-rabbitmq-rabbitmq-1 requesting resource cpu=500m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod placement-api-5b65bc5576-f9d4b requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod placement-api-5b65bc5576-kh6z7 requesting resource cpu=0m on Node controller-0
Jan  3 14:53:12.214: INFO: Pod sonobuoy requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod sonobuoy-e2e-job-08a7a8b2e6d84bc6 requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod sonobuoy-systemd-logs-daemon-set-5e158e08c5964d78-2qpxv requesting resource cpu=0m on Node controller-1
Jan  3 14:53:12.214: INFO: Pod sonobuoy-systemd-logs-daemon-set-5e158e08c5964d78-s6m8n requesting resource cpu=0m on Node controller-0
STEP: Starting Pods to consume most of the cluster CPU.
Jan  3 14:53:12.214: INFO: Creating a pod which consumes cpu=43400m on Node controller-0
Jan  3 14:53:12.217: INFO: Creating a pod which consumes cpu=43400m on Node controller-1
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79e68ad1-754c-4ff0-a2d6-986fcdd4d9e2.15e6681cc5eadd0b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4657/filler-pod-79e68ad1-754c-4ff0-a2d6-986fcdd4d9e2 to controller-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79e68ad1-754c-4ff0-a2d6-986fcdd4d9e2.15e6681e5c63847c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79e68ad1-754c-4ff0-a2d6-986fcdd4d9e2.15e6681e605db897], Reason = [Created], Message = [Created container filler-pod-79e68ad1-754c-4ff0-a2d6-986fcdd4d9e2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79e68ad1-754c-4ff0-a2d6-986fcdd4d9e2.15e6681e6fe944f3], Reason = [Started], Message = [Started container filler-pod-79e68ad1-754c-4ff0-a2d6-986fcdd4d9e2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a58ba455-7178-48aa-b37d-b5949dd5c2ee.15e6681cc5eb70e3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4657/filler-pod-a58ba455-7178-48aa-b37d-b5949dd5c2ee to controller-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a58ba455-7178-48aa-b37d-b5949dd5c2ee.15e6681e4a8d34fb], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a58ba455-7178-48aa-b37d-b5949dd5c2ee.15e6681e4d3bd08f], Reason = [Created], Message = [Created container filler-pod-a58ba455-7178-48aa-b37d-b5949dd5c2ee]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a58ba455-7178-48aa-b37d-b5949dd5c2ee.15e6681e5ef9e419], Reason = [Started], Message = [Started container filler-pod-a58ba455-7178-48aa-b37d-b5949dd5c2ee]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15e6681ea33223cd], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node controller-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node controller-1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:53:21.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4657" for this suite.
Jan  3 14:53:27.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:53:27.317: INFO: namespace sched-pred-4657 deletion completed in 6.054850441s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:15.248 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:53:27.317: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Jan  3 14:53:27.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-1418'
Jan  3 14:53:27.586: INFO: stderr: ""
Jan  3 14:53:27.586: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan  3 14:53:27.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1418'
Jan  3 14:53:27.689: INFO: stderr: ""
Jan  3 14:53:27.689: INFO: stdout: "update-demo-nautilus-nqbp6 update-demo-nautilus-tv2sc "
Jan  3 14:53:27.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-nqbp6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1418'
Jan  3 14:53:27.760: INFO: stderr: ""
Jan  3 14:53:27.760: INFO: stdout: ""
Jan  3 14:53:27.760: INFO: update-demo-nautilus-nqbp6 is created but not running
Jan  3 14:53:32.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1418'
Jan  3 14:53:32.833: INFO: stderr: ""
Jan  3 14:53:32.833: INFO: stdout: "update-demo-nautilus-nqbp6 update-demo-nautilus-tv2sc "
Jan  3 14:53:32.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-nqbp6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1418'
Jan  3 14:53:32.902: INFO: stderr: ""
Jan  3 14:53:32.902: INFO: stdout: ""
Jan  3 14:53:32.902: INFO: update-demo-nautilus-nqbp6 is created but not running
Jan  3 14:53:37.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1418'
Jan  3 14:53:38.036: INFO: stderr: ""
Jan  3 14:53:38.036: INFO: stdout: "update-demo-nautilus-nqbp6 update-demo-nautilus-tv2sc "
Jan  3 14:53:38.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-nqbp6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1418'
Jan  3 14:53:38.095: INFO: stderr: ""
Jan  3 14:53:38.095: INFO: stdout: "true"
Jan  3 14:53:38.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-nqbp6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1418'
Jan  3 14:53:38.157: INFO: stderr: ""
Jan  3 14:53:38.157: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan  3 14:53:38.157: INFO: validating pod update-demo-nautilus-nqbp6
Jan  3 14:53:38.159: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 14:53:38.159: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 14:53:38.159: INFO: update-demo-nautilus-nqbp6 is verified up and running
Jan  3 14:53:38.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-tv2sc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1418'
Jan  3 14:53:38.216: INFO: stderr: ""
Jan  3 14:53:38.216: INFO: stdout: "true"
Jan  3 14:53:38.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-tv2sc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1418'
Jan  3 14:53:38.278: INFO: stderr: ""
Jan  3 14:53:38.278: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan  3 14:53:38.278: INFO: validating pod update-demo-nautilus-tv2sc
Jan  3 14:53:38.280: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 14:53:38.280: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 14:53:38.280: INFO: update-demo-nautilus-tv2sc is verified up and running
STEP: using delete to clean up resources
Jan  3 14:53:38.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete --grace-period=0 --force -f - --namespace=kubectl-1418'
Jan  3 14:53:38.335: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 14:53:38.335: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  3 14:53:38.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1418'
Jan  3 14:53:38.399: INFO: stderr: "No resources found in kubectl-1418 namespace.\n"
Jan  3 14:53:38.399: INFO: stdout: ""
Jan  3 14:53:38.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -l name=update-demo --namespace=kubectl-1418 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  3 14:53:38.466: INFO: stderr: ""
Jan  3 14:53:38.466: INFO: stdout: "update-demo-nautilus-nqbp6\nupdate-demo-nautilus-tv2sc\n"
Jan  3 14:53:38.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1418'
Jan  3 14:53:39.193: INFO: stderr: "No resources found in kubectl-1418 namespace.\n"
Jan  3 14:53:39.193: INFO: stdout: ""
Jan  3 14:53:39.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -l name=update-demo --namespace=kubectl-1418 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  3 14:53:39.404: INFO: stderr: ""
Jan  3 14:53:39.404: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:53:39.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1418" for this suite.
Jan  3 14:53:51.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:53:51.490: INFO: namespace kubectl-1418 deletion completed in 12.084383497s

• [SLOW TEST:24.173 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:53:51.491: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-9f076e58-2264-49ff-a0b5-4bb28f636829
STEP: Creating a pod to test consume configMaps
Jan  3 14:53:51.512: INFO: Waiting up to 5m0s for pod "pod-configmaps-583ada7c-05c8-43b3-9a74-06e27d553b93" in namespace "configmap-5596" to be "success or failure"
Jan  3 14:53:51.516: INFO: Pod "pod-configmaps-583ada7c-05c8-43b3-9a74-06e27d553b93": Phase="Pending", Reason="", readiness=false. Elapsed: 3.778257ms
Jan  3 14:53:53.517: INFO: Pod "pod-configmaps-583ada7c-05c8-43b3-9a74-06e27d553b93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005204423s
Jan  3 14:53:55.519: INFO: Pod "pod-configmaps-583ada7c-05c8-43b3-9a74-06e27d553b93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006970618s
Jan  3 14:53:57.521: INFO: Pod "pod-configmaps-583ada7c-05c8-43b3-9a74-06e27d553b93": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008940995s
Jan  3 14:53:59.525: INFO: Pod "pod-configmaps-583ada7c-05c8-43b3-9a74-06e27d553b93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013645883s
STEP: Saw pod success
Jan  3 14:53:59.526: INFO: Pod "pod-configmaps-583ada7c-05c8-43b3-9a74-06e27d553b93" satisfied condition "success or failure"
Jan  3 14:53:59.527: INFO: Trying to get logs from node controller-0 pod pod-configmaps-583ada7c-05c8-43b3-9a74-06e27d553b93 container configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 14:53:59.541: INFO: Waiting for pod pod-configmaps-583ada7c-05c8-43b3-9a74-06e27d553b93 to disappear
Jan  3 14:53:59.542: INFO: Pod pod-configmaps-583ada7c-05c8-43b3-9a74-06e27d553b93 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:53:59.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5596" for this suite.
Jan  3 14:54:05.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:54:05.588: INFO: namespace configmap-5596 deletion completed in 6.044725913s

• [SLOW TEST:14.097 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:54:05.588: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7454
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan  3 14:54:05.610: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan  3 14:54:37.644: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.166.145:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7454 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:54:37.644: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:54:37.825: INFO: Found all expected endpoints: [netserver-0]
Jan  3 14:54:37.827: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.192.87:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7454 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:54:37.827: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:54:38.034: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:54:38.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7454" for this suite.
Jan  3 14:54:50.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:54:50.068: INFO: namespace pod-network-test-7454 deletion completed in 12.031902958s

• [SLOW TEST:44.480 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:54:50.069: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan  3 14:54:57.099: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:54:57.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1895" for this suite.
Jan  3 14:55:03.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:55:03.149: INFO: namespace container-runtime-1895 deletion completed in 6.04194112s

• [SLOW TEST:13.081 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:55:03.151: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-4124
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan  3 14:55:03.165: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan  3 14:55:39.199: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.192.102:8080/dial?request=hostName&protocol=udp&host=172.16.166.148&port=8081&tries=1'] Namespace:pod-network-test-4124 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:55:39.199: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:55:39.452: INFO: Waiting for endpoints: map[]
Jan  3 14:55:39.454: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.192.102:8080/dial?request=hostName&protocol=udp&host=172.16.192.89&port=8081&tries=1'] Namespace:pod-network-test-4124 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan  3 14:55:39.455: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
Jan  3 14:55:39.809: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:55:39.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4124" for this suite.
Jan  3 14:55:51.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:55:51.858: INFO: namespace pod-network-test-4124 deletion completed in 12.04475872s

• [SLOW TEST:48.707 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:55:51.858: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jan  3 14:56:00.395: INFO: Successfully updated pod "annotationupdateec92c689-a8df-42b0-af8c-5ea32f1aff27"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:56:04.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5186" for this suite.
Jan  3 14:56:16.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:56:16.476: INFO: namespace downward-api-5186 deletion completed in 12.048208755s

• [SLOW TEST:24.618 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:56:16.476: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:56:16.491: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan  3 14:56:20.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-3780 create -f -'
Jan  3 14:56:20.429: INFO: stderr: ""
Jan  3 14:56:20.429: INFO: stdout: "e2e-test-crd-publish-openapi-9992-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  3 14:56:20.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-3780 delete e2e-test-crd-publish-openapi-9992-crds test-cr'
Jan  3 14:56:20.491: INFO: stderr: ""
Jan  3 14:56:20.491: INFO: stdout: "e2e-test-crd-publish-openapi-9992-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan  3 14:56:20.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-3780 apply -f -'
Jan  3 14:56:20.633: INFO: stderr: ""
Jan  3 14:56:20.633: INFO: stdout: "e2e-test-crd-publish-openapi-9992-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  3 14:56:20.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 --namespace=crd-publish-openapi-3780 delete e2e-test-crd-publish-openapi-9992-crds test-cr'
Jan  3 14:56:20.696: INFO: stderr: ""
Jan  3 14:56:20.696: INFO: stdout: "e2e-test-crd-publish-openapi-9992-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan  3 14:56:20.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 explain e2e-test-crd-publish-openapi-9992-crds'
Jan  3 14:56:20.832: INFO: stderr: ""
Jan  3 14:56:20.832: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9992-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:56:23.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3780" for this suite.
Jan  3 14:56:29.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:56:29.923: INFO: namespace crd-publish-openapi-3780 deletion completed in 6.194725477s

• [SLOW TEST:13.446 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:56:29.923: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-fps7
STEP: Creating a pod to test atomic-volume-subpath
Jan  3 14:56:29.958: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-fps7" in namespace "subpath-8980" to be "success or failure"
Jan  3 14:56:29.959: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.249158ms
Jan  3 14:56:31.962: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00420765s
Jan  3 14:56:33.964: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006022632s
Jan  3 14:56:35.965: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007429274s
Jan  3 14:56:37.967: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Running", Reason="", readiness=true. Elapsed: 8.009254195s
Jan  3 14:56:39.970: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Running", Reason="", readiness=true. Elapsed: 10.012064947s
Jan  3 14:56:41.972: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Running", Reason="", readiness=true. Elapsed: 12.014250751s
Jan  3 14:56:43.976: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Running", Reason="", readiness=true. Elapsed: 14.018499907s
Jan  3 14:56:45.985: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Running", Reason="", readiness=true. Elapsed: 16.026813027s
Jan  3 14:56:47.997: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Running", Reason="", readiness=true. Elapsed: 18.039552741s
Jan  3 14:56:49.999: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Running", Reason="", readiness=true. Elapsed: 20.041280483s
Jan  3 14:56:52.001: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Running", Reason="", readiness=true. Elapsed: 22.042966488s
Jan  3 14:56:54.003: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Running", Reason="", readiness=true. Elapsed: 24.044737299s
Jan  3 14:56:56.004: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Running", Reason="", readiness=true. Elapsed: 26.046136847s
Jan  3 14:56:58.017: INFO: Pod "pod-subpath-test-projected-fps7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.059082341s
STEP: Saw pod success
Jan  3 14:56:58.017: INFO: Pod "pod-subpath-test-projected-fps7" satisfied condition "success or failure"
Jan  3 14:56:58.031: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-projected-fps7 container test-container-subpath-projected-fps7: <nil>
STEP: delete the pod
Jan  3 14:56:58.095: INFO: Waiting for pod pod-subpath-test-projected-fps7 to disappear
Jan  3 14:56:58.095: INFO: Pod pod-subpath-test-projected-fps7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-fps7
Jan  3 14:56:58.095: INFO: Deleting pod "pod-subpath-test-projected-fps7" in namespace "subpath-8980"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:56:58.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8980" for this suite.
Jan  3 14:57:04.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:57:04.201: INFO: namespace subpath-8980 deletion completed in 6.099554525s

• [SLOW TEST:34.278 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:57:04.204: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 14:57:04.224: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c1042bb-a475-4dc8-a822-7916493e2634" in namespace "downward-api-8844" to be "success or failure"
Jan  3 14:57:04.246: INFO: Pod "downwardapi-volume-5c1042bb-a475-4dc8-a822-7916493e2634": Phase="Pending", Reason="", readiness=false. Elapsed: 21.500517ms
Jan  3 14:57:06.249: INFO: Pod "downwardapi-volume-5c1042bb-a475-4dc8-a822-7916493e2634": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024887376s
Jan  3 14:57:08.251: INFO: Pod "downwardapi-volume-5c1042bb-a475-4dc8-a822-7916493e2634": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026712033s
Jan  3 14:57:10.256: INFO: Pod "downwardapi-volume-5c1042bb-a475-4dc8-a822-7916493e2634": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032332274s
Jan  3 14:57:12.266: INFO: Pod "downwardapi-volume-5c1042bb-a475-4dc8-a822-7916493e2634": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.042071113s
STEP: Saw pod success
Jan  3 14:57:12.266: INFO: Pod "downwardapi-volume-5c1042bb-a475-4dc8-a822-7916493e2634" satisfied condition "success or failure"
Jan  3 14:57:12.282: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-5c1042bb-a475-4dc8-a822-7916493e2634 container client-container: <nil>
STEP: delete the pod
Jan  3 14:57:12.296: INFO: Waiting for pod downwardapi-volume-5c1042bb-a475-4dc8-a822-7916493e2634 to disappear
Jan  3 14:57:12.297: INFO: Pod downwardapi-volume-5c1042bb-a475-4dc8-a822-7916493e2634 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:57:12.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8844" for this suite.
Jan  3 14:57:18.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:57:18.360: INFO: namespace downward-api-8844 deletion completed in 6.05990673s

• [SLOW TEST:14.156 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:57:18.361: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9754
I0103 14:57:18.382304      23 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9754, replica count: 1
I0103 14:57:19.433202      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:57:20.433311      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:57:21.433440      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:57:22.433583      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:57:23.453306      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:57:24.453546      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:57:25.453841      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 14:57:26.454633      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 14:57:26.560: INFO: Created: latency-svc-xnj8r
Jan  3 14:57:26.563: INFO: Got endpoints: latency-svc-xnj8r [8.598461ms]
Jan  3 14:57:26.568: INFO: Created: latency-svc-cgkrt
Jan  3 14:57:26.573: INFO: Created: latency-svc-4ltb5
Jan  3 14:57:26.573: INFO: Got endpoints: latency-svc-cgkrt [10.24074ms]
Jan  3 14:57:26.574: INFO: Got endpoints: latency-svc-4ltb5 [9.541809ms]
Jan  3 14:57:26.576: INFO: Created: latency-svc-8ntrh
Jan  3 14:57:26.588: INFO: Got endpoints: latency-svc-8ntrh [24.766125ms]
Jan  3 14:57:26.589: INFO: Created: latency-svc-4jr4m
Jan  3 14:57:26.593: INFO: Got endpoints: latency-svc-4jr4m [29.06524ms]
Jan  3 14:57:26.595: INFO: Created: latency-svc-7t67h
Jan  3 14:57:26.598: INFO: Got endpoints: latency-svc-7t67h [34.31468ms]
Jan  3 14:57:26.599: INFO: Created: latency-svc-qmnkr
Jan  3 14:57:26.603: INFO: Got endpoints: latency-svc-qmnkr [39.299871ms]
Jan  3 14:57:26.610: INFO: Created: latency-svc-qdm4g
Jan  3 14:57:26.612: INFO: Created: latency-svc-xlx72
Jan  3 14:57:26.616: INFO: Got endpoints: latency-svc-xlx72 [52.530989ms]
Jan  3 14:57:26.617: INFO: Got endpoints: latency-svc-qdm4g [52.919041ms]
Jan  3 14:57:26.617: INFO: Created: latency-svc-pqjvq
Jan  3 14:57:26.618: INFO: Got endpoints: latency-svc-pqjvq [52.756166ms]
Jan  3 14:57:26.621: INFO: Created: latency-svc-8q6zb
Jan  3 14:57:26.623: INFO: Got endpoints: latency-svc-8q6zb [58.46727ms]
Jan  3 14:57:26.624: INFO: Created: latency-svc-k9xmp
Jan  3 14:57:26.626: INFO: Got endpoints: latency-svc-k9xmp [61.456921ms]
Jan  3 14:57:26.629: INFO: Created: latency-svc-cjsjm
Jan  3 14:57:26.631: INFO: Got endpoints: latency-svc-cjsjm [65.768998ms]
Jan  3 14:57:26.631: INFO: Created: latency-svc-cxssl
Jan  3 14:57:26.633: INFO: Got endpoints: latency-svc-cxssl [69.514456ms]
Jan  3 14:57:26.638: INFO: Created: latency-svc-btb94
Jan  3 14:57:26.639: INFO: Got endpoints: latency-svc-btb94 [74.576192ms]
Jan  3 14:57:26.639: INFO: Created: latency-svc-z2sfp
Jan  3 14:57:26.642: INFO: Got endpoints: latency-svc-z2sfp [77.759757ms]
Jan  3 14:57:26.648: INFO: Created: latency-svc-972tr
Jan  3 14:57:26.648: INFO: Got endpoints: latency-svc-972tr [73.229883ms]
Jan  3 14:57:26.648: INFO: Created: latency-svc-p27kv
Jan  3 14:57:26.650: INFO: Got endpoints: latency-svc-p27kv [76.709634ms]
Jan  3 14:57:26.651: INFO: Created: latency-svc-879zg
Jan  3 14:57:26.653: INFO: Got endpoints: latency-svc-879zg [64.341635ms]
Jan  3 14:57:26.658: INFO: Created: latency-svc-rmmcb
Jan  3 14:57:26.670: INFO: Got endpoints: latency-svc-rmmcb [77.114193ms]
Jan  3 14:57:26.670: INFO: Created: latency-svc-sdg2f
Jan  3 14:57:26.672: INFO: Got endpoints: latency-svc-sdg2f [73.777747ms]
Jan  3 14:57:26.675: INFO: Created: latency-svc-77cq4
Jan  3 14:57:26.678: INFO: Got endpoints: latency-svc-77cq4 [74.534227ms]
Jan  3 14:57:26.679: INFO: Created: latency-svc-ppcll
Jan  3 14:57:26.682: INFO: Got endpoints: latency-svc-ppcll [65.633204ms]
Jan  3 14:57:26.683: INFO: Created: latency-svc-z96kz
Jan  3 14:57:26.687: INFO: Got endpoints: latency-svc-z96kz [70.699717ms]
Jan  3 14:57:26.689: INFO: Created: latency-svc-j4jp2
Jan  3 14:57:26.693: INFO: Got endpoints: latency-svc-j4jp2 [75.051048ms]
Jan  3 14:57:26.693: INFO: Created: latency-svc-qmckz
Jan  3 14:57:26.695: INFO: Got endpoints: latency-svc-qmckz [71.450256ms]
Jan  3 14:57:26.697: INFO: Created: latency-svc-pf6zq
Jan  3 14:57:26.699: INFO: Got endpoints: latency-svc-pf6zq [72.214906ms]
Jan  3 14:57:26.702: INFO: Created: latency-svc-wr44w
Jan  3 14:57:26.705: INFO: Got endpoints: latency-svc-wr44w [74.36627ms]
Jan  3 14:57:26.705: INFO: Created: latency-svc-fjtqn
Jan  3 14:57:26.708: INFO: Got endpoints: latency-svc-fjtqn [74.578038ms]
Jan  3 14:57:26.711: INFO: Created: latency-svc-86nmz
Jan  3 14:57:26.716: INFO: Got endpoints: latency-svc-86nmz [76.758118ms]
Jan  3 14:57:26.717: INFO: Created: latency-svc-pmtw6
Jan  3 14:57:26.722: INFO: Got endpoints: latency-svc-pmtw6 [79.718936ms]
Jan  3 14:57:26.722: INFO: Created: latency-svc-f5kzj
Jan  3 14:57:26.725: INFO: Created: latency-svc-wpcps
Jan  3 14:57:26.737: INFO: Got endpoints: latency-svc-wpcps [86.857611ms]
Jan  3 14:57:26.743: INFO: Got endpoints: latency-svc-f5kzj [95.052102ms]
Jan  3 14:57:26.757: INFO: Created: latency-svc-zcvhz
Jan  3 14:57:26.764: INFO: Created: latency-svc-vjj94
Jan  3 14:57:26.765: INFO: Got endpoints: latency-svc-zcvhz [111.929666ms]
Jan  3 14:57:26.768: INFO: Created: latency-svc-nhpj2
Jan  3 14:57:26.785: INFO: Created: latency-svc-crdfc
Jan  3 14:57:26.789: INFO: Created: latency-svc-vm4lm
Jan  3 14:57:26.794: INFO: Created: latency-svc-6rm5z
Jan  3 14:57:26.798: INFO: Created: latency-svc-q982d
Jan  3 14:57:26.801: INFO: Created: latency-svc-pqswp
Jan  3 14:57:26.804: INFO: Created: latency-svc-qc59v
Jan  3 14:57:26.809: INFO: Created: latency-svc-ckgcl
Jan  3 14:57:26.813: INFO: Created: latency-svc-tzgvd
Jan  3 14:57:26.814: INFO: Got endpoints: latency-svc-vjj94 [144.189641ms]
Jan  3 14:57:26.816: INFO: Created: latency-svc-vxgcj
Jan  3 14:57:26.820: INFO: Created: latency-svc-vnczp
Jan  3 14:57:26.824: INFO: Created: latency-svc-zmmdf
Jan  3 14:57:26.828: INFO: Created: latency-svc-ksltl
Jan  3 14:57:26.831: INFO: Created: latency-svc-n5r2r
Jan  3 14:57:26.834: INFO: Created: latency-svc-9vfrn
Jan  3 14:57:26.873: INFO: Got endpoints: latency-svc-nhpj2 [200.510684ms]
Jan  3 14:57:26.894: INFO: Created: latency-svc-hn6l5
Jan  3 14:57:26.915: INFO: Got endpoints: latency-svc-crdfc [236.639985ms]
Jan  3 14:57:26.922: INFO: Created: latency-svc-5gn2v
Jan  3 14:57:26.964: INFO: Got endpoints: latency-svc-vm4lm [281.553518ms]
Jan  3 14:57:26.973: INFO: Created: latency-svc-v8mbh
Jan  3 14:57:27.013: INFO: Got endpoints: latency-svc-6rm5z [325.69151ms]
Jan  3 14:57:27.020: INFO: Created: latency-svc-wvbgb
Jan  3 14:57:27.068: INFO: Got endpoints: latency-svc-q982d [375.678642ms]
Jan  3 14:57:27.073: INFO: Created: latency-svc-7lj8b
Jan  3 14:57:27.116: INFO: Got endpoints: latency-svc-pqswp [421.252462ms]
Jan  3 14:57:27.122: INFO: Created: latency-svc-dzvvr
Jan  3 14:57:27.165: INFO: Got endpoints: latency-svc-qc59v [466.499561ms]
Jan  3 14:57:27.173: INFO: Created: latency-svc-cxv4l
Jan  3 14:57:27.213: INFO: Got endpoints: latency-svc-ckgcl [508.157902ms]
Jan  3 14:57:27.229: INFO: Created: latency-svc-n94xm
Jan  3 14:57:27.264: INFO: Got endpoints: latency-svc-tzgvd [556.457966ms]
Jan  3 14:57:27.274: INFO: Created: latency-svc-5n98s
Jan  3 14:57:27.314: INFO: Got endpoints: latency-svc-vxgcj [598.067963ms]
Jan  3 14:57:27.323: INFO: Created: latency-svc-xrzhj
Jan  3 14:57:27.394: INFO: Got endpoints: latency-svc-vnczp [672.469598ms]
Jan  3 14:57:27.411: INFO: Created: latency-svc-d9hk7
Jan  3 14:57:27.414: INFO: Got endpoints: latency-svc-zmmdf [676.775757ms]
Jan  3 14:57:27.421: INFO: Created: latency-svc-k9hhd
Jan  3 14:57:27.464: INFO: Got endpoints: latency-svc-ksltl [720.964505ms]
Jan  3 14:57:27.472: INFO: Created: latency-svc-l4mdw
Jan  3 14:57:27.514: INFO: Got endpoints: latency-svc-n5r2r [749.404692ms]
Jan  3 14:57:27.520: INFO: Created: latency-svc-dgfjt
Jan  3 14:57:27.563: INFO: Got endpoints: latency-svc-9vfrn [748.798457ms]
Jan  3 14:57:27.572: INFO: Created: latency-svc-zfmgw
Jan  3 14:57:27.630: INFO: Got endpoints: latency-svc-hn6l5 [757.591089ms]
Jan  3 14:57:27.640: INFO: Created: latency-svc-f2gzj
Jan  3 14:57:27.663: INFO: Got endpoints: latency-svc-5gn2v [748.697688ms]
Jan  3 14:57:27.669: INFO: Created: latency-svc-ph6ld
Jan  3 14:57:27.716: INFO: Got endpoints: latency-svc-v8mbh [751.343254ms]
Jan  3 14:57:27.722: INFO: Created: latency-svc-b8vtc
Jan  3 14:57:27.764: INFO: Got endpoints: latency-svc-wvbgb [751.121579ms]
Jan  3 14:57:27.781: INFO: Created: latency-svc-56xsd
Jan  3 14:57:27.814: INFO: Got endpoints: latency-svc-7lj8b [745.06801ms]
Jan  3 14:57:27.820: INFO: Created: latency-svc-llm4s
Jan  3 14:57:27.869: INFO: Got endpoints: latency-svc-dzvvr [752.596843ms]
Jan  3 14:57:27.875: INFO: Created: latency-svc-252fr
Jan  3 14:57:27.913: INFO: Got endpoints: latency-svc-cxv4l [747.620955ms]
Jan  3 14:57:27.933: INFO: Created: latency-svc-ltt4l
Jan  3 14:57:27.965: INFO: Got endpoints: latency-svc-n94xm [751.474277ms]
Jan  3 14:57:27.975: INFO: Created: latency-svc-jv2mv
Jan  3 14:57:28.014: INFO: Got endpoints: latency-svc-5n98s [749.401629ms]
Jan  3 14:57:28.022: INFO: Created: latency-svc-t27sd
Jan  3 14:57:28.064: INFO: Got endpoints: latency-svc-xrzhj [749.990942ms]
Jan  3 14:57:28.070: INFO: Created: latency-svc-6vd4l
Jan  3 14:57:28.114: INFO: Got endpoints: latency-svc-d9hk7 [719.434213ms]
Jan  3 14:57:28.121: INFO: Created: latency-svc-fp9cg
Jan  3 14:57:28.164: INFO: Got endpoints: latency-svc-k9hhd [749.775804ms]
Jan  3 14:57:28.172: INFO: Created: latency-svc-fqzl2
Jan  3 14:57:28.216: INFO: Got endpoints: latency-svc-l4mdw [751.72162ms]
Jan  3 14:57:28.223: INFO: Created: latency-svc-lhxrh
Jan  3 14:57:28.275: INFO: Got endpoints: latency-svc-dgfjt [760.988924ms]
Jan  3 14:57:28.282: INFO: Created: latency-svc-t5l8q
Jan  3 14:57:28.352: INFO: Got endpoints: latency-svc-zfmgw [788.287006ms]
Jan  3 14:57:28.363: INFO: Created: latency-svc-5thks
Jan  3 14:57:28.363: INFO: Got endpoints: latency-svc-f2gzj [732.454266ms]
Jan  3 14:57:28.372: INFO: Created: latency-svc-hr4r8
Jan  3 14:57:28.415: INFO: Got endpoints: latency-svc-ph6ld [751.347199ms]
Jan  3 14:57:28.420: INFO: Created: latency-svc-x5lqh
Jan  3 14:57:28.463: INFO: Got endpoints: latency-svc-b8vtc [747.693179ms]
Jan  3 14:57:28.469: INFO: Created: latency-svc-mt9h8
Jan  3 14:57:28.516: INFO: Got endpoints: latency-svc-56xsd [750.708859ms]
Jan  3 14:57:28.526: INFO: Created: latency-svc-l6csx
Jan  3 14:57:28.564: INFO: Got endpoints: latency-svc-llm4s [750.504897ms]
Jan  3 14:57:28.571: INFO: Created: latency-svc-wzb8t
Jan  3 14:57:28.613: INFO: Got endpoints: latency-svc-252fr [744.325762ms]
Jan  3 14:57:28.623: INFO: Created: latency-svc-7dk9c
Jan  3 14:57:28.666: INFO: Got endpoints: latency-svc-ltt4l [752.724572ms]
Jan  3 14:57:28.676: INFO: Created: latency-svc-cqbmv
Jan  3 14:57:28.717: INFO: Got endpoints: latency-svc-jv2mv [751.954599ms]
Jan  3 14:57:28.727: INFO: Created: latency-svc-j5cpw
Jan  3 14:57:28.766: INFO: Got endpoints: latency-svc-t27sd [751.873074ms]
Jan  3 14:57:28.783: INFO: Created: latency-svc-77d2b
Jan  3 14:57:28.813: INFO: Got endpoints: latency-svc-6vd4l [749.344033ms]
Jan  3 14:57:28.822: INFO: Created: latency-svc-kn5rd
Jan  3 14:57:28.866: INFO: Got endpoints: latency-svc-fp9cg [751.568226ms]
Jan  3 14:57:28.872: INFO: Created: latency-svc-jp8rk
Jan  3 14:57:28.923: INFO: Got endpoints: latency-svc-fqzl2 [759.425894ms]
Jan  3 14:57:28.938: INFO: Created: latency-svc-zmdlm
Jan  3 14:57:28.969: INFO: Got endpoints: latency-svc-lhxrh [753.203929ms]
Jan  3 14:57:28.977: INFO: Created: latency-svc-wn24q
Jan  3 14:57:29.013: INFO: Got endpoints: latency-svc-t5l8q [737.165938ms]
Jan  3 14:57:29.020: INFO: Created: latency-svc-pjwr7
Jan  3 14:57:29.068: INFO: Got endpoints: latency-svc-5thks [715.976896ms]
Jan  3 14:57:29.079: INFO: Created: latency-svc-m5xgm
Jan  3 14:57:29.116: INFO: Got endpoints: latency-svc-hr4r8 [753.346427ms]
Jan  3 14:57:29.131: INFO: Created: latency-svc-mzbbz
Jan  3 14:57:29.164: INFO: Got endpoints: latency-svc-x5lqh [748.990384ms]
Jan  3 14:57:29.170: INFO: Created: latency-svc-8swhj
Jan  3 14:57:29.216: INFO: Got endpoints: latency-svc-mt9h8 [752.156471ms]
Jan  3 14:57:29.227: INFO: Created: latency-svc-8485h
Jan  3 14:57:29.264: INFO: Got endpoints: latency-svc-l6csx [747.550572ms]
Jan  3 14:57:29.282: INFO: Created: latency-svc-dt7ps
Jan  3 14:57:29.314: INFO: Got endpoints: latency-svc-wzb8t [749.423508ms]
Jan  3 14:57:29.335: INFO: Created: latency-svc-8sq6m
Jan  3 14:57:29.376: INFO: Got endpoints: latency-svc-7dk9c [762.287116ms]
Jan  3 14:57:29.386: INFO: Created: latency-svc-s4pm8
Jan  3 14:57:29.414: INFO: Got endpoints: latency-svc-cqbmv [748.364963ms]
Jan  3 14:57:29.421: INFO: Created: latency-svc-mpzjl
Jan  3 14:57:29.465: INFO: Got endpoints: latency-svc-j5cpw [747.665665ms]
Jan  3 14:57:29.471: INFO: Created: latency-svc-d4zfz
Jan  3 14:57:29.514: INFO: Got endpoints: latency-svc-77d2b [748.359233ms]
Jan  3 14:57:29.548: INFO: Created: latency-svc-khsc5
Jan  3 14:57:29.566: INFO: Got endpoints: latency-svc-kn5rd [752.306919ms]
Jan  3 14:57:29.576: INFO: Created: latency-svc-drdc7
Jan  3 14:57:29.635: INFO: Got endpoints: latency-svc-jp8rk [768.959431ms]
Jan  3 14:57:29.655: INFO: Created: latency-svc-wgqfl
Jan  3 14:57:29.665: INFO: Got endpoints: latency-svc-zmdlm [734.401296ms]
Jan  3 14:57:29.680: INFO: Created: latency-svc-42hlp
Jan  3 14:57:29.726: INFO: Got endpoints: latency-svc-wn24q [756.987027ms]
Jan  3 14:57:29.748: INFO: Created: latency-svc-t7pwq
Jan  3 14:57:29.772: INFO: Got endpoints: latency-svc-pjwr7 [759.032292ms]
Jan  3 14:57:29.780: INFO: Created: latency-svc-vc5c6
Jan  3 14:57:29.825: INFO: Got endpoints: latency-svc-m5xgm [757.279321ms]
Jan  3 14:57:29.844: INFO: Created: latency-svc-5c95g
Jan  3 14:57:29.885: INFO: Got endpoints: latency-svc-mzbbz [768.607233ms]
Jan  3 14:57:29.914: INFO: Created: latency-svc-gvw95
Jan  3 14:57:29.918: INFO: Got endpoints: latency-svc-8swhj [754.345748ms]
Jan  3 14:57:29.931: INFO: Created: latency-svc-j6trk
Jan  3 14:57:29.965: INFO: Got endpoints: latency-svc-8485h [748.928767ms]
Jan  3 14:57:29.971: INFO: Created: latency-svc-vxk6w
Jan  3 14:57:30.021: INFO: Got endpoints: latency-svc-dt7ps [756.995921ms]
Jan  3 14:57:30.027: INFO: Created: latency-svc-mc75z
Jan  3 14:57:30.065: INFO: Got endpoints: latency-svc-8sq6m [750.772595ms]
Jan  3 14:57:30.072: INFO: Created: latency-svc-64vkv
Jan  3 14:57:30.113: INFO: Got endpoints: latency-svc-s4pm8 [737.302542ms]
Jan  3 14:57:30.118: INFO: Created: latency-svc-nvhgn
Jan  3 14:57:30.164: INFO: Got endpoints: latency-svc-mpzjl [749.75268ms]
Jan  3 14:57:30.169: INFO: Created: latency-svc-qbpsh
Jan  3 14:57:30.213: INFO: Got endpoints: latency-svc-d4zfz [747.745868ms]
Jan  3 14:57:30.222: INFO: Created: latency-svc-jbs5z
Jan  3 14:57:30.263: INFO: Got endpoints: latency-svc-khsc5 [748.774339ms]
Jan  3 14:57:30.268: INFO: Created: latency-svc-nf6wk
Jan  3 14:57:30.315: INFO: Got endpoints: latency-svc-drdc7 [749.495756ms]
Jan  3 14:57:30.321: INFO: Created: latency-svc-97v2x
Jan  3 14:57:30.363: INFO: Got endpoints: latency-svc-wgqfl [728.262733ms]
Jan  3 14:57:30.368: INFO: Created: latency-svc-vxk22
Jan  3 14:57:30.419: INFO: Got endpoints: latency-svc-42hlp [753.191807ms]
Jan  3 14:57:30.425: INFO: Created: latency-svc-lxrfq
Jan  3 14:57:30.464: INFO: Got endpoints: latency-svc-t7pwq [737.578473ms]
Jan  3 14:57:30.470: INFO: Created: latency-svc-bjr6t
Jan  3 14:57:30.514: INFO: Got endpoints: latency-svc-vc5c6 [742.283346ms]
Jan  3 14:57:30.520: INFO: Created: latency-svc-pdzt6
Jan  3 14:57:30.564: INFO: Got endpoints: latency-svc-5c95g [738.498323ms]
Jan  3 14:57:30.570: INFO: Created: latency-svc-vq5fr
Jan  3 14:57:30.613: INFO: Got endpoints: latency-svc-gvw95 [727.973538ms]
Jan  3 14:57:30.624: INFO: Created: latency-svc-n42jt
Jan  3 14:57:30.663: INFO: Got endpoints: latency-svc-j6trk [742.743251ms]
Jan  3 14:57:30.667: INFO: Created: latency-svc-58gvt
Jan  3 14:57:30.713: INFO: Got endpoints: latency-svc-vxk6w [748.103257ms]
Jan  3 14:57:30.717: INFO: Created: latency-svc-ntm54
Jan  3 14:57:30.763: INFO: Got endpoints: latency-svc-mc75z [741.788598ms]
Jan  3 14:57:30.768: INFO: Created: latency-svc-trk7v
Jan  3 14:57:30.816: INFO: Got endpoints: latency-svc-64vkv [750.691776ms]
Jan  3 14:57:30.820: INFO: Created: latency-svc-l9vh4
Jan  3 14:57:30.863: INFO: Got endpoints: latency-svc-nvhgn [749.953357ms]
Jan  3 14:57:30.869: INFO: Created: latency-svc-z9r2q
Jan  3 14:57:30.921: INFO: Got endpoints: latency-svc-qbpsh [756.569183ms]
Jan  3 14:57:30.937: INFO: Created: latency-svc-rqfw8
Jan  3 14:57:30.964: INFO: Got endpoints: latency-svc-jbs5z [751.071266ms]
Jan  3 14:57:30.974: INFO: Created: latency-svc-5hdnl
Jan  3 14:57:31.014: INFO: Got endpoints: latency-svc-nf6wk [750.959956ms]
Jan  3 14:57:31.025: INFO: Created: latency-svc-wfzg5
Jan  3 14:57:31.064: INFO: Got endpoints: latency-svc-97v2x [748.242731ms]
Jan  3 14:57:31.070: INFO: Created: latency-svc-p5szk
Jan  3 14:57:31.113: INFO: Got endpoints: latency-svc-vxk22 [750.279521ms]
Jan  3 14:57:31.131: INFO: Created: latency-svc-cgbpv
Jan  3 14:57:31.163: INFO: Got endpoints: latency-svc-lxrfq [744.444388ms]
Jan  3 14:57:31.169: INFO: Created: latency-svc-rkndx
Jan  3 14:57:31.213: INFO: Got endpoints: latency-svc-bjr6t [749.304338ms]
Jan  3 14:57:31.218: INFO: Created: latency-svc-4lqfd
Jan  3 14:57:31.263: INFO: Got endpoints: latency-svc-pdzt6 [749.184555ms]
Jan  3 14:57:31.273: INFO: Created: latency-svc-xntbr
Jan  3 14:57:31.314: INFO: Got endpoints: latency-svc-vq5fr [749.966349ms]
Jan  3 14:57:31.321: INFO: Created: latency-svc-n7t4b
Jan  3 14:57:31.363: INFO: Got endpoints: latency-svc-n42jt [749.901533ms]
Jan  3 14:57:31.372: INFO: Created: latency-svc-knkcb
Jan  3 14:57:31.415: INFO: Got endpoints: latency-svc-58gvt [752.180954ms]
Jan  3 14:57:31.422: INFO: Created: latency-svc-8ljss
Jan  3 14:57:31.468: INFO: Got endpoints: latency-svc-ntm54 [755.466849ms]
Jan  3 14:57:31.477: INFO: Created: latency-svc-9w6tz
Jan  3 14:57:31.520: INFO: Got endpoints: latency-svc-trk7v [757.510638ms]
Jan  3 14:57:31.527: INFO: Created: latency-svc-hkpzj
Jan  3 14:57:31.568: INFO: Got endpoints: latency-svc-l9vh4 [752.333552ms]
Jan  3 14:57:31.584: INFO: Created: latency-svc-dcgcb
Jan  3 14:57:31.617: INFO: Got endpoints: latency-svc-z9r2q [754.305171ms]
Jan  3 14:57:31.638: INFO: Created: latency-svc-6gbl2
Jan  3 14:57:31.663: INFO: Got endpoints: latency-svc-rqfw8 [741.950598ms]
Jan  3 14:57:31.668: INFO: Created: latency-svc-47ds5
Jan  3 14:57:31.715: INFO: Got endpoints: latency-svc-5hdnl [751.059333ms]
Jan  3 14:57:31.721: INFO: Created: latency-svc-xq2dl
Jan  3 14:57:31.776: INFO: Got endpoints: latency-svc-wfzg5 [761.257852ms]
Jan  3 14:57:31.781: INFO: Created: latency-svc-55jrn
Jan  3 14:57:31.813: INFO: Got endpoints: latency-svc-p5szk [749.609882ms]
Jan  3 14:57:31.819: INFO: Created: latency-svc-58frc
Jan  3 14:57:31.863: INFO: Got endpoints: latency-svc-cgbpv [749.526035ms]
Jan  3 14:57:31.871: INFO: Created: latency-svc-5r8zl
Jan  3 14:57:31.913: INFO: Got endpoints: latency-svc-rkndx [749.86762ms]
Jan  3 14:57:31.919: INFO: Created: latency-svc-zwns7
Jan  3 14:57:31.964: INFO: Got endpoints: latency-svc-4lqfd [750.590307ms]
Jan  3 14:57:31.971: INFO: Created: latency-svc-dch4r
Jan  3 14:57:32.014: INFO: Got endpoints: latency-svc-xntbr [751.039202ms]
Jan  3 14:57:32.022: INFO: Created: latency-svc-96hcq
Jan  3 14:57:32.070: INFO: Got endpoints: latency-svc-n7t4b [756.637769ms]
Jan  3 14:57:32.079: INFO: Created: latency-svc-xqqpd
Jan  3 14:57:32.114: INFO: Got endpoints: latency-svc-knkcb [751.251016ms]
Jan  3 14:57:32.121: INFO: Created: latency-svc-rhzsp
Jan  3 14:57:32.163: INFO: Got endpoints: latency-svc-8ljss [747.775396ms]
Jan  3 14:57:32.170: INFO: Created: latency-svc-76qgd
Jan  3 14:57:32.213: INFO: Got endpoints: latency-svc-9w6tz [744.326034ms]
Jan  3 14:57:32.221: INFO: Created: latency-svc-twdpf
Jan  3 14:57:32.263: INFO: Got endpoints: latency-svc-hkpzj [742.074372ms]
Jan  3 14:57:32.270: INFO: Created: latency-svc-hj6vp
Jan  3 14:57:32.313: INFO: Got endpoints: latency-svc-dcgcb [745.340021ms]
Jan  3 14:57:32.322: INFO: Created: latency-svc-h2rh4
Jan  3 14:57:32.365: INFO: Got endpoints: latency-svc-6gbl2 [747.037281ms]
Jan  3 14:57:32.379: INFO: Created: latency-svc-fqlll
Jan  3 14:57:32.414: INFO: Got endpoints: latency-svc-47ds5 [750.862456ms]
Jan  3 14:57:32.421: INFO: Created: latency-svc-568pl
Jan  3 14:57:32.463: INFO: Got endpoints: latency-svc-xq2dl [747.505866ms]
Jan  3 14:57:32.468: INFO: Created: latency-svc-sn8bg
Jan  3 14:57:32.513: INFO: Got endpoints: latency-svc-55jrn [737.71916ms]
Jan  3 14:57:32.518: INFO: Created: latency-svc-h5n5z
Jan  3 14:57:32.563: INFO: Got endpoints: latency-svc-58frc [749.383002ms]
Jan  3 14:57:32.568: INFO: Created: latency-svc-xlnbr
Jan  3 14:57:32.613: INFO: Got endpoints: latency-svc-5r8zl [750.232411ms]
Jan  3 14:57:32.618: INFO: Created: latency-svc-cmcrj
Jan  3 14:57:32.663: INFO: Got endpoints: latency-svc-zwns7 [749.956326ms]
Jan  3 14:57:32.668: INFO: Created: latency-svc-7xztn
Jan  3 14:57:32.712: INFO: Got endpoints: latency-svc-dch4r [748.692137ms]
Jan  3 14:57:32.717: INFO: Created: latency-svc-c2n6w
Jan  3 14:57:32.766: INFO: Got endpoints: latency-svc-96hcq [751.629018ms]
Jan  3 14:57:32.776: INFO: Created: latency-svc-4sjg7
Jan  3 14:57:32.814: INFO: Got endpoints: latency-svc-xqqpd [743.818613ms]
Jan  3 14:57:32.823: INFO: Created: latency-svc-95gfk
Jan  3 14:57:32.863: INFO: Got endpoints: latency-svc-rhzsp [748.72985ms]
Jan  3 14:57:32.869: INFO: Created: latency-svc-4sbrl
Jan  3 14:57:32.918: INFO: Got endpoints: latency-svc-76qgd [754.307512ms]
Jan  3 14:57:32.927: INFO: Created: latency-svc-ndf9x
Jan  3 14:57:32.964: INFO: Got endpoints: latency-svc-twdpf [750.523493ms]
Jan  3 14:57:32.970: INFO: Created: latency-svc-w8sgz
Jan  3 14:57:33.014: INFO: Got endpoints: latency-svc-hj6vp [750.686847ms]
Jan  3 14:57:33.023: INFO: Created: latency-svc-ds4bx
Jan  3 14:57:33.063: INFO: Got endpoints: latency-svc-h2rh4 [748.837071ms]
Jan  3 14:57:33.080: INFO: Created: latency-svc-krsms
Jan  3 14:57:33.113: INFO: Got endpoints: latency-svc-fqlll [747.979411ms]
Jan  3 14:57:33.122: INFO: Created: latency-svc-jl4dl
Jan  3 14:57:33.163: INFO: Got endpoints: latency-svc-568pl [748.848886ms]
Jan  3 14:57:33.168: INFO: Created: latency-svc-ljjcb
Jan  3 14:57:33.222: INFO: Got endpoints: latency-svc-sn8bg [759.47092ms]
Jan  3 14:57:33.230: INFO: Created: latency-svc-l9xd2
Jan  3 14:57:33.263: INFO: Got endpoints: latency-svc-h5n5z [749.752287ms]
Jan  3 14:57:33.270: INFO: Created: latency-svc-9fgkv
Jan  3 14:57:33.313: INFO: Got endpoints: latency-svc-xlnbr [750.475739ms]
Jan  3 14:57:33.318: INFO: Created: latency-svc-ssnj5
Jan  3 14:57:33.363: INFO: Got endpoints: latency-svc-cmcrj [749.982205ms]
Jan  3 14:57:33.371: INFO: Created: latency-svc-cmtfc
Jan  3 14:57:33.413: INFO: Got endpoints: latency-svc-7xztn [749.441444ms]
Jan  3 14:57:33.417: INFO: Created: latency-svc-bml4f
Jan  3 14:57:33.463: INFO: Got endpoints: latency-svc-c2n6w [750.629398ms]
Jan  3 14:57:33.474: INFO: Created: latency-svc-lsz67
Jan  3 14:57:33.512: INFO: Got endpoints: latency-svc-4sjg7 [746.077453ms]
Jan  3 14:57:33.517: INFO: Created: latency-svc-6x587
Jan  3 14:57:33.563: INFO: Got endpoints: latency-svc-95gfk [747.797083ms]
Jan  3 14:57:33.568: INFO: Created: latency-svc-98844
Jan  3 14:57:33.613: INFO: Got endpoints: latency-svc-4sbrl [750.086232ms]
Jan  3 14:57:33.619: INFO: Created: latency-svc-4z6g2
Jan  3 14:57:33.663: INFO: Got endpoints: latency-svc-ndf9x [744.862881ms]
Jan  3 14:57:33.669: INFO: Created: latency-svc-wm4rn
Jan  3 14:57:33.714: INFO: Got endpoints: latency-svc-w8sgz [750.395524ms]
Jan  3 14:57:33.719: INFO: Created: latency-svc-pwzp9
Jan  3 14:57:33.763: INFO: Got endpoints: latency-svc-ds4bx [749.210849ms]
Jan  3 14:57:33.768: INFO: Created: latency-svc-g8c2b
Jan  3 14:57:33.817: INFO: Got endpoints: latency-svc-krsms [753.797756ms]
Jan  3 14:57:33.823: INFO: Created: latency-svc-bndr7
Jan  3 14:57:33.863: INFO: Got endpoints: latency-svc-jl4dl [750.411569ms]
Jan  3 14:57:33.868: INFO: Created: latency-svc-68tct
Jan  3 14:57:33.916: INFO: Got endpoints: latency-svc-ljjcb [752.601848ms]
Jan  3 14:57:33.923: INFO: Created: latency-svc-xl5dn
Jan  3 14:57:33.966: INFO: Got endpoints: latency-svc-l9xd2 [743.18551ms]
Jan  3 14:57:33.971: INFO: Created: latency-svc-94btc
Jan  3 14:57:34.013: INFO: Got endpoints: latency-svc-9fgkv [749.396088ms]
Jan  3 14:57:34.020: INFO: Created: latency-svc-26dqv
Jan  3 14:57:34.078: INFO: Got endpoints: latency-svc-ssnj5 [764.021854ms]
Jan  3 14:57:34.128: INFO: Got endpoints: latency-svc-cmtfc [762.63911ms]
Jan  3 14:57:34.128: INFO: Created: latency-svc-m5vzn
Jan  3 14:57:34.136: INFO: Created: latency-svc-bgbxz
Jan  3 14:57:34.181: INFO: Got endpoints: latency-svc-bml4f [768.341469ms]
Jan  3 14:57:34.193: INFO: Created: latency-svc-99fjp
Jan  3 14:57:34.232: INFO: Got endpoints: latency-svc-lsz67 [768.404118ms]
Jan  3 14:57:34.240: INFO: Created: latency-svc-xrmp2
Jan  3 14:57:34.267: INFO: Got endpoints: latency-svc-6x587 [754.482126ms]
Jan  3 14:57:34.276: INFO: Created: latency-svc-76v5w
Jan  3 14:57:34.322: INFO: Got endpoints: latency-svc-98844 [759.006089ms]
Jan  3 14:57:34.355: INFO: Created: latency-svc-nl8nb
Jan  3 14:57:34.365: INFO: Got endpoints: latency-svc-4z6g2 [751.994768ms]
Jan  3 14:57:34.379: INFO: Created: latency-svc-5ck6f
Jan  3 14:57:34.418: INFO: Got endpoints: latency-svc-wm4rn [755.327832ms]
Jan  3 14:57:34.465: INFO: Got endpoints: latency-svc-pwzp9 [750.893485ms]
Jan  3 14:57:34.524: INFO: Got endpoints: latency-svc-g8c2b [761.164909ms]
Jan  3 14:57:34.581: INFO: Got endpoints: latency-svc-bndr7 [764.323717ms]
Jan  3 14:57:34.618: INFO: Got endpoints: latency-svc-68tct [754.605388ms]
Jan  3 14:57:34.665: INFO: Got endpoints: latency-svc-xl5dn [749.856564ms]
Jan  3 14:57:34.720: INFO: Got endpoints: latency-svc-94btc [753.792166ms]
Jan  3 14:57:34.775: INFO: Got endpoints: latency-svc-26dqv [762.066847ms]
Jan  3 14:57:34.820: INFO: Got endpoints: latency-svc-m5vzn [741.86016ms]
Jan  3 14:57:34.869: INFO: Got endpoints: latency-svc-bgbxz [741.104833ms]
Jan  3 14:57:34.920: INFO: Got endpoints: latency-svc-99fjp [738.352812ms]
Jan  3 14:57:34.965: INFO: Got endpoints: latency-svc-xrmp2 [733.480544ms]
Jan  3 14:57:35.015: INFO: Got endpoints: latency-svc-76v5w [747.960849ms]
Jan  3 14:57:35.068: INFO: Got endpoints: latency-svc-nl8nb [746.360794ms]
Jan  3 14:57:35.119: INFO: Got endpoints: latency-svc-5ck6f [753.761702ms]
Jan  3 14:57:35.120: INFO: Latencies: [9.541809ms 10.24074ms 24.766125ms 29.06524ms 34.31468ms 39.299871ms 52.530989ms 52.756166ms 52.919041ms 58.46727ms 61.456921ms 64.341635ms 65.633204ms 65.768998ms 69.514456ms 70.699717ms 71.450256ms 72.214906ms 73.229883ms 73.777747ms 74.36627ms 74.534227ms 74.576192ms 74.578038ms 75.051048ms 76.709634ms 76.758118ms 77.114193ms 77.759757ms 79.718936ms 86.857611ms 95.052102ms 111.929666ms 144.189641ms 200.510684ms 236.639985ms 281.553518ms 325.69151ms 375.678642ms 421.252462ms 466.499561ms 508.157902ms 556.457966ms 598.067963ms 672.469598ms 676.775757ms 715.976896ms 719.434213ms 720.964505ms 727.973538ms 728.262733ms 732.454266ms 733.480544ms 734.401296ms 737.165938ms 737.302542ms 737.578473ms 737.71916ms 738.352812ms 738.498323ms 741.104833ms 741.788598ms 741.86016ms 741.950598ms 742.074372ms 742.283346ms 742.743251ms 743.18551ms 743.818613ms 744.325762ms 744.326034ms 744.444388ms 744.862881ms 745.06801ms 745.340021ms 746.077453ms 746.360794ms 747.037281ms 747.505866ms 747.550572ms 747.620955ms 747.665665ms 747.693179ms 747.745868ms 747.775396ms 747.797083ms 747.960849ms 747.979411ms 748.103257ms 748.242731ms 748.359233ms 748.364963ms 748.692137ms 748.697688ms 748.72985ms 748.774339ms 748.798457ms 748.837071ms 748.848886ms 748.928767ms 748.990384ms 749.184555ms 749.210849ms 749.304338ms 749.344033ms 749.383002ms 749.396088ms 749.401629ms 749.404692ms 749.423508ms 749.441444ms 749.495756ms 749.526035ms 749.609882ms 749.752287ms 749.75268ms 749.775804ms 749.856564ms 749.86762ms 749.901533ms 749.953357ms 749.956326ms 749.966349ms 749.982205ms 749.990942ms 750.086232ms 750.232411ms 750.279521ms 750.395524ms 750.411569ms 750.475739ms 750.504897ms 750.523493ms 750.590307ms 750.629398ms 750.686847ms 750.691776ms 750.708859ms 750.772595ms 750.862456ms 750.893485ms 750.959956ms 751.039202ms 751.059333ms 751.071266ms 751.121579ms 751.251016ms 751.343254ms 751.347199ms 751.474277ms 751.568226ms 751.629018ms 751.72162ms 751.873074ms 751.954599ms 751.994768ms 752.156471ms 752.180954ms 752.306919ms 752.333552ms 752.596843ms 752.601848ms 752.724572ms 753.191807ms 753.203929ms 753.346427ms 753.761702ms 753.792166ms 753.797756ms 754.305171ms 754.307512ms 754.345748ms 754.482126ms 754.605388ms 755.327832ms 755.466849ms 756.569183ms 756.637769ms 756.987027ms 756.995921ms 757.279321ms 757.510638ms 757.591089ms 759.006089ms 759.032292ms 759.425894ms 759.47092ms 760.988924ms 761.164909ms 761.257852ms 762.066847ms 762.287116ms 762.63911ms 764.021854ms 764.323717ms 768.341469ms 768.404118ms 768.607233ms 768.959431ms 788.287006ms]
Jan  3 14:57:35.120: INFO: 50 %ile: 748.990384ms
Jan  3 14:57:35.120: INFO: 90 %ile: 757.279321ms
Jan  3 14:57:35.120: INFO: 99 %ile: 768.959431ms
Jan  3 14:57:35.120: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:57:35.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9754" for this suite.
Jan  3 14:57:51.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:57:51.176: INFO: namespace svc-latency-9754 deletion completed in 16.050503729s

• [SLOW TEST:32.816 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:57:51.177: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 14:57:51.194: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b2b3461-189c-4fed-a055-5d2485b76eff" in namespace "projected-875" to be "success or failure"
Jan  3 14:57:51.195: INFO: Pod "downwardapi-volume-2b2b3461-189c-4fed-a055-5d2485b76eff": Phase="Pending", Reason="", readiness=false. Elapsed: 962.004µs
Jan  3 14:57:53.196: INFO: Pod "downwardapi-volume-2b2b3461-189c-4fed-a055-5d2485b76eff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002250059s
Jan  3 14:57:55.198: INFO: Pod "downwardapi-volume-2b2b3461-189c-4fed-a055-5d2485b76eff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003952689s
Jan  3 14:57:57.201: INFO: Pod "downwardapi-volume-2b2b3461-189c-4fed-a055-5d2485b76eff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006967747s
Jan  3 14:57:59.204: INFO: Pod "downwardapi-volume-2b2b3461-189c-4fed-a055-5d2485b76eff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.010130822s
STEP: Saw pod success
Jan  3 14:57:59.205: INFO: Pod "downwardapi-volume-2b2b3461-189c-4fed-a055-5d2485b76eff" satisfied condition "success or failure"
Jan  3 14:57:59.206: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-2b2b3461-189c-4fed-a055-5d2485b76eff container client-container: <nil>
STEP: delete the pod
Jan  3 14:57:59.230: INFO: Waiting for pod downwardapi-volume-2b2b3461-189c-4fed-a055-5d2485b76eff to disappear
Jan  3 14:57:59.233: INFO: Pod downwardapi-volume-2b2b3461-189c-4fed-a055-5d2485b76eff no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:57:59.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-875" for this suite.
Jan  3 14:58:05.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:58:05.280: INFO: namespace projected-875 deletion completed in 6.044287318s

• [SLOW TEST:14.103 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:58:05.280: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:58:21.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8947" for this suite.
Jan  3 14:58:27.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:58:27.560: INFO: namespace resourcequota-8947 deletion completed in 6.041353043s

• [SLOW TEST:22.280 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:58:27.561: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:58:27.579: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan  3 14:58:32.580: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan  3 14:58:36.583: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jan  3 14:58:36.591: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3157 /apis/apps/v1/namespaces/deployment-3157/deployments/test-cleanup-deployment f5a4aeac-dc39-4f63-aefc-bd4790d47aa5 74664 1 2020-01-03 14:58:36 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002b167b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan  3 14:58:36.594: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-3157 /apis/apps/v1/namespaces/deployment-3157/replicasets/test-cleanup-deployment-65db99849b dff98965-f78d-403a-af65-83c748d4b1f9 74666 1 2020-01-03 14:58:36 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment f5a4aeac-dc39-4f63-aefc-bd4790d47aa5 0xc002b16c07 0xc002b16c08}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002b16c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 14:58:36.594: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan  3 14:58:36.595: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3157 /apis/apps/v1/namespaces/deployment-3157/replicasets/test-cleanup-controller 85e0e871-7f4d-4ccb-a919-a09aed78fc73 74665 1 2020-01-03 14:58:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment f5a4aeac-dc39-4f63-aefc-bd4790d47aa5 0xc002b16b37 0xc002b16b38}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002b16b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  3 14:58:36.596: INFO: Pod "test-cleanup-controller-gq6zv" is available:
&Pod{ObjectMeta:{test-cleanup-controller-gq6zv test-cleanup-controller- deployment-3157 /api/v1/namespaces/deployment-3157/pods/test-cleanup-controller-gq6zv 0a620e14-c73e-44f3-967e-b502229d47b5 74656 0 2020-01-03 14:58:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:172.16.166.164/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.164"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-cleanup-controller 85e0e871-7f4d-4ccb-a919-a09aed78fc73 0xc002b170b7 0xc002b170b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8dlwc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8dlwc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8dlwc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 14:58:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 14:58:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 14:58:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 14:58:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.61.12,PodIP:172.16.166.164,StartTime:2020-01-03 14:58:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-03 14:58:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://411d5841cc90dc6cd25683f526cddcc7acd79bc7d0d1cc8e486ee9393d8085df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:58:36.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3157" for this suite.
Jan  3 14:58:42.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:58:42.689: INFO: namespace deployment-3157 deletion completed in 6.088917226s

• [SLOW TEST:15.129 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:58:42.691: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan  3 14:58:42.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-3841'
Jan  3 14:58:42.849: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan  3 14:58:42.849: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Jan  3 14:58:44.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete deployment e2e-test-httpd-deployment --namespace=kubectl-3841'
Jan  3 14:58:45.317: INFO: stderr: ""
Jan  3 14:58:45.317: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:58:45.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3841" for this suite.
Jan  3 14:58:51.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:58:51.373: INFO: namespace kubectl-3841 deletion completed in 6.054618846s

• [SLOW TEST:8.682 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:58:51.374: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 14:58:51.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-1183'
Jan  3 14:58:51.607: INFO: stderr: ""
Jan  3 14:58:51.607: INFO: stdout: "replicationcontroller/redis-master created\n"
Jan  3 14:58:51.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-1183'
Jan  3 14:58:51.797: INFO: stderr: ""
Jan  3 14:58:51.797: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan  3 14:58:52.799: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 14:58:52.799: INFO: Found 0 / 1
Jan  3 14:58:53.800: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 14:58:53.800: INFO: Found 0 / 1
Jan  3 14:58:54.799: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 14:58:54.799: INFO: Found 0 / 1
Jan  3 14:58:55.799: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 14:58:55.799: INFO: Found 0 / 1
Jan  3 14:58:56.799: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 14:58:56.799: INFO: Found 0 / 1
Jan  3 14:58:57.800: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 14:58:57.800: INFO: Found 0 / 1
Jan  3 14:58:58.799: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 14:58:58.799: INFO: Found 0 / 1
Jan  3 14:58:59.831: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 14:58:59.831: INFO: Found 1 / 1
Jan  3 14:58:59.831: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  3 14:58:59.952: INFO: Selector matched 1 pods for map[app:redis]
Jan  3 14:58:59.952: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  3 14:58:59.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 describe pod redis-master-wr26l --namespace=kubectl-1183'
Jan  3 14:59:00.144: INFO: stderr: ""
Jan  3 14:59:00.144: INFO: stdout: "Name:         redis-master-wr26l\nNamespace:    kubectl-1183\nPriority:     0\nNode:         controller-1/10.10.61.12\nStart Time:   Fri, 03 Jan 2020 14:58:51 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.16.166.156/32\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"chain\",\n                    \"ips\": [\n                        \"172.16.166.156\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\nStatus:       Running\nIP:           172.16.166.156\nIPs:\n  IP:           172.16.166.156\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://37a2cf4bcf00a393ef269459b93e8c47b5a08f971d45b5bfc37338135005a412\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 03 Jan 2020 14:58:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5sb6f (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-5sb6f:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-5sb6f\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 30s\n                 node.kubernetes.io/unreachable:NoExecute for 30s\nEvents:\n  Type    Reason     Age        From                   Message\n  ----    ------     ----       ----                   -------\n  Normal  Scheduled  <unknown>  default-scheduler      Successfully assigned kubectl-1183/redis-master-wr26l to controller-1\n  Normal  Pulled     2s         kubelet, controller-1  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    2s         kubelet, controller-1  Created container redis-master\n  Normal  Started    2s         kubelet, controller-1  Started container redis-master\n"
Jan  3 14:59:00.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 describe rc redis-master --namespace=kubectl-1183'
Jan  3 14:59:00.231: INFO: stderr: ""
Jan  3 14:59:00.231: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1183\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  9s    replication-controller  Created pod: redis-master-wr26l\n"
Jan  3 14:59:00.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 describe service redis-master --namespace=kubectl-1183'
Jan  3 14:59:00.292: INFO: stderr: ""
Jan  3 14:59:00.292: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1183\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.96.137.219\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.16.166.156:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan  3 14:59:00.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 describe node controller-0'
Jan  3 14:59:00.424: INFO: stderr: ""
Jan  3 14:59:00.424: INFO: stdout: "Name:               controller-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=controller-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    openstack-compute-node=enabled\n                    openstack-control-plane=enabled\n                    openvswitch=enabled\n                    sriov=enabled\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.206.2/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.16.192.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 03 Jan 2020 09:58:57 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 03 Jan 2020 10:26:10 +0000   Fri, 03 Jan 2020 10:26:10 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 03 Jan 2020 14:59:00 +0000   Fri, 03 Jan 2020 09:58:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 03 Jan 2020 14:59:00 +0000   Fri, 03 Jan 2020 09:58:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 03 Jan 2020 14:59:00 +0000   Fri, 03 Jan 2020 09:58:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 03 Jan 2020 14:59:00 +0000   Fri, 03 Jan 2020 10:26:09 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.10.61.11\n  Hostname:    controller-0\nCapacity:\n cpu:                64\n ephemeral-storage:  10190100Ki\n hugepages-1Gi:      22Gi\n hugepages-2Mi:      0\n memory:             97527732Ki\n pods:               110\nAllocatable:\n cpu:                64\n ephemeral-storage:  9391196145\n hugepages-1Gi:      22Gi\n hugepages-2Mi:      0\n memory:             74356660Ki\n pods:               110\nSystem Info:\n Machine ID:                 316fa6b120fe49be9340d8ea91cf3421\n System UUID:                96DCF642-A8E1-E811-BD91-A4BF0165F4A2\n Boot ID:                    f2b52fcb-1717-48b5-83c9-b57dd96f0263\n Kernel Version:             3.10.0-957.21.3.el7.2.tis.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.6\n Kubelet Version:            v1.16.2\n Kube-Proxy Version:         v1.16.2\nPodCIDR:                     172.16.0.0/24\nPodCIDRs:                    172.16.0.0/24\nNon-terminated Pods:         (42 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                calico-node-rb47m                                          250m (0%)     0 (0%)      0 (0%)           0 (0%)         4h59m\n  kube-system                coredns-6bc668cd76-nt9h6                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     68m\n  kube-system                ingress-error-pages-5bcb8b5f6c-bn4nq                       100m (0%)     4 (6%)      128Mi (0%)       8000Mi (11%)   69m\n  kube-system                ingress-p6rb9                                              100m (0%)     4 (6%)      128Mi (0%)       8000Mi (11%)   68m\n  kube-system                kube-apiserver-controller-0                                250m (0%)     0 (0%)      0 (0%)           0 (0%)         4h58m\n  kube-system                kube-controller-manager-controller-0                       200m (0%)     0 (0%)      0 (0%)           0 (0%)         4h58m\n  kube-system                kube-multus-ds-amd64-6zbwh                                 100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      68m\n  kube-system                kube-proxy-5l8fd                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h59m\n  kube-system                kube-scheduler-controller-0                                100m (0%)     0 (0%)      0 (0%)           0 (0%)         4h58m\n  kube-system                kube-sriov-cni-ds-amd64-ddmnd                              100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      68m\n  kube-system                rbd-provisioner-7484d49cf6-zf49b                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  openstack                  cinder-api-7ff9984869-kgvv6                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  openstack                  cinder-backup-fd5f96bf4-z4xc2                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  openstack                  cinder-scheduler-664bb87785-p8t9k                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  openstack                  cinder-volume-549d7c447c-tr8gw                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  openstack                  fm-rest-api-b4bc757f4-4fzhx                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  openstack                  glance-api-747954666d-cmg85                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  openstack                  heat-api-58bf859968-7rz2w                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  heat-cfn-c8f5b9b4b-dfrgc                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  heat-engine-54745654c7-hbkrx                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  ingress-bc886876f-487zs                                    100m (0%)     4 (6%)      128Mi (0%)       8000Mi (11%)   68m\n  openstack                  ingress-error-pages-cf6c65b-vmlrg                          100m (0%)     4 (6%)      128Mi (0%)       8000Mi (11%)   68m\n  openstack                  keystone-api-7b4d98b8c5-2q8z7                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  libvirt-libvirt-default-psmq6                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  mariadb-ingress-5bb8b69fc8-xppj8                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  mariadb-server-1                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  neutron-dhcp-agent-controller-0-937646f6-prqt7             0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  neutron-l3-agent-controller-0-937646f6-ft64l               0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  neutron-metadata-agent-controller-0-937646f6-z2mz9         0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  neutron-ovs-agent-controller-0-937646f6-n2m6q              0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  neutron-server-54b46f798-bbhdw                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  openstack                  neutron-sriov-agent-controller-0-937646f6-dg699            0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  nova-api-metadata-7bdf79d754-h8r6m                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  openstack                  nova-api-osapi-68846d5959-w7jrd                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  nova-api-proxy-577495bf7f-k7tnr                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  nova-compute-controller-0-937646f6-vl8z6                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  nova-conductor-bb6d86d69-tgxnd                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  nova-novncproxy-6c54c7d98-ptb6t                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  openstack                  nova-scheduler-76b67b674d-lwrc2                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  openstack                  osh-openstack-rabbitmq-rabbitmq-0                          500m (0%)     4 (6%)      128Mi (0%)       8000Mi (11%)   68m\n  openstack                  placement-api-5b65bc5576-kh6z7                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-5e158e08c5964d78-s6m8n    0 (0%)        0 (0%)      0 (0%)           0 (0%)         123m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                2 (3%)      20200m (31%)\n  memory             810Mi (1%)  40270Mi (55%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                    From                      Message\n  ----    ------                   ----                   ----                      -------\n  Normal  NodeHasSufficientMemory  5h (x8 over 5h)        kubelet, controller-0     Node controller-0 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    5h (x8 over 5h)        kubelet, controller-0     Node controller-0 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     5h (x7 over 5h)        kubelet, controller-0     Node controller-0 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  5h                     kubelet, controller-0     Updated Node Allocatable limit across pods\n  Normal  Starting                 5h                     kubelet, controller-0     Starting kubelet.\n  Normal  Starting                 4h59m                  kube-proxy, controller-0  Starting kube-proxy.\n  Normal  NodeAllocatableEnforced  4h34m                  kubelet, controller-0     Updated Node Allocatable limit across pods\n  Normal  Starting                 4h34m                  kubelet, controller-0     Starting kubelet.\n  Normal  NodeHasSufficientMemory  4h34m (x8 over 4h34m)  kubelet, controller-0     Node controller-0 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    4h34m (x8 over 4h34m)  kubelet, controller-0     Node controller-0 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     4h34m (x7 over 4h34m)  kubelet, controller-0     Node controller-0 status is now: NodeHasSufficientPID\n  Normal  Starting                 4h33m                  kube-proxy, controller-0  Starting kube-proxy.\n  Normal  Starting                 4h32m                  kubelet, controller-0     Starting kubelet.\n  Normal  NodeHasSufficientMemory  4h32m                  kubelet, controller-0     Node controller-0 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    4h32m                  kubelet, controller-0     Node controller-0 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     4h32m                  kubelet, controller-0     Node controller-0 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             4h32m                  kubelet, controller-0     Node controller-0 status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  4h32m                  kubelet, controller-0     Updated Node Allocatable limit across pods\n  Normal  NodeReady                4h32m                  kubelet, controller-0     Node controller-0 status is now: NodeReady\n"
Jan  3 14:59:00.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 describe namespace kubectl-1183'
Jan  3 14:59:00.495: INFO: stderr: ""
Jan  3 14:59:00.495: INFO: stdout: "Name:         kubectl-1183\nLabels:       e2e-framework=kubectl\n              e2e-run=44774371-286b-4fa5-bb21-ee06b15f775f\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:59:00.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1183" for this suite.
Jan  3 14:59:12.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:59:12.578: INFO: namespace kubectl-1183 deletion completed in 12.0813526s

• [SLOW TEST:21.205 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:59:12.583: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 14:59:13.029: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 14:59:15.054: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660353, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660353, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660353, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660353, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:59:17.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660353, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660353, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660353, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660353, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 14:59:19.055: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660353, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660353, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660353, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660353, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 14:59:22.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:59:22.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1749" for this suite.
Jan  3 14:59:28.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:59:28.096: INFO: namespace webhook-1749 deletion completed in 6.032100842s
STEP: Destroying namespace "webhook-1749-markers" for this suite.
Jan  3 14:59:34.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:59:34.197: INFO: namespace webhook-1749-markers deletion completed in 6.101130761s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.636 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:59:34.220: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Jan  3 14:59:34.252: INFO: Waiting up to 5m0s for pod "client-containers-c4418598-975f-4abe-b50b-b4215b9b63c9" in namespace "containers-3320" to be "success or failure"
Jan  3 14:59:34.253: INFO: Pod "client-containers-c4418598-975f-4abe-b50b-b4215b9b63c9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.497629ms
Jan  3 14:59:36.255: INFO: Pod "client-containers-c4418598-975f-4abe-b50b-b4215b9b63c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002737279s
Jan  3 14:59:38.256: INFO: Pod "client-containers-c4418598-975f-4abe-b50b-b4215b9b63c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00416366s
Jan  3 14:59:40.258: INFO: Pod "client-containers-c4418598-975f-4abe-b50b-b4215b9b63c9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006003666s
Jan  3 14:59:42.260: INFO: Pod "client-containers-c4418598-975f-4abe-b50b-b4215b9b63c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007910969s
STEP: Saw pod success
Jan  3 14:59:42.260: INFO: Pod "client-containers-c4418598-975f-4abe-b50b-b4215b9b63c9" satisfied condition "success or failure"
Jan  3 14:59:42.261: INFO: Trying to get logs from node controller-0 pod client-containers-c4418598-975f-4abe-b50b-b4215b9b63c9 container test-container: <nil>
STEP: delete the pod
Jan  3 14:59:42.284: INFO: Waiting for pod client-containers-c4418598-975f-4abe-b50b-b4215b9b63c9 to disappear
Jan  3 14:59:42.286: INFO: Pod client-containers-c4418598-975f-4abe-b50b-b4215b9b63c9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:59:42.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3320" for this suite.
Jan  3 14:59:48.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 14:59:48.340: INFO: namespace containers-3320 deletion completed in 6.049536138s

• [SLOW TEST:14.121 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 14:59:48.341: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 14:59:54.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4883" for this suite.
Jan  3 15:00:00.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:00:00.492: INFO: namespace namespaces-4883 deletion completed in 6.092388879s
STEP: Destroying namespace "nsdeletetest-6988" for this suite.
Jan  3 15:00:00.493: INFO: Namespace nsdeletetest-6988 was already deleted
STEP: Destroying namespace "nsdeletetest-9210" for this suite.
Jan  3 15:00:06.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:00:06.557: INFO: namespace nsdeletetest-9210 deletion completed in 6.064512003s

• [SLOW TEST:18.217 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:00:06.558: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 15:00:07.217: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 15:00:09.227: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660407, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660407, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660407, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660407, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 15:00:11.232: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660407, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660407, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660407, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660407, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 15:00:13.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660407, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660407, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660407, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660407, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 15:00:16.322: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:00:16.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8176" for this suite.
Jan  3 15:00:22.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:00:22.415: INFO: namespace webhook-8176 deletion completed in 6.040382939s
STEP: Destroying namespace "webhook-8176-markers" for this suite.
Jan  3 15:00:28.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:00:28.458: INFO: namespace webhook-8176-markers deletion completed in 6.042833533s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.905 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:00:28.464: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-29c20957-84ca-480b-a15d-6ec718d7f0ad
STEP: Creating a pod to test consume secrets
Jan  3 15:00:28.485: INFO: Waiting up to 5m0s for pod "pod-secrets-8eb2dbb4-3117-4431-9179-a55839bb2913" in namespace "secrets-8684" to be "success or failure"
Jan  3 15:00:28.487: INFO: Pod "pod-secrets-8eb2dbb4-3117-4431-9179-a55839bb2913": Phase="Pending", Reason="", readiness=false. Elapsed: 1.884049ms
Jan  3 15:00:30.504: INFO: Pod "pod-secrets-8eb2dbb4-3117-4431-9179-a55839bb2913": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01929182s
Jan  3 15:00:32.506: INFO: Pod "pod-secrets-8eb2dbb4-3117-4431-9179-a55839bb2913": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021199399s
Jan  3 15:00:34.510: INFO: Pod "pod-secrets-8eb2dbb4-3117-4431-9179-a55839bb2913": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025227312s
Jan  3 15:00:36.512: INFO: Pod "pod-secrets-8eb2dbb4-3117-4431-9179-a55839bb2913": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.026934603s
STEP: Saw pod success
Jan  3 15:00:36.512: INFO: Pod "pod-secrets-8eb2dbb4-3117-4431-9179-a55839bb2913" satisfied condition "success or failure"
Jan  3 15:00:36.513: INFO: Trying to get logs from node controller-0 pod pod-secrets-8eb2dbb4-3117-4431-9179-a55839bb2913 container secret-env-test: <nil>
STEP: delete the pod
Jan  3 15:00:36.524: INFO: Waiting for pod pod-secrets-8eb2dbb4-3117-4431-9179-a55839bb2913 to disappear
Jan  3 15:00:36.525: INFO: Pod pod-secrets-8eb2dbb4-3117-4431-9179-a55839bb2913 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:00:36.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8684" for this suite.
Jan  3 15:00:42.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:00:42.590: INFO: namespace secrets-8684 deletion completed in 6.05967708s

• [SLOW TEST:14.126 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:00:42.591: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Jan  3 15:00:42.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-5117'
Jan  3 15:00:42.841: INFO: stderr: ""
Jan  3 15:00:42.841: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan  3 15:00:42.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5117'
Jan  3 15:00:43.067: INFO: stderr: ""
Jan  3 15:00:43.067: INFO: stdout: "update-demo-nautilus-2xhdb update-demo-nautilus-b474q "
Jan  3 15:00:43.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-2xhdb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5117'
Jan  3 15:00:43.431: INFO: stderr: ""
Jan  3 15:00:43.431: INFO: stdout: ""
Jan  3 15:00:43.431: INFO: update-demo-nautilus-2xhdb is created but not running
Jan  3 15:00:48.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5117'
Jan  3 15:00:48.496: INFO: stderr: ""
Jan  3 15:00:48.496: INFO: stdout: "update-demo-nautilus-2xhdb update-demo-nautilus-b474q "
Jan  3 15:00:48.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-2xhdb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5117'
Jan  3 15:00:48.554: INFO: stderr: ""
Jan  3 15:00:48.554: INFO: stdout: ""
Jan  3 15:00:48.554: INFO: update-demo-nautilus-2xhdb is created but not running
Jan  3 15:00:53.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5117'
Jan  3 15:00:53.612: INFO: stderr: ""
Jan  3 15:00:53.612: INFO: stdout: "update-demo-nautilus-2xhdb update-demo-nautilus-b474q "
Jan  3 15:00:53.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-2xhdb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5117'
Jan  3 15:00:53.668: INFO: stderr: ""
Jan  3 15:00:53.668: INFO: stdout: "true"
Jan  3 15:00:53.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-2xhdb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5117'
Jan  3 15:00:53.721: INFO: stderr: ""
Jan  3 15:00:53.721: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan  3 15:00:53.721: INFO: validating pod update-demo-nautilus-2xhdb
Jan  3 15:00:53.723: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 15:00:53.723: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 15:00:53.723: INFO: update-demo-nautilus-2xhdb is verified up and running
Jan  3 15:00:53.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-b474q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5117'
Jan  3 15:00:53.778: INFO: stderr: ""
Jan  3 15:00:53.778: INFO: stdout: "true"
Jan  3 15:00:53.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-nautilus-b474q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5117'
Jan  3 15:00:53.838: INFO: stderr: ""
Jan  3 15:00:53.838: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan  3 15:00:53.838: INFO: validating pod update-demo-nautilus-b474q
Jan  3 15:00:53.840: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 15:00:53.840: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 15:00:53.840: INFO: update-demo-nautilus-b474q is verified up and running
STEP: rolling-update to new replication controller
Jan  3 15:00:53.841: INFO: scanned /root for discovery docs: <nil>
Jan  3 15:00:53.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-5117'
Jan  3 15:01:33.595: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan  3 15:01:33.595: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan  3 15:01:33.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5117'
Jan  3 15:01:33.659: INFO: stderr: ""
Jan  3 15:01:33.659: INFO: stdout: "update-demo-kitten-cts7k update-demo-kitten-jprjc "
Jan  3 15:01:33.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-kitten-cts7k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5117'
Jan  3 15:01:33.715: INFO: stderr: ""
Jan  3 15:01:33.715: INFO: stdout: "true"
Jan  3 15:01:33.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-kitten-cts7k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5117'
Jan  3 15:01:33.795: INFO: stderr: ""
Jan  3 15:01:33.795: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan  3 15:01:33.795: INFO: validating pod update-demo-kitten-cts7k
Jan  3 15:01:33.797: INFO: got data: {
  "image": "kitten.jpg"
}

Jan  3 15:01:33.797: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan  3 15:01:33.797: INFO: update-demo-kitten-cts7k is verified up and running
Jan  3 15:01:33.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-kitten-jprjc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5117'
Jan  3 15:01:33.878: INFO: stderr: ""
Jan  3 15:01:33.878: INFO: stdout: "true"
Jan  3 15:01:33.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pods update-demo-kitten-jprjc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5117'
Jan  3 15:01:33.954: INFO: stderr: ""
Jan  3 15:01:33.954: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan  3 15:01:33.954: INFO: validating pod update-demo-kitten-jprjc
Jan  3 15:01:33.957: INFO: got data: {
  "image": "kitten.jpg"
}

Jan  3 15:01:33.957: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan  3 15:01:33.957: INFO: update-demo-kitten-jprjc is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:01:33.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5117" for this suite.
Jan  3 15:02:01.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:02:01.993: INFO: namespace kubectl-5117 deletion completed in 28.034231339s

• [SLOW TEST:79.403 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:02:01.993: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Jan  3 15:02:02.008: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jan  3 15:02:02.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-2329'
Jan  3 15:02:02.137: INFO: stderr: ""
Jan  3 15:02:02.137: INFO: stdout: "service/redis-slave created\n"
Jan  3 15:02:02.137: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jan  3 15:02:02.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-2329'
Jan  3 15:02:02.329: INFO: stderr: ""
Jan  3 15:02:02.329: INFO: stdout: "service/redis-master created\n"
Jan  3 15:02:02.330: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan  3 15:02:02.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-2329'
Jan  3 15:02:02.485: INFO: stderr: ""
Jan  3 15:02:02.485: INFO: stdout: "service/frontend created\n"
Jan  3 15:02:02.485: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jan  3 15:02:02.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-2329'
Jan  3 15:02:02.653: INFO: stderr: ""
Jan  3 15:02:02.653: INFO: stdout: "deployment.apps/frontend created\n"
Jan  3 15:02:02.654: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan  3 15:02:02.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-2329'
Jan  3 15:02:02.845: INFO: stderr: ""
Jan  3 15:02:02.845: INFO: stdout: "deployment.apps/redis-master created\n"
Jan  3 15:02:02.845: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jan  3 15:02:02.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 create -f - --namespace=kubectl-2329'
Jan  3 15:02:03.074: INFO: stderr: ""
Jan  3 15:02:03.074: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jan  3 15:02:03.074: INFO: Waiting for all frontend pods to be Running.
Jan  3 15:04:13.167: INFO: Waiting for frontend to serve content.
Jan  3 15:04:13.178: INFO: Trying to add a new entry to the guestbook.
Jan  3 15:04:13.189: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan  3 15:04:13.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete --grace-period=0 --force -f - --namespace=kubectl-2329'
Jan  3 15:04:13.449: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 15:04:13.450: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jan  3 15:04:13.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete --grace-period=0 --force -f - --namespace=kubectl-2329'
Jan  3 15:04:13.531: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 15:04:13.531: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan  3 15:04:13.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete --grace-period=0 --force -f - --namespace=kubectl-2329'
Jan  3 15:04:13.679: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 15:04:13.679: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan  3 15:04:13.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete --grace-period=0 --force -f - --namespace=kubectl-2329'
Jan  3 15:04:13.751: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 15:04:13.751: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan  3 15:04:13.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete --grace-period=0 --force -f - --namespace=kubectl-2329'
Jan  3 15:04:13.822: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 15:04:13.822: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan  3 15:04:13.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete --grace-period=0 --force -f - --namespace=kubectl-2329'
Jan  3 15:04:13.889: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 15:04:13.889: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:04:13.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2329" for this suite.
Jan  3 15:04:25.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:04:25.934: INFO: namespace kubectl-2329 deletion completed in 12.043570926s

• [SLOW TEST:143.941 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:04:25.934: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan  3 15:04:25.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-1386'
Jan  3 15:04:26.021: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan  3 15:04:26.021: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jan  3 15:04:26.023: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-tvxvx]
Jan  3 15:04:26.023: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-tvxvx" in namespace "kubectl-1386" to be "running and ready"
Jan  3 15:04:26.024: INFO: Pod "e2e-test-httpd-rc-tvxvx": Phase="Pending", Reason="", readiness=false. Elapsed: 854.57µs
Jan  3 15:04:28.026: INFO: Pod "e2e-test-httpd-rc-tvxvx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002727512s
Jan  3 15:04:30.027: INFO: Pod "e2e-test-httpd-rc-tvxvx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004148673s
Jan  3 15:04:32.029: INFO: Pod "e2e-test-httpd-rc-tvxvx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006023322s
Jan  3 15:04:34.030: INFO: Pod "e2e-test-httpd-rc-tvxvx": Phase="Running", Reason="", readiness=true. Elapsed: 8.007422151s
Jan  3 15:04:34.030: INFO: Pod "e2e-test-httpd-rc-tvxvx" satisfied condition "running and ready"
Jan  3 15:04:34.030: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-tvxvx]
Jan  3 15:04:34.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 logs rc/e2e-test-httpd-rc --namespace=kubectl-1386'
Jan  3 15:04:34.151: INFO: stderr: ""
Jan  3 15:04:34.151: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.16.166.182. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.16.166.182. Set the 'ServerName' directive globally to suppress this message\n[Fri Jan 03 15:04:32.781189 2020] [mpm_event:notice] [pid 1:tid 140439157566312] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Fri Jan 03 15:04:32.781228 2020] [core:notice] [pid 1:tid 140439157566312] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Jan  3 15:04:34.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete rc e2e-test-httpd-rc --namespace=kubectl-1386'
Jan  3 15:04:34.238: INFO: stderr: ""
Jan  3 15:04:34.238: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:04:34.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1386" for this suite.
Jan  3 15:04:40.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:04:40.289: INFO: namespace kubectl-1386 deletion completed in 6.049271469s

• [SLOW TEST:14.355 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:04:40.291: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 15:04:41.328: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 15:04:43.333: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660681, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660681, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 15:04:45.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660681, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660681, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 15:04:47.335: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660681, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660681, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 15:04:50.339: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:04:50.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5588" for this suite.
Jan  3 15:04:56.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:04:56.422: INFO: namespace webhook-5588 deletion completed in 6.034239631s
STEP: Destroying namespace "webhook-5588-markers" for this suite.
Jan  3 15:05:02.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:05:02.457: INFO: namespace webhook-5588-markers deletion completed in 6.035227549s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:22.171 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:05:02.463: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan  3 15:05:02.487: INFO: Number of nodes with available pods: 0
Jan  3 15:05:02.487: INFO: Node controller-0 is running more than one daemon pod
Jan  3 15:05:03.490: INFO: Number of nodes with available pods: 0
Jan  3 15:05:03.490: INFO: Node controller-0 is running more than one daemon pod
Jan  3 15:05:04.491: INFO: Number of nodes with available pods: 0
Jan  3 15:05:04.491: INFO: Node controller-0 is running more than one daemon pod
Jan  3 15:05:05.550: INFO: Number of nodes with available pods: 0
Jan  3 15:05:05.553: INFO: Node controller-0 is running more than one daemon pod
Jan  3 15:05:06.491: INFO: Number of nodes with available pods: 0
Jan  3 15:05:06.491: INFO: Node controller-0 is running more than one daemon pod
Jan  3 15:05:07.493: INFO: Number of nodes with available pods: 0
Jan  3 15:05:07.493: INFO: Node controller-0 is running more than one daemon pod
Jan  3 15:05:08.492: INFO: Number of nodes with available pods: 0
Jan  3 15:05:08.492: INFO: Node controller-0 is running more than one daemon pod
Jan  3 15:05:09.491: INFO: Number of nodes with available pods: 0
Jan  3 15:05:09.491: INFO: Node controller-0 is running more than one daemon pod
Jan  3 15:05:10.517: INFO: Number of nodes with available pods: 2
Jan  3 15:05:10.517: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan  3 15:05:10.545: INFO: Number of nodes with available pods: 1
Jan  3 15:05:10.552: INFO: Node controller-1 is running more than one daemon pod
Jan  3 15:05:11.566: INFO: Number of nodes with available pods: 1
Jan  3 15:05:11.567: INFO: Node controller-1 is running more than one daemon pod
Jan  3 15:05:12.562: INFO: Number of nodes with available pods: 1
Jan  3 15:05:12.563: INFO: Node controller-1 is running more than one daemon pod
Jan  3 15:05:13.589: INFO: Number of nodes with available pods: 1
Jan  3 15:05:13.589: INFO: Node controller-1 is running more than one daemon pod
Jan  3 15:05:14.574: INFO: Number of nodes with available pods: 1
Jan  3 15:05:14.575: INFO: Node controller-1 is running more than one daemon pod
Jan  3 15:05:15.664: INFO: Number of nodes with available pods: 1
Jan  3 15:05:15.665: INFO: Node controller-1 is running more than one daemon pod
Jan  3 15:05:16.556: INFO: Number of nodes with available pods: 1
Jan  3 15:05:16.556: INFO: Node controller-1 is running more than one daemon pod
Jan  3 15:05:17.564: INFO: Number of nodes with available pods: 1
Jan  3 15:05:17.564: INFO: Node controller-1 is running more than one daemon pod
Jan  3 15:05:18.556: INFO: Number of nodes with available pods: 1
Jan  3 15:05:18.557: INFO: Node controller-1 is running more than one daemon pod
Jan  3 15:05:19.556: INFO: Number of nodes with available pods: 2
Jan  3 15:05:19.556: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1124, will wait for the garbage collector to delete the pods
Jan  3 15:05:19.618: INFO: Deleting DaemonSet.extensions daemon-set took: 2.892937ms
Jan  3 15:05:20.018: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.151606ms
Jan  3 15:05:26.932: INFO: Number of nodes with available pods: 0
Jan  3 15:05:26.944: INFO: Number of running nodes: 0, number of available pods: 0
Jan  3 15:05:26.945: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1124/daemonsets","resourceVersion":"77085"},"items":null}

Jan  3 15:05:26.946: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1124/pods","resourceVersion":"77085"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:05:26.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1124" for this suite.
Jan  3 15:05:32.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:05:32.994: INFO: namespace daemonsets-1124 deletion completed in 6.042961309s

• [SLOW TEST:30.531 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:05:32.995: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:05:33.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7189" for this suite.
Jan  3 15:05:39.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:05:39.049: INFO: namespace services-7189 deletion completed in 6.036868842s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.054 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:05:39.049: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:05:50.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-425" for this suite.
Jan  3 15:05:56.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:05:56.139: INFO: namespace resourcequota-425 deletion completed in 6.039554082s

• [SLOW TEST:17.090 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:05:56.140: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Jan  3 15:05:56.162: INFO: Waiting up to 5m0s for pod "client-containers-7bfbff8d-77d1-4181-bb89-6aa18fbb3cdd" in namespace "containers-2452" to be "success or failure"
Jan  3 15:05:56.165: INFO: Pod "client-containers-7bfbff8d-77d1-4181-bb89-6aa18fbb3cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.670967ms
Jan  3 15:05:58.167: INFO: Pod "client-containers-7bfbff8d-77d1-4181-bb89-6aa18fbb3cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004593184s
Jan  3 15:06:00.169: INFO: Pod "client-containers-7bfbff8d-77d1-4181-bb89-6aa18fbb3cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006828717s
Jan  3 15:06:02.170: INFO: Pod "client-containers-7bfbff8d-77d1-4181-bb89-6aa18fbb3cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008332981s
Jan  3 15:06:04.172: INFO: Pod "client-containers-7bfbff8d-77d1-4181-bb89-6aa18fbb3cdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009929726s
STEP: Saw pod success
Jan  3 15:06:04.172: INFO: Pod "client-containers-7bfbff8d-77d1-4181-bb89-6aa18fbb3cdd" satisfied condition "success or failure"
Jan  3 15:06:04.173: INFO: Trying to get logs from node controller-0 pod client-containers-7bfbff8d-77d1-4181-bb89-6aa18fbb3cdd container test-container: <nil>
STEP: delete the pod
Jan  3 15:06:04.187: INFO: Waiting for pod client-containers-7bfbff8d-77d1-4181-bb89-6aa18fbb3cdd to disappear
Jan  3 15:06:04.188: INFO: Pod client-containers-7bfbff8d-77d1-4181-bb89-6aa18fbb3cdd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:06:04.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2452" for this suite.
Jan  3 15:06:10.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:06:10.246: INFO: namespace containers-2452 deletion completed in 6.056081564s

• [SLOW TEST:14.106 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:06:10.246: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jan  3 15:06:10.263: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  3 15:06:10.269: INFO: Waiting for terminating namespaces to be deleted...
Jan  3 15:06:10.270: INFO: 
Logging pods the kubelet thinks is on node controller-0 before test
Jan  3 15:06:10.293: INFO: neutron-sriov-agent-controller-0-937646f6-dg699 from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container neutron-sriov-agent ready: true, restart count 0
Jan  3 15:06:10.294: INFO: placement-api-5b65bc5576-kh6z7 from openstack started at 2020-01-03 13:50:47 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container placement-api ready: true, restart count 0
Jan  3 15:06:10.294: INFO: nova-compute-controller-0-937646f6-vl8z6 from openstack started at 2020-01-03 13:50:44 +0000 UTC (2 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container nova-compute ready: true, restart count 0
Jan  3 15:06:10.294: INFO: 	Container nova-compute-ssh ready: true, restart count 0
Jan  3 15:06:10.294: INFO: osh-openstack-rabbitmq-rabbitmq-0 from openstack started at 2020-01-03 13:50:53 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container rabbitmq ready: true, restart count 0
Jan  3 15:06:10.294: INFO: ceph-pools-audit-1578063300-mhksq from kube-system started at 2020-01-03 14:55:09 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 15:06:10.294: INFO: neutron-dhcp-agent-controller-0-937646f6-prqt7 from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container neutron-dhcp-agent ready: true, restart count 0
Jan  3 15:06:10.294: INFO: ceph-pools-audit-1578063900-tm8vk from kube-system started at 2020-01-03 15:05:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container ceph-pools-audit-ceph-store ready: true, restart count 0
Jan  3 15:06:10.294: INFO: libvirt-libvirt-default-psmq6 from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container libvirt ready: true, restart count 0
Jan  3 15:06:10.294: INFO: coredns-6bc668cd76-nt9h6 from kube-system started at 2020-01-03 13:50:47 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container coredns ready: true, restart count 0
Jan  3 15:06:10.294: INFO: cinder-backup-fd5f96bf4-z4xc2 from openstack started at 2020-01-03 13:50:47 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container cinder-backup ready: true, restart count 0
Jan  3 15:06:10.294: INFO: neutron-metadata-agent-controller-0-937646f6-z2mz9 from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container neutron-metadata-agent ready: true, restart count 0
Jan  3 15:06:10.294: INFO: keystone-api-7b4d98b8c5-2q8z7 from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container keystone-api ready: true, restart count 0
Jan  3 15:06:10.294: INFO: neutron-l3-agent-controller-0-937646f6-ft64l from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container neutron-l3-agent ready: true, restart count 0
Jan  3 15:06:10.294: INFO: cinder-scheduler-664bb87785-p8t9k from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container cinder-scheduler ready: true, restart count 0
Jan  3 15:06:10.294: INFO: ingress-error-pages-5bcb8b5f6c-bn4nq from kube-system started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 15:06:10.294: INFO: kube-proxy-5l8fd from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container kube-proxy ready: true, restart count 1
Jan  3 15:06:10.294: INFO: nova-api-metadata-7bdf79d754-h8r6m from openstack started at 2020-01-03 13:50:47 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container nova-api ready: true, restart count 0
Jan  3 15:06:10.294: INFO: ingress-error-pages-cf6c65b-vmlrg from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 15:06:10.294: INFO: glance-api-747954666d-cmg85 from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container glance-api ready: true, restart count 0
Jan  3 15:06:10.294: INFO: cinder-volume-usage-audit-1578063900-hvb7w from openstack started at 2020-01-03 15:05:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 15:06:10.294: INFO: heat-engine-54745654c7-hbkrx from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container heat-engine ready: true, restart count 0
Jan  3 15:06:10.294: INFO: sonobuoy-systemd-logs-daemon-set-5e158e08c5964d78-s6m8n from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 15:06:10.294: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jan  3 15:06:10.294: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 15:06:10.295: INFO: kube-apiserver-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container kube-apiserver ready: true, restart count 2
Jan  3 15:06:10.295: INFO: kube-multus-ds-amd64-6zbwh from kube-system started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container kube-multus ready: true, restart count 0
Jan  3 15:06:10.295: INFO: nova-scheduler-76b67b674d-lwrc2 from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container nova-scheduler ready: true, restart count 0
Jan  3 15:06:10.295: INFO: cinder-volume-549d7c447c-tr8gw from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container cinder-volume ready: true, restart count 0
Jan  3 15:06:10.295: INFO: kube-sriov-cni-ds-amd64-ddmnd from kube-system started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Jan  3 15:06:10.295: INFO: cinder-volume-usage-audit-1578063600-xb654 from openstack started at 2020-01-03 15:00:00 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 15:06:10.295: INFO: nova-conductor-bb6d86d69-tgxnd from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container nova-conductor ready: true, restart count 0
Jan  3 15:06:10.295: INFO: nova-api-osapi-68846d5959-w7jrd from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container nova-osapi ready: true, restart count 1
Jan  3 15:06:10.295: INFO: kube-scheduler-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan  3 15:06:10.295: INFO: rbd-provisioner-7484d49cf6-zf49b from kube-system started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container rbd-provisioner ready: true, restart count 0
Jan  3 15:06:10.295: INFO: mariadb-ingress-5bb8b69fc8-xppj8 from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container ingress ready: true, restart count 0
Jan  3 15:06:10.295: INFO: ingress-bc886876f-487zs from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container ingress ready: true, restart count 0
Jan  3 15:06:10.295: INFO: heat-engine-cleaner-1578063600-jzn9d from openstack started at 2020-01-03 15:00:00 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 15:06:10.295: INFO: neutron-ovs-agent-controller-0-937646f6-n2m6q from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container neutron-ovs-agent ready: true, restart count 0
Jan  3 15:06:10.295: INFO: heat-api-58bf859968-7rz2w from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container heat-api ready: true, restart count 0
Jan  3 15:06:10.295: INFO: neutron-server-54b46f798-bbhdw from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container neutron-server ready: true, restart count 0
Jan  3 15:06:10.295: INFO: kube-controller-manager-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan  3 15:06:10.295: INFO: heat-cfn-c8f5b9b4b-dfrgc from openstack started at 2020-01-03 13:51:06 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container heat-cfn ready: true, restart count 0
Jan  3 15:06:10.295: INFO: ceph-pools-audit-1578063000-76vlr from kube-system started at 2020-01-03 14:50:07 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 15:06:10.295: INFO: calico-node-rb47m from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container calico-node ready: true, restart count 3
Jan  3 15:06:10.295: INFO: ingress-p6rb9 from kube-system started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container ingress ready: true, restart count 0
Jan  3 15:06:10.295: INFO: nova-api-proxy-577495bf7f-k7tnr from openstack started at 2020-01-03 13:51:11 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container nova-api-proxy ready: true, restart count 0
Jan  3 15:06:10.295: INFO: nova-service-cleaner-1578060000-zbvnr from openstack started at 2020-01-03 14:00:06 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container nova-service-cleaner ready: false, restart count 0
Jan  3 15:06:10.295: INFO: mariadb-server-1 from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container mariadb ready: true, restart count 0
Jan  3 15:06:10.295: INFO: nova-novncproxy-6c54c7d98-ptb6t from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container nova-novncproxy ready: true, restart count 0
Jan  3 15:06:10.295: INFO: fm-rest-api-b4bc757f4-4fzhx from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container fm-rest-api ready: true, restart count 0
Jan  3 15:06:10.295: INFO: cinder-api-7ff9984869-kgvv6 from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.295: INFO: 	Container cinder-api ready: true, restart count 0
Jan  3 15:06:10.295: INFO: 
Logging pods the kubelet thinks is on node controller-1 before test
Jan  3 15:06:10.351: INFO: mariadb-server-0 from openstack started at 2020-01-03 11:20:52 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.351: INFO: 	Container mariadb ready: true, restart count 0
Jan  3 15:06:10.352: INFO: glance-ks-service-kbjlh from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.352: INFO: 	Container image-ks-service-registration ready: false, restart count 0
Jan  3 15:06:10.352: INFO: cinder-db-init-g8w2v from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.352: INFO: 	Container cinder-db-init-0 ready: false, restart count 0
Jan  3 15:06:10.352: INFO: osh-openstack-memcached-memcached-545668bdbd-xp7s7 from openstack started at 2020-01-03 13:50:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.352: INFO: 	Container memcached ready: true, restart count 0
Jan  3 15:06:10.352: INFO: heat-rabbit-init-mtk7j from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.352: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 15:06:10.352: INFO: heat-engine-cleaner-1578063300-6swqb from openstack started at 2020-01-03 14:55:09 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.353: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 15:06:10.353: INFO: kube-controller-manager-controller-1 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.353: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  3 15:06:10.353: INFO: glance-ks-user-h6kmn from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.353: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 15:06:10.353: INFO: placement-ks-user-swghx from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.353: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 15:06:10.353: INFO: neutron-ks-endpoints-xrvqm from openstack started at 2020-01-03 11:34:31 +0000 UTC (3 container statuses recorded)
Jan  3 15:06:10.353: INFO: 	Container network-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:06:10.354: INFO: 	Container network-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:06:10.354: INFO: 	Container network-ks-endpoints-public ready: false, restart count 0
Jan  3 15:06:10.354: INFO: ceph-pools-audit-1578063600-gz98x from kube-system started at 2020-01-03 15:00:00 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.354: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 15:06:10.354: INFO: kube-scheduler-controller-1 from kube-system started at 2020-01-03 10:52:37 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.354: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  3 15:06:10.354: INFO: placement-db-init-4t4cq from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.354: INFO: 	Container placement-db-init-0 ready: false, restart count 0
Jan  3 15:06:10.354: INFO: neutron-l3-agent-controller-1-cab72f56-nm27w from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.355: INFO: 	Container neutron-l3-agent ready: true, restart count 0
Jan  3 15:06:10.355: INFO: fm-ks-endpoints-rswss from openstack started at 2020-01-03 11:41:38 +0000 UTC (3 container statuses recorded)
Jan  3 15:06:10.355: INFO: 	Container faultmanagement-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:06:10.355: INFO: 	Container faultmanagement-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:06:10.355: INFO: 	Container faultmanagement-ks-endpoints-public ready: false, restart count 0
Jan  3 15:06:10.355: INFO: cinder-ks-user-rhjbx from openstack started at 2020-01-03 11:29:43 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.355: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 15:06:10.355: INFO: nova-ks-service-x57sz from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.355: INFO: 	Container compute-ks-service-registration ready: false, restart count 0
Jan  3 15:06:10.356: INFO: glance-db-init-9hfsz from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.356: INFO: 	Container glance-db-init-0 ready: false, restart count 0
Jan  3 15:06:10.356: INFO: cinder-scheduler-664bb87785-67xsw from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.356: INFO: 	Container cinder-scheduler ready: true, restart count 0
Jan  3 15:06:10.356: INFO: heat-api-58bf859968-c7cp2 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.356: INFO: 	Container heat-api ready: true, restart count 0
Jan  3 15:06:10.356: INFO: heat-db-init-dmbl7 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.356: INFO: 	Container heat-db-init-0 ready: false, restart count 0
Jan  3 15:06:10.356: INFO: sonobuoy from sonobuoy started at 2020-01-03 12:54:34 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.357: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  3 15:06:10.357: INFO: nova-service-cleaner-1578063600-x4bz4 from openstack started at 2020-01-03 15:00:00 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.357: INFO: 	Container nova-service-cleaner ready: false, restart count 0
Jan  3 15:06:10.357: INFO: calico-node-nkz88 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.357: INFO: 	Container calico-node ready: true, restart count 1
Jan  3 15:06:10.357: INFO: osh-openstack-rabbitmq-rabbitmq-1 from openstack started at 2020-01-03 11:22:47 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.357: INFO: 	Container rabbitmq ready: true, restart count 0
Jan  3 15:06:10.357: INFO: neutron-sriov-agent-controller-1-cab72f56-t65r5 from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.358: INFO: 	Container neutron-sriov-agent ready: true, restart count 0
Jan  3 15:06:10.358: INFO: heat-bootstrap-9xrdd from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.358: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 15:06:10.358: INFO: kube-sriov-cni-ds-amd64-jnv4d from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.358: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Jan  3 15:06:10.358: INFO: keystone-bootstrap-jmtqn from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.358: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 15:06:10.358: INFO: glance-api-747954666d-j2ldf from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container glance-api ready: true, restart count 0
Jan  3 15:06:10.359: INFO: fm-ks-service-nwst8 from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container faultmanagement-ks-service-registration ready: false, restart count 0
Jan  3 15:06:10.359: INFO: placement-api-5b65bc5576-f9d4b from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container placement-api ready: true, restart count 0
Jan  3 15:06:10.359: INFO: nova-compute-controller-1-cab72f56-c922s from openstack started at 2020-01-03 11:34:28 +0000 UTC (2 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container nova-compute ready: true, restart count 0
Jan  3 15:06:10.359: INFO: 	Container nova-compute-ssh ready: true, restart count 0
Jan  3 15:06:10.359: INFO: heat-engine-54745654c7-4ln84 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container heat-engine ready: true, restart count 0
Jan  3 15:06:10.359: INFO: heat-ks-user-hbk96 from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 15:06:10.359: INFO: kube-proxy-2fz94 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 15:06:10.359: INFO: ingress-bc886876f-b6vc9 from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container ingress ready: true, restart count 0
Jan  3 15:06:10.359: INFO: mariadb-ingress-5bb8b69fc8-r2qvn from openstack started at 2020-01-03 11:20:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container ingress ready: true, restart count 0
Jan  3 15:06:10.359: INFO: nova-ks-endpoints-qv68g from openstack started at 2020-01-03 11:34:26 +0000 UTC (3 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container compute-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container compute-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container compute-ks-endpoints-public ready: false, restart count 0
Jan  3 15:06:10.359: INFO: fm-rest-api-b4bc757f4-8tmgc from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container fm-rest-api ready: true, restart count 0
Jan  3 15:06:10.359: INFO: cinder-volume-usage-audit-1578063300-gmvws from openstack started at 2020-01-03 14:55:09 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 15:06:10.359: INFO: kube-multus-ds-amd64-ggv58 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container kube-multus ready: true, restart count 0
Jan  3 15:06:10.359: INFO: neutron-server-54b46f798-4f8d9 from openstack started at 2020-01-03 11:34:31 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container neutron-server ready: true, restart count 0
Jan  3 15:06:10.359: INFO: sonobuoy-systemd-logs-daemon-set-5e158e08c5964d78-2qpxv from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jan  3 15:06:10.359: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 15:06:10.359: INFO: heat-engine-cleaner-1578063900-s9wfq from openstack started at 2020-01-03 15:05:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 15:06:10.359: INFO: nova-db-init-86cfh from openstack started at 2020-01-03 11:34:29 +0000 UTC (3 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container nova-db-init-0 ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container nova-db-init-1 ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container nova-db-init-2 ready: false, restart count 0
Jan  3 15:06:10.359: INFO: fm-db-init-2t975 from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container fm-db-init-0 ready: false, restart count 0
Jan  3 15:06:10.359: INFO: sonobuoy-e2e-job-08a7a8b2e6d84bc6 from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container e2e ready: true, restart count 0
Jan  3 15:06:10.359: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 15:06:10.359: INFO: cinder-create-internal-tenant-twkzc from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container create-internal-tenant ready: false, restart count 0
Jan  3 15:06:10.359: INFO: cinder-backup-fd5f96bf4-vzc8x from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container cinder-backup ready: true, restart count 0
Jan  3 15:06:10.359: INFO: cinder-ks-endpoints-wvm8w from openstack started at 2020-01-03 11:29:43 +0000 UTC (9 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container volume-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container volume-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container volume-ks-endpoints-public ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container volumev2-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container volumev2-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container volumev2-ks-endpoints-public ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container volumev3-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container volumev3-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container volumev3-ks-endpoints-public ready: false, restart count 0
Jan  3 15:06:10.359: INFO: nova-api-metadata-7bdf79d754-kr2gk from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container nova-api ready: true, restart count 1
Jan  3 15:06:10.359: INFO: nova-api-proxy-577495bf7f-b5t9q from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container nova-api-proxy ready: true, restart count 0
Jan  3 15:06:10.359: INFO: nova-conductor-bb6d86d69-5tkcn from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container nova-conductor ready: true, restart count 0
Jan  3 15:06:10.359: INFO: neutron-ovs-agent-controller-1-cab72f56-mntzv from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container neutron-ovs-agent ready: true, restart count 0
Jan  3 15:06:10.359: INFO: heat-cfn-c8f5b9b4b-wd8h4 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container heat-cfn ready: true, restart count 0
Jan  3 15:06:10.359: INFO: ingress-5fgmr from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container ingress ready: true, restart count 0
Jan  3 15:06:10.359: INFO: ingress-error-pages-5bcb8b5f6c-8rf2d from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 15:06:10.359: INFO: mariadb-ingress-error-pages-847467b5d5-d57l2 from openstack started at 2020-01-03 11:20:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 15:06:10.359: INFO: libvirt-libvirt-default-np4lk from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container libvirt ready: true, restart count 0
Jan  3 15:06:10.359: INFO: tiller-deploy-d6b59fcb-zpz47 from kube-system started at 2020-01-03 13:50:02 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container tiller ready: true, restart count 0
Jan  3 15:06:10.359: INFO: heat-ks-endpoints-jjzrq from openstack started at 2020-01-03 11:40:01 +0000 UTC (6 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container cloudformation-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container cloudformation-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container cloudformation-ks-endpoints-public ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container orchestration-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container orchestration-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container orchestration-ks-endpoints-public ready: false, restart count 0
Jan  3 15:06:10.359: INFO: heat-ks-service-b7t2s from openstack started at 2020-01-03 11:40:01 +0000 UTC (2 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container cloudformation-ks-service-registration ready: false, restart count 0
Jan  3 15:06:10.359: INFO: 	Container orchestration-ks-service-registration ready: false, restart count 0
Jan  3 15:06:10.359: INFO: calico-kube-controllers-855577b7b5-n6tlk from kube-system started at 2020-01-03 13:50:00 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan  3 15:06:10.359: INFO: horizon-6865446ff5-c22fk from openstack started at 2020-01-03 13:50:00 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container horizon ready: true, restart count 0
Jan  3 15:06:10.359: INFO: nova-ks-user-gtl4m from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 15:06:10.359: INFO: neutron-metadata-agent-controller-1-cab72f56-xwlz5 from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container neutron-metadata-agent ready: true, restart count 0
Jan  3 15:06:10.359: INFO: ingress-error-pages-cf6c65b-zwhqk from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 15:06:10.359: INFO: keystone-api-7b4d98b8c5-8wjmf from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container keystone-api ready: true, restart count 0
Jan  3 15:06:10.359: INFO: neutron-dhcp-agent-controller-1-cab72f56-t5s8d from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container neutron-dhcp-agent ready: true, restart count 0
Jan  3 15:06:10.359: INFO: kube-apiserver-controller-1 from kube-system started at 2020-01-03 10:52:37 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  3 15:06:10.359: INFO: rbd-provisioner-7484d49cf6-2tb85 from kube-system started at 2020-01-03 10:55:54 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container rbd-provisioner ready: true, restart count 0
Jan  3 15:06:10.359: INFO: nova-api-osapi-68846d5959-ggf8r from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container nova-osapi ready: true, restart count 0
Jan  3 15:06:10.359: INFO: nova-scheduler-76b67b674d-4g9bx from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container nova-scheduler ready: true, restart count 0
Jan  3 15:06:10.359: INFO: coredns-6bc668cd76-9nlk5 from kube-system started at 2020-01-03 10:48:30 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container coredns ready: true, restart count 0
Jan  3 15:06:10.359: INFO: cinder-api-7ff9984869-9g8tp from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container cinder-api ready: true, restart count 0
Jan  3 15:06:10.359: INFO: cinder-volume-549d7c447c-2sjzd from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container cinder-volume ready: true, restart count 0
Jan  3 15:06:10.359: INFO: nova-novncproxy-6c54c7d98-d2pzp from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 15:06:10.359: INFO: 	Container nova-novncproxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-cc2f90c3-e7d2-4c08-955f-941a9f5c319b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-cc2f90c3-e7d2-4c08-955f-941a9f5c319b off the node controller-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-cc2f90c3-e7d2-4c08-955f-941a9f5c319b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:06:26.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2918" for this suite.
Jan  3 15:06:36.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:06:36.441: INFO: namespace sched-pred-2918 deletion completed in 10.032277195s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:26.195 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:06:36.441: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-b0301f82-3b5a-4c72-9b19-3201a130b04c
STEP: Creating a pod to test consume configMaps
Jan  3 15:06:36.457: INFO: Waiting up to 5m0s for pod "pod-configmaps-41e380af-99b9-446b-b72e-50708d1e430d" in namespace "configmap-1416" to be "success or failure"
Jan  3 15:06:36.458: INFO: Pod "pod-configmaps-41e380af-99b9-446b-b72e-50708d1e430d": Phase="Pending", Reason="", readiness=false. Elapsed: 890.837µs
Jan  3 15:06:38.460: INFO: Pod "pod-configmaps-41e380af-99b9-446b-b72e-50708d1e430d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002225328s
Jan  3 15:06:40.461: INFO: Pod "pod-configmaps-41e380af-99b9-446b-b72e-50708d1e430d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003888426s
Jan  3 15:06:42.611: INFO: Pod "pod-configmaps-41e380af-99b9-446b-b72e-50708d1e430d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.153422059s
Jan  3 15:06:44.670: INFO: Pod "pod-configmaps-41e380af-99b9-446b-b72e-50708d1e430d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.212479233s
STEP: Saw pod success
Jan  3 15:06:44.670: INFO: Pod "pod-configmaps-41e380af-99b9-446b-b72e-50708d1e430d" satisfied condition "success or failure"
Jan  3 15:06:44.672: INFO: Trying to get logs from node controller-1 pod pod-configmaps-41e380af-99b9-446b-b72e-50708d1e430d container configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 15:06:44.708: INFO: Waiting for pod pod-configmaps-41e380af-99b9-446b-b72e-50708d1e430d to disappear
Jan  3 15:06:44.744: INFO: Pod pod-configmaps-41e380af-99b9-446b-b72e-50708d1e430d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:06:44.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1416" for this suite.
Jan  3 15:06:50.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:06:50.799: INFO: namespace configmap-1416 deletion completed in 6.053572199s

• [SLOW TEST:14.358 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:06:50.802: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-db477b74-700c-4f4a-9891-70ccc1dedae6
STEP: Creating secret with name s-test-opt-upd-a966385f-eae0-4d60-bacc-bccc26de900c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-db477b74-700c-4f4a-9891-70ccc1dedae6
STEP: Updating secret s-test-opt-upd-a966385f-eae0-4d60-bacc-bccc26de900c
STEP: Creating secret with name s-test-opt-create-d4e4d517-8423-448d-989d-b15d6bacda24
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:08:09.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3136" for this suite.
Jan  3 15:08:21.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:08:21.177: INFO: namespace secrets-3136 deletion completed in 12.03680228s

• [SLOW TEST:90.375 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:08:21.177: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-60f50ec0-e126-4eb1-b3f7-bd0be2adf153
STEP: Creating a pod to test consume secrets
Jan  3 15:08:21.194: INFO: Waiting up to 5m0s for pod "pod-secrets-2a2661a7-aff7-4dde-9e39-c0df199105a6" in namespace "secrets-3736" to be "success or failure"
Jan  3 15:08:21.195: INFO: Pod "pod-secrets-2a2661a7-aff7-4dde-9e39-c0df199105a6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.02324ms
Jan  3 15:08:23.196: INFO: Pod "pod-secrets-2a2661a7-aff7-4dde-9e39-c0df199105a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002386446s
Jan  3 15:08:25.199: INFO: Pod "pod-secrets-2a2661a7-aff7-4dde-9e39-c0df199105a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005642789s
Jan  3 15:08:27.201: INFO: Pod "pod-secrets-2a2661a7-aff7-4dde-9e39-c0df199105a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007483489s
Jan  3 15:08:29.204: INFO: Pod "pod-secrets-2a2661a7-aff7-4dde-9e39-c0df199105a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.010250584s
STEP: Saw pod success
Jan  3 15:08:29.204: INFO: Pod "pod-secrets-2a2661a7-aff7-4dde-9e39-c0df199105a6" satisfied condition "success or failure"
Jan  3 15:08:29.205: INFO: Trying to get logs from node controller-0 pod pod-secrets-2a2661a7-aff7-4dde-9e39-c0df199105a6 container secret-volume-test: <nil>
STEP: delete the pod
Jan  3 15:08:29.216: INFO: Waiting for pod pod-secrets-2a2661a7-aff7-4dde-9e39-c0df199105a6 to disappear
Jan  3 15:08:29.217: INFO: Pod pod-secrets-2a2661a7-aff7-4dde-9e39-c0df199105a6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:08:29.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3736" for this suite.
Jan  3 15:08:35.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:08:35.258: INFO: namespace secrets-3736 deletion completed in 6.039604462s

• [SLOW TEST:14.081 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:08:35.258: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan  3 15:08:35.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6642'
Jan  3 15:08:35.408: INFO: stderr: ""
Jan  3 15:08:35.408: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan  3 15:08:45.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 get pod e2e-test-httpd-pod --namespace=kubectl-6642 -o json'
Jan  3 15:08:45.696: INFO: stderr: ""
Jan  3 15:08:45.696: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.16.192.119/32\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"chain\\\",\\n    \\\"ips\\\": [\\n        \\\"172.16.192.119\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2020-01-03T15:08:35Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6642\",\n        \"resourceVersion\": \"77974\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6642/pods/e2e-test-httpd-pod\",\n        \"uid\": \"ef2178ff-8a03-4f54-aa0f-ba8ce4472c6d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-55rc6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"controller-0\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 30\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 30\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-55rc6\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-55rc6\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-03T15:08:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-03T15:08:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-03T15:08:42Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-03T15:08:35Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://49be19caa237b6a556e23819b1b1f88430547b7de894b8849570bd00c7763c2f\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-01-03T15:08:42Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.61.11\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.192.119\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.192.119\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-01-03T15:08:35Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan  3 15:08:45.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 replace -f - --namespace=kubectl-6642'
Jan  3 15:08:46.266: INFO: stderr: ""
Jan  3 15:08:46.266: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Jan  3 15:08:46.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 delete pods e2e-test-httpd-pod --namespace=kubectl-6642'
Jan  3 15:08:48.102: INFO: stderr: ""
Jan  3 15:08:48.102: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:08:48.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6642" for this suite.
Jan  3 15:08:54.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:08:54.157: INFO: namespace kubectl-6642 deletion completed in 6.048245945s

• [SLOW TEST:18.899 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:08:54.158: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 15:08:54.193: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9cf6637-f60a-4707-9af9-d0cb3d0e216b" in namespace "downward-api-5871" to be "success or failure"
Jan  3 15:08:54.194: INFO: Pod "downwardapi-volume-d9cf6637-f60a-4707-9af9-d0cb3d0e216b": Phase="Pending", Reason="", readiness=false. Elapsed: 974.278µs
Jan  3 15:08:56.199: INFO: Pod "downwardapi-volume-d9cf6637-f60a-4707-9af9-d0cb3d0e216b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005275303s
Jan  3 15:08:58.200: INFO: Pod "downwardapi-volume-d9cf6637-f60a-4707-9af9-d0cb3d0e216b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006647807s
Jan  3 15:09:00.202: INFO: Pod "downwardapi-volume-d9cf6637-f60a-4707-9af9-d0cb3d0e216b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008189507s
Jan  3 15:09:02.203: INFO: Pod "downwardapi-volume-d9cf6637-f60a-4707-9af9-d0cb3d0e216b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009949199s
STEP: Saw pod success
Jan  3 15:09:02.203: INFO: Pod "downwardapi-volume-d9cf6637-f60a-4707-9af9-d0cb3d0e216b" satisfied condition "success or failure"
Jan  3 15:09:02.204: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-d9cf6637-f60a-4707-9af9-d0cb3d0e216b container client-container: <nil>
STEP: delete the pod
Jan  3 15:09:02.217: INFO: Waiting for pod downwardapi-volume-d9cf6637-f60a-4707-9af9-d0cb3d0e216b to disappear
Jan  3 15:09:02.218: INFO: Pod downwardapi-volume-d9cf6637-f60a-4707-9af9-d0cb3d0e216b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:09:02.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5871" for this suite.
Jan  3 15:09:08.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:09:08.260: INFO: namespace downward-api-5871 deletion completed in 6.040523458s

• [SLOW TEST:14.102 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:09:08.260: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 15:09:08.273: INFO: Creating deployment "test-recreate-deployment"
Jan  3 15:09:08.274: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan  3 15:09:08.277: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan  3 15:09:10.280: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan  3 15:09:10.281: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660948, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660948, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660948, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660948, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 15:09:12.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660948, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660948, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660948, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660948, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 15:09:14.293: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660948, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660948, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660948, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713660948, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 15:09:16.282: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan  3 15:09:16.286: INFO: Updating deployment test-recreate-deployment
Jan  3 15:09:16.286: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jan  3 15:09:16.341: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3832 /apis/apps/v1/namespaces/deployment-3832/deployments/test-recreate-deployment 4ce9391f-e2c4-4982-9aea-fccddc0d3635 78179 2 2020-01-03 15:09:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ee6978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-01-03 15:09:16 +0000 UTC,LastTransitionTime:2020-01-03 15:09:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-01-03 15:09:16 +0000 UTC,LastTransitionTime:2020-01-03 15:09:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan  3 15:09:16.346: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-3832 /apis/apps/v1/namespaces/deployment-3832/replicasets/test-recreate-deployment-5f94c574ff 55262d5f-8251-4bc7-9469-7ce4a761c5f4 78178 1 2020-01-03 15:09:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4ce9391f-e2c4-4982-9aea-fccddc0d3635 0xc003ee6d67 0xc003ee6d68}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ee6dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 15:09:16.346: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan  3 15:09:16.346: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-3832 /apis/apps/v1/namespaces/deployment-3832/replicasets/test-recreate-deployment-68fc85c7bb 0fa66f6a-e8e4-43fc-9dbe-44ad80d49498 78170 2 2020-01-03 15:09:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4ce9391f-e2c4-4982-9aea-fccddc0d3635 0xc003ee6e37 0xc003ee6e38}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ee6e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 15:09:16.370: INFO: Pod "test-recreate-deployment-5f94c574ff-mpnfn" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-mpnfn test-recreate-deployment-5f94c574ff- deployment-3832 /api/v1/namespaces/deployment-3832/pods/test-recreate-deployment-5f94c574ff-mpnfn a383b252-893e-4e8d-ab5d-25d22f1ce9b4 78180 0 2020-01-03 15:09:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 55262d5f-8251-4bc7-9469-7ce4a761c5f4 0xc00453c8c7 0xc00453c8c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m8j47,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m8j47,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m8j47,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-03 15:09:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:09:16.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3832" for this suite.
Jan  3 15:09:22.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:09:22.405: INFO: namespace deployment-3832 deletion completed in 6.033654865s

• [SLOW TEST:14.145 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:09:22.407: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1522
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-1522
I0103 15:09:22.431802      23 runners.go:184] Created replication controller with name: externalname-service, namespace: services-1522, replica count: 2
I0103 15:09:25.482345      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 15:09:28.482522      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 15:09:31.482721      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 15:09:31.482: INFO: Creating new exec pod
Jan  3 15:09:40.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=services-1522 execpodwnclk -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jan  3 15:09:40.974: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  3 15:09:40.974: INFO: stdout: ""
Jan  3 15:09:40.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=services-1522 execpodwnclk -- /bin/sh -x -c nc -zv -t -w 2 10.108.138.145 80'
Jan  3 15:09:41.745: INFO: stderr: "+ nc -zv -t -w 2 10.108.138.145 80\nConnection to 10.108.138.145 80 port [tcp/http] succeeded!\n"
Jan  3 15:09:41.745: INFO: stdout: ""
Jan  3 15:09:41.745: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:09:41.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1522" for this suite.
Jan  3 15:09:47.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:09:47.824: INFO: namespace services-1522 deletion completed in 6.058700134s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:25.417 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:09:47.825: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 15:09:47.844: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:09:49.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1260" for this suite.
Jan  3 15:09:55.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:09:55.751: INFO: namespace custom-resource-definition-1260 deletion completed in 6.043571355s

• [SLOW TEST:7.927 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:09:55.752: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan  3 15:09:55.773: INFO: Waiting up to 5m0s for pod "pod-10d4966c-b9ef-4a3c-9fbe-f0f1eb4939ee" in namespace "emptydir-906" to be "success or failure"
Jan  3 15:09:55.780: INFO: Pod "pod-10d4966c-b9ef-4a3c-9fbe-f0f1eb4939ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.541178ms
Jan  3 15:09:57.781: INFO: Pod "pod-10d4966c-b9ef-4a3c-9fbe-f0f1eb4939ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008049837s
Jan  3 15:09:59.783: INFO: Pod "pod-10d4966c-b9ef-4a3c-9fbe-f0f1eb4939ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009513555s
Jan  3 15:10:01.785: INFO: Pod "pod-10d4966c-b9ef-4a3c-9fbe-f0f1eb4939ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011547629s
Jan  3 15:10:03.787: INFO: Pod "pod-10d4966c-b9ef-4a3c-9fbe-f0f1eb4939ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013403916s
STEP: Saw pod success
Jan  3 15:10:03.787: INFO: Pod "pod-10d4966c-b9ef-4a3c-9fbe-f0f1eb4939ee" satisfied condition "success or failure"
Jan  3 15:10:03.788: INFO: Trying to get logs from node controller-1 pod pod-10d4966c-b9ef-4a3c-9fbe-f0f1eb4939ee container test-container: <nil>
STEP: delete the pod
Jan  3 15:10:03.797: INFO: Waiting for pod pod-10d4966c-b9ef-4a3c-9fbe-f0f1eb4939ee to disappear
Jan  3 15:10:03.800: INFO: Pod pod-10d4966c-b9ef-4a3c-9fbe-f0f1eb4939ee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:10:03.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-906" for this suite.
Jan  3 15:10:09.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:10:09.857: INFO: namespace emptydir-906 deletion completed in 6.055820191s

• [SLOW TEST:14.105 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:10:09.858: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3765
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Jan  3 15:10:09.915: INFO: Found 1 stateful pods, waiting for 3
Jan  3 15:10:19.917: INFO: Found 2 stateful pods, waiting for 3
Jan  3 15:10:29.917: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 15:10:29.917: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 15:10:29.917: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan  3 15:10:39.917: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 15:10:39.918: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 15:10:39.919: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 15:10:39.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-3765 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 15:10:40.166: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 15:10:40.166: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 15:10:40.166: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jan  3 15:10:50.185: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan  3 15:11:00.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-3765 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 15:11:00.428: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 15:11:00.428: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 15:11:00.428: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 15:11:10.436: INFO: Waiting for StatefulSet statefulset-3765/ss2 to complete update
Jan  3 15:11:10.436: INFO: Waiting for Pod statefulset-3765/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan  3 15:11:10.436: INFO: Waiting for Pod statefulset-3765/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan  3 15:11:20.438: INFO: Waiting for StatefulSet statefulset-3765/ss2 to complete update
Jan  3 15:11:20.438: INFO: Waiting for Pod statefulset-3765/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan  3 15:11:30.439: INFO: Waiting for StatefulSet statefulset-3765/ss2 to complete update
STEP: Rolling back to a previous revision
Jan  3 15:11:40.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-3765 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 15:11:40.669: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 15:11:40.670: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 15:11:40.670: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 15:11:50.690: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan  3 15:12:00.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937082782 exec --namespace=statefulset-3765 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 15:12:01.026: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 15:12:01.026: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 15:12:01.034: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 15:12:11.114: INFO: Waiting for StatefulSet statefulset-3765/ss2 to complete update
Jan  3 15:12:11.114: INFO: Waiting for Pod statefulset-3765/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan  3 15:12:11.114: INFO: Waiting for Pod statefulset-3765/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan  3 15:12:21.123: INFO: Waiting for StatefulSet statefulset-3765/ss2 to complete update
Jan  3 15:12:21.123: INFO: Waiting for Pod statefulset-3765/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan  3 15:12:31.122: INFO: Waiting for StatefulSet statefulset-3765/ss2 to complete update
Jan  3 15:12:31.122: INFO: Waiting for Pod statefulset-3765/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan  3 15:12:41.117: INFO: Waiting for StatefulSet statefulset-3765/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jan  3 15:12:51.124: INFO: Deleting all statefulset in ns statefulset-3765
Jan  3 15:12:51.126: INFO: Scaling statefulset ss2 to 0
Jan  3 15:13:11.242: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 15:13:11.245: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:13:11.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3765" for this suite.
Jan  3 15:13:17.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:13:17.341: INFO: namespace statefulset-3765 deletion completed in 6.069056293s

• [SLOW TEST:187.483 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:13:17.341: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:13:35.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2789" for this suite.
Jan  3 15:13:41.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:13:41.572: INFO: namespace job-2789 deletion completed in 6.192349563s

• [SLOW TEST:24.231 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:13:41.572: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jan  3 15:13:41.618: INFO: PodSpec: initContainers in spec.initContainers
Jan  3 15:14:33.007: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-cd7d8c10-4fc2-4b86-ac4c-08c68634fbf1", GenerateName:"", Namespace:"init-container-4958", SelfLink:"/api/v1/namespaces/init-container-4958/pods/pod-init-cd7d8c10-4fc2-4b86-ac4c-08c68634fbf1", UID:"b2d8978a-68ad-40da-afb5-46350d6b02d4", ResourceVersion:"80063", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63713661221, loc:(*time.Location)(0x84c02a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"618641571"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.16.192.83/32", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.192.83\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-7vsnr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002e90000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7vsnr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7vsnr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7vsnr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00359e098), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"controller-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003c3a120), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00359e120)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00359e140)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00359e148), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00359e14c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661221, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661221, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661221, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661221, loc:(*time.Location)(0x84c02a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.61.11", PodIP:"172.16.192.83", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.16.192.83"}}, StartTime:(*v1.Time)(0xc002f84080), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006024d0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000602540)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://8d1a2197452e1bfe1eaf019cb557f3ff29f862471086074c2fa7d84cf07792cb", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f840c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f840a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00359e1cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:14:33.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4958" for this suite.
Jan  3 15:15:01.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:15:01.074: INFO: namespace init-container-4958 deletion completed in 28.057990764s

• [SLOW TEST:79.502 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:15:01.076: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jan  3 15:15:01.090: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  3 15:15:01.094: INFO: Waiting for terminating namespaces to be deleted...
Jan  3 15:15:01.096: INFO: 
Logging pods the kubelet thinks is on node controller-0 before test
Jan  3 15:15:01.122: INFO: mariadb-server-1 from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.122: INFO: 	Container mariadb ready: true, restart count 0
Jan  3 15:15:01.122: INFO: nova-novncproxy-6c54c7d98-ptb6t from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container nova-novncproxy ready: true, restart count 0
Jan  3 15:15:01.123: INFO: fm-rest-api-b4bc757f4-4fzhx from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container fm-rest-api ready: true, restart count 0
Jan  3 15:15:01.123: INFO: cinder-api-7ff9984869-kgvv6 from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container cinder-api ready: true, restart count 0
Jan  3 15:15:01.123: INFO: neutron-sriov-agent-controller-0-937646f6-dg699 from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container neutron-sriov-agent ready: true, restart count 0
Jan  3 15:15:01.123: INFO: placement-api-5b65bc5576-kh6z7 from openstack started at 2020-01-03 13:50:47 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container placement-api ready: true, restart count 0
Jan  3 15:15:01.123: INFO: nova-compute-controller-0-937646f6-vl8z6 from openstack started at 2020-01-03 13:50:44 +0000 UTC (2 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container nova-compute ready: true, restart count 0
Jan  3 15:15:01.123: INFO: 	Container nova-compute-ssh ready: true, restart count 0
Jan  3 15:15:01.123: INFO: osh-openstack-rabbitmq-rabbitmq-0 from openstack started at 2020-01-03 13:50:53 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container rabbitmq ready: true, restart count 0
Jan  3 15:15:01.123: INFO: neutron-dhcp-agent-controller-0-937646f6-prqt7 from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container neutron-dhcp-agent ready: true, restart count 0
Jan  3 15:15:01.123: INFO: ceph-pools-audit-1578063900-tm8vk from kube-system started at 2020-01-03 15:05:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 15:15:01.123: INFO: coredns-6bc668cd76-nt9h6 from kube-system started at 2020-01-03 13:50:47 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container coredns ready: true, restart count 0
Jan  3 15:15:01.123: INFO: libvirt-libvirt-default-psmq6 from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container libvirt ready: true, restart count 0
Jan  3 15:15:01.123: INFO: cinder-backup-fd5f96bf4-z4xc2 from openstack started at 2020-01-03 13:50:47 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container cinder-backup ready: true, restart count 0
Jan  3 15:15:01.123: INFO: neutron-metadata-agent-controller-0-937646f6-z2mz9 from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container neutron-metadata-agent ready: true, restart count 0
Jan  3 15:15:01.123: INFO: keystone-api-7b4d98b8c5-2q8z7 from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container keystone-api ready: true, restart count 0
Jan  3 15:15:01.123: INFO: ingress-error-pages-5bcb8b5f6c-bn4nq from kube-system started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 15:15:01.123: INFO: neutron-l3-agent-controller-0-937646f6-ft64l from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container neutron-l3-agent ready: true, restart count 0
Jan  3 15:15:01.123: INFO: cinder-scheduler-664bb87785-p8t9k from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container cinder-scheduler ready: true, restart count 0
Jan  3 15:15:01.123: INFO: kube-proxy-5l8fd from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container kube-proxy ready: true, restart count 1
Jan  3 15:15:01.123: INFO: nova-api-metadata-7bdf79d754-h8r6m from openstack started at 2020-01-03 13:50:47 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container nova-api ready: true, restart count 0
Jan  3 15:15:01.123: INFO: ingress-error-pages-cf6c65b-vmlrg from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 15:15:01.123: INFO: glance-api-747954666d-cmg85 from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container glance-api ready: true, restart count 0
Jan  3 15:15:01.123: INFO: cinder-volume-usage-audit-1578063900-hvb7w from openstack started at 2020-01-03 15:05:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.123: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 15:15:01.124: INFO: sonobuoy-systemd-logs-daemon-set-5e158e08c5964d78-s6m8n from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 15:15:01.124: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jan  3 15:15:01.124: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 15:15:01.124: INFO: heat-engine-54745654c7-hbkrx from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.124: INFO: 	Container heat-engine ready: true, restart count 0
Jan  3 15:15:01.124: INFO: nova-scheduler-76b67b674d-lwrc2 from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.124: INFO: 	Container nova-scheduler ready: true, restart count 0
Jan  3 15:15:01.124: INFO: kube-apiserver-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.124: INFO: 	Container kube-apiserver ready: true, restart count 2
Jan  3 15:15:01.124: INFO: kube-multus-ds-amd64-6zbwh from kube-system started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.124: INFO: 	Container kube-multus ready: true, restart count 0
Jan  3 15:15:01.124: INFO: cinder-volume-549d7c447c-tr8gw from openstack started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.124: INFO: 	Container cinder-volume ready: true, restart count 0
Jan  3 15:15:01.124: INFO: kube-sriov-cni-ds-amd64-ddmnd from kube-system started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.124: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Jan  3 15:15:01.124: INFO: cinder-volume-usage-audit-1578063600-xb654 from openstack started at 2020-01-03 15:00:00 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.124: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 15:15:01.125: INFO: nova-conductor-bb6d86d69-tgxnd from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.125: INFO: 	Container nova-conductor ready: true, restart count 0
Jan  3 15:15:01.125: INFO: nova-api-osapi-68846d5959-w7jrd from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.125: INFO: 	Container nova-osapi ready: true, restart count 1
Jan  3 15:15:01.125: INFO: mariadb-ingress-5bb8b69fc8-xppj8 from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.125: INFO: 	Container ingress ready: true, restart count 0
Jan  3 15:15:01.125: INFO: kube-scheduler-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.125: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan  3 15:15:01.125: INFO: rbd-provisioner-7484d49cf6-zf49b from kube-system started at 2020-01-03 13:50:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.125: INFO: 	Container rbd-provisioner ready: true, restart count 0
Jan  3 15:15:01.125: INFO: ingress-bc886876f-487zs from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.126: INFO: 	Container ingress ready: true, restart count 0
Jan  3 15:15:01.126: INFO: heat-engine-cleaner-1578063600-jzn9d from openstack started at 2020-01-03 15:00:00 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.126: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 15:15:01.126: INFO: heat-engine-cleaner-1578064200-hm2r6 from openstack started at 2020-01-03 15:10:02 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.126: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 15:15:01.126: INFO: neutron-server-54b46f798-bbhdw from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.126: INFO: 	Container neutron-server ready: true, restart count 0
Jan  3 15:15:01.126: INFO: neutron-ovs-agent-controller-0-937646f6-n2m6q from openstack started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.126: INFO: 	Container neutron-ovs-agent ready: true, restart count 0
Jan  3 15:15:01.126: INFO: heat-api-58bf859968-7rz2w from openstack started at 2020-01-03 13:50:49 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.126: INFO: 	Container heat-api ready: true, restart count 0
Jan  3 15:15:01.126: INFO: kube-controller-manager-controller-0 from kube-system started at 2020-01-03 10:24:46 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.127: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan  3 15:15:01.127: INFO: heat-cfn-c8f5b9b4b-dfrgc from openstack started at 2020-01-03 13:51:06 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.127: INFO: 	Container heat-cfn ready: true, restart count 0
Jan  3 15:15:01.127: INFO: nova-api-proxy-577495bf7f-k7tnr from openstack started at 2020-01-03 13:51:11 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.127: INFO: 	Container nova-api-proxy ready: true, restart count 0
Jan  3 15:15:01.127: INFO: nova-service-cleaner-1578060000-zbvnr from openstack started at 2020-01-03 14:00:06 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.127: INFO: 	Container nova-service-cleaner ready: false, restart count 0
Jan  3 15:15:01.127: INFO: calico-node-rb47m from kube-system started at 2020-01-03 09:59:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.127: INFO: 	Container calico-node ready: true, restart count 3
Jan  3 15:15:01.127: INFO: ingress-p6rb9 from kube-system started at 2020-01-03 13:50:44 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.127: INFO: 	Container ingress ready: true, restart count 0
Jan  3 15:15:01.127: INFO: 
Logging pods the kubelet thinks is on node controller-1 before test
Jan  3 15:15:01.161: INFO: nova-ks-user-gtl4m from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.161: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 15:15:01.161: INFO: neutron-metadata-agent-controller-1-cab72f56-xwlz5 from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.161: INFO: 	Container neutron-metadata-agent ready: true, restart count 0
Jan  3 15:15:01.161: INFO: ingress-error-pages-cf6c65b-zwhqk from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.161: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 15:15:01.161: INFO: keystone-api-7b4d98b8c5-8wjmf from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.161: INFO: 	Container keystone-api ready: true, restart count 0
Jan  3 15:15:01.161: INFO: neutron-dhcp-agent-controller-1-cab72f56-t5s8d from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container neutron-dhcp-agent ready: true, restart count 0
Jan  3 15:15:01.162: INFO: kube-apiserver-controller-1 from kube-system started at 2020-01-03 10:52:37 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  3 15:15:01.162: INFO: rbd-provisioner-7484d49cf6-2tb85 from kube-system started at 2020-01-03 10:55:54 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container rbd-provisioner ready: true, restart count 0
Jan  3 15:15:01.162: INFO: nova-api-osapi-68846d5959-ggf8r from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container nova-osapi ready: true, restart count 0
Jan  3 15:15:01.162: INFO: nova-scheduler-76b67b674d-4g9bx from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container nova-scheduler ready: true, restart count 0
Jan  3 15:15:01.162: INFO: coredns-6bc668cd76-9nlk5 from kube-system started at 2020-01-03 10:48:30 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container coredns ready: true, restart count 0
Jan  3 15:15:01.162: INFO: cinder-api-7ff9984869-9g8tp from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container cinder-api ready: true, restart count 0
Jan  3 15:15:01.162: INFO: cinder-volume-549d7c447c-2sjzd from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container cinder-volume ready: true, restart count 0
Jan  3 15:15:01.162: INFO: nova-novncproxy-6c54c7d98-d2pzp from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container nova-novncproxy ready: true, restart count 0
Jan  3 15:15:01.162: INFO: mariadb-server-0 from openstack started at 2020-01-03 11:20:52 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container mariadb ready: true, restart count 0
Jan  3 15:15:01.162: INFO: glance-ks-service-kbjlh from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container image-ks-service-registration ready: false, restart count 0
Jan  3 15:15:01.162: INFO: cinder-db-init-g8w2v from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container cinder-db-init-0 ready: false, restart count 0
Jan  3 15:15:01.162: INFO: osh-openstack-memcached-memcached-545668bdbd-xp7s7 from openstack started at 2020-01-03 13:50:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container memcached ready: true, restart count 0
Jan  3 15:15:01.162: INFO: kube-controller-manager-controller-1 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan  3 15:15:01.162: INFO: glance-ks-user-h6kmn from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 15:15:01.162: INFO: placement-ks-user-swghx from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 15:15:01.162: INFO: neutron-ks-endpoints-xrvqm from openstack started at 2020-01-03 11:34:31 +0000 UTC (3 container statuses recorded)
Jan  3 15:15:01.162: INFO: 	Container network-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:15:01.163: INFO: 	Container network-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:15:01.163: INFO: 	Container network-ks-endpoints-public ready: false, restart count 0
Jan  3 15:15:01.163: INFO: heat-rabbit-init-mtk7j from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container rabbit-init ready: false, restart count 0
Jan  3 15:15:01.163: INFO: kube-scheduler-controller-1 from kube-system started at 2020-01-03 10:52:37 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan  3 15:15:01.163: INFO: placement-db-init-4t4cq from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container placement-db-init-0 ready: false, restart count 0
Jan  3 15:15:01.163: INFO: neutron-l3-agent-controller-1-cab72f56-nm27w from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container neutron-l3-agent ready: true, restart count 0
Jan  3 15:15:01.163: INFO: fm-ks-endpoints-rswss from openstack started at 2020-01-03 11:41:38 +0000 UTC (3 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container faultmanagement-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:15:01.163: INFO: 	Container faultmanagement-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:15:01.163: INFO: 	Container faultmanagement-ks-endpoints-public ready: false, restart count 0
Jan  3 15:15:01.163: INFO: ceph-pools-audit-1578063600-gz98x from kube-system started at 2020-01-03 15:00:00 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 15:15:01.163: INFO: ceph-pools-audit-1578064200-tshtg from kube-system started at 2020-01-03 15:10:02 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jan  3 15:15:01.163: INFO: cinder-ks-user-rhjbx from openstack started at 2020-01-03 11:29:43 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 15:15:01.163: INFO: nova-ks-service-x57sz from openstack started at 2020-01-03 11:34:29 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container compute-ks-service-registration ready: false, restart count 0
Jan  3 15:15:01.163: INFO: glance-db-init-9hfsz from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container glance-db-init-0 ready: false, restart count 0
Jan  3 15:15:01.163: INFO: cinder-scheduler-664bb87785-67xsw from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container cinder-scheduler ready: true, restart count 0
Jan  3 15:15:01.163: INFO: heat-api-58bf859968-c7cp2 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container heat-api ready: true, restart count 0
Jan  3 15:15:01.163: INFO: heat-db-init-dmbl7 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container heat-db-init-0 ready: false, restart count 0
Jan  3 15:15:01.163: INFO: sonobuoy from sonobuoy started at 2020-01-03 12:54:34 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  3 15:15:01.163: INFO: nova-service-cleaner-1578063600-x4bz4 from openstack started at 2020-01-03 15:00:00 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container nova-service-cleaner ready: false, restart count 0
Jan  3 15:15:01.163: INFO: calico-node-nkz88 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container calico-node ready: true, restart count 1
Jan  3 15:15:01.163: INFO: osh-openstack-rabbitmq-rabbitmq-1 from openstack started at 2020-01-03 11:22:47 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container rabbitmq ready: true, restart count 0
Jan  3 15:15:01.163: INFO: neutron-sriov-agent-controller-1-cab72f56-t65r5 from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container neutron-sriov-agent ready: true, restart count 0
Jan  3 15:15:01.163: INFO: heat-bootstrap-9xrdd from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.163: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 15:15:01.164: INFO: cinder-volume-usage-audit-1578064200-vc8v8 from openstack started at 2020-01-03 15:10:02 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container cinder-volume-usage-audit ready: false, restart count 0
Jan  3 15:15:01.164: INFO: kube-sriov-cni-ds-amd64-jnv4d from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Jan  3 15:15:01.164: INFO: keystone-bootstrap-jmtqn from openstack started at 2020-01-03 11:23:45 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container bootstrap ready: false, restart count 0
Jan  3 15:15:01.164: INFO: glance-api-747954666d-j2ldf from openstack started at 2020-01-03 11:25:03 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container glance-api ready: true, restart count 0
Jan  3 15:15:01.164: INFO: fm-ks-service-nwst8 from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container faultmanagement-ks-service-registration ready: false, restart count 0
Jan  3 15:15:01.164: INFO: heat-ks-user-hbk96 from openstack started at 2020-01-03 11:40:02 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container ks-user ready: false, restart count 0
Jan  3 15:15:01.164: INFO: kube-proxy-2fz94 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 15:15:01.164: INFO: ingress-bc886876f-b6vc9 from openstack started at 2020-01-03 11:20:27 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container ingress ready: true, restart count 0
Jan  3 15:15:01.164: INFO: mariadb-ingress-5bb8b69fc8-r2qvn from openstack started at 2020-01-03 11:20:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container ingress ready: true, restart count 0
Jan  3 15:15:01.164: INFO: nova-ks-endpoints-qv68g from openstack started at 2020-01-03 11:34:26 +0000 UTC (3 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container compute-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container compute-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container compute-ks-endpoints-public ready: false, restart count 0
Jan  3 15:15:01.164: INFO: placement-api-5b65bc5576-f9d4b from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container placement-api ready: true, restart count 0
Jan  3 15:15:01.164: INFO: nova-compute-controller-1-cab72f56-c922s from openstack started at 2020-01-03 11:34:28 +0000 UTC (2 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container nova-compute ready: true, restart count 0
Jan  3 15:15:01.164: INFO: 	Container nova-compute-ssh ready: true, restart count 0
Jan  3 15:15:01.164: INFO: heat-engine-54745654c7-4ln84 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container heat-engine ready: true, restart count 0
Jan  3 15:15:01.164: INFO: fm-rest-api-b4bc757f4-8tmgc from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container fm-rest-api ready: true, restart count 0
Jan  3 15:15:01.164: INFO: kube-multus-ds-amd64-ggv58 from kube-system started at 2020-01-03 10:48:17 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container kube-multus ready: true, restart count 0
Jan  3 15:15:01.164: INFO: neutron-server-54b46f798-4f8d9 from openstack started at 2020-01-03 11:34:31 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container neutron-server ready: true, restart count 0
Jan  3 15:15:01.164: INFO: sonobuoy-systemd-logs-daemon-set-5e158e08c5964d78-2qpxv from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jan  3 15:15:01.164: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 15:15:01.164: INFO: heat-engine-cleaner-1578063900-s9wfq from openstack started at 2020-01-03 15:05:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container heat-engine-cleaner ready: false, restart count 0
Jan  3 15:15:01.164: INFO: cinder-create-internal-tenant-twkzc from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container create-internal-tenant ready: false, restart count 0
Jan  3 15:15:01.164: INFO: cinder-backup-fd5f96bf4-vzc8x from openstack started at 2020-01-03 11:29:42 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container cinder-backup ready: true, restart count 0
Jan  3 15:15:01.164: INFO: cinder-ks-endpoints-wvm8w from openstack started at 2020-01-03 11:29:43 +0000 UTC (9 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container volume-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container volume-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container volume-ks-endpoints-public ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container volumev2-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container volumev2-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container volumev2-ks-endpoints-public ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container volumev3-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container volumev3-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container volumev3-ks-endpoints-public ready: false, restart count 0
Jan  3 15:15:01.164: INFO: nova-api-metadata-7bdf79d754-kr2gk from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container nova-api ready: true, restart count 1
Jan  3 15:15:01.164: INFO: nova-db-init-86cfh from openstack started at 2020-01-03 11:34:29 +0000 UTC (3 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container nova-db-init-0 ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container nova-db-init-1 ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container nova-db-init-2 ready: false, restart count 0
Jan  3 15:15:01.164: INFO: fm-db-init-2t975 from openstack started at 2020-01-03 11:41:38 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container fm-db-init-0 ready: false, restart count 0
Jan  3 15:15:01.164: INFO: sonobuoy-e2e-job-08a7a8b2e6d84bc6 from sonobuoy started at 2020-01-03 12:55:43 +0000 UTC (2 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container e2e ready: true, restart count 0
Jan  3 15:15:01.164: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 15:15:01.164: INFO: heat-cfn-c8f5b9b4b-wd8h4 from openstack started at 2020-01-03 11:40:01 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container heat-cfn ready: true, restart count 0
Jan  3 15:15:01.164: INFO: ingress-5fgmr from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container ingress ready: true, restart count 0
Jan  3 15:15:01.164: INFO: ingress-error-pages-5bcb8b5f6c-8rf2d from kube-system started at 2020-01-03 11:20:05 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 15:15:01.164: INFO: mariadb-ingress-error-pages-847467b5d5-d57l2 from openstack started at 2020-01-03 11:20:48 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container ingress-error-pages ready: true, restart count 0
Jan  3 15:15:01.164: INFO: libvirt-libvirt-default-np4lk from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container libvirt ready: true, restart count 0
Jan  3 15:15:01.164: INFO: nova-api-proxy-577495bf7f-b5t9q from openstack started at 2020-01-03 11:34:26 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container nova-api-proxy ready: true, restart count 0
Jan  3 15:15:01.164: INFO: nova-conductor-bb6d86d69-5tkcn from openstack started at 2020-01-03 11:34:28 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container nova-conductor ready: true, restart count 0
Jan  3 15:15:01.164: INFO: neutron-ovs-agent-controller-1-cab72f56-mntzv from openstack started at 2020-01-03 11:34:30 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container neutron-ovs-agent ready: true, restart count 0
Jan  3 15:15:01.164: INFO: heat-ks-endpoints-jjzrq from openstack started at 2020-01-03 11:40:01 +0000 UTC (6 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container cloudformation-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container cloudformation-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container cloudformation-ks-endpoints-public ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container orchestration-ks-endpoints-admin ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container orchestration-ks-endpoints-internal ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container orchestration-ks-endpoints-public ready: false, restart count 0
Jan  3 15:15:01.164: INFO: heat-ks-service-b7t2s from openstack started at 2020-01-03 11:40:01 +0000 UTC (2 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container cloudformation-ks-service-registration ready: false, restart count 0
Jan  3 15:15:01.164: INFO: 	Container orchestration-ks-service-registration ready: false, restart count 0
Jan  3 15:15:01.164: INFO: calico-kube-controllers-855577b7b5-n6tlk from kube-system started at 2020-01-03 13:50:00 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan  3 15:15:01.164: INFO: horizon-6865446ff5-c22fk from openstack started at 2020-01-03 13:50:00 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container horizon ready: true, restart count 0
Jan  3 15:15:01.164: INFO: tiller-deploy-d6b59fcb-zpz47 from kube-system started at 2020-01-03 13:50:02 +0000 UTC (1 container statuses recorded)
Jan  3 15:15:01.164: INFO: 	Container tiller ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d5649bec-9cbb-490e-82b2-2f94867f5516 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-d5649bec-9cbb-490e-82b2-2f94867f5516 off the node controller-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d5649bec-9cbb-490e-82b2-2f94867f5516
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:20:17.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2563" for this suite.
Jan  3 15:20:25.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:20:25.283: INFO: namespace sched-pred-2563 deletion completed in 8.043851593s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:324.208 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:20:25.284: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Jan  3 15:20:25.296: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 15:21:25.366: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jan  3 15:21:25.367: INFO: Starting informer...
STEP: Starting pod...
Jan  3 15:21:25.573: INFO: Pod is running on controller-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jan  3 15:21:25.580: INFO: Pod wasn't evicted. Proceeding
Jan  3 15:21:25.580: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jan  3 15:22:40.650: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:22:40.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1740" for this suite.
Jan  3 15:23:08.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:23:09.371: INFO: namespace taint-single-pod-1740 deletion completed in 28.681561569s

• [SLOW TEST:164.087 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:23:09.374: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:23:29.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5372" for this suite.
Jan  3 15:23:35.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:23:35.725: INFO: namespace kubelet-test-5372 deletion completed in 6.040458861s

• [SLOW TEST:26.352 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:23:35.726: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 15:23:35.745: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e155015-ded6-406b-ad59-80ab314ca0dc" in namespace "downward-api-2685" to be "success or failure"
Jan  3 15:23:35.750: INFO: Pod "downwardapi-volume-0e155015-ded6-406b-ad59-80ab314ca0dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.962822ms
Jan  3 15:23:37.752: INFO: Pod "downwardapi-volume-0e155015-ded6-406b-ad59-80ab314ca0dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006158942s
Jan  3 15:23:39.753: INFO: Pod "downwardapi-volume-0e155015-ded6-406b-ad59-80ab314ca0dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007932751s
Jan  3 15:23:41.796: INFO: Pod "downwardapi-volume-0e155015-ded6-406b-ad59-80ab314ca0dc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050314625s
Jan  3 15:23:43.807: INFO: Pod "downwardapi-volume-0e155015-ded6-406b-ad59-80ab314ca0dc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.061535534s
Jan  3 15:23:45.815: INFO: Pod "downwardapi-volume-0e155015-ded6-406b-ad59-80ab314ca0dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.069338332s
STEP: Saw pod success
Jan  3 15:23:45.815: INFO: Pod "downwardapi-volume-0e155015-ded6-406b-ad59-80ab314ca0dc" satisfied condition "success or failure"
Jan  3 15:23:45.817: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-0e155015-ded6-406b-ad59-80ab314ca0dc container client-container: <nil>
STEP: delete the pod
Jan  3 15:23:45.996: INFO: Waiting for pod downwardapi-volume-0e155015-ded6-406b-ad59-80ab314ca0dc to disappear
Jan  3 15:23:45.998: INFO: Pod downwardapi-volume-0e155015-ded6-406b-ad59-80ab314ca0dc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:23:45.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2685" for this suite.
Jan  3 15:23:52.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:23:52.169: INFO: namespace downward-api-2685 deletion completed in 6.167337151s

• [SLOW TEST:16.443 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:23:52.171: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-4chx
STEP: Creating a pod to test atomic-volume-subpath
Jan  3 15:23:52.204: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-4chx" in namespace "subpath-3970" to be "success or failure"
Jan  3 15:23:52.213: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Pending", Reason="", readiness=false. Elapsed: 9.10382ms
Jan  3 15:23:54.214: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010439125s
Jan  3 15:23:56.217: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012661646s
Jan  3 15:23:58.218: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014330857s
Jan  3 15:24:00.220: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 8.016100559s
Jan  3 15:24:02.225: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 10.021125157s
Jan  3 15:24:04.227: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 12.022604013s
Jan  3 15:24:06.228: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 14.024369611s
Jan  3 15:24:08.230: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 16.026109572s
Jan  3 15:24:10.232: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 18.027605025s
Jan  3 15:24:12.233: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 20.029117956s
Jan  3 15:24:14.235: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 22.030666037s
Jan  3 15:24:16.237: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 24.032700017s
Jan  3 15:24:18.238: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 26.034355063s
Jan  3 15:24:20.261: INFO: Pod "pod-subpath-test-downwardapi-4chx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.056683759s
STEP: Saw pod success
Jan  3 15:24:20.261: INFO: Pod "pod-subpath-test-downwardapi-4chx" satisfied condition "success or failure"
Jan  3 15:24:20.262: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-downwardapi-4chx container test-container-subpath-downwardapi-4chx: <nil>
STEP: delete the pod
Jan  3 15:24:20.276: INFO: Waiting for pod pod-subpath-test-downwardapi-4chx to disappear
Jan  3 15:24:20.278: INFO: Pod pod-subpath-test-downwardapi-4chx no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-4chx
Jan  3 15:24:20.278: INFO: Deleting pod "pod-subpath-test-downwardapi-4chx" in namespace "subpath-3970"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:24:20.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3970" for this suite.
Jan  3 15:24:26.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:24:26.350: INFO: namespace subpath-3970 deletion completed in 6.068545007s

• [SLOW TEST:34.179 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:24:26.350: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:24:37.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2423" for this suite.
Jan  3 15:24:43.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:24:43.468: INFO: namespace resourcequota-2423 deletion completed in 6.063647253s

• [SLOW TEST:17.118 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:24:43.468: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jan  3 15:24:43.503: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a4fcfb7-bf04-473a-af62-f175461789f9" in namespace "projected-4417" to be "success or failure"
Jan  3 15:24:43.506: INFO: Pod "downwardapi-volume-5a4fcfb7-bf04-473a-af62-f175461789f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.516111ms
Jan  3 15:24:45.508: INFO: Pod "downwardapi-volume-5a4fcfb7-bf04-473a-af62-f175461789f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004625807s
Jan  3 15:24:47.510: INFO: Pod "downwardapi-volume-5a4fcfb7-bf04-473a-af62-f175461789f9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006535662s
Jan  3 15:24:49.532: INFO: Pod "downwardapi-volume-5a4fcfb7-bf04-473a-af62-f175461789f9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028505734s
Jan  3 15:24:51.540: INFO: Pod "downwardapi-volume-5a4fcfb7-bf04-473a-af62-f175461789f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.036621569s
STEP: Saw pod success
Jan  3 15:24:51.540: INFO: Pod "downwardapi-volume-5a4fcfb7-bf04-473a-af62-f175461789f9" satisfied condition "success or failure"
Jan  3 15:24:51.541: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-5a4fcfb7-bf04-473a-af62-f175461789f9 container client-container: <nil>
STEP: delete the pod
Jan  3 15:24:51.562: INFO: Waiting for pod downwardapi-volume-5a4fcfb7-bf04-473a-af62-f175461789f9 to disappear
Jan  3 15:24:51.577: INFO: Pod downwardapi-volume-5a4fcfb7-bf04-473a-af62-f175461789f9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:24:51.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4417" for this suite.
Jan  3 15:24:57.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:24:57.635: INFO: namespace projected-4417 deletion completed in 6.055403223s

• [SLOW TEST:14.167 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:24:57.636: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan  3 15:24:58.025: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 15:25:00.035: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661898, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661898, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661898, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661898, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 15:25:02.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661898, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661898, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661898, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661898, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 15:25:04.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661898, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661898, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661898, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713661898, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan  3 15:25:07.041: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:25:07.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1311" for this suite.
Jan  3 15:25:13.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:25:13.411: INFO: namespace webhook-1311 deletion completed in 6.181510752s
STEP: Destroying namespace "webhook-1311-markers" for this suite.
Jan  3 15:25:19.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:25:19.491: INFO: namespace webhook-1311-markers deletion completed in 6.080261038s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.860 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan  3 15:25:19.496: INFO: >>> kubeConfig: /tmp/kubeconfig-937082782
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-a9ab4db0-92b8-456b-9ed6-5b755fdd02d8
STEP: Creating a pod to test consume configMaps
Jan  3 15:25:19.524: INFO: Waiting up to 5m0s for pod "pod-configmaps-737bf4f5-ad7b-4d1c-acd6-3cc78050fc6e" in namespace "configmap-9006" to be "success or failure"
Jan  3 15:25:19.525: INFO: Pod "pod-configmaps-737bf4f5-ad7b-4d1c-acd6-3cc78050fc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.018556ms
Jan  3 15:25:21.530: INFO: Pod "pod-configmaps-737bf4f5-ad7b-4d1c-acd6-3cc78050fc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006068005s
Jan  3 15:25:23.532: INFO: Pod "pod-configmaps-737bf4f5-ad7b-4d1c-acd6-3cc78050fc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008351892s
Jan  3 15:25:25.538: INFO: Pod "pod-configmaps-737bf4f5-ad7b-4d1c-acd6-3cc78050fc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014581781s
Jan  3 15:25:27.544: INFO: Pod "pod-configmaps-737bf4f5-ad7b-4d1c-acd6-3cc78050fc6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020389003s
STEP: Saw pod success
Jan  3 15:25:27.544: INFO: Pod "pod-configmaps-737bf4f5-ad7b-4d1c-acd6-3cc78050fc6e" satisfied condition "success or failure"
Jan  3 15:25:27.545: INFO: Trying to get logs from node controller-0 pod pod-configmaps-737bf4f5-ad7b-4d1c-acd6-3cc78050fc6e container configmap-volume-test: <nil>
STEP: delete the pod
Jan  3 15:25:27.567: INFO: Waiting for pod pod-configmaps-737bf4f5-ad7b-4d1c-acd6-3cc78050fc6e to disappear
Jan  3 15:25:27.568: INFO: Pod pod-configmaps-737bf4f5-ad7b-4d1c-acd6-3cc78050fc6e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan  3 15:25:27.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9006" for this suite.
Jan  3 15:25:33.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan  3 15:25:33.625: INFO: namespace configmap-9006 deletion completed in 6.053355037s

• [SLOW TEST:14.129 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSJan  3 15:25:33.626: INFO: Running AfterSuite actions on all nodes
Jan  3 15:25:33.626: INFO: Running AfterSuite actions on node 1
Jan  3 15:25:33.626: INFO: Skipping dumping logs from cluster

Ran 276 of 4897 Specs in 8814.900 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4621 Skipped
PASS

Ginkgo ran 1 suite in 2h26m56.504741118s
Test Suite Passed
