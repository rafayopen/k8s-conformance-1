I1218 18:18:02.212762      27 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-207688788
I1218 18:18:02.212912      27 e2e.go:92] Starting e2e run "0c59fe68-2173-4430-b06c-f27acbe8d08d" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1576693081 - Will randomize all specs
Will run 276 of 4897 specs

Dec 18 18:18:02.225: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:18:02.227: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 18 18:18:02.239: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 18 18:18:02.266: INFO: The status of Pod ceph-pools-audit-1576692300-fprj7 is Succeeded, skipping waiting
Dec 18 18:18:02.266: INFO: The status of Pod ceph-pools-audit-1576692600-hv868 is Succeeded, skipping waiting
Dec 18 18:18:02.266: INFO: The status of Pod ceph-pools-audit-1576692900-lnvgw is Succeeded, skipping waiting
Dec 18 18:18:02.266: INFO: The status of Pod storage-init-rbd-provisioner-vnvt6 is Succeeded, skipping waiting
Dec 18 18:18:02.266: INFO: 28 / 32 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 18 18:18:02.266: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Dec 18 18:18:02.266: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 18 18:18:02.272: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec 18 18:18:02.272: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds-amd64' (0 seconds elapsed)
Dec 18 18:18:02.272: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 18 18:18:02.272: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-sriov-cni-ds-amd64' (0 seconds elapsed)
Dec 18 18:18:02.272: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-sriov-device-plugin-amd64' (0 seconds elapsed)
Dec 18 18:18:02.272: INFO: e2e test version: v1.16.2
Dec 18 18:18:02.272: INFO: kube-apiserver version: v1.16.2
Dec 18 18:18:02.272: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:18:02.275: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:18:02.275: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
Dec 18 18:18:02.290: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 18 18:18:02.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-5575'
Dec 18 18:18:02.447: INFO: stderr: ""
Dec 18 18:18:02.447: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 18 18:18:12.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pod e2e-test-httpd-pod --namespace=kubectl-5575 -o json'
Dec 18 18:18:12.561: INFO: stderr: ""
Dec 18 18:18:12.561: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.16.192.114/32\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"chain\\\",\\n    \\\"ips\\\": [\\n        \\\"172.16.192.114\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2019-12-18T18:18:02Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5575\",\n        \"resourceVersion\": \"59668\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5575/pods/e2e-test-httpd-pod\",\n        \"uid\": \"31521725-727c-4dd2-8113-8716ff81c41e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-dkczs\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"controller-0\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 30\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 30\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-dkczs\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-dkczs\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-18T18:18:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-18T18:18:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-18T18:18:09Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-18T18:18:02Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://c98c85f536cc2c80eaf9d13f7813f91e8e8595af540e873762eb5547dbbe9e4b\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-18T18:18:09Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.204.2\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.192.114\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.192.114\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-18T18:18:02Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 18 18:18:12.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 replace -f - --namespace=kubectl-5575'
Dec 18 18:18:12.781: INFO: stderr: ""
Dec 18 18:18:12.781: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Dec 18 18:18:12.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete pods e2e-test-httpd-pod --namespace=kubectl-5575'
Dec 18 18:18:23.309: INFO: stderr: ""
Dec 18 18:18:23.309: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:18:23.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5575" for this suite.
Dec 18 18:18:29.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:18:29.346: INFO: namespace kubectl-5575 deletion completed in 6.034615578s

• [SLOW TEST:27.071 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:18:29.346: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:18:40.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9923" for this suite.
Dec 18 18:18:46.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:18:46.420: INFO: namespace resourcequota-9923 deletion completed in 6.03475769s

• [SLOW TEST:17.074 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:18:46.420: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:18:46.436: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-966f67af-9ef3-4306-a4bd-5c2c7b92fd4b" in namespace "security-context-test-6446" to be "success or failure"
Dec 18 18:18:46.438: INFO: Pod "busybox-privileged-false-966f67af-9ef3-4306-a4bd-5c2c7b92fd4b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.279819ms
Dec 18 18:18:48.439: INFO: Pod "busybox-privileged-false-966f67af-9ef3-4306-a4bd-5c2c7b92fd4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00316421s
Dec 18 18:18:50.441: INFO: Pod "busybox-privileged-false-966f67af-9ef3-4306-a4bd-5c2c7b92fd4b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004957423s
Dec 18 18:18:52.443: INFO: Pod "busybox-privileged-false-966f67af-9ef3-4306-a4bd-5c2c7b92fd4b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006514648s
Dec 18 18:18:54.445: INFO: Pod "busybox-privileged-false-966f67af-9ef3-4306-a4bd-5c2c7b92fd4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008427476s
Dec 18 18:18:54.445: INFO: Pod "busybox-privileged-false-966f67af-9ef3-4306-a4bd-5c2c7b92fd4b" satisfied condition "success or failure"
Dec 18 18:18:54.456: INFO: Got logs for pod "busybox-privileged-false-966f67af-9ef3-4306-a4bd-5c2c7b92fd4b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:18:54.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6446" for this suite.
Dec 18 18:19:00.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:19:00.493: INFO: namespace security-context-test-6446 deletion completed in 6.034541723s

• [SLOW TEST:14.073 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:19:00.493: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 18 18:19:09.518: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:19:10.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-940" for this suite.
Dec 18 18:19:38.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:19:38.560: INFO: namespace replicaset-940 deletion completed in 28.03400183s

• [SLOW TEST:38.067 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:19:38.560: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:19:38.575: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 18 18:19:43.577: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 18 18:19:47.580: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 18 18:19:49.582: INFO: Creating deployment "test-rollover-deployment"
Dec 18 18:19:49.584: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 18 18:19:51.587: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 18 18:19:51.591: INFO: Ensure that both replica sets have 1 created replica
Dec 18 18:19:51.593: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 18 18:19:51.596: INFO: Updating deployment test-rollover-deployment
Dec 18 18:19:51.596: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 18 18:19:53.598: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 18 18:19:53.601: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 18 18:19:53.603: INFO: all replica sets need to contain the pod-template-hash label
Dec 18 18:19:53.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289991, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:19:55.607: INFO: all replica sets need to contain the pod-template-hash label
Dec 18 18:19:55.607: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289991, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:19:57.607: INFO: all replica sets need to contain the pod-template-hash label
Dec 18 18:19:57.607: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289991, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:19:59.607: INFO: all replica sets need to contain the pod-template-hash label
Dec 18 18:19:59.607: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289998, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:20:01.607: INFO: all replica sets need to contain the pod-template-hash label
Dec 18 18:20:01.607: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289998, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:20:03.608: INFO: all replica sets need to contain the pod-template-hash label
Dec 18 18:20:03.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289998, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:20:05.607: INFO: all replica sets need to contain the pod-template-hash label
Dec 18 18:20:05.607: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289998, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:20:07.607: INFO: all replica sets need to contain the pod-template-hash label
Dec 18 18:20:07.607: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289998, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712289989, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:20:09.607: INFO: 
Dec 18 18:20:09.607: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 18 18:20:09.610: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-653 /apis/apps/v1/namespaces/deployment-653/deployments/test-rollover-deployment 1f81df8d-594c-4ca1-ba87-006188a33ca7 60254 2 2019-12-18 18:19:49 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0034b35b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-18 18:19:49 +0000 UTC,LastTransitionTime:2019-12-18 18:19:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2019-12-18 18:20:08 +0000 UTC,LastTransitionTime:2019-12-18 18:19:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 18 18:20:09.612: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-653 /apis/apps/v1/namespaces/deployment-653/replicasets/test-rollover-deployment-7d7dc6548c 01e8abe9-4326-4507-b3ba-7c029929d8e2 60243 2 2019-12-18 18:19:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1f81df8d-594c-4ca1-ba87-006188a33ca7 0xc0034b3c77 0xc0034b3c78}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0034b3cd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 18 18:20:09.612: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 18 18:20:09.612: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-653 /apis/apps/v1/namespaces/deployment-653/replicasets/test-rollover-controller ad46fbaf-cc54-4c74-937f-63351a0edf25 60252 2 2019-12-18 18:19:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1f81df8d-594c-4ca1-ba87-006188a33ca7 0xc0034b3ba7 0xc0034b3ba8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0034b3c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 18 18:20:09.612: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-653 /apis/apps/v1/namespaces/deployment-653/replicasets/test-rollover-deployment-f6c94f66c 48dfad47-885d-4be6-bd76-34ea9e7487c7 60161 2 2019-12-18 18:19:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1f81df8d-594c-4ca1-ba87-006188a33ca7 0xc0034b3d40 0xc0034b3d41}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0034b3db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 18 18:20:09.613: INFO: Pod "test-rollover-deployment-7d7dc6548c-6ckc7" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-6ckc7 test-rollover-deployment-7d7dc6548c- deployment-653 /api/v1/namespaces/deployment-653/pods/test-rollover-deployment-7d7dc6548c-6ckc7 ade92b8e-0873-48a7-a440-1ce656751e62 60201 0 2019-12-18 18:19:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:172.16.192.116/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.116"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 01e8abe9-4326-4507-b3ba-7c029929d8e2 0xc002fb7e57 0xc002fb7e58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvvzj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvvzj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvvzj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 18:19:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 18:19:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 18:19:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 18:19:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:172.16.192.116,StartTime:2019-12-18 18:19:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-18 18:19:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://6d552318dadc1629cb9e7a2f7fcd6be854a23d4b4071c25bfc01c249066e0b41,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:20:09.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-653" for this suite.
Dec 18 18:20:15.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:20:15.649: INFO: namespace deployment-653 deletion completed in 6.033890698s

• [SLOW TEST:37.089 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:20:15.649: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 18:20:16.407: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 18 18:20:18.412: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290016, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290016, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290016, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290016, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:20:20.413: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290016, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290016, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290016, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290016, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:20:22.413: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290016, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290016, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290016, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290016, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 18:20:25.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:20:35.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2323" for this suite.
Dec 18 18:20:41.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:20:41.503: INFO: namespace webhook-2323 deletion completed in 6.033934706s
STEP: Destroying namespace "webhook-2323-markers" for this suite.
Dec 18 18:20:47.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:20:47.537: INFO: namespace webhook-2323-markers deletion completed in 6.034099337s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:31.892 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:20:47.542: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-c622682e-ec72-4fef-a42d-6bcc1b7d0ce4
Dec 18 18:20:47.557: INFO: Pod name my-hostname-basic-c622682e-ec72-4fef-a42d-6bcc1b7d0ce4: Found 0 pods out of 1
Dec 18 18:20:52.560: INFO: Pod name my-hostname-basic-c622682e-ec72-4fef-a42d-6bcc1b7d0ce4: Found 1 pods out of 1
Dec 18 18:20:52.560: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c622682e-ec72-4fef-a42d-6bcc1b7d0ce4" are running
Dec 18 18:20:54.562: INFO: Pod "my-hostname-basic-c622682e-ec72-4fef-a42d-6bcc1b7d0ce4-c5mz7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-18 18:20:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-18 18:20:47 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-c622682e-ec72-4fef-a42d-6bcc1b7d0ce4]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-18 18:20:47 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-c622682e-ec72-4fef-a42d-6bcc1b7d0ce4]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-18 18:20:47 +0000 UTC Reason: Message:}])
Dec 18 18:20:54.562: INFO: Trying to dial the pod
Dec 18 18:20:59.567: INFO: Controller my-hostname-basic-c622682e-ec72-4fef-a42d-6bcc1b7d0ce4: Got expected result from replica 1 [my-hostname-basic-c622682e-ec72-4fef-a42d-6bcc1b7d0ce4-c5mz7]: "my-hostname-basic-c622682e-ec72-4fef-a42d-6bcc1b7d0ce4-c5mz7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:20:59.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1368" for this suite.
Dec 18 18:21:05.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:21:05.604: INFO: namespace replication-controller-1368 deletion completed in 6.035284925s

• [SLOW TEST:18.063 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:21:05.605: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:21:21.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2194" for this suite.
Dec 18 18:21:27.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:21:27.690: INFO: namespace resourcequota-2194 deletion completed in 6.035003293s

• [SLOW TEST:22.085 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:21:27.690: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:21:27.704: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 18 18:21:30.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-3354 create -f -'
Dec 18 18:21:31.028: INFO: stderr: ""
Dec 18 18:21:31.028: INFO: stdout: "e2e-test-crd-publish-openapi-7758-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 18 18:21:31.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-3354 delete e2e-test-crd-publish-openapi-7758-crds test-cr'
Dec 18 18:21:31.094: INFO: stderr: ""
Dec 18 18:21:31.094: INFO: stdout: "e2e-test-crd-publish-openapi-7758-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 18 18:21:31.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-3354 apply -f -'
Dec 18 18:21:31.240: INFO: stderr: ""
Dec 18 18:21:31.240: INFO: stdout: "e2e-test-crd-publish-openapi-7758-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 18 18:21:31.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-3354 delete e2e-test-crd-publish-openapi-7758-crds test-cr'
Dec 18 18:21:31.305: INFO: stderr: ""
Dec 18 18:21:31.305: INFO: stdout: "e2e-test-crd-publish-openapi-7758-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 18 18:21:31.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 explain e2e-test-crd-publish-openapi-7758-crds'
Dec 18 18:21:31.443: INFO: stderr: ""
Dec 18 18:21:31.443: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7758-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:21:34.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3354" for this suite.
Dec 18 18:21:40.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:21:41.008: INFO: namespace crd-publish-openapi-3354 deletion completed in 6.034285779s

• [SLOW TEST:13.319 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:21:41.009: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Dec 18 18:21:41.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-492'
Dec 18 18:21:41.218: INFO: stderr: ""
Dec 18 18:21:41.218: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 18 18:21:41.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-492'
Dec 18 18:21:41.282: INFO: stderr: ""
Dec 18 18:21:41.282: INFO: stdout: "update-demo-nautilus-7h6l6 update-demo-nautilus-pctn5 "
Dec 18 18:21:41.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-7h6l6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-492'
Dec 18 18:21:41.341: INFO: stderr: ""
Dec 18 18:21:41.341: INFO: stdout: ""
Dec 18 18:21:41.341: INFO: update-demo-nautilus-7h6l6 is created but not running
Dec 18 18:21:46.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-492'
Dec 18 18:21:46.407: INFO: stderr: ""
Dec 18 18:21:46.407: INFO: stdout: "update-demo-nautilus-7h6l6 update-demo-nautilus-pctn5 "
Dec 18 18:21:46.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-7h6l6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-492'
Dec 18 18:21:46.467: INFO: stderr: ""
Dec 18 18:21:46.467: INFO: stdout: ""
Dec 18 18:21:46.467: INFO: update-demo-nautilus-7h6l6 is created but not running
Dec 18 18:21:51.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-492'
Dec 18 18:21:51.531: INFO: stderr: ""
Dec 18 18:21:51.531: INFO: stdout: "update-demo-nautilus-7h6l6 update-demo-nautilus-pctn5 "
Dec 18 18:21:51.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-7h6l6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-492'
Dec 18 18:21:51.593: INFO: stderr: ""
Dec 18 18:21:51.593: INFO: stdout: "true"
Dec 18 18:21:51.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-7h6l6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-492'
Dec 18 18:21:51.656: INFO: stderr: ""
Dec 18 18:21:51.656: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 18 18:21:51.656: INFO: validating pod update-demo-nautilus-7h6l6
Dec 18 18:21:51.658: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 18 18:21:51.658: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 18 18:21:51.658: INFO: update-demo-nautilus-7h6l6 is verified up and running
Dec 18 18:21:51.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-pctn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-492'
Dec 18 18:21:51.719: INFO: stderr: ""
Dec 18 18:21:51.719: INFO: stdout: "true"
Dec 18 18:21:51.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-pctn5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-492'
Dec 18 18:21:51.778: INFO: stderr: ""
Dec 18 18:21:51.778: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 18 18:21:51.778: INFO: validating pod update-demo-nautilus-pctn5
Dec 18 18:21:51.780: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 18 18:21:51.780: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 18 18:21:51.780: INFO: update-demo-nautilus-pctn5 is verified up and running
STEP: rolling-update to new replication controller
Dec 18 18:21:51.781: INFO: scanned /root for discovery docs: <nil>
Dec 18 18:21:51.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-492'
Dec 18 18:22:22.041: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 18 18:22:22.041: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 18 18:22:22.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-492'
Dec 18 18:22:22.109: INFO: stderr: ""
Dec 18 18:22:22.109: INFO: stdout: "update-demo-kitten-m87nq update-demo-kitten-rpnfn "
Dec 18 18:22:22.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-kitten-m87nq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-492'
Dec 18 18:22:22.172: INFO: stderr: ""
Dec 18 18:22:22.172: INFO: stdout: "true"
Dec 18 18:22:22.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-kitten-m87nq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-492'
Dec 18 18:22:22.235: INFO: stderr: ""
Dec 18 18:22:22.235: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 18 18:22:22.235: INFO: validating pod update-demo-kitten-m87nq
Dec 18 18:22:22.237: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 18 18:22:22.237: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 18 18:22:22.237: INFO: update-demo-kitten-m87nq is verified up and running
Dec 18 18:22:22.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-kitten-rpnfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-492'
Dec 18 18:22:22.298: INFO: stderr: ""
Dec 18 18:22:22.298: INFO: stdout: "true"
Dec 18 18:22:22.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-kitten-rpnfn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-492'
Dec 18 18:22:22.359: INFO: stderr: ""
Dec 18 18:22:22.359: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 18 18:22:22.359: INFO: validating pod update-demo-kitten-rpnfn
Dec 18 18:22:22.360: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 18 18:22:22.360: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 18 18:22:22.361: INFO: update-demo-kitten-rpnfn is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:22:22.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-492" for this suite.
Dec 18 18:22:34.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:22:34.397: INFO: namespace kubectl-492 deletion completed in 12.034533181s

• [SLOW TEST:53.388 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:22:34.397: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:22:34.420: INFO: Create a RollingUpdate DaemonSet
Dec 18 18:22:34.422: INFO: Check that daemon pods launch on every node of the cluster
Dec 18 18:22:34.426: INFO: Number of nodes with available pods: 0
Dec 18 18:22:34.426: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:22:35.430: INFO: Number of nodes with available pods: 0
Dec 18 18:22:35.430: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:22:36.430: INFO: Number of nodes with available pods: 0
Dec 18 18:22:36.430: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:22:37.430: INFO: Number of nodes with available pods: 0
Dec 18 18:22:37.430: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:22:38.429: INFO: Number of nodes with available pods: 0
Dec 18 18:22:38.429: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:22:39.429: INFO: Number of nodes with available pods: 0
Dec 18 18:22:39.430: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:22:40.430: INFO: Number of nodes with available pods: 0
Dec 18 18:22:40.430: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:22:41.429: INFO: Number of nodes with available pods: 2
Dec 18 18:22:41.429: INFO: Node controller-0 is running more than one daemon pod
Dec 18 18:22:42.429: INFO: Number of nodes with available pods: 4
Dec 18 18:22:42.429: INFO: Number of running nodes: 4, number of available pods: 4
Dec 18 18:22:42.429: INFO: Update the DaemonSet to trigger a rollout
Dec 18 18:22:42.432: INFO: Updating DaemonSet daemon-set
Dec 18 18:22:46.440: INFO: Roll back the DaemonSet before rollout is complete
Dec 18 18:22:46.443: INFO: Updating DaemonSet daemon-set
Dec 18 18:22:46.443: INFO: Make sure DaemonSet rollback is complete
Dec 18 18:22:46.448: INFO: Wrong image for pod: daemon-set-s27f4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 18 18:22:46.448: INFO: Pod daemon-set-s27f4 is not available
Dec 18 18:22:47.456: INFO: Wrong image for pod: daemon-set-s27f4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 18 18:22:47.456: INFO: Pod daemon-set-s27f4 is not available
Dec 18 18:22:48.456: INFO: Pod daemon-set-j568x is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8310, will wait for the garbage collector to delete the pods
Dec 18 18:22:48.514: INFO: Deleting DaemonSet.extensions daemon-set took: 2.761616ms
Dec 18 18:22:48.914: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.134599ms
Dec 18 18:22:54.015: INFO: Number of nodes with available pods: 0
Dec 18 18:22:54.015: INFO: Number of running nodes: 0, number of available pods: 0
Dec 18 18:22:54.018: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8310/daemonsets","resourceVersion":"61297"},"items":null}

Dec 18 18:22:54.019: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8310/pods","resourceVersion":"61297"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:22:54.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8310" for this suite.
Dec 18 18:23:00.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:23:00.061: INFO: namespace daemonsets-8310 deletion completed in 6.034712426s

• [SLOW TEST:25.664 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:23:00.061: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:23:08.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1235" for this suite.
Dec 18 18:23:14.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:23:14.119: INFO: namespace kubelet-test-1235 deletion completed in 6.035924996s

• [SLOW TEST:14.058 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:23:14.119: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 18:23:14.626: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 18 18:23:16.631: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290194, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290194, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290194, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290194, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:23:18.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290194, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290194, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290194, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290194, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:23:20.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290194, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290194, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290194, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290194, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 18:23:23.640: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:23:23.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3594" for this suite.
Dec 18 18:23:29.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:23:29.701: INFO: namespace webhook-3594 deletion completed in 6.035258073s
STEP: Destroying namespace "webhook-3594-markers" for this suite.
Dec 18 18:23:35.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:23:35.736: INFO: namespace webhook-3594-markers deletion completed in 6.035239348s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.621 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:23:35.741: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Dec 18 18:23:36.261: INFO: created pod pod-service-account-defaultsa
Dec 18 18:23:36.261: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 18 18:23:36.262: INFO: created pod pod-service-account-mountsa
Dec 18 18:23:36.262: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 18 18:23:36.264: INFO: created pod pod-service-account-nomountsa
Dec 18 18:23:36.264: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 18 18:23:36.267: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 18 18:23:36.267: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 18 18:23:36.269: INFO: created pod pod-service-account-mountsa-mountspec
Dec 18 18:23:36.269: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 18 18:23:36.271: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 18 18:23:36.271: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 18 18:23:36.274: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 18 18:23:36.274: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 18 18:23:36.275: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 18 18:23:36.275: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 18 18:23:36.277: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 18 18:23:36.277: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:23:36.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9795" for this suite.
Dec 18 18:23:48.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:23:48.313: INFO: namespace svcaccounts-9795 deletion completed in 12.034191079s

• [SLOW TEST:12.573 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:23:48.314: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:24:18.334: INFO: Container started at 2019-12-18 18:23:54 +0000 UTC, pod became ready at 2019-12-18 18:24:16 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:24:18.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5477" for this suite.
Dec 18 18:24:46.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:24:46.372: INFO: namespace container-probe-5477 deletion completed in 28.035775319s

• [SLOW TEST:58.058 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:24:46.372: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9274
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 18 18:24:46.385: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 18 18:25:20.421: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.166.159 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9274 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 18:25:20.421: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:25:21.550: INFO: Found all expected endpoints: [netserver-0]
Dec 18 18:25:21.552: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.192.65 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9274 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 18:25:21.552: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:25:22.700: INFO: Found all expected endpoints: [netserver-1]
Dec 18 18:25:22.701: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.103.154 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9274 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 18:25:22.701: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:25:23.858: INFO: Found all expected endpoints: [netserver-2]
Dec 18 18:25:23.859: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.154.39 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9274 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 18:25:23.859: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:25:25.011: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:25:25.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9274" for this suite.
Dec 18 18:25:37.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:25:37.051: INFO: namespace pod-network-test-9274 deletion completed in 12.038012271s

• [SLOW TEST:50.680 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:25:37.052: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:25:37.066: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 18 18:25:40.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-1021 create -f -'
Dec 18 18:25:40.893: INFO: stderr: ""
Dec 18 18:25:40.893: INFO: stdout: "e2e-test-crd-publish-openapi-9597-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 18 18:25:40.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-1021 delete e2e-test-crd-publish-openapi-9597-crds test-cr'
Dec 18 18:25:40.959: INFO: stderr: ""
Dec 18 18:25:40.959: INFO: stdout: "e2e-test-crd-publish-openapi-9597-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 18 18:25:40.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-1021 apply -f -'
Dec 18 18:25:41.105: INFO: stderr: ""
Dec 18 18:25:41.105: INFO: stdout: "e2e-test-crd-publish-openapi-9597-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 18 18:25:41.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-1021 delete e2e-test-crd-publish-openapi-9597-crds test-cr'
Dec 18 18:25:41.170: INFO: stderr: ""
Dec 18 18:25:41.170: INFO: stdout: "e2e-test-crd-publish-openapi-9597-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 18 18:25:41.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 explain e2e-test-crd-publish-openapi-9597-crds'
Dec 18 18:25:41.313: INFO: stderr: ""
Dec 18 18:25:41.313: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9597-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:25:44.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1021" for this suite.
Dec 18 18:25:50.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:25:50.873: INFO: namespace crd-publish-openapi-1021 deletion completed in 6.036642027s

• [SLOW TEST:13.821 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:25:50.873: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:25:55.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1648" for this suite.
Dec 18 18:26:01.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:26:01.957: INFO: namespace watch-1648 deletion completed in 6.130651948s

• [SLOW TEST:11.084 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:26:01.957: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-23f646b9-71a3-473b-b37c-bc0e2d68e896
STEP: Creating a pod to test consume secrets
Dec 18 18:26:01.974: INFO: Waiting up to 5m0s for pod "pod-secrets-baa6ccb5-7d94-453b-8a12-16a660c9af19" in namespace "secrets-583" to be "success or failure"
Dec 18 18:26:01.976: INFO: Pod "pod-secrets-baa6ccb5-7d94-453b-8a12-16a660c9af19": Phase="Pending", Reason="", readiness=false. Elapsed: 1.567105ms
Dec 18 18:26:03.977: INFO: Pod "pod-secrets-baa6ccb5-7d94-453b-8a12-16a660c9af19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003004528s
Dec 18 18:26:05.979: INFO: Pod "pod-secrets-baa6ccb5-7d94-453b-8a12-16a660c9af19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004711519s
Dec 18 18:26:07.981: INFO: Pod "pod-secrets-baa6ccb5-7d94-453b-8a12-16a660c9af19": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006390114s
Dec 18 18:26:09.982: INFO: Pod "pod-secrets-baa6ccb5-7d94-453b-8a12-16a660c9af19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007988491s
STEP: Saw pod success
Dec 18 18:26:09.982: INFO: Pod "pod-secrets-baa6ccb5-7d94-453b-8a12-16a660c9af19" satisfied condition "success or failure"
Dec 18 18:26:09.983: INFO: Trying to get logs from node controller-1 pod pod-secrets-baa6ccb5-7d94-453b-8a12-16a660c9af19 container secret-volume-test: <nil>
STEP: delete the pod
Dec 18 18:26:09.999: INFO: Waiting for pod pod-secrets-baa6ccb5-7d94-453b-8a12-16a660c9af19 to disappear
Dec 18 18:26:10.000: INFO: Pod pod-secrets-baa6ccb5-7d94-453b-8a12-16a660c9af19 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:26:10.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-583" for this suite.
Dec 18 18:26:16.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:26:16.038: INFO: namespace secrets-583 deletion completed in 6.035839631s

• [SLOW TEST:14.080 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:26:16.038: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-29687c36-8bd3-4873-b61d-fddb9fcc51a0 in namespace container-probe-6568
Dec 18 18:26:24.056: INFO: Started pod test-webserver-29687c36-8bd3-4873-b61d-fddb9fcc51a0 in namespace container-probe-6568
STEP: checking the pod's current state and verifying that restartCount is present
Dec 18 18:26:24.057: INFO: Initial restart count of pod test-webserver-29687c36-8bd3-4873-b61d-fddb9fcc51a0 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:30:24.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6568" for this suite.
Dec 18 18:30:30.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:30:30.319: INFO: namespace container-probe-6568 deletion completed in 6.034478488s

• [SLOW TEST:254.281 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:30:30.319: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9504.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9504.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9504.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9504.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9504.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9504.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 18 18:30:38.351: INFO: DNS probes using dns-9504/dns-test-acdabfea-b7ab-4f92-a797-e6c8e9d368ec succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:30:38.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9504" for this suite.
Dec 18 18:30:44.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:30:44.391: INFO: namespace dns-9504 deletion completed in 6.034119824s

• [SLOW TEST:14.072 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:30:44.391: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:30:44.412: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 18 18:30:44.416: INFO: Number of nodes with available pods: 0
Dec 18 18:30:44.416: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:30:45.420: INFO: Number of nodes with available pods: 0
Dec 18 18:30:45.420: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:30:46.420: INFO: Number of nodes with available pods: 0
Dec 18 18:30:46.420: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:30:47.420: INFO: Number of nodes with available pods: 0
Dec 18 18:30:47.420: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:30:48.420: INFO: Number of nodes with available pods: 0
Dec 18 18:30:48.420: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:30:49.427: INFO: Number of nodes with available pods: 0
Dec 18 18:30:49.427: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:30:50.420: INFO: Number of nodes with available pods: 0
Dec 18 18:30:50.420: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:30:51.420: INFO: Number of nodes with available pods: 3
Dec 18 18:30:51.420: INFO: Node compute-1 is running more than one daemon pod
Dec 18 18:30:52.420: INFO: Number of nodes with available pods: 4
Dec 18 18:30:52.420: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 18 18:30:52.429: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:52.430: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:52.430: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:52.430: INFO: Wrong image for pod: daemon-set-nv99q. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:53.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:53.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:53.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:53.434: INFO: Wrong image for pod: daemon-set-nv99q. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:54.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:54.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:54.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:54.434: INFO: Wrong image for pod: daemon-set-nv99q. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:54.434: INFO: Pod daemon-set-nv99q is not available
Dec 18 18:30:55.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:55.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:55.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:55.434: INFO: Wrong image for pod: daemon-set-nv99q. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:55.434: INFO: Pod daemon-set-nv99q is not available
Dec 18 18:30:56.435: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:56.435: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:56.435: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:56.435: INFO: Wrong image for pod: daemon-set-nv99q. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:56.435: INFO: Pod daemon-set-nv99q is not available
Dec 18 18:30:57.435: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:57.435: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:57.435: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:57.435: INFO: Wrong image for pod: daemon-set-nv99q. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:57.435: INFO: Pod daemon-set-nv99q is not available
Dec 18 18:30:58.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:58.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:58.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:58.434: INFO: Wrong image for pod: daemon-set-nv99q. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:58.434: INFO: Pod daemon-set-nv99q is not available
Dec 18 18:30:59.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:59.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:59.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:59.434: INFO: Wrong image for pod: daemon-set-nv99q. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:30:59.434: INFO: Pod daemon-set-nv99q is not available
Dec 18 18:31:00.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:00.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:00.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:00.434: INFO: Wrong image for pod: daemon-set-nv99q. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:00.434: INFO: Pod daemon-set-nv99q is not available
Dec 18 18:31:01.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:01.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:01.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:01.434: INFO: Wrong image for pod: daemon-set-nv99q. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:01.434: INFO: Pod daemon-set-nv99q is not available
Dec 18 18:31:02.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:02.435: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:02.435: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:02.435: INFO: Wrong image for pod: daemon-set-nv99q. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:02.435: INFO: Pod daemon-set-nv99q is not available
Dec 18 18:31:03.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:03.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:03.434: INFO: Pod daemon-set-98pf4 is not available
Dec 18 18:31:03.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:04.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:04.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:04.434: INFO: Pod daemon-set-98pf4 is not available
Dec 18 18:31:04.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:05.435: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:05.435: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:05.435: INFO: Pod daemon-set-98pf4 is not available
Dec 18 18:31:05.435: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:06.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:06.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:06.434: INFO: Pod daemon-set-98pf4 is not available
Dec 18 18:31:06.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:07.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:07.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:07.434: INFO: Pod daemon-set-98pf4 is not available
Dec 18 18:31:07.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:08.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:08.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:08.434: INFO: Pod daemon-set-98pf4 is not available
Dec 18 18:31:08.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:09.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:09.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:09.434: INFO: Pod daemon-set-98pf4 is not available
Dec 18 18:31:09.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:10.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:10.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:10.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:11.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:11.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:11.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:12.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:12.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:12.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:12.434: INFO: Pod daemon-set-djpr7 is not available
Dec 18 18:31:13.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:13.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:13.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:13.434: INFO: Pod daemon-set-djpr7 is not available
Dec 18 18:31:14.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:14.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:14.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:14.434: INFO: Pod daemon-set-djpr7 is not available
Dec 18 18:31:15.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:15.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:15.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:15.434: INFO: Pod daemon-set-djpr7 is not available
Dec 18 18:31:16.435: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:16.435: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:16.435: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:16.435: INFO: Pod daemon-set-djpr7 is not available
Dec 18 18:31:17.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:17.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:17.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:17.434: INFO: Pod daemon-set-djpr7 is not available
Dec 18 18:31:18.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:18.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:18.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:18.434: INFO: Pod daemon-set-djpr7 is not available
Dec 18 18:31:19.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:19.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:19.434: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:19.434: INFO: Pod daemon-set-djpr7 is not available
Dec 18 18:31:20.435: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:20.435: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:20.435: INFO: Wrong image for pod: daemon-set-djpr7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:20.435: INFO: Pod daemon-set-djpr7 is not available
Dec 18 18:31:21.435: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:21.435: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:21.435: INFO: Pod daemon-set-ttp6q is not available
Dec 18 18:31:22.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:22.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:22.434: INFO: Pod daemon-set-ttp6q is not available
Dec 18 18:31:23.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:23.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:23.434: INFO: Pod daemon-set-ttp6q is not available
Dec 18 18:31:24.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:24.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:24.434: INFO: Pod daemon-set-ttp6q is not available
Dec 18 18:31:25.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:25.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:25.434: INFO: Pod daemon-set-ttp6q is not available
Dec 18 18:31:26.435: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:26.435: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:26.435: INFO: Pod daemon-set-ttp6q is not available
Dec 18 18:31:27.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:27.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:27.434: INFO: Pod daemon-set-ttp6q is not available
Dec 18 18:31:28.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:28.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:29.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:29.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:29.434: INFO: Pod daemon-set-5cmsw is not available
Dec 18 18:31:30.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:30.434: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:30.434: INFO: Pod daemon-set-5cmsw is not available
Dec 18 18:31:31.435: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:31.435: INFO: Wrong image for pod: daemon-set-5cmsw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:31.435: INFO: Pod daemon-set-5cmsw is not available
Dec 18 18:31:32.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:32.434: INFO: Pod daemon-set-wknx7 is not available
Dec 18 18:31:33.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:33.434: INFO: Pod daemon-set-wknx7 is not available
Dec 18 18:31:34.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:34.434: INFO: Pod daemon-set-wknx7 is not available
Dec 18 18:31:35.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:35.434: INFO: Pod daemon-set-wknx7 is not available
Dec 18 18:31:36.435: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:36.435: INFO: Pod daemon-set-wknx7 is not available
Dec 18 18:31:37.435: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:37.435: INFO: Pod daemon-set-wknx7 is not available
Dec 18 18:31:38.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:38.434: INFO: Pod daemon-set-wknx7 is not available
Dec 18 18:31:39.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:40.434: INFO: Wrong image for pod: daemon-set-4k4dt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 18 18:31:40.434: INFO: Pod daemon-set-4k4dt is not available
Dec 18 18:31:41.434: INFO: Pod daemon-set-pk4dz is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 18 18:31:41.438: INFO: Number of nodes with available pods: 3
Dec 18 18:31:41.438: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:31:42.442: INFO: Number of nodes with available pods: 3
Dec 18 18:31:42.442: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:31:43.442: INFO: Number of nodes with available pods: 3
Dec 18 18:31:43.442: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:31:44.442: INFO: Number of nodes with available pods: 3
Dec 18 18:31:44.442: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:31:45.442: INFO: Number of nodes with available pods: 3
Dec 18 18:31:45.442: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:31:46.443: INFO: Number of nodes with available pods: 3
Dec 18 18:31:46.443: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:31:47.442: INFO: Number of nodes with available pods: 3
Dec 18 18:31:47.442: INFO: Node compute-0 is running more than one daemon pod
Dec 18 18:31:48.442: INFO: Number of nodes with available pods: 4
Dec 18 18:31:48.442: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9880, will wait for the garbage collector to delete the pods
Dec 18 18:31:48.501: INFO: Deleting DaemonSet.extensions daemon-set took: 2.248923ms
Dec 18 18:31:48.902: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.20798ms
Dec 18 18:32:02.203: INFO: Number of nodes with available pods: 0
Dec 18 18:32:02.203: INFO: Number of running nodes: 0, number of available pods: 0
Dec 18 18:32:02.204: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9880/daemonsets","resourceVersion":"63976"},"items":null}

Dec 18 18:32:02.205: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9880/pods","resourceVersion":"63976"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:32:02.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9880" for this suite.
Dec 18 18:32:08.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:32:08.247: INFO: namespace daemonsets-9880 deletion completed in 6.034566439s

• [SLOW TEST:83.855 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:32:08.247: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Dec 18 18:32:08.261: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 18 18:33:08.274: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:33:08.275: INFO: Starting informer...
STEP: Starting pod...
Dec 18 18:33:08.480: INFO: Pod is running on controller-0. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec 18 18:33:08.488: INFO: Pod wasn't evicted. Proceeding
Dec 18 18:33:08.488: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec 18 18:34:23.496: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:34:23.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2205" for this suite.
Dec 18 18:34:35.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:34:35.534: INFO: namespace taint-single-pod-2205 deletion completed in 12.034867611s

• [SLOW TEST:147.287 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:34:35.534: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 18 18:34:35.550: INFO: Waiting up to 5m0s for pod "pod-454c0e0e-2d8c-4a3f-bc3f-bf4797957cd6" in namespace "emptydir-5010" to be "success or failure"
Dec 18 18:34:35.551: INFO: Pod "pod-454c0e0e-2d8c-4a3f-bc3f-bf4797957cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 851.489µs
Dec 18 18:34:37.553: INFO: Pod "pod-454c0e0e-2d8c-4a3f-bc3f-bf4797957cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002995869s
Dec 18 18:34:39.555: INFO: Pod "pod-454c0e0e-2d8c-4a3f-bc3f-bf4797957cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004534177s
Dec 18 18:34:41.556: INFO: Pod "pod-454c0e0e-2d8c-4a3f-bc3f-bf4797957cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006133275s
Dec 18 18:34:43.558: INFO: Pod "pod-454c0e0e-2d8c-4a3f-bc3f-bf4797957cd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007734912s
STEP: Saw pod success
Dec 18 18:34:43.558: INFO: Pod "pod-454c0e0e-2d8c-4a3f-bc3f-bf4797957cd6" satisfied condition "success or failure"
Dec 18 18:34:43.559: INFO: Trying to get logs from node controller-0 pod pod-454c0e0e-2d8c-4a3f-bc3f-bf4797957cd6 container test-container: <nil>
STEP: delete the pod
Dec 18 18:34:43.574: INFO: Waiting for pod pod-454c0e0e-2d8c-4a3f-bc3f-bf4797957cd6 to disappear
Dec 18 18:34:43.575: INFO: Pod pod-454c0e0e-2d8c-4a3f-bc3f-bf4797957cd6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:34:43.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5010" for this suite.
Dec 18 18:34:49.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:34:49.612: INFO: namespace emptydir-5010 deletion completed in 6.034878632s

• [SLOW TEST:14.078 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:34:49.612: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 18:34:50.098: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 18 18:34:52.102: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290890, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290890, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290890, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290890, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:34:54.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290890, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290890, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290890, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290890, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:34:56.104: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290890, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290890, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290890, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290890, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 18:34:59.110: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:34:59.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1902" for this suite.
Dec 18 18:35:05.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:35:05.170: INFO: namespace webhook-1902 deletion completed in 6.035475674s
STEP: Destroying namespace "webhook-1902-markers" for this suite.
Dec 18 18:35:11.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:35:11.206: INFO: namespace webhook-1902-markers deletion completed in 6.035326491s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.598 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:35:11.210: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:35:11.236: INFO: (0) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 10.404905ms)
Dec 18 18:35:11.238: INFO: (1) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.697933ms)
Dec 18 18:35:11.239: INFO: (2) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.760005ms)
Dec 18 18:35:11.241: INFO: (3) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.989851ms)
Dec 18 18:35:11.243: INFO: (4) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.897926ms)
Dec 18 18:35:11.245: INFO: (5) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.892975ms)
Dec 18 18:35:11.247: INFO: (6) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.845443ms)
Dec 18 18:35:11.249: INFO: (7) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.583876ms)
Dec 18 18:35:11.251: INFO: (8) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.821736ms)
Dec 18 18:35:11.253: INFO: (9) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.984485ms)
Dec 18 18:35:11.256: INFO: (10) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 3.852755ms)
Dec 18 18:35:11.258: INFO: (11) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.591008ms)
Dec 18 18:35:11.260: INFO: (12) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.527066ms)
Dec 18 18:35:11.261: INFO: (13) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.551702ms)
Dec 18 18:35:11.263: INFO: (14) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.733482ms)
Dec 18 18:35:11.265: INFO: (15) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.718022ms)
Dec 18 18:35:11.266: INFO: (16) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.791253ms)
Dec 18 18:35:11.268: INFO: (17) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.706667ms)
Dec 18 18:35:11.270: INFO: (18) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.740532ms)
Dec 18 18:35:11.272: INFO: (19) /api/v1/nodes/compute-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.683531ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:35:11.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3059" for this suite.
Dec 18 18:35:17.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:35:17.307: INFO: namespace proxy-3059 deletion completed in 6.033372224s

• [SLOW TEST:6.097 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:35:17.308: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:35:17.324: INFO: (0) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.900883ms)
Dec 18 18:35:17.326: INFO: (1) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.601025ms)
Dec 18 18:35:17.328: INFO: (2) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.681829ms)
Dec 18 18:35:17.332: INFO: (3) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 4.441498ms)
Dec 18 18:35:17.334: INFO: (4) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.827328ms)
Dec 18 18:35:17.335: INFO: (5) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.642635ms)
Dec 18 18:35:17.337: INFO: (6) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.743164ms)
Dec 18 18:35:17.339: INFO: (7) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.831558ms)
Dec 18 18:35:17.341: INFO: (8) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.660401ms)
Dec 18 18:35:17.342: INFO: (9) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.689924ms)
Dec 18 18:35:17.344: INFO: (10) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.831998ms)
Dec 18 18:35:17.346: INFO: (11) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.901591ms)
Dec 18 18:35:17.348: INFO: (12) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.761137ms)
Dec 18 18:35:17.350: INFO: (13) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.806123ms)
Dec 18 18:35:17.352: INFO: (14) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.791525ms)
Dec 18 18:35:17.353: INFO: (15) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.757662ms)
Dec 18 18:35:17.355: INFO: (16) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.70235ms)
Dec 18 18:35:17.357: INFO: (17) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.738927ms)
Dec 18 18:35:17.359: INFO: (18) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 1.943475ms)
Dec 18 18:35:17.363: INFO: (19) /api/v1/nodes/compute-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</... (200; 4.354172ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:35:17.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6498" for this suite.
Dec 18 18:35:23.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:35:23.399: INFO: namespace proxy-6498 deletion completed in 6.034262882s

• [SLOW TEST:6.092 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:35:23.399: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:35:23.413: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Creating first CR 
Dec 18 18:35:23.926: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-18T18:35:23Z generation:1 name:name1 resourceVersion:65019 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:32fba65a-5313-4cdb-aae5-fb8ac8d75338] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 18 18:35:33.928: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-18T18:35:33Z generation:1 name:name2 resourceVersion:65052 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:732a28e7-d1a4-42ce-ad94-1a385eb4b241] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 18 18:35:43.931: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-18T18:35:23Z generation:2 name:name1 resourceVersion:65084 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:32fba65a-5313-4cdb-aae5-fb8ac8d75338] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 18 18:35:53.934: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-18T18:35:33Z generation:2 name:name2 resourceVersion:65116 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:732a28e7-d1a4-42ce-ad94-1a385eb4b241] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 18 18:36:03.938: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-18T18:35:23Z generation:2 name:name1 resourceVersion:65148 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:32fba65a-5313-4cdb-aae5-fb8ac8d75338] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 18 18:36:13.941: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-18T18:35:33Z generation:2 name:name2 resourceVersion:65181 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:732a28e7-d1a4-42ce-ad94-1a385eb4b241] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:36:24.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9529" for this suite.
Dec 18 18:36:30.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:36:30.482: INFO: namespace crd-watch-9529 deletion completed in 6.034943409s

• [SLOW TEST:67.083 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:36:30.482: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 18:36:30.832: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 18 18:36:32.836: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:36:34.838: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:36:36.838: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:36:38.838: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712290990, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 18:36:41.844: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:36:41.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-674" for this suite.
Dec 18 18:36:47.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:36:47.948: INFO: namespace webhook-674 deletion completed in 6.049188052s
STEP: Destroying namespace "webhook-674-markers" for this suite.
Dec 18 18:36:53.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:36:53.982: INFO: namespace webhook-674-markers deletion completed in 6.033829411s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.504 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:36:53.986: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 18 18:36:54.006: INFO: Waiting up to 5m0s for pod "pod-db4614c8-927e-4c50-80f2-d38c44bbd056" in namespace "emptydir-6792" to be "success or failure"
Dec 18 18:36:54.007: INFO: Pod "pod-db4614c8-927e-4c50-80f2-d38c44bbd056": Phase="Pending", Reason="", readiness=false. Elapsed: 949.75µs
Dec 18 18:36:56.008: INFO: Pod "pod-db4614c8-927e-4c50-80f2-d38c44bbd056": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002550524s
Dec 18 18:36:58.010: INFO: Pod "pod-db4614c8-927e-4c50-80f2-d38c44bbd056": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004235432s
Dec 18 18:37:00.011: INFO: Pod "pod-db4614c8-927e-4c50-80f2-d38c44bbd056": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005758247s
Dec 18 18:37:02.013: INFO: Pod "pod-db4614c8-927e-4c50-80f2-d38c44bbd056": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007401808s
STEP: Saw pod success
Dec 18 18:37:02.013: INFO: Pod "pod-db4614c8-927e-4c50-80f2-d38c44bbd056" satisfied condition "success or failure"
Dec 18 18:37:02.014: INFO: Trying to get logs from node controller-1 pod pod-db4614c8-927e-4c50-80f2-d38c44bbd056 container test-container: <nil>
STEP: delete the pod
Dec 18 18:37:02.030: INFO: Waiting for pod pod-db4614c8-927e-4c50-80f2-d38c44bbd056 to disappear
Dec 18 18:37:02.031: INFO: Pod pod-db4614c8-927e-4c50-80f2-d38c44bbd056 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:37:02.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6792" for this suite.
Dec 18 18:37:08.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:37:08.067: INFO: namespace emptydir-6792 deletion completed in 6.034893682s

• [SLOW TEST:14.081 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:37:08.068: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Dec 18 18:37:08.082: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 18 18:38:08.095: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:38:08.096: INFO: Starting informer...
STEP: Starting pods...
Dec 18 18:38:08.302: INFO: Pod1 is running on controller-1. Tainting Node
Dec 18 18:38:16.509: INFO: Pod2 is running on controller-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec 18 18:38:24.523: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 18 18:38:52.103: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:38:52.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7879" for this suite.
Dec 18 18:38:58.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:38:58.146: INFO: namespace taint-multiple-pods-7879 deletion completed in 6.036215301s

• [SLOW TEST:110.079 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:38:58.147: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-b30198a1-f3aa-4cd8-af5d-22f46bb2bb98
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:39:06.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2786" for this suite.
Dec 18 18:39:18.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:39:18.220: INFO: namespace configmap-2786 deletion completed in 12.034664286s

• [SLOW TEST:20.073 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:39:18.220: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 18 18:39:18.236: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 18 18:39:23.238: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:39:24.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9312" for this suite.
Dec 18 18:39:30.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:39:30.289: INFO: namespace replication-controller-9312 deletion completed in 6.03474062s

• [SLOW TEST:12.069 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:39:30.289: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-89kjv in namespace proxy-639
I1218 18:39:30.309604      27 runners.go:184] Created replication controller with name: proxy-service-89kjv, namespace: proxy-639, replica count: 1
I1218 18:39:31.359956      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 18:39:32.360125      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 18:39:33.360381      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 18:39:34.360634      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 18:39:35.360839      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 18:39:36.361038      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 18:39:37.361203      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1218 18:39:38.361485      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1218 18:39:39.361722      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1218 18:39:40.361905      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1218 18:39:41.362076      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1218 18:39:42.362307      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1218 18:39:43.362497      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1218 18:39:44.362706      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1218 18:39:45.362899      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1218 18:39:46.363073      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1218 18:39:47.363266      27 runners.go:184] proxy-service-89kjv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 18 18:39:47.365: INFO: setup took 17.062360887s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 18 18:39:47.367: INFO: (0) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.841348ms)
Dec 18 18:39:47.368: INFO: (0) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 2.924777ms)
Dec 18 18:39:47.368: INFO: (0) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 3.216974ms)
Dec 18 18:39:47.368: INFO: (0) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 3.162297ms)
Dec 18 18:39:47.368: INFO: (0) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 3.265711ms)
Dec 18 18:39:47.368: INFO: (0) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 3.353651ms)
Dec 18 18:39:47.368: INFO: (0) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 3.182987ms)
Dec 18 18:39:47.368: INFO: (0) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 3.311482ms)
Dec 18 18:39:47.368: INFO: (0) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 3.216976ms)
Dec 18 18:39:47.369: INFO: (0) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 3.878944ms)
Dec 18 18:39:47.369: INFO: (0) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 3.697839ms)
Dec 18 18:39:47.373: INFO: (0) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 7.738133ms)
Dec 18 18:39:47.373: INFO: (0) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 7.803309ms)
Dec 18 18:39:47.373: INFO: (0) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 7.741243ms)
Dec 18 18:39:47.373: INFO: (0) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 7.734323ms)
Dec 18 18:39:47.373: INFO: (0) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 7.790479ms)
Dec 18 18:39:47.374: INFO: (1) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.29389ms)
Dec 18 18:39:47.374: INFO: (1) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.342747ms)
Dec 18 18:39:47.374: INFO: (1) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.36567ms)
Dec 18 18:39:47.374: INFO: (1) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.43155ms)
Dec 18 18:39:47.375: INFO: (1) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.73056ms)
Dec 18 18:39:47.375: INFO: (1) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.704335ms)
Dec 18 18:39:47.375: INFO: (1) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.795212ms)
Dec 18 18:39:47.375: INFO: (1) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.883079ms)
Dec 18 18:39:47.375: INFO: (1) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.982988ms)
Dec 18 18:39:47.375: INFO: (1) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.953184ms)
Dec 18 18:39:47.375: INFO: (1) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 2.017628ms)
Dec 18 18:39:47.375: INFO: (1) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.93147ms)
Dec 18 18:39:47.375: INFO: (1) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.955119ms)
Dec 18 18:39:47.375: INFO: (1) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 2.000487ms)
Dec 18 18:39:47.375: INFO: (1) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.970864ms)
Dec 18 18:39:47.375: INFO: (1) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.997411ms)
Dec 18 18:39:47.376: INFO: (2) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.231662ms)
Dec 18 18:39:47.376: INFO: (2) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.396923ms)
Dec 18 18:39:47.376: INFO: (2) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.377191ms)
Dec 18 18:39:47.376: INFO: (2) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.422264ms)
Dec 18 18:39:47.376: INFO: (2) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.384379ms)
Dec 18 18:39:47.376: INFO: (2) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.624579ms)
Dec 18 18:39:47.376: INFO: (2) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.609192ms)
Dec 18 18:39:47.376: INFO: (2) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.608287ms)
Dec 18 18:39:47.376: INFO: (2) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.653546ms)
Dec 18 18:39:47.376: INFO: (2) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.578331ms)
Dec 18 18:39:47.376: INFO: (2) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.564337ms)
Dec 18 18:39:47.376: INFO: (2) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.58584ms)
Dec 18 18:39:47.376: INFO: (2) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.609224ms)
Dec 18 18:39:47.377: INFO: (2) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.73609ms)
Dec 18 18:39:47.377: INFO: (2) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.786571ms)
Dec 18 18:39:47.377: INFO: (2) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.775181ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.015102ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.210641ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.331658ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.433928ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.441014ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.421312ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.524571ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.532961ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.547493ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.63617ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.535058ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.694877ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.710618ms)
Dec 18 18:39:47.378: INFO: (3) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.716314ms)
Dec 18 18:39:47.379: INFO: (3) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.826587ms)
Dec 18 18:39:47.379: INFO: (3) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.81474ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.0576ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.171322ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.13424ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.211534ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.264516ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.310541ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.371473ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.496036ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.519135ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.660546ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.710152ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.726669ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.791682ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.743095ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.754875ms)
Dec 18 18:39:47.380: INFO: (4) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.742503ms)
Dec 18 18:39:47.381: INFO: (5) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.04363ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.223099ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.387982ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.416547ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.405908ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.416111ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.448057ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.478074ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.518959ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.485309ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.50062ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.522437ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.691881ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.684894ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.720896ms)
Dec 18 18:39:47.382: INFO: (5) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.75124ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.156349ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.263969ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.277762ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.314457ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.328702ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.408712ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.406179ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.541756ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.580991ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.562632ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.624353ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.611858ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.845276ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.841509ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.840217ms)
Dec 18 18:39:47.384: INFO: (6) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.830602ms)
Dec 18 18:39:47.385: INFO: (7) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.140685ms)
Dec 18 18:39:47.385: INFO: (7) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.227129ms)
Dec 18 18:39:47.385: INFO: (7) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.28029ms)
Dec 18 18:39:47.385: INFO: (7) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.248706ms)
Dec 18 18:39:47.385: INFO: (7) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.229923ms)
Dec 18 18:39:47.386: INFO: (7) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.431815ms)
Dec 18 18:39:47.386: INFO: (7) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.400712ms)
Dec 18 18:39:47.386: INFO: (7) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.361006ms)
Dec 18 18:39:47.386: INFO: (7) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.477738ms)
Dec 18 18:39:47.386: INFO: (7) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.514801ms)
Dec 18 18:39:47.386: INFO: (7) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.569785ms)
Dec 18 18:39:47.386: INFO: (7) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.588106ms)
Dec 18 18:39:47.386: INFO: (7) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.573264ms)
Dec 18 18:39:47.386: INFO: (7) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.681987ms)
Dec 18 18:39:47.386: INFO: (7) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.658202ms)
Dec 18 18:39:47.386: INFO: (7) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.864291ms)
Dec 18 18:39:47.387: INFO: (8) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.099806ms)
Dec 18 18:39:47.387: INFO: (8) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.095445ms)
Dec 18 18:39:47.387: INFO: (8) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.241832ms)
Dec 18 18:39:47.387: INFO: (8) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.299911ms)
Dec 18 18:39:47.388: INFO: (8) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.343645ms)
Dec 18 18:39:47.388: INFO: (8) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.395805ms)
Dec 18 18:39:47.388: INFO: (8) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.428974ms)
Dec 18 18:39:47.388: INFO: (8) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.531278ms)
Dec 18 18:39:47.388: INFO: (8) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.587478ms)
Dec 18 18:39:47.388: INFO: (8) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.674438ms)
Dec 18 18:39:47.388: INFO: (8) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.679348ms)
Dec 18 18:39:47.388: INFO: (8) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.673419ms)
Dec 18 18:39:47.388: INFO: (8) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.682082ms)
Dec 18 18:39:47.388: INFO: (8) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.684056ms)
Dec 18 18:39:47.388: INFO: (8) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.789715ms)
Dec 18 18:39:47.388: INFO: (8) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.817409ms)
Dec 18 18:39:47.389: INFO: (9) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.072522ms)
Dec 18 18:39:47.389: INFO: (9) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.079894ms)
Dec 18 18:39:47.389: INFO: (9) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.150259ms)
Dec 18 18:39:47.389: INFO: (9) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.288868ms)
Dec 18 18:39:47.389: INFO: (9) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.273775ms)
Dec 18 18:39:47.390: INFO: (9) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.435693ms)
Dec 18 18:39:47.390: INFO: (9) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.586831ms)
Dec 18 18:39:47.390: INFO: (9) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.646388ms)
Dec 18 18:39:47.390: INFO: (9) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.621036ms)
Dec 18 18:39:47.390: INFO: (9) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.632912ms)
Dec 18 18:39:47.390: INFO: (9) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.650941ms)
Dec 18 18:39:47.390: INFO: (9) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.641228ms)
Dec 18 18:39:47.390: INFO: (9) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.706173ms)
Dec 18 18:39:47.390: INFO: (9) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.72641ms)
Dec 18 18:39:47.390: INFO: (9) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.72008ms)
Dec 18 18:39:47.390: INFO: (9) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.69575ms)
Dec 18 18:39:47.391: INFO: (10) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.046725ms)
Dec 18 18:39:47.391: INFO: (10) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.209634ms)
Dec 18 18:39:47.391: INFO: (10) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.127137ms)
Dec 18 18:39:47.391: INFO: (10) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.17303ms)
Dec 18 18:39:47.391: INFO: (10) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.241378ms)
Dec 18 18:39:47.391: INFO: (10) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.254046ms)
Dec 18 18:39:47.391: INFO: (10) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.211487ms)
Dec 18 18:39:47.391: INFO: (10) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.326354ms)
Dec 18 18:39:47.391: INFO: (10) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.542714ms)
Dec 18 18:39:47.391: INFO: (10) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.547055ms)
Dec 18 18:39:47.391: INFO: (10) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.570338ms)
Dec 18 18:39:47.391: INFO: (10) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.576335ms)
Dec 18 18:39:47.392: INFO: (10) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.774921ms)
Dec 18 18:39:47.392: INFO: (10) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.818094ms)
Dec 18 18:39:47.392: INFO: (10) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.837185ms)
Dec 18 18:39:47.392: INFO: (10) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.839354ms)
Dec 18 18:39:47.393: INFO: (11) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.202819ms)
Dec 18 18:39:47.393: INFO: (11) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.270706ms)
Dec 18 18:39:47.393: INFO: (11) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.279946ms)
Dec 18 18:39:47.393: INFO: (11) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.565899ms)
Dec 18 18:39:47.393: INFO: (11) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.514701ms)
Dec 18 18:39:47.393: INFO: (11) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.548896ms)
Dec 18 18:39:47.393: INFO: (11) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.542707ms)
Dec 18 18:39:47.393: INFO: (11) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.589486ms)
Dec 18 18:39:47.393: INFO: (11) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.570869ms)
Dec 18 18:39:47.394: INFO: (11) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.737328ms)
Dec 18 18:39:47.394: INFO: (11) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.798145ms)
Dec 18 18:39:47.394: INFO: (11) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.805792ms)
Dec 18 18:39:47.394: INFO: (11) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.889634ms)
Dec 18 18:39:47.394: INFO: (11) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.88861ms)
Dec 18 18:39:47.394: INFO: (11) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.899288ms)
Dec 18 18:39:47.394: INFO: (11) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.91714ms)
Dec 18 18:39:47.395: INFO: (12) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.108623ms)
Dec 18 18:39:47.395: INFO: (12) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.10704ms)
Dec 18 18:39:47.395: INFO: (12) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.213386ms)
Dec 18 18:39:47.395: INFO: (12) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.425895ms)
Dec 18 18:39:47.395: INFO: (12) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.495508ms)
Dec 18 18:39:47.395: INFO: (12) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.545192ms)
Dec 18 18:39:47.395: INFO: (12) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.542669ms)
Dec 18 18:39:47.395: INFO: (12) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.614297ms)
Dec 18 18:39:47.395: INFO: (12) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.569539ms)
Dec 18 18:39:47.396: INFO: (12) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.730407ms)
Dec 18 18:39:47.396: INFO: (12) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.735174ms)
Dec 18 18:39:47.396: INFO: (12) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.744542ms)
Dec 18 18:39:47.396: INFO: (12) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.898407ms)
Dec 18 18:39:47.396: INFO: (12) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.919202ms)
Dec 18 18:39:47.396: INFO: (12) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.944007ms)
Dec 18 18:39:47.396: INFO: (12) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.970924ms)
Dec 18 18:39:47.397: INFO: (13) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.129758ms)
Dec 18 18:39:47.397: INFO: (13) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.247597ms)
Dec 18 18:39:47.397: INFO: (13) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.289765ms)
Dec 18 18:39:47.397: INFO: (13) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.346829ms)
Dec 18 18:39:47.397: INFO: (13) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.278679ms)
Dec 18 18:39:47.397: INFO: (13) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.38089ms)
Dec 18 18:39:47.397: INFO: (13) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.368528ms)
Dec 18 18:39:47.397: INFO: (13) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.403995ms)
Dec 18 18:39:47.397: INFO: (13) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.542038ms)
Dec 18 18:39:47.397: INFO: (13) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.524426ms)
Dec 18 18:39:47.397: INFO: (13) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.532441ms)
Dec 18 18:39:47.397: INFO: (13) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.621967ms)
Dec 18 18:39:47.398: INFO: (13) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.722208ms)
Dec 18 18:39:47.398: INFO: (13) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.698686ms)
Dec 18 18:39:47.398: INFO: (13) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.746542ms)
Dec 18 18:39:47.398: INFO: (13) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.735835ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.024356ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.140218ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.387302ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.379252ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.449705ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.522089ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.547687ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.607647ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.601019ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.627489ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.670897ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.73978ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.691724ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.758089ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.793917ms)
Dec 18 18:39:47.399: INFO: (14) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.820523ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.120193ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.084483ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.190067ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.271058ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.327722ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.311982ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.337486ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.455002ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.49675ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.540806ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.579274ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.640837ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.68455ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.663611ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.795058ms)
Dec 18 18:39:47.401: INFO: (15) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.82233ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.431669ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.490458ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.669339ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.611997ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.630556ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.666749ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.689181ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.713189ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.714644ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.68372ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.739481ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.784696ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.796069ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.784688ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.782469ms)
Dec 18 18:39:47.403: INFO: (16) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.792382ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.27509ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.306574ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.345875ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.367065ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.370171ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.474556ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.397602ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.424348ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.48504ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.491532ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.503488ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.566779ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.81355ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.844562ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.886317ms)
Dec 18 18:39:47.405: INFO: (17) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.974613ms)
Dec 18 18:39:47.406: INFO: (18) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.14751ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.253395ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.308751ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.332218ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.400194ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.37998ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.39385ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.595045ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.536597ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.519069ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.580971ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.702327ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.699683ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.756236ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.694178ms)
Dec 18 18:39:47.407: INFO: (18) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.776477ms)
Dec 18 18:39:47.408: INFO: (19) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.022163ms)
Dec 18 18:39:47.408: INFO: (19) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">t... (200; 1.185101ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:1080/proxy/rewriteme">test</... (200; 1.341915ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:160/proxy/: foo (200; 1.494165ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:462/proxy/: tls qux (200; 1.504913ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.510365ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname1/proxy/: foo (200; 1.570079ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname1/proxy/: tls baz (200; 1.649857ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:460/proxy/: tls baz (200; 1.630929ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/proxy-service-89kjv-fqr5l/proxy/rewriteme">test</a> (200; 1.63191ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/pods/http:proxy-service-89kjv-fqr5l:162/proxy/: bar (200; 1.685797ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/: <a href="/api/v1/namespaces/proxy-639/pods/https:proxy-service-89kjv-fqr5l:443/proxy/tlsrewriteme... (200; 1.713933ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname1/proxy/: foo (200; 1.754605ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/services/https:proxy-service-89kjv:tlsportname2/proxy/: tls qux (200; 1.743621ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/services/proxy-service-89kjv:portname2/proxy/: bar (200; 1.795662ms)
Dec 18 18:39:47.409: INFO: (19) /api/v1/namespaces/proxy-639/services/http:proxy-service-89kjv:portname2/proxy/: bar (200; 1.854028ms)
STEP: deleting ReplicationController proxy-service-89kjv in namespace proxy-639, will wait for the garbage collector to delete the pods
Dec 18 18:39:47.463: INFO: Deleting ReplicationController proxy-service-89kjv took: 2.333322ms
Dec 18 18:39:47.863: INFO: Terminating ReplicationController proxy-service-89kjv pods took: 400.183052ms
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:39:50.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-639" for this suite.
Dec 18 18:39:56.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:39:56.199: INFO: namespace proxy-639 deletion completed in 6.034645238s

• [SLOW TEST:25.910 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:39:56.200: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1855
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 18 18:39:56.212: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 18 18:40:26.247: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.192.85:8080/dial?request=hostName&protocol=udp&host=172.16.154.42&port=8081&tries=1'] Namespace:pod-network-test-1855 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 18:40:26.247: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:40:26.385: INFO: Waiting for endpoints: map[]
Dec 18 18:40:26.387: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.192.85:8080/dial?request=hostName&protocol=udp&host=172.16.103.157&port=8081&tries=1'] Namespace:pod-network-test-1855 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 18:40:26.387: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:40:26.534: INFO: Waiting for endpoints: map[]
Dec 18 18:40:26.536: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.192.85:8080/dial?request=hostName&protocol=udp&host=172.16.166.173&port=8081&tries=1'] Namespace:pod-network-test-1855 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 18:40:26.536: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:40:26.689: INFO: Waiting for endpoints: map[]
Dec 18 18:40:26.691: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.192.85:8080/dial?request=hostName&protocol=udp&host=172.16.192.83&port=8081&tries=1'] Namespace:pod-network-test-1855 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 18:40:26.691: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:40:26.842: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:40:26.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1855" for this suite.
Dec 18 18:40:38.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:40:38.880: INFO: namespace pod-network-test-1855 deletion completed in 12.035771624s

• [SLOW TEST:42.681 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:40:38.881: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:40:46.907: INFO: Waiting up to 5m0s for pod "client-envvars-da4308d0-449f-4845-b5c4-d8b283899063" in namespace "pods-8294" to be "success or failure"
Dec 18 18:40:46.908: INFO: Pod "client-envvars-da4308d0-449f-4845-b5c4-d8b283899063": Phase="Pending", Reason="", readiness=false. Elapsed: 1.065949ms
Dec 18 18:40:48.911: INFO: Pod "client-envvars-da4308d0-449f-4845-b5c4-d8b283899063": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003411007s
Dec 18 18:40:50.912: INFO: Pod "client-envvars-da4308d0-449f-4845-b5c4-d8b283899063": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00501612s
Dec 18 18:40:52.914: INFO: Pod "client-envvars-da4308d0-449f-4845-b5c4-d8b283899063": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006684763s
Dec 18 18:40:54.916: INFO: Pod "client-envvars-da4308d0-449f-4845-b5c4-d8b283899063": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008382322s
STEP: Saw pod success
Dec 18 18:40:54.916: INFO: Pod "client-envvars-da4308d0-449f-4845-b5c4-d8b283899063" satisfied condition "success or failure"
Dec 18 18:40:54.917: INFO: Trying to get logs from node controller-1 pod client-envvars-da4308d0-449f-4845-b5c4-d8b283899063 container env3cont: <nil>
STEP: delete the pod
Dec 18 18:40:54.931: INFO: Waiting for pod client-envvars-da4308d0-449f-4845-b5c4-d8b283899063 to disappear
Dec 18 18:40:54.932: INFO: Pod client-envvars-da4308d0-449f-4845-b5c4-d8b283899063 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:40:54.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8294" for this suite.
Dec 18 18:41:22.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:41:22.968: INFO: namespace pods-8294 deletion completed in 28.033785006s

• [SLOW TEST:44.087 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:41:22.968: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 18:41:23.341: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 18 18:41:25.345: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291283, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291283, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291283, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291283, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:41:27.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291283, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291283, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291283, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291283, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:41:29.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291283, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291283, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291283, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291283, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 18:41:32.353: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:41:32.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1377" for this suite.
Dec 18 18:41:44.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:41:44.614: INFO: namespace webhook-1377 deletion completed in 12.037343648s
STEP: Destroying namespace "webhook-1377-markers" for this suite.
Dec 18 18:41:50.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:41:50.647: INFO: namespace webhook-1377-markers deletion completed in 6.033114892s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.683 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:41:50.651: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-09668f9b-7aeb-4297-9214-a8e63d3718f0
STEP: Creating a pod to test consume secrets
Dec 18 18:41:50.668: INFO: Waiting up to 5m0s for pod "pod-secrets-f88f0bca-81f8-4af2-96ee-5d150f9444d7" in namespace "secrets-5218" to be "success or failure"
Dec 18 18:41:50.669: INFO: Pod "pod-secrets-f88f0bca-81f8-4af2-96ee-5d150f9444d7": Phase="Pending", Reason="", readiness=false. Elapsed: 803.807µs
Dec 18 18:41:52.670: INFO: Pod "pod-secrets-f88f0bca-81f8-4af2-96ee-5d150f9444d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002274235s
Dec 18 18:41:54.672: INFO: Pod "pod-secrets-f88f0bca-81f8-4af2-96ee-5d150f9444d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004371679s
Dec 18 18:41:56.676: INFO: Pod "pod-secrets-f88f0bca-81f8-4af2-96ee-5d150f9444d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008220952s
Dec 18 18:41:58.678: INFO: Pod "pod-secrets-f88f0bca-81f8-4af2-96ee-5d150f9444d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.010082456s
STEP: Saw pod success
Dec 18 18:41:58.678: INFO: Pod "pod-secrets-f88f0bca-81f8-4af2-96ee-5d150f9444d7" satisfied condition "success or failure"
Dec 18 18:41:58.679: INFO: Trying to get logs from node controller-1 pod pod-secrets-f88f0bca-81f8-4af2-96ee-5d150f9444d7 container secret-env-test: <nil>
STEP: delete the pod
Dec 18 18:41:58.686: INFO: Waiting for pod pod-secrets-f88f0bca-81f8-4af2-96ee-5d150f9444d7 to disappear
Dec 18 18:41:58.687: INFO: Pod pod-secrets-f88f0bca-81f8-4af2-96ee-5d150f9444d7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:41:58.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5218" for this suite.
Dec 18 18:42:04.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:42:04.723: INFO: namespace secrets-5218 deletion completed in 6.034802305s

• [SLOW TEST:14.072 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:42:04.724: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 18 18:42:04.738: INFO: Waiting up to 5m0s for pod "downward-api-27112f84-ab99-447b-bf17-b81cc6eb3598" in namespace "downward-api-1198" to be "success or failure"
Dec 18 18:42:04.739: INFO: Pod "downward-api-27112f84-ab99-447b-bf17-b81cc6eb3598": Phase="Pending", Reason="", readiness=false. Elapsed: 854.105µs
Dec 18 18:42:06.741: INFO: Pod "downward-api-27112f84-ab99-447b-bf17-b81cc6eb3598": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002593411s
Dec 18 18:42:08.744: INFO: Pod "downward-api-27112f84-ab99-447b-bf17-b81cc6eb3598": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005152868s
Dec 18 18:42:10.746: INFO: Pod "downward-api-27112f84-ab99-447b-bf17-b81cc6eb3598": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007313337s
Dec 18 18:42:12.747: INFO: Pod "downward-api-27112f84-ab99-447b-bf17-b81cc6eb3598": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009055781s
STEP: Saw pod success
Dec 18 18:42:12.747: INFO: Pod "downward-api-27112f84-ab99-447b-bf17-b81cc6eb3598" satisfied condition "success or failure"
Dec 18 18:42:12.748: INFO: Trying to get logs from node controller-1 pod downward-api-27112f84-ab99-447b-bf17-b81cc6eb3598 container dapi-container: <nil>
STEP: delete the pod
Dec 18 18:42:12.758: INFO: Waiting for pod downward-api-27112f84-ab99-447b-bf17-b81cc6eb3598 to disappear
Dec 18 18:42:12.760: INFO: Pod downward-api-27112f84-ab99-447b-bf17-b81cc6eb3598 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:42:12.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1198" for this suite.
Dec 18 18:42:18.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:42:18.797: INFO: namespace downward-api-1198 deletion completed in 6.034896217s

• [SLOW TEST:14.073 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:42:18.797: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:42:18.813: INFO: Waiting up to 5m0s for pod "busybox-user-65534-903a8fab-376e-483b-9c6b-d9618ff6d887" in namespace "security-context-test-6756" to be "success or failure"
Dec 18 18:42:18.814: INFO: Pod "busybox-user-65534-903a8fab-376e-483b-9c6b-d9618ff6d887": Phase="Pending", Reason="", readiness=false. Elapsed: 912.662µs
Dec 18 18:42:20.816: INFO: Pod "busybox-user-65534-903a8fab-376e-483b-9c6b-d9618ff6d887": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002669999s
Dec 18 18:42:22.818: INFO: Pod "busybox-user-65534-903a8fab-376e-483b-9c6b-d9618ff6d887": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004803954s
Dec 18 18:42:24.819: INFO: Pod "busybox-user-65534-903a8fab-376e-483b-9c6b-d9618ff6d887": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006442687s
Dec 18 18:42:26.822: INFO: Pod "busybox-user-65534-903a8fab-376e-483b-9c6b-d9618ff6d887": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008624831s
Dec 18 18:42:26.822: INFO: Pod "busybox-user-65534-903a8fab-376e-483b-9c6b-d9618ff6d887" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:42:26.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6756" for this suite.
Dec 18 18:42:32.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:42:32.859: INFO: namespace security-context-test-6756 deletion completed in 6.035281864s

• [SLOW TEST:14.062 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:42:32.859: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Dec 18 18:42:32.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-8847 -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 18 18:42:33.011: INFO: stderr: ""
Dec 18 18:42:33.011: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Dec 18 18:42:33.011: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 18 18:42:33.011: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8847" to be "running and ready, or succeeded"
Dec 18 18:42:33.012: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.104229ms
Dec 18 18:42:35.014: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00281575s
Dec 18 18:42:37.016: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004655355s
Dec 18 18:42:39.017: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00625844s
Dec 18 18:42:41.019: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 8.00786162s
Dec 18 18:42:41.019: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 18 18:42:41.019: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 18 18:42:41.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 logs logs-generator logs-generator --namespace=kubectl-8847'
Dec 18 18:42:41.101: INFO: stderr: ""
Dec 18 18:42:41.101: INFO: stdout: "I1218 18:42:39.563560       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/rnl 569\nI1218 18:42:39.763738       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/bfv 586\nI1218 18:42:39.963667       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/wv8 418\nI1218 18:42:40.163699       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/6bv2 503\nI1218 18:42:40.363700       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/vld 489\nI1218 18:42:40.563718       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/gln5 419\nI1218 18:42:40.763661       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/5m2 308\nI1218 18:42:40.963703       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/vgr 499\n"
STEP: limiting log lines
Dec 18 18:42:41.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 logs logs-generator logs-generator --namespace=kubectl-8847 --tail=1'
Dec 18 18:42:41.173: INFO: stderr: ""
Dec 18 18:42:41.173: INFO: stdout: "I1218 18:42:41.163674       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/g6k 487\n"
STEP: limiting log bytes
Dec 18 18:42:41.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 logs logs-generator logs-generator --namespace=kubectl-8847 --limit-bytes=1'
Dec 18 18:42:41.244: INFO: stderr: ""
Dec 18 18:42:41.244: INFO: stdout: "I"
STEP: exposing timestamps
Dec 18 18:42:41.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 logs logs-generator logs-generator --namespace=kubectl-8847 --tail=1 --timestamps'
Dec 18 18:42:41.314: INFO: stderr: ""
Dec 18 18:42:41.314: INFO: stdout: "2019-12-18T18:42:41.163793287Z I1218 18:42:41.163674       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/g6k 487\n"
STEP: restricting to a time range
Dec 18 18:42:43.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 logs logs-generator logs-generator --namespace=kubectl-8847 --since=1s'
Dec 18 18:42:43.887: INFO: stderr: ""
Dec 18 18:42:43.887: INFO: stdout: "I1218 18:42:42.963649       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/n79j 324\nI1218 18:42:43.163739       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/jcw 341\nI1218 18:42:43.363671       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/2h9 511\nI1218 18:42:43.563760       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/nl7w 516\nI1218 18:42:43.763756       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/bqd 422\n"
Dec 18 18:42:43.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 logs logs-generator logs-generator --namespace=kubectl-8847 --since=24h'
Dec 18 18:42:43.958: INFO: stderr: ""
Dec 18 18:42:43.958: INFO: stdout: "I1218 18:42:39.563560       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/rnl 569\nI1218 18:42:39.763738       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/bfv 586\nI1218 18:42:39.963667       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/wv8 418\nI1218 18:42:40.163699       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/6bv2 503\nI1218 18:42:40.363700       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/vld 489\nI1218 18:42:40.563718       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/gln5 419\nI1218 18:42:40.763661       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/5m2 308\nI1218 18:42:40.963703       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/vgr 499\nI1218 18:42:41.163674       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/g6k 487\nI1218 18:42:41.363743       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/hskr 343\nI1218 18:42:41.563798       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/6dnt 529\nI1218 18:42:41.763659       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/mjx 452\nI1218 18:42:41.963703       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/sf9q 297\nI1218 18:42:42.163749       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/lsqp 593\nI1218 18:42:42.363720       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/2sqd 359\nI1218 18:42:42.563746       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/277h 286\nI1218 18:42:42.763760       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/bn56 322\nI1218 18:42:42.963649       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/n79j 324\nI1218 18:42:43.163739       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/jcw 341\nI1218 18:42:43.363671       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/2h9 511\nI1218 18:42:43.563760       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/nl7w 516\nI1218 18:42:43.763756       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/bqd 422\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Dec 18 18:42:43.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete pod logs-generator --namespace=kubectl-8847'
Dec 18 18:42:46.006: INFO: stderr: ""
Dec 18 18:42:46.006: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:42:46.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8847" for this suite.
Dec 18 18:42:52.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:42:52.042: INFO: namespace kubectl-8847 deletion completed in 6.033937375s

• [SLOW TEST:19.183 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:42:52.042: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:42:52.058: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-a7793ce8-cd3b-4efc-bbd7-50cc8e0fa062" in namespace "security-context-test-6425" to be "success or failure"
Dec 18 18:42:52.059: INFO: Pod "busybox-readonly-false-a7793ce8-cd3b-4efc-bbd7-50cc8e0fa062": Phase="Pending", Reason="", readiness=false. Elapsed: 816.136µs
Dec 18 18:42:54.061: INFO: Pod "busybox-readonly-false-a7793ce8-cd3b-4efc-bbd7-50cc8e0fa062": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002843556s
Dec 18 18:42:56.063: INFO: Pod "busybox-readonly-false-a7793ce8-cd3b-4efc-bbd7-50cc8e0fa062": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005126802s
Dec 18 18:42:58.065: INFO: Pod "busybox-readonly-false-a7793ce8-cd3b-4efc-bbd7-50cc8e0fa062": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006900593s
Dec 18 18:43:00.066: INFO: Pod "busybox-readonly-false-a7793ce8-cd3b-4efc-bbd7-50cc8e0fa062": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00846375s
Dec 18 18:43:00.066: INFO: Pod "busybox-readonly-false-a7793ce8-cd3b-4efc-bbd7-50cc8e0fa062" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:43:00.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6425" for this suite.
Dec 18 18:43:06.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:43:06.103: INFO: namespace security-context-test-6425 deletion completed in 6.034694862s

• [SLOW TEST:14.061 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:43:06.103: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7426
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7426
I1218 18:43:06.128185      27 runners.go:184] Created replication controller with name: externalname-service, namespace: services-7426, replica count: 2
I1218 18:43:09.178618      27 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 18:43:12.178798      27 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 18 18:43:15.179: INFO: Creating new exec pod
I1218 18:43:15.178986      27 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 18 18:43:24.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=services-7426 execpodkp846 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 18 18:43:24.386: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 18 18:43:24.386: INFO: stdout: ""
Dec 18 18:43:24.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=services-7426 execpodkp846 -- /bin/sh -x -c nc -zv -t -w 2 10.105.76.18 80'
Dec 18 18:43:24.585: INFO: stderr: "+ nc -zv -t -w 2 10.105.76.18 80\nConnection to 10.105.76.18 80 port [tcp/http] succeeded!\n"
Dec 18 18:43:24.585: INFO: stdout: ""
Dec 18 18:43:24.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=services-7426 execpodkp846 -- /bin/sh -x -c nc -zv -t -w 2 192.168.204.40 32113'
Dec 18 18:43:24.779: INFO: stderr: "+ nc -zv -t -w 2 192.168.204.40 32113\nConnection to 192.168.204.40 32113 port [tcp/32113] succeeded!\n"
Dec 18 18:43:24.779: INFO: stdout: ""
Dec 18 18:43:24.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=services-7426 execpodkp846 -- /bin/sh -x -c nc -zv -t -w 2 192.168.204.183 32113'
Dec 18 18:43:24.995: INFO: stderr: "+ nc -zv -t -w 2 192.168.204.183 32113\nConnection to 192.168.204.183 32113 port [tcp/32113] succeeded!\n"
Dec 18 18:43:24.995: INFO: stdout: ""
Dec 18 18:43:24.996: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:43:25.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7426" for this suite.
Dec 18 18:43:31.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:43:31.041: INFO: namespace services-7426 deletion completed in 6.034622005s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:24.938 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:43:31.042: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 18:43:31.057: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02c9c078-dc19-44f9-8274-b175a6c060ce" in namespace "projected-7344" to be "success or failure"
Dec 18 18:43:31.058: INFO: Pod "downwardapi-volume-02c9c078-dc19-44f9-8274-b175a6c060ce": Phase="Pending", Reason="", readiness=false. Elapsed: 1.210559ms
Dec 18 18:43:33.060: INFO: Pod "downwardapi-volume-02c9c078-dc19-44f9-8274-b175a6c060ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002874053s
Dec 18 18:43:35.061: INFO: Pod "downwardapi-volume-02c9c078-dc19-44f9-8274-b175a6c060ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004364953s
Dec 18 18:43:37.063: INFO: Pod "downwardapi-volume-02c9c078-dc19-44f9-8274-b175a6c060ce": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006671527s
Dec 18 18:43:39.065: INFO: Pod "downwardapi-volume-02c9c078-dc19-44f9-8274-b175a6c060ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00817732s
STEP: Saw pod success
Dec 18 18:43:39.065: INFO: Pod "downwardapi-volume-02c9c078-dc19-44f9-8274-b175a6c060ce" satisfied condition "success or failure"
Dec 18 18:43:39.066: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-02c9c078-dc19-44f9-8274-b175a6c060ce container client-container: <nil>
STEP: delete the pod
Dec 18 18:43:39.074: INFO: Waiting for pod downwardapi-volume-02c9c078-dc19-44f9-8274-b175a6c060ce to disappear
Dec 18 18:43:39.075: INFO: Pod downwardapi-volume-02c9c078-dc19-44f9-8274-b175a6c060ce no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:43:39.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7344" for this suite.
Dec 18 18:43:45.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:43:45.112: INFO: namespace projected-7344 deletion completed in 6.035862558s

• [SLOW TEST:14.071 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:43:45.113: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-b051e0af-4a60-41c8-b2a0-af2adf03c07a
STEP: Creating a pod to test consume configMaps
Dec 18 18:43:45.129: INFO: Waiting up to 5m0s for pod "pod-configmaps-d6788e60-7207-4238-b036-6f1d885bd7b2" in namespace "configmap-6259" to be "success or failure"
Dec 18 18:43:45.130: INFO: Pod "pod-configmaps-d6788e60-7207-4238-b036-6f1d885bd7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 916.286µs
Dec 18 18:43:47.133: INFO: Pod "pod-configmaps-d6788e60-7207-4238-b036-6f1d885bd7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003304465s
Dec 18 18:43:49.134: INFO: Pod "pod-configmaps-d6788e60-7207-4238-b036-6f1d885bd7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004926217s
Dec 18 18:43:51.136: INFO: Pod "pod-configmaps-d6788e60-7207-4238-b036-6f1d885bd7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00648285s
Dec 18 18:43:53.137: INFO: Pod "pod-configmaps-d6788e60-7207-4238-b036-6f1d885bd7b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008081079s
STEP: Saw pod success
Dec 18 18:43:53.137: INFO: Pod "pod-configmaps-d6788e60-7207-4238-b036-6f1d885bd7b2" satisfied condition "success or failure"
Dec 18 18:43:53.139: INFO: Trying to get logs from node controller-1 pod pod-configmaps-d6788e60-7207-4238-b036-6f1d885bd7b2 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 18:43:53.153: INFO: Waiting for pod pod-configmaps-d6788e60-7207-4238-b036-6f1d885bd7b2 to disappear
Dec 18 18:43:53.154: INFO: Pod pod-configmaps-d6788e60-7207-4238-b036-6f1d885bd7b2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:43:53.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6259" for this suite.
Dec 18 18:43:59.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:43:59.189: INFO: namespace configmap-6259 deletion completed in 6.033646477s

• [SLOW TEST:14.077 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:43:59.189: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7082
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Dec 18 18:43:59.208: INFO: Found 0 stateful pods, waiting for 3
Dec 18 18:44:09.210: INFO: Found 2 stateful pods, waiting for 3
Dec 18 18:44:19.210: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 18:44:19.210: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 18:44:19.210: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 18 18:44:29.210: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 18:44:29.210: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 18:44:29.210: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 18 18:44:29.227: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 18 18:44:39.244: INFO: Updating stateful set ss2
Dec 18 18:44:39.247: INFO: Waiting for Pod statefulset-7082/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Dec 18 18:44:49.265: INFO: Found 2 stateful pods, waiting for 3
Dec 18 18:44:59.267: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 18:44:59.267: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 18:44:59.267: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 18 18:45:09.267: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 18:45:09.267: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 18:45:09.267: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 18 18:45:19.267: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 18:45:19.267: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 18:45:19.267: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 18 18:45:19.282: INFO: Updating stateful set ss2
Dec 18 18:45:19.285: INFO: Waiting for Pod statefulset-7082/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 18 18:45:29.301: INFO: Updating stateful set ss2
Dec 18 18:45:29.303: INFO: Waiting for StatefulSet statefulset-7082/ss2 to complete update
Dec 18 18:45:29.303: INFO: Waiting for Pod statefulset-7082/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 18 18:45:39.306: INFO: Waiting for StatefulSet statefulset-7082/ss2 to complete update
Dec 18 18:45:39.306: INFO: Waiting for Pod statefulset-7082/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 18 18:45:49.306: INFO: Deleting all statefulset in ns statefulset-7082
Dec 18 18:45:49.307: INFO: Scaling statefulset ss2 to 0
Dec 18 18:46:19.314: INFO: Waiting for statefulset status.replicas updated to 0
Dec 18 18:46:19.315: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:46:19.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7082" for this suite.
Dec 18 18:46:25.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:46:25.357: INFO: namespace statefulset-7082 deletion completed in 6.036565962s

• [SLOW TEST:146.168 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:46:25.357: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:46:25.370: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 18 18:46:25.372: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 18 18:46:30.374: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 18 18:46:34.378: INFO: Creating deployment "test-rolling-update-deployment"
Dec 18 18:46:34.379: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 18 18:46:34.382: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 18 18:46:36.385: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 18 18:46:36.386: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291594, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291594, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291594, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291594, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:46:38.388: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291594, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291594, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291594, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291594, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:46:40.388: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291594, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291594, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291594, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291594, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:46:42.388: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 18 18:46:42.392: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8952 /apis/apps/v1/namespaces/deployment-8952/deployments/test-rolling-update-deployment 9fde0f50-4722-40e9-881f-531b3526189b 68688 1 2019-12-18 18:46:34 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005a20568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-18 18:46:34 +0000 UTC,LastTransitionTime:2019-12-18 18:46:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2019-12-18 18:46:41 +0000 UTC,LastTransitionTime:2019-12-18 18:46:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 18 18:46:42.393: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-8952 /apis/apps/v1/namespaces/deployment-8952/replicasets/test-rolling-update-deployment-55d946486 eb5081cd-75b3-4fee-b6d1-88781122b732 68677 1 2019-12-18 18:46:34 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9fde0f50-4722-40e9-881f-531b3526189b 0xc005a20a60 0xc005a20a61}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005a20ac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 18 18:46:42.393: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 18 18:46:42.393: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8952 /apis/apps/v1/namespaces/deployment-8952/replicasets/test-rolling-update-controller 1f67790d-ae3c-4024-b3f1-dcb30942b094 68686 2 2019-12-18 18:46:25 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9fde0f50-4722-40e9-881f-531b3526189b 0xc005a20997 0xc005a20998}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005a209f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 18 18:46:42.395: INFO: Pod "test-rolling-update-deployment-55d946486-ktrfx" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-ktrfx test-rolling-update-deployment-55d946486- deployment-8952 /api/v1/namespaces/deployment-8952/pods/test-rolling-update-deployment-55d946486-ktrfx 2dd4fc21-e9e4-48ae-856e-05a3e2b2a786 68676 0 2019-12-18 18:46:34 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:172.16.192.95/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.95"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 eb5081cd-75b3-4fee-b6d1-88781122b732 0xc005a21100 0xc005a21101}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k464n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k464n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k464n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 18:46:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 18:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 18:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 18:46:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:172.16.192.95,StartTime:2019-12-18 18:46:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-18 18:46:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://23c73ae89fcea4272be20137492e99243bef11e0d957a54f5050a8ce79ce78a0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:46:42.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8952" for this suite.
Dec 18 18:46:48.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:46:48.431: INFO: namespace deployment-8952 deletion completed in 6.034510873s

• [SLOW TEST:23.074 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:46:48.431: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-tl4j
STEP: Creating a pod to test atomic-volume-subpath
Dec 18 18:46:48.450: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-tl4j" in namespace "subpath-5972" to be "success or failure"
Dec 18 18:46:48.451: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Pending", Reason="", readiness=false. Elapsed: 853.825µs
Dec 18 18:46:50.453: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002270011s
Dec 18 18:46:52.454: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003852354s
Dec 18 18:46:54.456: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00545598s
Dec 18 18:46:56.458: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Running", Reason="", readiness=true. Elapsed: 8.007226202s
Dec 18 18:46:58.459: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Running", Reason="", readiness=true. Elapsed: 10.009013823s
Dec 18 18:47:00.461: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Running", Reason="", readiness=true. Elapsed: 12.010572068s
Dec 18 18:47:02.463: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Running", Reason="", readiness=true. Elapsed: 14.012460471s
Dec 18 18:47:04.465: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Running", Reason="", readiness=true. Elapsed: 16.014329018s
Dec 18 18:47:06.467: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Running", Reason="", readiness=true. Elapsed: 18.016146164s
Dec 18 18:47:08.468: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Running", Reason="", readiness=true. Elapsed: 20.017753076s
Dec 18 18:47:10.470: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Running", Reason="", readiness=true. Elapsed: 22.0199216s
Dec 18 18:47:12.473: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Running", Reason="", readiness=true. Elapsed: 24.022044332s
Dec 18 18:47:14.475: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Running", Reason="", readiness=true. Elapsed: 26.024313865s
Dec 18 18:47:16.477: INFO: Pod "pod-subpath-test-downwardapi-tl4j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.026156309s
STEP: Saw pod success
Dec 18 18:47:16.477: INFO: Pod "pod-subpath-test-downwardapi-tl4j" satisfied condition "success or failure"
Dec 18 18:47:16.478: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-downwardapi-tl4j container test-container-subpath-downwardapi-tl4j: <nil>
STEP: delete the pod
Dec 18 18:47:16.493: INFO: Waiting for pod pod-subpath-test-downwardapi-tl4j to disappear
Dec 18 18:47:16.494: INFO: Pod pod-subpath-test-downwardapi-tl4j no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-tl4j
Dec 18 18:47:16.494: INFO: Deleting pod "pod-subpath-test-downwardapi-tl4j" in namespace "subpath-5972"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:47:16.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5972" for this suite.
Dec 18 18:47:22.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:47:22.531: INFO: namespace subpath-5972 deletion completed in 6.03388741s

• [SLOW TEST:34.100 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:47:22.531: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 18:47:22.921: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 18 18:47:24.925: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291642, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291642, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291642, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291642, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:47:26.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291642, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291642, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291642, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291642, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:47:28.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291642, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291642, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291642, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712291642, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 18:47:31.933: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:47:43.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8371" for this suite.
Dec 18 18:47:49.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:47:50.025: INFO: namespace webhook-8371 deletion completed in 6.037891495s
STEP: Destroying namespace "webhook-8371-markers" for this suite.
Dec 18 18:47:56.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:47:56.061: INFO: namespace webhook-8371-markers deletion completed in 6.035942094s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.534 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:47:56.065: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Dec 18 18:48:26.593: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1218 18:48:26.593277      27 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:48:26.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2835" for this suite.
Dec 18 18:48:32.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:48:32.629: INFO: namespace gc-2835 deletion completed in 6.03433073s

• [SLOW TEST:36.564 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:48:32.629: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 18:48:32.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-088e6c5e-9081-4136-91bb-f6960af624eb" in namespace "downward-api-1923" to be "success or failure"
Dec 18 18:48:32.646: INFO: Pod "downwardapi-volume-088e6c5e-9081-4136-91bb-f6960af624eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.018204ms
Dec 18 18:48:34.649: INFO: Pod "downwardapi-volume-088e6c5e-9081-4136-91bb-f6960af624eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003526093s
Dec 18 18:48:36.651: INFO: Pod "downwardapi-volume-088e6c5e-9081-4136-91bb-f6960af624eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005598625s
Dec 18 18:48:38.653: INFO: Pod "downwardapi-volume-088e6c5e-9081-4136-91bb-f6960af624eb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007280509s
Dec 18 18:48:40.655: INFO: Pod "downwardapi-volume-088e6c5e-9081-4136-91bb-f6960af624eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00919222s
STEP: Saw pod success
Dec 18 18:48:40.655: INFO: Pod "downwardapi-volume-088e6c5e-9081-4136-91bb-f6960af624eb" satisfied condition "success or failure"
Dec 18 18:48:40.656: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-088e6c5e-9081-4136-91bb-f6960af624eb container client-container: <nil>
STEP: delete the pod
Dec 18 18:48:40.662: INFO: Waiting for pod downwardapi-volume-088e6c5e-9081-4136-91bb-f6960af624eb to disappear
Dec 18 18:48:40.663: INFO: Pod downwardapi-volume-088e6c5e-9081-4136-91bb-f6960af624eb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:48:40.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1923" for this suite.
Dec 18 18:48:46.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:48:46.700: INFO: namespace downward-api-1923 deletion completed in 6.035490876s

• [SLOW TEST:14.071 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:48:46.701: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec 18 18:48:56.724: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W1218 18:48:56.724644      27 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 18 18:48:56.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6868" for this suite.
Dec 18 18:49:02.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:49:02.759: INFO: namespace gc-6868 deletion completed in 6.033146322s

• [SLOW TEST:16.058 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:49:02.759: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-pxw6
STEP: Creating a pod to test atomic-volume-subpath
Dec 18 18:49:02.776: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pxw6" in namespace "subpath-5832" to be "success or failure"
Dec 18 18:49:02.777: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Pending", Reason="", readiness=false. Elapsed: 882.488µs
Dec 18 18:49:04.779: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002607229s
Dec 18 18:49:06.781: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004149059s
Dec 18 18:49:08.783: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006141721s
Dec 18 18:49:10.784: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Running", Reason="", readiness=true. Elapsed: 8.00776437s
Dec 18 18:49:12.786: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Running", Reason="", readiness=true. Elapsed: 10.009458475s
Dec 18 18:49:14.788: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Running", Reason="", readiness=true. Elapsed: 12.011213495s
Dec 18 18:49:16.789: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Running", Reason="", readiness=true. Elapsed: 14.012797185s
Dec 18 18:49:18.791: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Running", Reason="", readiness=true. Elapsed: 16.014301303s
Dec 18 18:49:20.793: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Running", Reason="", readiness=true. Elapsed: 18.01623887s
Dec 18 18:49:22.794: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Running", Reason="", readiness=true. Elapsed: 20.017871046s
Dec 18 18:49:24.796: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Running", Reason="", readiness=true. Elapsed: 22.01947319s
Dec 18 18:49:26.798: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Running", Reason="", readiness=true. Elapsed: 24.021286527s
Dec 18 18:49:28.799: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Running", Reason="", readiness=true. Elapsed: 26.022856649s
Dec 18 18:49:30.801: INFO: Pod "pod-subpath-test-configmap-pxw6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.024399957s
STEP: Saw pod success
Dec 18 18:49:30.801: INFO: Pod "pod-subpath-test-configmap-pxw6" satisfied condition "success or failure"
Dec 18 18:49:30.802: INFO: Trying to get logs from node controller-0 pod pod-subpath-test-configmap-pxw6 container test-container-subpath-configmap-pxw6: <nil>
STEP: delete the pod
Dec 18 18:49:30.816: INFO: Waiting for pod pod-subpath-test-configmap-pxw6 to disappear
Dec 18 18:49:30.817: INFO: Pod pod-subpath-test-configmap-pxw6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pxw6
Dec 18 18:49:30.817: INFO: Deleting pod "pod-subpath-test-configmap-pxw6" in namespace "subpath-5832"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:49:30.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5832" for this suite.
Dec 18 18:49:36.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:49:36.853: INFO: namespace subpath-5832 deletion completed in 6.034100323s

• [SLOW TEST:34.094 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:49:36.853: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-639
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-639
Dec 18 18:49:36.872: INFO: Found 0 stateful pods, waiting for 1
Dec 18 18:49:46.874: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 18 18:49:46.880: INFO: Deleting all statefulset in ns statefulset-639
Dec 18 18:49:46.881: INFO: Scaling statefulset ss to 0
Dec 18 18:49:56.890: INFO: Waiting for statefulset status.replicas updated to 0
Dec 18 18:49:56.891: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:49:56.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-639" for this suite.
Dec 18 18:50:02.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:50:02.930: INFO: namespace statefulset-639 deletion completed in 6.033444473s

• [SLOW TEST:26.076 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:50:02.930: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 18 18:50:02.945: INFO: Waiting up to 5m0s for pod "downward-api-4f07442a-171a-43f8-8d68-90301e518462" in namespace "downward-api-3614" to be "success or failure"
Dec 18 18:50:02.946: INFO: Pod "downward-api-4f07442a-171a-43f8-8d68-90301e518462": Phase="Pending", Reason="", readiness=false. Elapsed: 1.495048ms
Dec 18 18:50:04.948: INFO: Pod "downward-api-4f07442a-171a-43f8-8d68-90301e518462": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003075302s
Dec 18 18:50:06.949: INFO: Pod "downward-api-4f07442a-171a-43f8-8d68-90301e518462": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004623229s
Dec 18 18:50:08.951: INFO: Pod "downward-api-4f07442a-171a-43f8-8d68-90301e518462": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006778058s
Dec 18 18:50:10.953: INFO: Pod "downward-api-4f07442a-171a-43f8-8d68-90301e518462": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008440702s
STEP: Saw pod success
Dec 18 18:50:10.953: INFO: Pod "downward-api-4f07442a-171a-43f8-8d68-90301e518462" satisfied condition "success or failure"
Dec 18 18:50:10.954: INFO: Trying to get logs from node controller-0 pod downward-api-4f07442a-171a-43f8-8d68-90301e518462 container dapi-container: <nil>
STEP: delete the pod
Dec 18 18:50:10.962: INFO: Waiting for pod downward-api-4f07442a-171a-43f8-8d68-90301e518462 to disappear
Dec 18 18:50:10.963: INFO: Pod downward-api-4f07442a-171a-43f8-8d68-90301e518462 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:50:10.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3614" for this suite.
Dec 18 18:50:16.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:50:16.999: INFO: namespace downward-api-3614 deletion completed in 6.035020926s

• [SLOW TEST:14.070 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:50:17.000: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 18 18:50:33.026: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 18 18:50:33.027: INFO: Pod pod-with-prestop-http-hook still exists
Dec 18 18:50:35.027: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 18 18:50:35.029: INFO: Pod pod-with-prestop-http-hook still exists
Dec 18 18:50:37.027: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 18 18:50:37.029: INFO: Pod pod-with-prestop-http-hook still exists
Dec 18 18:50:39.027: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 18 18:50:39.029: INFO: Pod pod-with-prestop-http-hook still exists
Dec 18 18:50:41.027: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 18 18:50:41.029: INFO: Pod pod-with-prestop-http-hook still exists
Dec 18 18:50:43.027: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 18 18:50:43.029: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:50:43.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1528" for this suite.
Dec 18 18:50:55.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:50:55.076: INFO: namespace container-lifecycle-hook-1528 deletion completed in 12.034783889s

• [SLOW TEST:38.077 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:50:55.077: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Dec 18 18:50:55.091: INFO: namespace kubectl-699
Dec 18 18:50:55.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-699'
Dec 18 18:50:55.285: INFO: stderr: ""
Dec 18 18:50:55.285: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 18 18:50:56.287: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 18:50:56.287: INFO: Found 0 / 1
Dec 18 18:50:57.287: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 18:50:57.287: INFO: Found 0 / 1
Dec 18 18:50:58.287: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 18:50:58.287: INFO: Found 0 / 1
Dec 18 18:50:59.287: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 18:50:59.287: INFO: Found 0 / 1
Dec 18 18:51:00.287: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 18:51:00.287: INFO: Found 0 / 1
Dec 18 18:51:01.287: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 18:51:01.287: INFO: Found 0 / 1
Dec 18 18:51:02.287: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 18:51:02.287: INFO: Found 1 / 1
Dec 18 18:51:02.287: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 18 18:51:02.289: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 18:51:02.289: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 18 18:51:02.289: INFO: wait on redis-master startup in kubectl-699 
Dec 18 18:51:02.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 logs redis-master-zdtnw redis-master --namespace=kubectl-699'
Dec 18 18:51:02.359: INFO: stderr: ""
Dec 18 18:51:02.359: INFO: stdout: "1:C 18 Dec 2019 18:51:01.857 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 18 Dec 2019 18:51:01.857 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 18 Dec 2019 18:51:01.857 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 18 Dec 2019 18:51:01.858 * Running mode=standalone, port=6379.\n1:M 18 Dec 2019 18:51:01.858 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 18 Dec 2019 18:51:01.858 # Server initialized\n1:M 18 Dec 2019 18:51:01.858 * Ready to accept connections\n"
STEP: exposing RC
Dec 18 18:51:02.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-699'
Dec 18 18:51:02.437: INFO: stderr: ""
Dec 18 18:51:02.437: INFO: stdout: "service/rm2 exposed\n"
Dec 18 18:51:02.438: INFO: Service rm2 in namespace kubectl-699 found.
STEP: exposing service
Dec 18 18:51:04.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-699'
Dec 18 18:51:04.513: INFO: stderr: ""
Dec 18 18:51:04.513: INFO: stdout: "service/rm3 exposed\n"
Dec 18 18:51:04.514: INFO: Service rm3 in namespace kubectl-699 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:51:06.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-699" for this suite.
Dec 18 18:51:34.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:51:34.553: INFO: namespace kubectl-699 deletion completed in 28.034937343s

• [SLOW TEST:39.477 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:51:34.554: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-c0e0f900-13b2-42d3-b0ee-1b821a45ccea
STEP: Creating configMap with name cm-test-opt-upd-bed873ae-8a34-4e2b-aef0-dd82caa39314
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c0e0f900-13b2-42d3-b0ee-1b821a45ccea
STEP: Updating configmap cm-test-opt-upd-bed873ae-8a34-4e2b-aef0-dd82caa39314
STEP: Creating configMap with name cm-test-opt-create-c9eef5cc-a93e-4522-8d83-24b4766217d3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:51:46.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2331" for this suite.
Dec 18 18:51:58.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:51:58.652: INFO: namespace configmap-2331 deletion completed in 12.036624771s

• [SLOW TEST:24.098 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:51:58.653: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 18:51:58.669: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b7862d23-7fae-4e8e-bbaa-176ed94fc53a" in namespace "projected-1875" to be "success or failure"
Dec 18 18:51:58.670: INFO: Pod "downwardapi-volume-b7862d23-7fae-4e8e-bbaa-176ed94fc53a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.298975ms
Dec 18 18:52:00.672: INFO: Pod "downwardapi-volume-b7862d23-7fae-4e8e-bbaa-176ed94fc53a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003058769s
Dec 18 18:52:02.673: INFO: Pod "downwardapi-volume-b7862d23-7fae-4e8e-bbaa-176ed94fc53a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004790152s
Dec 18 18:52:04.676: INFO: Pod "downwardapi-volume-b7862d23-7fae-4e8e-bbaa-176ed94fc53a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007138361s
Dec 18 18:52:06.678: INFO: Pod "downwardapi-volume-b7862d23-7fae-4e8e-bbaa-176ed94fc53a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009097384s
STEP: Saw pod success
Dec 18 18:52:06.678: INFO: Pod "downwardapi-volume-b7862d23-7fae-4e8e-bbaa-176ed94fc53a" satisfied condition "success or failure"
Dec 18 18:52:06.679: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-b7862d23-7fae-4e8e-bbaa-176ed94fc53a container client-container: <nil>
STEP: delete the pod
Dec 18 18:52:06.688: INFO: Waiting for pod downwardapi-volume-b7862d23-7fae-4e8e-bbaa-176ed94fc53a to disappear
Dec 18 18:52:06.689: INFO: Pod downwardapi-volume-b7862d23-7fae-4e8e-bbaa-176ed94fc53a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:52:06.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1875" for this suite.
Dec 18 18:52:12.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:52:12.724: INFO: namespace projected-1875 deletion completed in 6.033507423s

• [SLOW TEST:14.071 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:52:12.724: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-pm5j
STEP: Creating a pod to test atomic-volume-subpath
Dec 18 18:52:12.745: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pm5j" in namespace "subpath-2766" to be "success or failure"
Dec 18 18:52:12.746: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Pending", Reason="", readiness=false. Elapsed: 1.204684ms
Dec 18 18:52:14.748: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003030927s
Dec 18 18:52:16.750: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005054586s
Dec 18 18:52:18.751: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006681793s
Dec 18 18:52:20.754: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Running", Reason="", readiness=true. Elapsed: 8.009138636s
Dec 18 18:52:22.756: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Running", Reason="", readiness=true. Elapsed: 10.01100528s
Dec 18 18:52:24.758: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Running", Reason="", readiness=true. Elapsed: 12.01334887s
Dec 18 18:52:26.760: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Running", Reason="", readiness=true. Elapsed: 14.015004252s
Dec 18 18:52:28.762: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Running", Reason="", readiness=true. Elapsed: 16.017115387s
Dec 18 18:52:30.763: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Running", Reason="", readiness=true. Elapsed: 18.018686138s
Dec 18 18:52:32.765: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Running", Reason="", readiness=true. Elapsed: 20.020368885s
Dec 18 18:52:34.767: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Running", Reason="", readiness=true. Elapsed: 22.022073337s
Dec 18 18:52:36.769: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Running", Reason="", readiness=true. Elapsed: 24.024330856s
Dec 18 18:52:38.770: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Running", Reason="", readiness=true. Elapsed: 26.02586568s
Dec 18 18:52:40.772: INFO: Pod "pod-subpath-test-configmap-pm5j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.027552924s
STEP: Saw pod success
Dec 18 18:52:40.772: INFO: Pod "pod-subpath-test-configmap-pm5j" satisfied condition "success or failure"
Dec 18 18:52:40.773: INFO: Trying to get logs from node controller-0 pod pod-subpath-test-configmap-pm5j container test-container-subpath-configmap-pm5j: <nil>
STEP: delete the pod
Dec 18 18:52:40.784: INFO: Waiting for pod pod-subpath-test-configmap-pm5j to disappear
Dec 18 18:52:40.785: INFO: Pod pod-subpath-test-configmap-pm5j no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pm5j
Dec 18 18:52:40.785: INFO: Deleting pod "pod-subpath-test-configmap-pm5j" in namespace "subpath-2766"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:52:40.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2766" for this suite.
Dec 18 18:52:46.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:52:46.824: INFO: namespace subpath-2766 deletion completed in 6.036806189s

• [SLOW TEST:34.100 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:52:46.824: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-21e69b51-30b4-4b40-872e-4a5f95d5612c
STEP: Creating a pod to test consume secrets
Dec 18 18:52:46.852: INFO: Waiting up to 5m0s for pod "pod-secrets-80f2fbdd-d0c4-4340-b016-cc2211b798b0" in namespace "secrets-7376" to be "success or failure"
Dec 18 18:52:46.853: INFO: Pod "pod-secrets-80f2fbdd-d0c4-4340-b016-cc2211b798b0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.254348ms
Dec 18 18:52:48.855: INFO: Pod "pod-secrets-80f2fbdd-d0c4-4340-b016-cc2211b798b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00313582s
Dec 18 18:52:50.856: INFO: Pod "pod-secrets-80f2fbdd-d0c4-4340-b016-cc2211b798b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004723997s
Dec 18 18:52:52.858: INFO: Pod "pod-secrets-80f2fbdd-d0c4-4340-b016-cc2211b798b0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006267663s
Dec 18 18:52:54.859: INFO: Pod "pod-secrets-80f2fbdd-d0c4-4340-b016-cc2211b798b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007842228s
STEP: Saw pod success
Dec 18 18:52:54.860: INFO: Pod "pod-secrets-80f2fbdd-d0c4-4340-b016-cc2211b798b0" satisfied condition "success or failure"
Dec 18 18:52:54.861: INFO: Trying to get logs from node controller-1 pod pod-secrets-80f2fbdd-d0c4-4340-b016-cc2211b798b0 container secret-volume-test: <nil>
STEP: delete the pod
Dec 18 18:52:54.870: INFO: Waiting for pod pod-secrets-80f2fbdd-d0c4-4340-b016-cc2211b798b0 to disappear
Dec 18 18:52:54.871: INFO: Pod pod-secrets-80f2fbdd-d0c4-4340-b016-cc2211b798b0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:52:54.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7376" for this suite.
Dec 18 18:53:00.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:53:00.906: INFO: namespace secrets-7376 deletion completed in 6.033421786s
STEP: Destroying namespace "secret-namespace-3523" for this suite.
Dec 18 18:53:06.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:53:06.941: INFO: namespace secret-namespace-3523 deletion completed in 6.035050281s

• [SLOW TEST:20.117 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:53:06.941: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 18 18:53:06.955: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 18 18:53:06.960: INFO: Waiting for terminating namespaces to be deleted...
Dec 18 18:53:06.961: INFO: 
Logging pods the kubelet thinks is on node compute-0 before test
Dec 18 18:53:06.973: INFO: kube-multus-ds-amd64-fczk7 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.973: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 18:53:06.973: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-jzbct from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 18:53:06.973: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 18 18:53:06.973: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 18:53:06.973: INFO: kube-proxy-lq2c8 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.973: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 18 18:53:06.973: INFO: calico-node-ntnk8 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.973: INFO: 	Container calico-node ready: true, restart count 1
Dec 18 18:53:06.973: INFO: sonobuoy from sonobuoy started at 2019-12-18 18:17:38 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.973: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 18 18:53:06.973: INFO: kube-sriov-cni-ds-amd64-nlfxr from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.973: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 18:53:06.973: INFO: 
Logging pods the kubelet thinks is on node compute-1 before test
Dec 18 18:53:06.988: INFO: kube-sriov-cni-ds-amd64-v94kl from kube-system started at 2019-12-18 15:11:51 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.988: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 18:53:06.988: INFO: kube-proxy-w7hfz from kube-system started at 2019-12-18 15:11:52 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.988: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 18 18:53:06.988: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-49m5k from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 18:53:06.988: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 18 18:53:06.988: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 18:53:06.988: INFO: calico-node-6s2pv from kube-system started at 2019-12-18 15:11:51 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.988: INFO: 	Container calico-node ready: true, restart count 1
Dec 18 18:53:06.988: INFO: kube-multus-ds-amd64-trmzx from kube-system started at 2019-12-18 15:11:52 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.988: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 18:53:06.988: INFO: 
Logging pods the kubelet thinks is on node controller-0 before test
Dec 18 18:53:06.993: INFO: coredns-6bc668cd76-ks6bd from kube-system started at 2019-12-18 18:33:17 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container coredns ready: true, restart count 0
Dec 18 18:53:06.993: INFO: calico-node-7bzgw from kube-system started at 2019-12-18 14:17:53 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container calico-node ready: true, restart count 3
Dec 18 18:53:06.993: INFO: kube-multus-ds-amd64-plsvq from kube-system started at 2019-12-18 18:33:43 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 18:53:06.993: INFO: tiller-deploy-d6b59fcb-kj5hv from kube-system started at 2019-12-18 18:38:16 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container tiller ready: true, restart count 0
Dec 18 18:53:06.993: INFO: ceph-pools-audit-1576694700-9r62g from kube-system started at 2019-12-18 18:45:09 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Dec 18 18:53:06.993: INFO: ceph-pools-audit-1576694400-m2l46 from kube-system started at 2019-12-18 18:40:09 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Dec 18 18:53:06.993: INFO: kube-apiserver-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container kube-apiserver ready: true, restart count 2
Dec 18 18:53:06.993: INFO: rbd-provisioner-7484d49cf6-858lp from kube-system started at 2019-12-18 18:33:13 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container rbd-provisioner ready: true, restart count 0
Dec 18 18:53:06.993: INFO: kube-controller-manager-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 18 18:53:06.993: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-q7fcg from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 18 18:53:06.993: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 18:53:06.993: INFO: kube-scheduler-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 18 18:53:06.993: INFO: kube-proxy-hs98d from kube-system started at 2019-12-18 14:17:53 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container kube-proxy ready: true, restart count 1
Dec 18 18:53:06.993: INFO: kube-sriov-cni-ds-amd64-97g65 from kube-system started at 2019-12-18 18:33:43 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 18:53:06.993: INFO: calico-kube-controllers-855577b7b5-2gktm from kube-system started at 2019-12-18 18:38:16 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.993: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 18 18:53:06.993: INFO: 
Logging pods the kubelet thinks is on node controller-1 before test
Dec 18 18:53:06.998: INFO: kube-controller-manager-controller-1 from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.998: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 18 18:53:06.998: INFO: calico-node-8q8ss from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.998: INFO: 	Container calico-node ready: true, restart count 4
Dec 18 18:53:06.998: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-rz72p from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 18:53:06.998: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 18 18:53:06.998: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 18:53:06.998: INFO: kube-sriov-cni-ds-amd64-rb5tl from kube-system started at 2019-12-18 18:38:52 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.998: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 18:53:06.998: INFO: rbd-provisioner-7484d49cf6-p98d2 from kube-system started at 2019-12-18 18:38:56 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.998: INFO: 	Container rbd-provisioner ready: true, restart count 0
Dec 18 18:53:06.998: INFO: kube-apiserver-controller-1 from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.998: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 18 18:53:06.998: INFO: kube-proxy-fds2l from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.998: INFO: 	Container kube-proxy ready: true, restart count 1
Dec 18 18:53:06.998: INFO: kube-scheduler-controller-1 from kube-system started at 2019-12-18 15:22:52 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.998: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 18 18:53:06.998: INFO: kube-multus-ds-amd64-rvvrh from kube-system started at 2019-12-18 18:38:52 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.998: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 18:53:06.998: INFO: coredns-6bc668cd76-86sgt from kube-system started at 2019-12-18 18:38:56 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.998: INFO: 	Container coredns ready: true, restart count 0
Dec 18 18:53:06.998: INFO: sonobuoy-e2e-job-8824c3aaef6e4480 from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 18:53:06.998: INFO: 	Container e2e ready: true, restart count 0
Dec 18 18:53:06.998: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 18 18:53:06.998: INFO: ceph-pools-audit-1576695000-jbq2c from kube-system started at 2019-12-18 18:50:09 +0000 UTC (1 container statuses recorded)
Dec 18 18:53:06.998: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node compute-0
STEP: verifying the node has the label node compute-1
STEP: verifying the node has the label node controller-0
STEP: verifying the node has the label node controller-1
Dec 18 18:53:07.026: INFO: Pod calico-kube-controllers-855577b7b5-2gktm requesting resource cpu=0m on Node controller-0
Dec 18 18:53:07.026: INFO: Pod calico-node-6s2pv requesting resource cpu=250m on Node compute-1
Dec 18 18:53:07.026: INFO: Pod calico-node-7bzgw requesting resource cpu=250m on Node controller-0
Dec 18 18:53:07.026: INFO: Pod calico-node-8q8ss requesting resource cpu=250m on Node controller-1
Dec 18 18:53:07.026: INFO: Pod calico-node-ntnk8 requesting resource cpu=250m on Node compute-0
Dec 18 18:53:07.026: INFO: Pod coredns-6bc668cd76-86sgt requesting resource cpu=100m on Node controller-1
Dec 18 18:53:07.026: INFO: Pod coredns-6bc668cd76-ks6bd requesting resource cpu=100m on Node controller-0
Dec 18 18:53:07.026: INFO: Pod kube-apiserver-controller-0 requesting resource cpu=250m on Node controller-0
Dec 18 18:53:07.026: INFO: Pod kube-apiserver-controller-1 requesting resource cpu=250m on Node controller-1
Dec 18 18:53:07.026: INFO: Pod kube-controller-manager-controller-0 requesting resource cpu=200m on Node controller-0
Dec 18 18:53:07.026: INFO: Pod kube-controller-manager-controller-1 requesting resource cpu=200m on Node controller-1
Dec 18 18:53:07.026: INFO: Pod kube-multus-ds-amd64-fczk7 requesting resource cpu=100m on Node compute-0
Dec 18 18:53:07.026: INFO: Pod kube-multus-ds-amd64-plsvq requesting resource cpu=100m on Node controller-0
Dec 18 18:53:07.026: INFO: Pod kube-multus-ds-amd64-rvvrh requesting resource cpu=100m on Node controller-1
Dec 18 18:53:07.026: INFO: Pod kube-multus-ds-amd64-trmzx requesting resource cpu=100m on Node compute-1
Dec 18 18:53:07.026: INFO: Pod kube-proxy-fds2l requesting resource cpu=0m on Node controller-1
Dec 18 18:53:07.026: INFO: Pod kube-proxy-hs98d requesting resource cpu=0m on Node controller-0
Dec 18 18:53:07.026: INFO: Pod kube-proxy-lq2c8 requesting resource cpu=0m on Node compute-0
Dec 18 18:53:07.026: INFO: Pod kube-proxy-w7hfz requesting resource cpu=0m on Node compute-1
Dec 18 18:53:07.026: INFO: Pod kube-scheduler-controller-0 requesting resource cpu=100m on Node controller-0
Dec 18 18:53:07.026: INFO: Pod kube-scheduler-controller-1 requesting resource cpu=100m on Node controller-1
Dec 18 18:53:07.026: INFO: Pod kube-sriov-cni-ds-amd64-97g65 requesting resource cpu=100m on Node controller-0
Dec 18 18:53:07.026: INFO: Pod kube-sriov-cni-ds-amd64-nlfxr requesting resource cpu=100m on Node compute-0
Dec 18 18:53:07.026: INFO: Pod kube-sriov-cni-ds-amd64-rb5tl requesting resource cpu=100m on Node controller-1
Dec 18 18:53:07.026: INFO: Pod kube-sriov-cni-ds-amd64-v94kl requesting resource cpu=100m on Node compute-1
Dec 18 18:53:07.026: INFO: Pod rbd-provisioner-7484d49cf6-858lp requesting resource cpu=0m on Node controller-0
Dec 18 18:53:07.026: INFO: Pod rbd-provisioner-7484d49cf6-p98d2 requesting resource cpu=0m on Node controller-1
Dec 18 18:53:07.026: INFO: Pod tiller-deploy-d6b59fcb-kj5hv requesting resource cpu=0m on Node controller-0
Dec 18 18:53:07.026: INFO: Pod sonobuoy requesting resource cpu=0m on Node compute-0
Dec 18 18:53:07.026: INFO: Pod sonobuoy-e2e-job-8824c3aaef6e4480 requesting resource cpu=0m on Node controller-1
Dec 18 18:53:07.026: INFO: Pod sonobuoy-systemd-logs-daemon-set-c91b178dff064717-49m5k requesting resource cpu=0m on Node compute-1
Dec 18 18:53:07.026: INFO: Pod sonobuoy-systemd-logs-daemon-set-c91b178dff064717-jzbct requesting resource cpu=0m on Node compute-0
Dec 18 18:53:07.026: INFO: Pod sonobuoy-systemd-logs-daemon-set-c91b178dff064717-q7fcg requesting resource cpu=0m on Node controller-0
Dec 18 18:53:07.026: INFO: Pod sonobuoy-systemd-logs-daemon-set-c91b178dff064717-rz72p requesting resource cpu=0m on Node controller-1
STEP: Starting Pods to consume most of the cluster CPU.
Dec 18 18:53:07.026: INFO: Creating a pod which consumes cpu=12985m on Node compute-0
Dec 18 18:53:07.029: INFO: Creating a pod which consumes cpu=12985m on Node compute-1
Dec 18 18:53:07.030: INFO: Creating a pod which consumes cpu=13230m on Node controller-0
Dec 18 18:53:07.032: INFO: Creating a pod which consumes cpu=13230m on Node controller-1
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e372c87-653b-42ac-871a-d40bafbc1a98.15e18beb3e8853ff], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9301/filler-pod-4e372c87-653b-42ac-871a-d40bafbc1a98 to controller-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e372c87-653b-42ac-871a-d40bafbc1a98.15e18becbbc3f262], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e372c87-653b-42ac-871a-d40bafbc1a98.15e18becbd8b1fa4], Reason = [Created], Message = [Created container filler-pod-4e372c87-653b-42ac-871a-d40bafbc1a98]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e372c87-653b-42ac-871a-d40bafbc1a98.15e18becc5eb136f], Reason = [Started], Message = [Started container filler-pod-4e372c87-653b-42ac-871a-d40bafbc1a98]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9259e5d9-7488-41c1-abe6-cf9f10521534.15e18beb3e88cff9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9301/filler-pod-9259e5d9-7488-41c1-abe6-cf9f10521534 to controller-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9259e5d9-7488-41c1-abe6-cf9f10521534.15e18becbc593e63], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9259e5d9-7488-41c1-abe6-cf9f10521534.15e18becbe6ed67a], Reason = [Created], Message = [Created container filler-pod-9259e5d9-7488-41c1-abe6-cf9f10521534]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9259e5d9-7488-41c1-abe6-cf9f10521534.15e18becc75da368], Reason = [Started], Message = [Started container filler-pod-9259e5d9-7488-41c1-abe6-cf9f10521534]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b5beb75c-1f6d-4bdc-8e62-e9947dfc2728.15e18beb3e46404c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9301/filler-pod-b5beb75c-1f6d-4bdc-8e62-e9947dfc2728 to compute-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b5beb75c-1f6d-4bdc-8e62-e9947dfc2728.15e18becc96ea93e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b5beb75c-1f6d-4bdc-8e62-e9947dfc2728.15e18beccbe4f1f7], Reason = [Created], Message = [Created container filler-pod-b5beb75c-1f6d-4bdc-8e62-e9947dfc2728]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b5beb75c-1f6d-4bdc-8e62-e9947dfc2728.15e18becdb7ad01f], Reason = [Started], Message = [Started container filler-pod-b5beb75c-1f6d-4bdc-8e62-e9947dfc2728]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df10336a-a86a-412d-b02e-0718e9dc66f6.15e18beb3e5db7b3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9301/filler-pod-df10336a-a86a-412d-b02e-0718e9dc66f6 to compute-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df10336a-a86a-412d-b02e-0718e9dc66f6.15e18becc27bc455], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df10336a-a86a-412d-b02e-0718e9dc66f6.15e18becc4918f06], Reason = [Created], Message = [Created container filler-pod-df10336a-a86a-412d-b02e-0718e9dc66f6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df10336a-a86a-412d-b02e-0718e9dc66f6.15e18beccdb6f88e], Reason = [Started], Message = [Started container filler-pod-df10336a-a86a-412d-b02e-0718e9dc66f6]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15e18bed1bf62cbe], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node controller-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node compute-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node compute-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node controller-0
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:53:16.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9301" for this suite.
Dec 18 18:53:22.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:53:22.103: INFO: namespace sched-pred-9301 deletion completed in 6.035036808s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:15.161 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:53:22.103: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 18 18:53:22.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6041'
Dec 18 18:53:22.272: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 18 18:53:22.272: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Dec 18 18:53:22.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete deployment e2e-test-httpd-deployment --namespace=kubectl-6041'
Dec 18 18:53:22.339: INFO: stderr: ""
Dec 18 18:53:22.339: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:53:22.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6041" for this suite.
Dec 18 18:53:28.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:53:28.374: INFO: namespace kubectl-6041 deletion completed in 6.033415938s

• [SLOW TEST:6.272 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:53:28.375: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 18 18:53:28.388: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:53:37.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1158" for this suite.
Dec 18 18:53:43.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:53:43.677: INFO: namespace init-container-1158 deletion completed in 6.038402288s

• [SLOW TEST:15.302 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:53:43.677: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:53:54.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3882" for this suite.
Dec 18 18:54:00.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:54:00.744: INFO: namespace resourcequota-3882 deletion completed in 6.03524724s

• [SLOW TEST:17.067 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:54:00.744: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:54:00.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9905" for this suite.
Dec 18 18:54:06.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:54:06.800: INFO: namespace resourcequota-9905 deletion completed in 6.034304195s

• [SLOW TEST:6.056 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:54:06.800: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:54:06.814: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec 18 18:54:10.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-3428 create -f -'
Dec 18 18:54:10.637: INFO: stderr: ""
Dec 18 18:54:10.637: INFO: stdout: "e2e-test-crd-publish-openapi-126-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 18 18:54:10.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-3428 delete e2e-test-crd-publish-openapi-126-crds test-foo'
Dec 18 18:54:10.699: INFO: stderr: ""
Dec 18 18:54:10.699: INFO: stdout: "e2e-test-crd-publish-openapi-126-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 18 18:54:10.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-3428 apply -f -'
Dec 18 18:54:10.843: INFO: stderr: ""
Dec 18 18:54:10.843: INFO: stdout: "e2e-test-crd-publish-openapi-126-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 18 18:54:10.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-3428 delete e2e-test-crd-publish-openapi-126-crds test-foo'
Dec 18 18:54:10.909: INFO: stderr: ""
Dec 18 18:54:10.909: INFO: stdout: "e2e-test-crd-publish-openapi-126-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 18 18:54:10.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-3428 create -f -'
Dec 18 18:54:11.041: INFO: rc: 1
Dec 18 18:54:11.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-3428 apply -f -'
Dec 18 18:54:11.185: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec 18 18:54:11.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-3428 create -f -'
Dec 18 18:54:11.316: INFO: rc: 1
Dec 18 18:54:11.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-3428 apply -f -'
Dec 18 18:54:11.450: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 18 18:54:11.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 explain e2e-test-crd-publish-openapi-126-crds'
Dec 18 18:54:11.589: INFO: stderr: ""
Dec 18 18:54:11.589: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-126-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 18 18:54:11.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 explain e2e-test-crd-publish-openapi-126-crds.metadata'
Dec 18 18:54:11.734: INFO: stderr: ""
Dec 18 18:54:11.734: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-126-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 18 18:54:11.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 explain e2e-test-crd-publish-openapi-126-crds.spec'
Dec 18 18:54:11.872: INFO: stderr: ""
Dec 18 18:54:11.872: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-126-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 18 18:54:11.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 explain e2e-test-crd-publish-openapi-126-crds.spec.bars'
Dec 18 18:54:12.008: INFO: stderr: ""
Dec 18 18:54:12.008: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-126-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 18 18:54:12.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 explain e2e-test-crd-publish-openapi-126-crds.spec.bars2'
Dec 18 18:54:12.150: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:54:14.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3428" for this suite.
Dec 18 18:54:20.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:54:20.213: INFO: namespace crd-publish-openapi-3428 deletion completed in 6.034633254s

• [SLOW TEST:13.413 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:54:20.213: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:54:20.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 version'
Dec 18 18:54:20.285: INFO: stderr: ""
Dec 18 18:54:20.285: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:18:23Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:09:08Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:54:20.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1791" for this suite.
Dec 18 18:54:26.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:54:26.323: INFO: namespace kubectl-1791 deletion completed in 6.035683738s

• [SLOW TEST:6.109 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:54:26.323: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-984.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-984.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-984.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-984.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-984.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-984.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 18 18:54:34.355: INFO: DNS probes using dns-984/dns-test-cf8e3f3e-6bef-4d22-bd61-f254355c26a5 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:54:34.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-984" for this suite.
Dec 18 18:54:40.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:54:40.400: INFO: namespace dns-984 deletion completed in 6.034581259s

• [SLOW TEST:14.077 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:54:40.400: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 18:54:40.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2af92d32-a5f4-45a3-a3fb-6c4c60f5ead5" in namespace "downward-api-973" to be "success or failure"
Dec 18 18:54:40.416: INFO: Pod "downwardapi-volume-2af92d32-a5f4-45a3-a3fb-6c4c60f5ead5": Phase="Pending", Reason="", readiness=false. Elapsed: 875.279µs
Dec 18 18:54:42.418: INFO: Pod "downwardapi-volume-2af92d32-a5f4-45a3-a3fb-6c4c60f5ead5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002754527s
Dec 18 18:54:44.420: INFO: Pod "downwardapi-volume-2af92d32-a5f4-45a3-a3fb-6c4c60f5ead5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004313311s
Dec 18 18:54:46.421: INFO: Pod "downwardapi-volume-2af92d32-a5f4-45a3-a3fb-6c4c60f5ead5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006034109s
Dec 18 18:54:48.423: INFO: Pod "downwardapi-volume-2af92d32-a5f4-45a3-a3fb-6c4c60f5ead5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008057512s
STEP: Saw pod success
Dec 18 18:54:48.423: INFO: Pod "downwardapi-volume-2af92d32-a5f4-45a3-a3fb-6c4c60f5ead5" satisfied condition "success or failure"
Dec 18 18:54:48.424: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-2af92d32-a5f4-45a3-a3fb-6c4c60f5ead5 container client-container: <nil>
STEP: delete the pod
Dec 18 18:54:48.440: INFO: Waiting for pod downwardapi-volume-2af92d32-a5f4-45a3-a3fb-6c4c60f5ead5 to disappear
Dec 18 18:54:48.441: INFO: Pod downwardapi-volume-2af92d32-a5f4-45a3-a3fb-6c4c60f5ead5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:54:48.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-973" for this suite.
Dec 18 18:54:54.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:54:54.477: INFO: namespace downward-api-973 deletion completed in 6.034641869s

• [SLOW TEST:14.077 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:54:54.477: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 18:54:54.937: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 18 18:54:56.942: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292094, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292094, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292094, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292094, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:54:58.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292094, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292094, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292094, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292094, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:55:00.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292094, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292094, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292094, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292094, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 18:55:03.949: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:55:03.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1980" for this suite.
Dec 18 18:55:09.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:55:10.008: INFO: namespace webhook-1980 deletion completed in 6.033846786s
STEP: Destroying namespace "webhook-1980-markers" for this suite.
Dec 18 18:55:16.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:55:16.044: INFO: namespace webhook-1980-markers deletion completed in 6.035195905s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.571 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:55:16.048: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 18:55:16.064: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82ab4a62-7fc0-46c0-a552-9ebe67c17b01" in namespace "downward-api-5526" to be "success or failure"
Dec 18 18:55:16.064: INFO: Pod "downwardapi-volume-82ab4a62-7fc0-46c0-a552-9ebe67c17b01": Phase="Pending", Reason="", readiness=false. Elapsed: 905.897µs
Dec 18 18:55:18.066: INFO: Pod "downwardapi-volume-82ab4a62-7fc0-46c0-a552-9ebe67c17b01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002521618s
Dec 18 18:55:20.068: INFO: Pod "downwardapi-volume-82ab4a62-7fc0-46c0-a552-9ebe67c17b01": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004302295s
Dec 18 18:55:22.070: INFO: Pod "downwardapi-volume-82ab4a62-7fc0-46c0-a552-9ebe67c17b01": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006512992s
Dec 18 18:55:24.072: INFO: Pod "downwardapi-volume-82ab4a62-7fc0-46c0-a552-9ebe67c17b01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008558832s
STEP: Saw pod success
Dec 18 18:55:24.072: INFO: Pod "downwardapi-volume-82ab4a62-7fc0-46c0-a552-9ebe67c17b01" satisfied condition "success or failure"
Dec 18 18:55:24.073: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-82ab4a62-7fc0-46c0-a552-9ebe67c17b01 container client-container: <nil>
STEP: delete the pod
Dec 18 18:55:24.081: INFO: Waiting for pod downwardapi-volume-82ab4a62-7fc0-46c0-a552-9ebe67c17b01 to disappear
Dec 18 18:55:24.082: INFO: Pod downwardapi-volume-82ab4a62-7fc0-46c0-a552-9ebe67c17b01 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:55:24.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5526" for this suite.
Dec 18 18:55:30.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:55:30.118: INFO: namespace downward-api-5526 deletion completed in 6.034182639s

• [SLOW TEST:14.070 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:55:30.118: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-a1822f47-4ed6-4dff-9390-b1df7f1554ee
STEP: Creating a pod to test consume secrets
Dec 18 18:55:30.134: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0f3836a4-8a52-4271-b1cf-f52c973586f6" in namespace "projected-746" to be "success or failure"
Dec 18 18:55:30.135: INFO: Pod "pod-projected-secrets-0f3836a4-8a52-4271-b1cf-f52c973586f6": Phase="Pending", Reason="", readiness=false. Elapsed: 847.364µs
Dec 18 18:55:32.137: INFO: Pod "pod-projected-secrets-0f3836a4-8a52-4271-b1cf-f52c973586f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002467395s
Dec 18 18:55:34.139: INFO: Pod "pod-projected-secrets-0f3836a4-8a52-4271-b1cf-f52c973586f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004324747s
Dec 18 18:55:36.141: INFO: Pod "pod-projected-secrets-0f3836a4-8a52-4271-b1cf-f52c973586f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006077087s
Dec 18 18:55:38.143: INFO: Pod "pod-projected-secrets-0f3836a4-8a52-4271-b1cf-f52c973586f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008320126s
STEP: Saw pod success
Dec 18 18:55:38.143: INFO: Pod "pod-projected-secrets-0f3836a4-8a52-4271-b1cf-f52c973586f6" satisfied condition "success or failure"
Dec 18 18:55:38.144: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-0f3836a4-8a52-4271-b1cf-f52c973586f6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 18 18:55:38.158: INFO: Waiting for pod pod-projected-secrets-0f3836a4-8a52-4271-b1cf-f52c973586f6 to disappear
Dec 18 18:55:38.159: INFO: Pod pod-projected-secrets-0f3836a4-8a52-4271-b1cf-f52c973586f6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:55:38.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-746" for this suite.
Dec 18 18:55:44.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:55:44.194: INFO: namespace projected-746 deletion completed in 6.033198241s

• [SLOW TEST:14.076 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:55:44.194: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 18 18:55:52.722: INFO: Successfully updated pod "annotationupdate1d11cca8-04cb-456a-bc68-4806e2f1b9d0"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:55:56.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1365" for this suite.
Dec 18 18:56:08.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:56:08.774: INFO: namespace downward-api-1365 deletion completed in 12.035067627s

• [SLOW TEST:24.580 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:56:08.774: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Dec 18 18:56:08.787: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-207688788 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:56:08.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3095" for this suite.
Dec 18 18:56:14.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:56:14.880: INFO: namespace kubectl-3095 deletion completed in 6.033400121s

• [SLOW TEST:6.106 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:56:14.881: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 18:56:14.894: INFO: Creating deployment "test-recreate-deployment"
Dec 18 18:56:14.895: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 18 18:56:14.898: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 18 18:56:16.902: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 18 18:56:16.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292174, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292174, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292174, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292174, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:56:18.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292174, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292174, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292174, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292174, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:56:20.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292174, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292174, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292174, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292174, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:56:22.905: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 18 18:56:22.908: INFO: Updating deployment test-recreate-deployment
Dec 18 18:56:22.908: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 18 18:56:22.931: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-338 /apis/apps/v1/namespaces/deployment-338/deployments/test-recreate-deployment d4f77627-ef39-48a1-89af-7c66143597e1 71860 2 2019-12-18 18:56:14 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0019eae68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-18 18:56:22 +0000 UTC,LastTransitionTime:2019-12-18 18:56:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2019-12-18 18:56:22 +0000 UTC,LastTransitionTime:2019-12-18 18:56:14 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 18 18:56:22.932: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-338 /apis/apps/v1/namespaces/deployment-338/replicasets/test-recreate-deployment-5f94c574ff 8d5cc479-a562-470d-9367-9f4ecbae4189 71858 1 2019-12-18 18:56:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d4f77627-ef39-48a1-89af-7c66143597e1 0xc0019eb407 0xc0019eb408}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0019eb478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 18 18:56:22.932: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 18 18:56:22.932: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-338 /apis/apps/v1/namespaces/deployment-338/replicasets/test-recreate-deployment-68fc85c7bb 891b8ff7-27d0-4071-af52-de0f004ce411 71849 2 2019-12-18 18:56:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d4f77627-ef39-48a1-89af-7c66143597e1 0xc0019eb4f7 0xc0019eb4f8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0019eb558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 18 18:56:22.934: INFO: Pod "test-recreate-deployment-5f94c574ff-n5lvh" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-n5lvh test-recreate-deployment-5f94c574ff- deployment-338 /api/v1/namespaces/deployment-338/pods/test-recreate-deployment-5f94c574ff-n5lvh 91ffb1c9-cc3e-4b36-9c6f-0d7eff046823 71861 0 2019-12-18 18:56:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 8d5cc479-a562-470d-9367-9f4ecbae4189 0xc0019ebc57 0xc0019ebc58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nvqbv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nvqbv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nvqbv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 18:56:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 18:56:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 18:56:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 18:56:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:,StartTime:2019-12-18 18:56:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:56:22.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-338" for this suite.
Dec 18 18:56:28.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:56:28.969: INFO: namespace deployment-338 deletion completed in 6.033978228s

• [SLOW TEST:14.089 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:56:28.970: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 18:56:29.524: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 18 18:56:31.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292189, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292189, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292189, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292189, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:56:33.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292189, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292189, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292189, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292189, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:56:35.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292189, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292189, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292189, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292189, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 18:56:38.536: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:56:38.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2261" for this suite.
Dec 18 18:56:44.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:56:44.643: INFO: namespace webhook-2261 deletion completed in 6.034542019s
STEP: Destroying namespace "webhook-2261-markers" for this suite.
Dec 18 18:56:50.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:56:50.677: INFO: namespace webhook-2261-markers deletion completed in 6.034355138s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.712 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:56:50.682: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-b5e11625-2aab-4ea8-b511-392a4d1f3290
STEP: Creating a pod to test consume configMaps
Dec 18 18:56:50.698: INFO: Waiting up to 5m0s for pod "pod-configmaps-a9fdc9a6-2453-43a1-a64b-d8356e9f0b98" in namespace "configmap-6937" to be "success or failure"
Dec 18 18:56:50.699: INFO: Pod "pod-configmaps-a9fdc9a6-2453-43a1-a64b-d8356e9f0b98": Phase="Pending", Reason="", readiness=false. Elapsed: 830.108µs
Dec 18 18:56:52.701: INFO: Pod "pod-configmaps-a9fdc9a6-2453-43a1-a64b-d8356e9f0b98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002541939s
Dec 18 18:56:54.703: INFO: Pod "pod-configmaps-a9fdc9a6-2453-43a1-a64b-d8356e9f0b98": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004802696s
Dec 18 18:56:56.705: INFO: Pod "pod-configmaps-a9fdc9a6-2453-43a1-a64b-d8356e9f0b98": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00702191s
Dec 18 18:56:58.707: INFO: Pod "pod-configmaps-a9fdc9a6-2453-43a1-a64b-d8356e9f0b98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008710337s
STEP: Saw pod success
Dec 18 18:56:58.707: INFO: Pod "pod-configmaps-a9fdc9a6-2453-43a1-a64b-d8356e9f0b98" satisfied condition "success or failure"
Dec 18 18:56:58.708: INFO: Trying to get logs from node controller-1 pod pod-configmaps-a9fdc9a6-2453-43a1-a64b-d8356e9f0b98 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 18:56:58.716: INFO: Waiting for pod pod-configmaps-a9fdc9a6-2453-43a1-a64b-d8356e9f0b98 to disappear
Dec 18 18:56:58.717: INFO: Pod pod-configmaps-a9fdc9a6-2453-43a1-a64b-d8356e9f0b98 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:56:58.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6937" for this suite.
Dec 18 18:57:04.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:57:04.756: INFO: namespace configmap-6937 deletion completed in 6.036815661s

• [SLOW TEST:14.074 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:57:04.756: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 18 18:57:04.772: INFO: Waiting up to 5m0s for pod "pod-c74cb783-1bcb-4a8a-babf-beaffbb73200" in namespace "emptydir-8360" to be "success or failure"
Dec 18 18:57:04.773: INFO: Pod "pod-c74cb783-1bcb-4a8a-babf-beaffbb73200": Phase="Pending", Reason="", readiness=false. Elapsed: 1.391712ms
Dec 18 18:57:06.774: INFO: Pod "pod-c74cb783-1bcb-4a8a-babf-beaffbb73200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002922863s
Dec 18 18:57:08.777: INFO: Pod "pod-c74cb783-1bcb-4a8a-babf-beaffbb73200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005657504s
Dec 18 18:57:10.779: INFO: Pod "pod-c74cb783-1bcb-4a8a-babf-beaffbb73200": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007219378s
Dec 18 18:57:12.781: INFO: Pod "pod-c74cb783-1bcb-4a8a-babf-beaffbb73200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009329229s
STEP: Saw pod success
Dec 18 18:57:12.781: INFO: Pod "pod-c74cb783-1bcb-4a8a-babf-beaffbb73200" satisfied condition "success or failure"
Dec 18 18:57:12.782: INFO: Trying to get logs from node controller-1 pod pod-c74cb783-1bcb-4a8a-babf-beaffbb73200 container test-container: <nil>
STEP: delete the pod
Dec 18 18:57:12.789: INFO: Waiting for pod pod-c74cb783-1bcb-4a8a-babf-beaffbb73200 to disappear
Dec 18 18:57:12.790: INFO: Pod pod-c74cb783-1bcb-4a8a-babf-beaffbb73200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:57:12.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8360" for this suite.
Dec 18 18:57:18.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:57:18.826: INFO: namespace emptydir-8360 deletion completed in 6.034088947s

• [SLOW TEST:14.070 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:57:18.826: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:57:26.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7165" for this suite.
Dec 18 18:57:32.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:57:32.891: INFO: namespace emptydir-wrapper-7165 deletion completed in 6.03312605s

• [SLOW TEST:14.065 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:57:32.892: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2930
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 18 18:57:32.904: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 18 18:58:06.933: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.103.161:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2930 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 18:58:06.933: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:58:07.062: INFO: Found all expected endpoints: [netserver-0]
Dec 18 18:58:07.064: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.166.142:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2930 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 18:58:07.064: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:58:07.206: INFO: Found all expected endpoints: [netserver-1]
Dec 18 18:58:07.208: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.154.45:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2930 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 18:58:07.208: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:58:07.352: INFO: Found all expected endpoints: [netserver-2]
Dec 18 18:58:07.354: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.192.110:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2930 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 18:58:07.354: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 18:58:07.495: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:58:07.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2930" for this suite.
Dec 18 18:58:19.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:58:19.531: INFO: namespace pod-network-test-2930 deletion completed in 12.03454417s

• [SLOW TEST:46.640 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:58:19.532: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Dec 18 18:58:19.547: INFO: Waiting up to 5m0s for pod "client-containers-b5abada8-d2fd-41d4-bc76-cc8a39375324" in namespace "containers-2717" to be "success or failure"
Dec 18 18:58:19.548: INFO: Pod "client-containers-b5abada8-d2fd-41d4-bc76-cc8a39375324": Phase="Pending", Reason="", readiness=false. Elapsed: 833.48µs
Dec 18 18:58:21.550: INFO: Pod "client-containers-b5abada8-d2fd-41d4-bc76-cc8a39375324": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002516981s
Dec 18 18:58:23.551: INFO: Pod "client-containers-b5abada8-d2fd-41d4-bc76-cc8a39375324": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004239667s
Dec 18 18:58:25.553: INFO: Pod "client-containers-b5abada8-d2fd-41d4-bc76-cc8a39375324": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005983064s
Dec 18 18:58:27.555: INFO: Pod "client-containers-b5abada8-d2fd-41d4-bc76-cc8a39375324": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008044985s
STEP: Saw pod success
Dec 18 18:58:27.555: INFO: Pod "client-containers-b5abada8-d2fd-41d4-bc76-cc8a39375324" satisfied condition "success or failure"
Dec 18 18:58:27.556: INFO: Trying to get logs from node controller-0 pod client-containers-b5abada8-d2fd-41d4-bc76-cc8a39375324 container test-container: <nil>
STEP: delete the pod
Dec 18 18:58:27.571: INFO: Waiting for pod client-containers-b5abada8-d2fd-41d4-bc76-cc8a39375324 to disappear
Dec 18 18:58:27.572: INFO: Pod client-containers-b5abada8-d2fd-41d4-bc76-cc8a39375324 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:58:27.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2717" for this suite.
Dec 18 18:58:33.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:58:33.607: INFO: namespace containers-2717 deletion completed in 6.033737413s

• [SLOW TEST:14.076 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:58:33.608: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 18:58:34.221: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 18 18:58:36.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292314, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292314, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292314, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292314, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:58:38.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292314, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292314, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292314, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292314, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 18:58:40.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292314, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292314, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292314, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292314, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 18:58:43.232: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:58:43.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7206" for this suite.
Dec 18 18:58:49.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:58:49.288: INFO: namespace webhook-7206 deletion completed in 6.037109136s
STEP: Destroying namespace "webhook-7206-markers" for this suite.
Dec 18 18:58:55.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:58:55.321: INFO: namespace webhook-7206-markers deletion completed in 6.033157079s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.718 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:58:55.326: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6234.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6234.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6234.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6234.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6234.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6234.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6234.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6234.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6234.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6234.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 18 18:59:03.348: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:03.350: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:03.353: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:03.357: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:03.358: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:03.363: INFO: Lookups using dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6234.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local]

Dec 18 18:59:08.366: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:08.367: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:08.373: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:08.374: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:08.379: INFO: Lookups using dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local]

Dec 18 18:59:13.366: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:13.367: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:13.374: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:13.375: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:13.380: INFO: Lookups using dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local]

Dec 18 18:59:18.365: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:18.367: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:18.373: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:18.375: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:18.380: INFO: Lookups using dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local]

Dec 18 18:59:23.366: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:23.368: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:23.375: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:23.376: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:23.381: INFO: Lookups using dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local]

Dec 18 18:59:28.366: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:28.367: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:28.373: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:28.375: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local from pod dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828: the server could not find the requested resource (get pods dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828)
Dec 18 18:59:28.380: INFO: Lookups using dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6234.svc.cluster.local]

Dec 18 18:59:33.380: INFO: DNS probes using dns-6234/dns-test-e5d9d21d-8c81-4b43-bf66-bfad5ca66828 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:59:33.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6234" for this suite.
Dec 18 18:59:39.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:59:39.425: INFO: namespace dns-6234 deletion completed in 6.034088676s

• [SLOW TEST:44.099 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:59:39.425: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9488.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9488.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 18 18:59:47.457: INFO: DNS probes using dns-9488/dns-test-8cadb60a-0c46-4daa-9e78-360db457e21f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 18:59:47.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9488" for this suite.
Dec 18 18:59:53.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 18:59:53.498: INFO: namespace dns-9488 deletion completed in 6.035094136s

• [SLOW TEST:14.073 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 18:59:53.498: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 18:59:53.514: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6863b734-2d7f-4459-ac40-66982f93fcef" in namespace "downward-api-1159" to be "success or failure"
Dec 18 18:59:53.516: INFO: Pod "downwardapi-volume-6863b734-2d7f-4459-ac40-66982f93fcef": Phase="Pending", Reason="", readiness=false. Elapsed: 1.640446ms
Dec 18 18:59:55.517: INFO: Pod "downwardapi-volume-6863b734-2d7f-4459-ac40-66982f93fcef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003466266s
Dec 18 18:59:57.519: INFO: Pod "downwardapi-volume-6863b734-2d7f-4459-ac40-66982f93fcef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005275829s
Dec 18 18:59:59.522: INFO: Pod "downwardapi-volume-6863b734-2d7f-4459-ac40-66982f93fcef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00755043s
Dec 18 19:00:01.523: INFO: Pod "downwardapi-volume-6863b734-2d7f-4459-ac40-66982f93fcef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009185559s
STEP: Saw pod success
Dec 18 19:00:01.523: INFO: Pod "downwardapi-volume-6863b734-2d7f-4459-ac40-66982f93fcef" satisfied condition "success or failure"
Dec 18 19:00:01.524: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-6863b734-2d7f-4459-ac40-66982f93fcef container client-container: <nil>
STEP: delete the pod
Dec 18 19:00:01.541: INFO: Waiting for pod downwardapi-volume-6863b734-2d7f-4459-ac40-66982f93fcef to disappear
Dec 18 19:00:01.541: INFO: Pod downwardapi-volume-6863b734-2d7f-4459-ac40-66982f93fcef no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:00:01.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1159" for this suite.
Dec 18 19:00:07.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:00:07.577: INFO: namespace downward-api-1159 deletion completed in 6.03428191s

• [SLOW TEST:14.079 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:00:07.578: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 18 19:00:07.604: INFO: Number of nodes with available pods: 0
Dec 18 19:00:07.604: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:08.607: INFO: Number of nodes with available pods: 0
Dec 18 19:00:08.607: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:09.607: INFO: Number of nodes with available pods: 0
Dec 18 19:00:09.607: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:10.608: INFO: Number of nodes with available pods: 0
Dec 18 19:00:10.608: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:11.608: INFO: Number of nodes with available pods: 0
Dec 18 19:00:11.608: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:12.607: INFO: Number of nodes with available pods: 0
Dec 18 19:00:12.607: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:13.607: INFO: Number of nodes with available pods: 0
Dec 18 19:00:13.607: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:14.608: INFO: Number of nodes with available pods: 1
Dec 18 19:00:14.608: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:15.609: INFO: Number of nodes with available pods: 4
Dec 18 19:00:15.609: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 18 19:00:15.616: INFO: Number of nodes with available pods: 3
Dec 18 19:00:15.616: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:16.620: INFO: Number of nodes with available pods: 3
Dec 18 19:00:16.620: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:17.619: INFO: Number of nodes with available pods: 3
Dec 18 19:00:17.619: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:18.620: INFO: Number of nodes with available pods: 3
Dec 18 19:00:18.620: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:19.619: INFO: Number of nodes with available pods: 3
Dec 18 19:00:19.619: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:20.620: INFO: Number of nodes with available pods: 3
Dec 18 19:00:20.620: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:21.619: INFO: Number of nodes with available pods: 3
Dec 18 19:00:21.619: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:22.619: INFO: Number of nodes with available pods: 3
Dec 18 19:00:22.620: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:23.620: INFO: Number of nodes with available pods: 3
Dec 18 19:00:23.620: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:24.620: INFO: Number of nodes with available pods: 3
Dec 18 19:00:24.620: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:25.620: INFO: Number of nodes with available pods: 3
Dec 18 19:00:25.620: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:26.620: INFO: Number of nodes with available pods: 3
Dec 18 19:00:26.620: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:27.620: INFO: Number of nodes with available pods: 3
Dec 18 19:00:27.620: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:28.620: INFO: Number of nodes with available pods: 3
Dec 18 19:00:28.620: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:29.619: INFO: Number of nodes with available pods: 3
Dec 18 19:00:29.619: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:30.619: INFO: Number of nodes with available pods: 3
Dec 18 19:00:30.619: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:31.620: INFO: Number of nodes with available pods: 3
Dec 18 19:00:31.620: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:00:32.619: INFO: Number of nodes with available pods: 4
Dec 18 19:00:32.619: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8415, will wait for the garbage collector to delete the pods
Dec 18 19:00:32.674: INFO: Deleting DaemonSet.extensions daemon-set took: 2.081046ms
Dec 18 19:00:32.774: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.169071ms
Dec 18 19:00:43.975: INFO: Number of nodes with available pods: 0
Dec 18 19:00:43.975: INFO: Number of running nodes: 0, number of available pods: 0
Dec 18 19:00:43.976: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8415/daemonsets","resourceVersion":"73418"},"items":null}

Dec 18 19:00:43.977: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8415/pods","resourceVersion":"73418"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:00:43.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8415" for this suite.
Dec 18 19:00:49.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:00:50.019: INFO: namespace daemonsets-8415 deletion completed in 6.034072246s

• [SLOW TEST:42.441 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:00:50.019: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Dec 18 19:00:51.046: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1218 19:00:51.046843      27 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:00:51.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6444" for this suite.
Dec 18 19:00:57.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:00:57.083: INFO: namespace gc-6444 deletion completed in 6.034982618s

• [SLOW TEST:7.064 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:00:57.083: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6610.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6610.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6610.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6610.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 18 19:01:05.108: INFO: DNS probes using dns-test-c59c1ed6-3de5-4521-aea1-9e07486de374 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6610.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6610.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6610.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6610.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 18 19:01:13.123: INFO: File wheezy_udp@dns-test-service-3.dns-6610.svc.cluster.local from pod  dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 18 19:01:13.124: INFO: File jessie_udp@dns-test-service-3.dns-6610.svc.cluster.local from pod  dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 18 19:01:13.124: INFO: Lookups using dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 failed for: [wheezy_udp@dns-test-service-3.dns-6610.svc.cluster.local jessie_udp@dns-test-service-3.dns-6610.svc.cluster.local]

Dec 18 19:01:18.127: INFO: File wheezy_udp@dns-test-service-3.dns-6610.svc.cluster.local from pod  dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 18 19:01:18.128: INFO: File jessie_udp@dns-test-service-3.dns-6610.svc.cluster.local from pod  dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 18 19:01:18.128: INFO: Lookups using dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 failed for: [wheezy_udp@dns-test-service-3.dns-6610.svc.cluster.local jessie_udp@dns-test-service-3.dns-6610.svc.cluster.local]

Dec 18 19:01:23.127: INFO: File wheezy_udp@dns-test-service-3.dns-6610.svc.cluster.local from pod  dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 18 19:01:23.128: INFO: File jessie_udp@dns-test-service-3.dns-6610.svc.cluster.local from pod  dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 18 19:01:23.128: INFO: Lookups using dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 failed for: [wheezy_udp@dns-test-service-3.dns-6610.svc.cluster.local jessie_udp@dns-test-service-3.dns-6610.svc.cluster.local]

Dec 18 19:01:28.126: INFO: File wheezy_udp@dns-test-service-3.dns-6610.svc.cluster.local from pod  dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 18 19:01:28.128: INFO: File jessie_udp@dns-test-service-3.dns-6610.svc.cluster.local from pod  dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 18 19:01:28.128: INFO: Lookups using dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 failed for: [wheezy_udp@dns-test-service-3.dns-6610.svc.cluster.local jessie_udp@dns-test-service-3.dns-6610.svc.cluster.local]

Dec 18 19:01:33.127: INFO: File wheezy_udp@dns-test-service-3.dns-6610.svc.cluster.local from pod  dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 18 19:01:33.128: INFO: File jessie_udp@dns-test-service-3.dns-6610.svc.cluster.local from pod  dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 18 19:01:33.128: INFO: Lookups using dns-6610/dns-test-77bae8bc-22c7-41af-a835-094da059e239 failed for: [wheezy_udp@dns-test-service-3.dns-6610.svc.cluster.local jessie_udp@dns-test-service-3.dns-6610.svc.cluster.local]

Dec 18 19:01:38.128: INFO: DNS probes using dns-test-77bae8bc-22c7-41af-a835-094da059e239 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6610.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6610.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6610.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6610.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 18 19:01:46.157: INFO: DNS probes using dns-test-3d070982-4139-4a56-8da9-8b2b11cf00e5 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:01:46.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6610" for this suite.
Dec 18 19:01:52.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:01:52.203: INFO: namespace dns-6610 deletion completed in 6.033907845s

• [SLOW TEST:55.120 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:01:52.203: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:02:27.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3726" for this suite.
Dec 18 19:02:33.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:02:33.284: INFO: namespace namespaces-3726 deletion completed in 6.034144015s
STEP: Destroying namespace "nsdeletetest-9339" for this suite.
Dec 18 19:02:33.285: INFO: Namespace nsdeletetest-9339 was already deleted
STEP: Destroying namespace "nsdeletetest-749" for this suite.
Dec 18 19:02:39.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:02:39.319: INFO: namespace nsdeletetest-749 deletion completed in 6.034203054s

• [SLOW TEST:47.117 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:02:39.320: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Dec 18 19:02:39.336: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1047" to be "success or failure"
Dec 18 19:02:39.337: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 883.856µs
Dec 18 19:02:41.339: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002813641s
Dec 18 19:02:43.340: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004354915s
Dec 18 19:02:45.342: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006006079s
Dec 18 19:02:47.344: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008233122s
STEP: Saw pod success
Dec 18 19:02:47.344: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec 18 19:02:47.345: INFO: Trying to get logs from node controller-0 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec 18 19:02:47.358: INFO: Waiting for pod pod-host-path-test to disappear
Dec 18 19:02:47.360: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:02:47.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1047" for this suite.
Dec 18 19:02:53.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:02:53.397: INFO: namespace hostpath-1047 deletion completed in 6.034907538s

• [SLOW TEST:14.077 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:02:53.397: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 18 19:02:53.413: INFO: Waiting up to 5m0s for pod "pod-65709be1-5116-4d1e-b7cd-384ff405f628" in namespace "emptydir-3606" to be "success or failure"
Dec 18 19:02:53.415: INFO: Pod "pod-65709be1-5116-4d1e-b7cd-384ff405f628": Phase="Pending", Reason="", readiness=false. Elapsed: 1.772843ms
Dec 18 19:02:55.417: INFO: Pod "pod-65709be1-5116-4d1e-b7cd-384ff405f628": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003343767s
Dec 18 19:02:57.419: INFO: Pod "pod-65709be1-5116-4d1e-b7cd-384ff405f628": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005167663s
Dec 18 19:02:59.421: INFO: Pod "pod-65709be1-5116-4d1e-b7cd-384ff405f628": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007542454s
Dec 18 19:03:01.423: INFO: Pod "pod-65709be1-5116-4d1e-b7cd-384ff405f628": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009143182s
STEP: Saw pod success
Dec 18 19:03:01.423: INFO: Pod "pod-65709be1-5116-4d1e-b7cd-384ff405f628" satisfied condition "success or failure"
Dec 18 19:03:01.424: INFO: Trying to get logs from node controller-1 pod pod-65709be1-5116-4d1e-b7cd-384ff405f628 container test-container: <nil>
STEP: delete the pod
Dec 18 19:03:01.440: INFO: Waiting for pod pod-65709be1-5116-4d1e-b7cd-384ff405f628 to disappear
Dec 18 19:03:01.441: INFO: Pod pod-65709be1-5116-4d1e-b7cd-384ff405f628 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:03:01.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3606" for this suite.
Dec 18 19:03:07.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:03:07.479: INFO: namespace emptydir-3606 deletion completed in 6.035768909s

• [SLOW TEST:14.081 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:03:07.479: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 19:03:07.493: INFO: Creating deployment "webserver-deployment"
Dec 18 19:03:07.494: INFO: Waiting for observed generation 1
Dec 18 19:03:09.497: INFO: Waiting for all required pods to come up
Dec 18 19:03:09.499: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 18 19:03:17.503: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 18 19:03:17.505: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 18 19:03:17.508: INFO: Updating deployment webserver-deployment
Dec 18 19:03:17.508: INFO: Waiting for observed generation 2
Dec 18 19:03:19.511: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 18 19:03:19.512: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 18 19:03:19.513: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 18 19:03:19.516: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 18 19:03:19.516: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 18 19:03:19.517: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 18 19:03:19.519: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 18 19:03:19.519: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 18 19:03:19.521: INFO: Updating deployment webserver-deployment
Dec 18 19:03:19.521: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 18 19:03:19.523: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 18 19:03:19.524: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 18 19:03:21.528: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8621 /apis/apps/v1/namespaces/deployment-8621/deployments/webserver-deployment 431876af-8c6b-4640-a340-611974da6f54 74574 3 2019-12-18 19:03:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004f8cf98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-18 19:03:19 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2019-12-18 19:03:19 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 18 19:03:21.530: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-8621 /apis/apps/v1/namespaces/deployment-8621/replicasets/webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 74550 3 2019-12-18 19:03:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 431876af-8c6b-4640-a340-611974da6f54 0xc004f8d4b7 0xc004f8d4b8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004f8d528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 18 19:03:21.530: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 18 19:03:21.530: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-8621 /apis/apps/v1/namespaces/deployment-8621/replicasets/webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 74573 3 2019-12-18 19:03:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 431876af-8c6b-4640-a340-611974da6f54 0xc004f8d3f7 0xc004f8d3f8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004f8d458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 18 19:03:21.533: INFO: Pod "webserver-deployment-595b5b9587-5g2j8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5g2j8 webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-5g2j8 7ab03060-ce3b-4041-90d0-b81941db0aee 74386 0 2019-12-18 19:03:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.166.153/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.153"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc004f8da27 0xc004f8da28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.3,PodIP:172.16.166.153,StartTime:2019-12-18 19:03:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-18 19:03:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://37706a8cdf8152fe4d39027528c8ea37ba78817193e90b134cfa3b4b67d2d62d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.533: INFO: Pod "webserver-deployment-595b5b9587-6jvz4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6jvz4 webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-6jvz4 60d891c9-9817-47c0-8647-2675a66cfbe1 74555 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc004f8dba7 0xc004f8dba8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.3,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.533: INFO: Pod "webserver-deployment-595b5b9587-7bxsd" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7bxsd webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-7bxsd 2463ba18-e8fc-433a-bae5-fc9bfd0224ee 74371 0 2019-12-18 19:03:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.103.163/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.103.163"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc004f8dd07 0xc004f8dd08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:compute-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.183,PodIP:172.16.103.163,StartTime:2019-12-18 19:03:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-18 19:03:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f2dd6e9988b0fc487782b85c0ba133ca0f5aa1e0cddeded536358b958079a0a4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.103.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.533: INFO: Pod "webserver-deployment-595b5b9587-8jsdh" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8jsdh webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-8jsdh 9db75ecb-d335-47c6-b293-ab8027bfe1e2 74377 0 2019-12-18 19:03:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.192.123/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.123"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc004f8de87 0xc004f8de88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:172.16.192.123,StartTime:2019-12-18 19:03:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-18 19:03:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e6ae5e85f8949543d7d62ab85c36f0bd1593fde6983ce7fcea06106e9a6b9d1a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.533: INFO: Pod "webserver-deployment-595b5b9587-9s7qh" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9s7qh webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-9s7qh 598190b8-ca07-4574-aa25-eb870251b140 74560 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036fe007 0xc0036fe008}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.533: INFO: Pod "webserver-deployment-595b5b9587-dm5mr" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dm5mr webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-dm5mr afc796f2-4ce7-4147-a307-fb714f98a62f 74557 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036fe817 0xc0036fe818}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:compute-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.40,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.533: INFO: Pod "webserver-deployment-595b5b9587-f57tj" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-f57tj webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-f57tj e1cab6af-344c-45ed-8f2c-3d3e1ab087a8 74389 0 2019-12-18 19:03:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.166.150/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.150"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036fec57 0xc0036fec58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.3,PodIP:172.16.166.150,StartTime:2019-12-18 19:03:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-18 19:03:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://18604f3413bac07d0cb862baefd2489d0685674f0918e997591e8cc727ec4192,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.534: INFO: Pod "webserver-deployment-595b5b9587-hcffm" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hcffm webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-hcffm 80d3e4bf-42c5-4451-b770-a88e1ed540df 74383 0 2019-12-18 19:03:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.166.151/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.151"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036fef27 0xc0036fef28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.3,PodIP:172.16.166.151,StartTime:2019-12-18 19:03:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-18 19:03:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5d94fad3536598ac8f9fea332f88d43565c6ce740cc2d56db2d6d0ba5966a073,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.534: INFO: Pod "webserver-deployment-595b5b9587-hjtvk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hjtvk webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-hjtvk dff79df2-cc7d-47e6-9382-f2c9426b7e12 74567 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036ff0a7 0xc0036ff0a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:compute-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.40,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.534: INFO: Pod "webserver-deployment-595b5b9587-lf4bf" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lf4bf webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-lf4bf 10c829d1-8744-4c34-92e9-e83744a4bb9a 74380 0 2019-12-18 19:03:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.192.119/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.119"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036ff207 0xc0036ff208}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:172.16.192.119,StartTime:2019-12-18 19:03:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-18 19:03:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e87b7dd1ab3bfb4bd9ce88de1d483123d8eae70c9a35a333bd644e23d84e5373,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.534: INFO: Pod "webserver-deployment-595b5b9587-lfkxj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lfkxj webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-lfkxj 45ed778c-63f4-46e0-96d8-baede9a0372c 74566 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036ff387 0xc0036ff388}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:compute-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.40,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.534: INFO: Pod "webserver-deployment-595b5b9587-qjq2g" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qjq2g webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-qjq2g a68d34c1-d7b4-48f2-aa6b-46cfe0c6befc 74564 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036ff4e7 0xc0036ff4e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:compute-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.40,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.534: INFO: Pod "webserver-deployment-595b5b9587-qksvw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qksvw webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-qksvw d2a4d63f-8da7-4976-b99e-8320232e6ce9 74561 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036ff647 0xc0036ff648}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.3,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.534: INFO: Pod "webserver-deployment-595b5b9587-rlpg9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rlpg9 webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-rlpg9 ea52662e-5402-4072-8a65-c2b8763fd244 74374 0 2019-12-18 19:03:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.192.121/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.121"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036ff7a7 0xc0036ff7a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:172.16.192.121,StartTime:2019-12-18 19:03:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-18 19:03:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8b97d2734fd2bdf43239e394359d2ea274a3a9e57b47447ac0eb482e5aa329bb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.534: INFO: Pod "webserver-deployment-595b5b9587-vb9hj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vb9hj webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-vb9hj 5bdab93f-62ac-4ec3-a435-7e6a2ec4588a 74571 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036ff927 0xc0036ff928}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:compute-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.183,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.535: INFO: Pod "webserver-deployment-595b5b9587-vbxs4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vbxs4 webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-vbxs4 373b31b0-687b-462e-87a5-427678367597 74562 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036ffa87 0xc0036ffa88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.535: INFO: Pod "webserver-deployment-595b5b9587-wcjfg" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wcjfg webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-wcjfg 66e23dd9-b4a4-4828-8240-5e1b8cc72bce 74402 0 2019-12-18 19:03:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.16.103.164/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.103.164"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036ffbe7 0xc0036ffbe8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:compute-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.183,PodIP:172.16.103.164,StartTime:2019-12-18 19:03:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-18 19:03:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://7452b14c4067d7fce18ad1b7422849f1860256c1e8cc74f7ce002cc79035f7a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.103.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.535: INFO: Pod "webserver-deployment-595b5b9587-xdjzt" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xdjzt webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-xdjzt 9a6ed70b-f8da-4d6c-8baa-523f47074b48 74578 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036ffd67 0xc0036ffd68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.3,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.535: INFO: Pod "webserver-deployment-595b5b9587-zr6r9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zr6r9 webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-zr6r9 7bd320c5-c384-4c74-83e1-e2aee85c6cd1 74568 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc0036ffec7 0xc0036ffec8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:compute-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.183,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.535: INFO: Pod "webserver-deployment-595b5b9587-ztknf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ztknf webserver-deployment-595b5b9587- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-595b5b9587-ztknf 37363b45-8a88-4cca-94b9-6f15a0f28cc7 74552 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 b98ad903-05d2-40c7-9647-b1b86cabb89a 0xc00131e047 0xc00131e048}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.535: INFO: Pod "webserver-deployment-c7997dcc8-45p47" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-45p47 webserver-deployment-c7997dcc8- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-c7997dcc8-45p47 ce9113b6-6fa0-42fe-a232-1194486e72d4 74565 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 0xc00131e1a7 0xc00131e1a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:compute-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.40,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.535: INFO: Pod "webserver-deployment-c7997dcc8-6nn7g" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-6nn7g webserver-deployment-c7997dcc8- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-c7997dcc8-6nn7g bb241094-4fc9-4090-8292-3c1b02109df2 74563 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 0xc00131e647 0xc00131e648}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.536: INFO: Pod "webserver-deployment-c7997dcc8-cn2xv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cn2xv webserver-deployment-c7997dcc8- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-c7997dcc8-cn2xv 02d1d20e-14fb-4f35-8353-6532b5c0d905 74570 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 0xc00131e927 0xc00131e928}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:compute-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.183,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.536: INFO: Pod "webserver-deployment-c7997dcc8-dz24h" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-dz24h webserver-deployment-c7997dcc8- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-c7997dcc8-dz24h 340804f8-f07b-4beb-9328-0842885b450a 74559 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 0xc00131eaa7 0xc00131eaa8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.3,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.536: INFO: Pod "webserver-deployment-c7997dcc8-j78gx" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-j78gx webserver-deployment-c7997dcc8- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-c7997dcc8-j78gx e9d6676e-87cb-4050-9984-25c66df9fdaf 74444 0 2019-12-18 19:03:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 0xc00131ec37 0xc00131ec38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.3,PodIP:,StartTime:2019-12-18 19:03:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.536: INFO: Pod "webserver-deployment-c7997dcc8-lstsf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lstsf webserver-deployment-c7997dcc8- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-c7997dcc8-lstsf 9120cfe3-9aad-4b79-9eb8-e7346a1c7bfb 74556 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 0xc00131edb7 0xc00131edb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.536: INFO: Pod "webserver-deployment-c7997dcc8-n4pr9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-n4pr9 webserver-deployment-c7997dcc8- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-c7997dcc8-n4pr9 3f7fd5cb-78e0-4cf2-84f6-2dcbaefd103f 74558 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 0xc00131ef37 0xc00131ef38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.3,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.536: INFO: Pod "webserver-deployment-c7997dcc8-pwvbc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-pwvbc webserver-deployment-c7997dcc8- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-c7997dcc8-pwvbc c4b0bd28-05f1-4748-95cb-d048a89e3ae6 74545 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 0xc00131f0b7 0xc00131f0b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.3,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.536: INFO: Pod "webserver-deployment-c7997dcc8-qhsnq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qhsnq webserver-deployment-c7997dcc8- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-c7997dcc8-qhsnq 54c749f2-6275-43c9-95b4-54c07643e79d 74464 0 2019-12-18 19:03:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 0xc00131f237 0xc00131f238}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:compute-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.40,PodIP:,StartTime:2019-12-18 19:03:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.536: INFO: Pod "webserver-deployment-c7997dcc8-w8jvk" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-w8jvk webserver-deployment-c7997dcc8- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-c7997dcc8-w8jvk 1c6e312e-b902-42a7-8cc0-a3ad230fb18e 74460 0 2019-12-18 19:03:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 0xc00131f3b7 0xc00131f3b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:,StartTime:2019-12-18 19:03:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.536: INFO: Pod "webserver-deployment-c7997dcc8-wqxwn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wqxwn webserver-deployment-c7997dcc8- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-c7997dcc8-wqxwn 0878d600-8b87-4f1a-859f-44f0865ca0ce 74445 0 2019-12-18 19:03:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 0xc00131f587 0xc00131f588}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:,StartTime:2019-12-18 19:03:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.537: INFO: Pod "webserver-deployment-c7997dcc8-x5smv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-x5smv webserver-deployment-c7997dcc8- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-c7997dcc8-x5smv 951fe581-5490-4a8a-8ec4-3967d9383005 74455 0 2019-12-18 19:03:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 0xc00131f707 0xc00131f708}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:compute-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.183,PodIP:,StartTime:2019-12-18 19:03:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:03:21.537: INFO: Pod "webserver-deployment-c7997dcc8-zxz5d" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zxz5d webserver-deployment-c7997dcc8- deployment-8621 /api/v1/namespaces/deployment-8621/pods/webserver-deployment-c7997dcc8-zxz5d 1d8c1d44-4134-441a-bf71-d1da13bc0f1b 74572 0 2019-12-18 19:03:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 86592904-30f2-465f-892d-d880717ea2ba 0xc00131f887 0xc00131f888}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j5d6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:compute-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.183,PodIP:,StartTime:2019-12-18 19:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:03:21.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8621" for this suite.
Dec 18 19:03:27.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:03:27.573: INFO: namespace deployment-8621 deletion completed in 6.034788894s

• [SLOW TEST:20.094 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:03:27.573: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 18 19:03:35.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec pod-sharedvolume-d0810b0a-92eb-45fe-ae93-cd51d64dd957 -c busybox-main-container --namespace=emptydir-7100 -- cat /usr/share/volumeshare/shareddata.txt'
Dec 18 19:03:35.792: INFO: stderr: ""
Dec 18 19:03:35.792: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:03:35.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7100" for this suite.
Dec 18 19:03:41.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:03:41.829: INFO: namespace emptydir-7100 deletion completed in 6.033941701s

• [SLOW TEST:14.255 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:03:41.829: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 18 19:03:50.353: INFO: Successfully updated pod "pod-update-activedeadlineseconds-70d2a492-7bab-49ea-9894-3090b9c15736"
Dec 18 19:03:50.353: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-70d2a492-7bab-49ea-9894-3090b9c15736" in namespace "pods-5602" to be "terminated due to deadline exceeded"
Dec 18 19:03:50.355: INFO: Pod "pod-update-activedeadlineseconds-70d2a492-7bab-49ea-9894-3090b9c15736": Phase="Running", Reason="", readiness=true. Elapsed: 1.394646ms
Dec 18 19:03:52.356: INFO: Pod "pod-update-activedeadlineseconds-70d2a492-7bab-49ea-9894-3090b9c15736": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.002959744s
Dec 18 19:03:52.356: INFO: Pod "pod-update-activedeadlineseconds-70d2a492-7bab-49ea-9894-3090b9c15736" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:03:52.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5602" for this suite.
Dec 18 19:03:58.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:03:58.393: INFO: namespace pods-5602 deletion completed in 6.035011397s

• [SLOW TEST:16.564 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:03:58.394: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 19:03:59.050: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 18 19:04:01.055: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292639, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292639, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292639, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292639, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 19:04:03.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292639, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292639, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292639, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292639, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 19:04:05.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292639, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292639, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292639, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292639, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 19:04:08.063: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:04:08.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8150" for this suite.
Dec 18 19:04:14.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:04:14.126: INFO: namespace webhook-8150 deletion completed in 6.03447097s
STEP: Destroying namespace "webhook-8150-markers" for this suite.
Dec 18 19:04:20.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:04:20.161: INFO: namespace webhook-8150-markers deletion completed in 6.03528835s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.772 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:04:20.166: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:04:20.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8719" for this suite.
Dec 18 19:04:26.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:04:26.216: INFO: namespace services-8719 deletion completed in 6.034851463s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.051 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:04:26.217: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 18 19:04:26.232: INFO: Waiting up to 5m0s for pod "pod-b6f516f7-3551-469a-b3e1-cc699400cc6c" in namespace "emptydir-5298" to be "success or failure"
Dec 18 19:04:26.234: INFO: Pod "pod-b6f516f7-3551-469a-b3e1-cc699400cc6c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.416777ms
Dec 18 19:04:28.235: INFO: Pod "pod-b6f516f7-3551-469a-b3e1-cc699400cc6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002981715s
Dec 18 19:04:30.237: INFO: Pod "pod-b6f516f7-3551-469a-b3e1-cc699400cc6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004593037s
Dec 18 19:04:32.238: INFO: Pod "pod-b6f516f7-3551-469a-b3e1-cc699400cc6c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00609328s
Dec 18 19:04:34.240: INFO: Pod "pod-b6f516f7-3551-469a-b3e1-cc699400cc6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007619644s
STEP: Saw pod success
Dec 18 19:04:34.240: INFO: Pod "pod-b6f516f7-3551-469a-b3e1-cc699400cc6c" satisfied condition "success or failure"
Dec 18 19:04:34.241: INFO: Trying to get logs from node controller-0 pod pod-b6f516f7-3551-469a-b3e1-cc699400cc6c container test-container: <nil>
STEP: delete the pod
Dec 18 19:04:34.254: INFO: Waiting for pod pod-b6f516f7-3551-469a-b3e1-cc699400cc6c to disappear
Dec 18 19:04:34.255: INFO: Pod pod-b6f516f7-3551-469a-b3e1-cc699400cc6c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:04:34.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5298" for this suite.
Dec 18 19:04:40.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:04:40.291: INFO: namespace emptydir-5298 deletion completed in 6.034385815s

• [SLOW TEST:14.074 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:04:40.291: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 19:04:41.078: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 18 19:04:43.082: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292681, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292681, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 19:04:45.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292681, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292681, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 19:04:47.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292681, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292681, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 19:04:50.091: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 19:04:50.093: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3246-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:04:51.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3123" for this suite.
Dec 18 19:04:57.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:04:57.425: INFO: namespace webhook-3123 deletion completed in 6.035951993s
STEP: Destroying namespace "webhook-3123-markers" for this suite.
Dec 18 19:05:03.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:05:03.459: INFO: namespace webhook-3123-markers deletion completed in 6.034527627s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.172 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:05:03.463: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 18 19:05:03.479: INFO: Waiting up to 5m0s for pod "downward-api-1fb6c792-8f62-4499-b75c-987cedb9c812" in namespace "downward-api-2493" to be "success or failure"
Dec 18 19:05:03.480: INFO: Pod "downward-api-1fb6c792-8f62-4499-b75c-987cedb9c812": Phase="Pending", Reason="", readiness=false. Elapsed: 1.457271ms
Dec 18 19:05:05.482: INFO: Pod "downward-api-1fb6c792-8f62-4499-b75c-987cedb9c812": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002910992s
Dec 18 19:05:07.484: INFO: Pod "downward-api-1fb6c792-8f62-4499-b75c-987cedb9c812": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00522396s
Dec 18 19:05:09.486: INFO: Pod "downward-api-1fb6c792-8f62-4499-b75c-987cedb9c812": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007615908s
Dec 18 19:05:11.488: INFO: Pod "downward-api-1fb6c792-8f62-4499-b75c-987cedb9c812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009682619s
STEP: Saw pod success
Dec 18 19:05:11.488: INFO: Pod "downward-api-1fb6c792-8f62-4499-b75c-987cedb9c812" satisfied condition "success or failure"
Dec 18 19:05:11.490: INFO: Trying to get logs from node controller-0 pod downward-api-1fb6c792-8f62-4499-b75c-987cedb9c812 container dapi-container: <nil>
STEP: delete the pod
Dec 18 19:05:11.498: INFO: Waiting for pod downward-api-1fb6c792-8f62-4499-b75c-987cedb9c812 to disappear
Dec 18 19:05:11.499: INFO: Pod downward-api-1fb6c792-8f62-4499-b75c-987cedb9c812 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:05:11.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2493" for this suite.
Dec 18 19:05:17.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:05:17.537: INFO: namespace downward-api-2493 deletion completed in 6.03633904s

• [SLOW TEST:14.074 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:05:17.537: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Dec 18 19:05:17.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-1803'
Dec 18 19:05:17.799: INFO: stderr: ""
Dec 18 19:05:17.799: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 18 19:05:17.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1803'
Dec 18 19:05:17.865: INFO: stderr: ""
Dec 18 19:05:17.865: INFO: stdout: "update-demo-nautilus-nnrwf update-demo-nautilus-xdl44 "
Dec 18 19:05:17.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-nnrwf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1803'
Dec 18 19:05:17.929: INFO: stderr: ""
Dec 18 19:05:17.929: INFO: stdout: ""
Dec 18 19:05:17.929: INFO: update-demo-nautilus-nnrwf is created but not running
Dec 18 19:05:22.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1803'
Dec 18 19:05:22.996: INFO: stderr: ""
Dec 18 19:05:22.996: INFO: stdout: "update-demo-nautilus-nnrwf update-demo-nautilus-xdl44 "
Dec 18 19:05:22.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-nnrwf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1803'
Dec 18 19:05:23.055: INFO: stderr: ""
Dec 18 19:05:23.055: INFO: stdout: ""
Dec 18 19:05:23.055: INFO: update-demo-nautilus-nnrwf is created but not running
Dec 18 19:05:28.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1803'
Dec 18 19:05:28.125: INFO: stderr: ""
Dec 18 19:05:28.125: INFO: stdout: "update-demo-nautilus-nnrwf update-demo-nautilus-xdl44 "
Dec 18 19:05:28.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-nnrwf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1803'
Dec 18 19:05:28.187: INFO: stderr: ""
Dec 18 19:05:28.187: INFO: stdout: "true"
Dec 18 19:05:28.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-nnrwf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1803'
Dec 18 19:05:28.249: INFO: stderr: ""
Dec 18 19:05:28.249: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 18 19:05:28.249: INFO: validating pod update-demo-nautilus-nnrwf
Dec 18 19:05:28.252: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 18 19:05:28.252: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 18 19:05:28.252: INFO: update-demo-nautilus-nnrwf is verified up and running
Dec 18 19:05:28.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-xdl44 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1803'
Dec 18 19:05:28.313: INFO: stderr: ""
Dec 18 19:05:28.313: INFO: stdout: "true"
Dec 18 19:05:28.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-xdl44 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1803'
Dec 18 19:05:28.374: INFO: stderr: ""
Dec 18 19:05:28.374: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 18 19:05:28.374: INFO: validating pod update-demo-nautilus-xdl44
Dec 18 19:05:28.376: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 18 19:05:28.376: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 18 19:05:28.376: INFO: update-demo-nautilus-xdl44 is verified up and running
STEP: using delete to clean up resources
Dec 18 19:05:28.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete --grace-period=0 --force -f - --namespace=kubectl-1803'
Dec 18 19:05:28.438: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 18 19:05:28.438: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 18 19:05:28.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1803'
Dec 18 19:05:28.507: INFO: stderr: "No resources found in kubectl-1803 namespace.\n"
Dec 18 19:05:28.507: INFO: stdout: ""
Dec 18 19:05:28.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -l name=update-demo --namespace=kubectl-1803 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 18 19:05:28.572: INFO: stderr: ""
Dec 18 19:05:28.572: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:05:28.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1803" for this suite.
Dec 18 19:05:34.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:05:34.610: INFO: namespace kubectl-1803 deletion completed in 6.035945848s

• [SLOW TEST:17.073 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:05:34.610: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 19:05:35.143: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 18 19:05:37.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292735, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292735, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292735, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292735, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 19:05:39.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292735, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292735, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292735, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292735, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 19:05:41.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292735, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292735, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292735, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712292735, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 19:05:44.157: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 19:05:44.158: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8560-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:05:45.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3425" for this suite.
Dec 18 19:05:51.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:05:51.255: INFO: namespace webhook-3425 deletion completed in 6.033847733s
STEP: Destroying namespace "webhook-3425-markers" for this suite.
Dec 18 19:05:57.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:05:57.293: INFO: namespace webhook-3425-markers deletion completed in 6.038284623s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:22.688 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:05:57.299: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-7428/configmap-test-135071b2-11dc-4986-adce-ea400604c9b2
STEP: Creating a pod to test consume configMaps
Dec 18 19:05:57.316: INFO: Waiting up to 5m0s for pod "pod-configmaps-a17e76ee-9878-4dd1-b284-4eae2f3ead67" in namespace "configmap-7428" to be "success or failure"
Dec 18 19:05:57.317: INFO: Pod "pod-configmaps-a17e76ee-9878-4dd1-b284-4eae2f3ead67": Phase="Pending", Reason="", readiness=false. Elapsed: 856.909µs
Dec 18 19:05:59.318: INFO: Pod "pod-configmaps-a17e76ee-9878-4dd1-b284-4eae2f3ead67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002491406s
Dec 18 19:06:01.320: INFO: Pod "pod-configmaps-a17e76ee-9878-4dd1-b284-4eae2f3ead67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00398997s
Dec 18 19:06:03.321: INFO: Pod "pod-configmaps-a17e76ee-9878-4dd1-b284-4eae2f3ead67": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00547073s
Dec 18 19:06:05.323: INFO: Pod "pod-configmaps-a17e76ee-9878-4dd1-b284-4eae2f3ead67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007051795s
STEP: Saw pod success
Dec 18 19:06:05.323: INFO: Pod "pod-configmaps-a17e76ee-9878-4dd1-b284-4eae2f3ead67" satisfied condition "success or failure"
Dec 18 19:06:05.324: INFO: Trying to get logs from node controller-1 pod pod-configmaps-a17e76ee-9878-4dd1-b284-4eae2f3ead67 container env-test: <nil>
STEP: delete the pod
Dec 18 19:06:05.339: INFO: Waiting for pod pod-configmaps-a17e76ee-9878-4dd1-b284-4eae2f3ead67 to disappear
Dec 18 19:06:05.340: INFO: Pod pod-configmaps-a17e76ee-9878-4dd1-b284-4eae2f3ead67 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:06:05.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7428" for this suite.
Dec 18 19:06:11.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:06:11.374: INFO: namespace configmap-7428 deletion completed in 6.032967204s

• [SLOW TEST:14.076 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:06:11.374: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:06:54.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5826" for this suite.
Dec 18 19:07:00.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:07:00.531: INFO: namespace container-runtime-5826 deletion completed in 6.033506556s

• [SLOW TEST:49.157 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:07:00.532: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-4fd1b93e-b7c5-4020-ae47-23d9888c5c93
STEP: Creating a pod to test consume secrets
Dec 18 19:07:00.549: INFO: Waiting up to 5m0s for pod "pod-secrets-1c875435-45ad-4a66-882d-7e3cfe61fecf" in namespace "secrets-4456" to be "success or failure"
Dec 18 19:07:00.550: INFO: Pod "pod-secrets-1c875435-45ad-4a66-882d-7e3cfe61fecf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.41643ms
Dec 18 19:07:02.552: INFO: Pod "pod-secrets-1c875435-45ad-4a66-882d-7e3cfe61fecf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003562472s
Dec 18 19:07:04.554: INFO: Pod "pod-secrets-1c875435-45ad-4a66-882d-7e3cfe61fecf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005561908s
Dec 18 19:07:06.556: INFO: Pod "pod-secrets-1c875435-45ad-4a66-882d-7e3cfe61fecf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007257703s
Dec 18 19:07:08.558: INFO: Pod "pod-secrets-1c875435-45ad-4a66-882d-7e3cfe61fecf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008837901s
STEP: Saw pod success
Dec 18 19:07:08.558: INFO: Pod "pod-secrets-1c875435-45ad-4a66-882d-7e3cfe61fecf" satisfied condition "success or failure"
Dec 18 19:07:08.559: INFO: Trying to get logs from node controller-1 pod pod-secrets-1c875435-45ad-4a66-882d-7e3cfe61fecf container secret-volume-test: <nil>
STEP: delete the pod
Dec 18 19:07:08.566: INFO: Waiting for pod pod-secrets-1c875435-45ad-4a66-882d-7e3cfe61fecf to disappear
Dec 18 19:07:08.567: INFO: Pod pod-secrets-1c875435-45ad-4a66-882d-7e3cfe61fecf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:07:08.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4456" for this suite.
Dec 18 19:07:14.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:07:14.606: INFO: namespace secrets-4456 deletion completed in 6.037007395s

• [SLOW TEST:14.074 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:07:14.606: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 18 19:07:23.145: INFO: Successfully updated pod "annotationupdatebb719343-81d1-4714-8033-b88586771140"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:07:27.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1322" for this suite.
Dec 18 19:07:49.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:07:49.198: INFO: namespace projected-1322 deletion completed in 22.034462234s

• [SLOW TEST:34.592 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:07:49.198: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 19:07:49.213: INFO: Waiting up to 5m0s for pod "downwardapi-volume-803743c1-4ec2-43a2-8d82-7e0462a4134f" in namespace "downward-api-7661" to be "success or failure"
Dec 18 19:07:49.214: INFO: Pod "downwardapi-volume-803743c1-4ec2-43a2-8d82-7e0462a4134f": Phase="Pending", Reason="", readiness=false. Elapsed: 863.867µs
Dec 18 19:07:51.216: INFO: Pod "downwardapi-volume-803743c1-4ec2-43a2-8d82-7e0462a4134f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00276577s
Dec 18 19:07:53.218: INFO: Pod "downwardapi-volume-803743c1-4ec2-43a2-8d82-7e0462a4134f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004574127s
Dec 18 19:07:55.219: INFO: Pod "downwardapi-volume-803743c1-4ec2-43a2-8d82-7e0462a4134f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006130455s
Dec 18 19:07:57.222: INFO: Pod "downwardapi-volume-803743c1-4ec2-43a2-8d82-7e0462a4134f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008566638s
STEP: Saw pod success
Dec 18 19:07:57.222: INFO: Pod "downwardapi-volume-803743c1-4ec2-43a2-8d82-7e0462a4134f" satisfied condition "success or failure"
Dec 18 19:07:57.223: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-803743c1-4ec2-43a2-8d82-7e0462a4134f container client-container: <nil>
STEP: delete the pod
Dec 18 19:07:57.231: INFO: Waiting for pod downwardapi-volume-803743c1-4ec2-43a2-8d82-7e0462a4134f to disappear
Dec 18 19:07:57.231: INFO: Pod downwardapi-volume-803743c1-4ec2-43a2-8d82-7e0462a4134f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:07:57.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7661" for this suite.
Dec 18 19:08:03.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:08:03.268: INFO: namespace downward-api-7661 deletion completed in 6.034525464s

• [SLOW TEST:14.070 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:08:03.268: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2352
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2352
STEP: Creating statefulset with conflicting port in namespace statefulset-2352
STEP: Waiting until pod test-pod will start running in namespace statefulset-2352
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2352
Dec 18 19:08:11.292: INFO: Observed stateful pod in namespace: statefulset-2352, name: ss-0, uid: f87ae9ee-8e45-4e85-b18d-77cef50dfdcb, status phase: Pending. Waiting for statefulset controller to delete.
Dec 18 19:08:11.690: INFO: Observed stateful pod in namespace: statefulset-2352, name: ss-0, uid: f87ae9ee-8e45-4e85-b18d-77cef50dfdcb, status phase: Failed. Waiting for statefulset controller to delete.
Dec 18 19:08:11.692: INFO: Observed stateful pod in namespace: statefulset-2352, name: ss-0, uid: f87ae9ee-8e45-4e85-b18d-77cef50dfdcb, status phase: Failed. Waiting for statefulset controller to delete.
Dec 18 19:08:11.693: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2352
STEP: Removing pod with conflicting port in namespace statefulset-2352
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2352 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 18 19:08:19.704: INFO: Deleting all statefulset in ns statefulset-2352
Dec 18 19:08:19.705: INFO: Scaling statefulset ss to 0
Dec 18 19:08:39.710: INFO: Waiting for statefulset status.replicas updated to 0
Dec 18 19:08:39.711: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:08:39.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2352" for this suite.
Dec 18 19:08:45.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:08:45.750: INFO: namespace statefulset-2352 deletion completed in 6.033514344s

• [SLOW TEST:42.483 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:08:45.751: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-ffdc4e41-3a4e-4805-8069-dccf97cc03be
STEP: Creating a pod to test consume configMaps
Dec 18 19:08:45.768: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a3facad5-4618-45cd-9129-1198865c35d4" in namespace "projected-3435" to be "success or failure"
Dec 18 19:08:45.768: INFO: Pod "pod-projected-configmaps-a3facad5-4618-45cd-9129-1198865c35d4": Phase="Pending", Reason="", readiness=false. Elapsed: 893.706µs
Dec 18 19:08:47.770: INFO: Pod "pod-projected-configmaps-a3facad5-4618-45cd-9129-1198865c35d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002825023s
Dec 18 19:08:49.772: INFO: Pod "pod-projected-configmaps-a3facad5-4618-45cd-9129-1198865c35d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0045337s
Dec 18 19:08:51.774: INFO: Pod "pod-projected-configmaps-a3facad5-4618-45cd-9129-1198865c35d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006073884s
Dec 18 19:08:53.775: INFO: Pod "pod-projected-configmaps-a3facad5-4618-45cd-9129-1198865c35d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007651529s
STEP: Saw pod success
Dec 18 19:08:53.775: INFO: Pod "pod-projected-configmaps-a3facad5-4618-45cd-9129-1198865c35d4" satisfied condition "success or failure"
Dec 18 19:08:53.776: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-a3facad5-4618-45cd-9129-1198865c35d4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 19:08:53.791: INFO: Waiting for pod pod-projected-configmaps-a3facad5-4618-45cd-9129-1198865c35d4 to disappear
Dec 18 19:08:53.792: INFO: Pod pod-projected-configmaps-a3facad5-4618-45cd-9129-1198865c35d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:08:53.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3435" for this suite.
Dec 18 19:08:59.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:08:59.828: INFO: namespace projected-3435 deletion completed in 6.03414207s

• [SLOW TEST:14.077 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:08:59.829: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:08:59.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3442" for this suite.
Dec 18 19:09:05.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:09:05.884: INFO: namespace kubelet-test-3442 deletion completed in 6.033711751s

• [SLOW TEST:6.055 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:09:05.884: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Dec 18 19:09:05.899: INFO: Waiting up to 5m0s for pod "var-expansion-9882742d-f6d3-4e70-8cad-42d96d22debb" in namespace "var-expansion-3240" to be "success or failure"
Dec 18 19:09:05.900: INFO: Pod "var-expansion-9882742d-f6d3-4e70-8cad-42d96d22debb": Phase="Pending", Reason="", readiness=false. Elapsed: 894.443µs
Dec 18 19:09:07.902: INFO: Pod "var-expansion-9882742d-f6d3-4e70-8cad-42d96d22debb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002547222s
Dec 18 19:09:09.904: INFO: Pod "var-expansion-9882742d-f6d3-4e70-8cad-42d96d22debb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004595932s
Dec 18 19:09:11.906: INFO: Pod "var-expansion-9882742d-f6d3-4e70-8cad-42d96d22debb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006238143s
Dec 18 19:09:13.907: INFO: Pod "var-expansion-9882742d-f6d3-4e70-8cad-42d96d22debb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007864033s
STEP: Saw pod success
Dec 18 19:09:13.907: INFO: Pod "var-expansion-9882742d-f6d3-4e70-8cad-42d96d22debb" satisfied condition "success or failure"
Dec 18 19:09:13.908: INFO: Trying to get logs from node controller-0 pod var-expansion-9882742d-f6d3-4e70-8cad-42d96d22debb container dapi-container: <nil>
STEP: delete the pod
Dec 18 19:09:13.915: INFO: Waiting for pod var-expansion-9882742d-f6d3-4e70-8cad-42d96d22debb to disappear
Dec 18 19:09:13.916: INFO: Pod var-expansion-9882742d-f6d3-4e70-8cad-42d96d22debb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:09:13.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3240" for this suite.
Dec 18 19:09:19.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:09:19.953: INFO: namespace var-expansion-3240 deletion completed in 6.035680864s

• [SLOW TEST:14.069 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:09:19.954: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-034ae6f6-2f3c-487d-8726-e1bf9c8ab0aa in namespace container-probe-5462
Dec 18 19:09:27.972: INFO: Started pod liveness-034ae6f6-2f3c-487d-8726-e1bf9c8ab0aa in namespace container-probe-5462
STEP: checking the pod's current state and verifying that restartCount is present
Dec 18 19:09:27.973: INFO: Initial restart count of pod liveness-034ae6f6-2f3c-487d-8726-e1bf9c8ab0aa is 0
Dec 18 19:09:51.999: INFO: Restart count of pod container-probe-5462/liveness-034ae6f6-2f3c-487d-8726-e1bf9c8ab0aa is now 1 (24.025252515s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:09:52.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5462" for this suite.
Dec 18 19:09:58.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:09:58.039: INFO: namespace container-probe-5462 deletion completed in 6.035504904s

• [SLOW TEST:38.085 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:09:58.039: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 19:09:58.055: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3cd77be-44e4-4b4e-b704-7dae76315b55" in namespace "downward-api-6428" to be "success or failure"
Dec 18 19:09:58.056: INFO: Pod "downwardapi-volume-e3cd77be-44e4-4b4e-b704-7dae76315b55": Phase="Pending", Reason="", readiness=false. Elapsed: 921.407µs
Dec 18 19:10:00.058: INFO: Pod "downwardapi-volume-e3cd77be-44e4-4b4e-b704-7dae76315b55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003018483s
Dec 18 19:10:02.060: INFO: Pod "downwardapi-volume-e3cd77be-44e4-4b4e-b704-7dae76315b55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004857739s
Dec 18 19:10:04.062: INFO: Pod "downwardapi-volume-e3cd77be-44e4-4b4e-b704-7dae76315b55": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006544735s
Dec 18 19:10:06.064: INFO: Pod "downwardapi-volume-e3cd77be-44e4-4b4e-b704-7dae76315b55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008913292s
STEP: Saw pod success
Dec 18 19:10:06.064: INFO: Pod "downwardapi-volume-e3cd77be-44e4-4b4e-b704-7dae76315b55" satisfied condition "success or failure"
Dec 18 19:10:06.065: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-e3cd77be-44e4-4b4e-b704-7dae76315b55 container client-container: <nil>
STEP: delete the pod
Dec 18 19:10:06.073: INFO: Waiting for pod downwardapi-volume-e3cd77be-44e4-4b4e-b704-7dae76315b55 to disappear
Dec 18 19:10:06.074: INFO: Pod downwardapi-volume-e3cd77be-44e4-4b4e-b704-7dae76315b55 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:10:06.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6428" for this suite.
Dec 18 19:10:12.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:10:12.110: INFO: namespace downward-api-6428 deletion completed in 6.033833778s

• [SLOW TEST:14.070 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:10:12.110: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:10:20.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2607" for this suite.
Dec 18 19:10:28.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:10:28.169: INFO: namespace containers-2607 deletion completed in 8.034119111s

• [SLOW TEST:16.059 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:10:28.169: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 18 19:10:28.183: INFO: PodSpec: initContainers in spec.initContainers
Dec 18 19:11:19.700: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3f54c713-b228-4870-9e48-9952b2f516f5", GenerateName:"", Namespace:"init-container-8244", SelfLink:"/api/v1/namespaces/init-container-8244/pods/pod-init-3f54c713-b228-4870-9e48-9952b2f516f5", UID:"3c6aa232-8431-4c86-b685-32817afb58c9", ResourceVersion:"77622", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63712293028, loc:(*time.Location)(0x84c02a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"183374545"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.16.166.168/32", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.166.168\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-xswdr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003952f80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xswdr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xswdr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xswdr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00348a338), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"controller-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002693980), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00348a3c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00348a3e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00348a3e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00348a3ec), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293028, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293028, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293028, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293028, loc:(*time.Location)(0x84c02a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.204.3", PodIP:"172.16.166.168", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.16.166.168"}}, StartTime:(*v1.Time)(0xc003892300), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001c86380)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001c863f0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://95fac746e3396e2318d579c1459a12ac037749245215c7c754fc4b0a5786599d", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003892360), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003892340), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00348a464)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:11:19.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8244" for this suite.
Dec 18 19:11:47.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:11:47.737: INFO: namespace init-container-8244 deletion completed in 28.034844271s

• [SLOW TEST:79.568 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:11:47.737: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 18 19:11:47.751: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:11:55.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-910" for this suite.
Dec 18 19:12:01.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:12:01.932: INFO: namespace init-container-910 deletion completed in 6.034853301s

• [SLOW TEST:14.195 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:12:01.932: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-c3b08e72-5acc-4c8a-88ce-0b210e22aee2
STEP: Creating secret with name s-test-opt-upd-966d4ae7-373f-475c-8235-37d81cc904e8
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c3b08e72-5acc-4c8a-88ce-0b210e22aee2
STEP: Updating secret s-test-opt-upd-966d4ae7-373f-475c-8235-37d81cc904e8
STEP: Creating secret with name s-test-opt-create-1fdc0d36-f6a2-40de-b026-52ed5a9e5969
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:12:11.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1725" for this suite.
Dec 18 19:12:39.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:12:40.030: INFO: namespace projected-1725 deletion completed in 28.035371922s

• [SLOW TEST:38.097 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:12:40.030: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 18 19:12:47.060: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:12:47.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4580" for this suite.
Dec 18 19:12:53.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:12:53.101: INFO: namespace container-runtime-4580 deletion completed in 6.034590217s

• [SLOW TEST:13.071 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:12:53.101: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-e377e59d-dc82-4f4c-8113-1eacd4dee8f7
STEP: Creating secret with name s-test-opt-upd-2b192f2c-2fd8-482e-aece-3a8c1381358f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e377e59d-dc82-4f4c-8113-1eacd4dee8f7
STEP: Updating secret s-test-opt-upd-2b192f2c-2fd8-482e-aece-3a8c1381358f
STEP: Creating secret with name s-test-opt-create-7d677748-f401-4bb5-9e86-69b81927af22
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:13:05.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5864" for this suite.
Dec 18 19:13:17.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:13:17.206: INFO: namespace secrets-5864 deletion completed in 12.035204718s

• [SLOW TEST:24.105 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:13:17.206: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 18 19:13:25.729: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4818 pod-service-account-60501c68-c699-4e3e-adba-d43aeaf620e2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 18 19:13:25.928: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4818 pod-service-account-60501c68-c699-4e3e-adba-d43aeaf620e2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 18 19:13:26.120: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4818 pod-service-account-60501c68-c699-4e3e-adba-d43aeaf620e2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:13:26.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4818" for this suite.
Dec 18 19:13:32.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:13:32.356: INFO: namespace svcaccounts-4818 deletion completed in 6.033864519s

• [SLOW TEST:15.150 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:13:32.356: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:13:32.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9147" for this suite.
Dec 18 19:13:38.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:13:38.413: INFO: namespace custom-resource-definition-9147 deletion completed in 6.0359061s

• [SLOW TEST:6.056 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:13:38.413: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 18 19:13:46.939: INFO: Successfully updated pod "labelsupdateb9c0d8a9-3f66-4923-96de-a43a208aed95"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:13:50.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1443" for this suite.
Dec 18 19:14:02.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:14:02.992: INFO: namespace downward-api-1443 deletion completed in 12.035997212s

• [SLOW TEST:24.579 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:14:02.992: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 19:14:03.013: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"aac4847f-e2e7-4e4b-8586-420229983e7f", Controller:(*bool)(0xc00230cf86), BlockOwnerDeletion:(*bool)(0xc00230cf87)}}
Dec 18 19:14:03.017: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a19dc280-03a8-423a-9dec-89dd90d38da7", Controller:(*bool)(0xc001ea9676), BlockOwnerDeletion:(*bool)(0xc001ea9677)}}
Dec 18 19:14:03.023: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ed4197ca-ab83-4cda-8165-664ce04b5721", Controller:(*bool)(0xc0028eecba), BlockOwnerDeletion:(*bool)(0xc0028eecbb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:14:08.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5822" for this suite.
Dec 18 19:14:14.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:14:14.063: INFO: namespace gc-5822 deletion completed in 6.034788811s

• [SLOW TEST:11.071 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:14:14.064: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 18 19:14:14.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-834'
Dec 18 19:14:14.150: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 18 19:14:14.150: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Dec 18 19:14:16.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete deployment e2e-test-httpd-deployment --namespace=kubectl-834'
Dec 18 19:14:16.221: INFO: stderr: ""
Dec 18 19:14:16.221: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:14:16.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-834" for this suite.
Dec 18 19:14:22.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:14:22.257: INFO: namespace kubectl-834 deletion completed in 6.034759243s

• [SLOW TEST:8.194 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:14:22.258: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 19:14:22.273: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96a62fc9-7a5a-465e-b8bd-de25204ea22e" in namespace "projected-6900" to be "success or failure"
Dec 18 19:14:22.274: INFO: Pod "downwardapi-volume-96a62fc9-7a5a-465e-b8bd-de25204ea22e": Phase="Pending", Reason="", readiness=false. Elapsed: 857.556µs
Dec 18 19:14:24.275: INFO: Pod "downwardapi-volume-96a62fc9-7a5a-465e-b8bd-de25204ea22e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002529741s
Dec 18 19:14:26.277: INFO: Pod "downwardapi-volume-96a62fc9-7a5a-465e-b8bd-de25204ea22e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004218915s
Dec 18 19:14:28.279: INFO: Pod "downwardapi-volume-96a62fc9-7a5a-465e-b8bd-de25204ea22e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005962392s
Dec 18 19:14:30.280: INFO: Pod "downwardapi-volume-96a62fc9-7a5a-465e-b8bd-de25204ea22e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007603142s
STEP: Saw pod success
Dec 18 19:14:30.280: INFO: Pod "downwardapi-volume-96a62fc9-7a5a-465e-b8bd-de25204ea22e" satisfied condition "success or failure"
Dec 18 19:14:30.281: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-96a62fc9-7a5a-465e-b8bd-de25204ea22e container client-container: <nil>
STEP: delete the pod
Dec 18 19:14:30.295: INFO: Waiting for pod downwardapi-volume-96a62fc9-7a5a-465e-b8bd-de25204ea22e to disappear
Dec 18 19:14:30.296: INFO: Pod downwardapi-volume-96a62fc9-7a5a-465e-b8bd-de25204ea22e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:14:30.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6900" for this suite.
Dec 18 19:14:36.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:14:36.333: INFO: namespace projected-6900 deletion completed in 6.035044126s

• [SLOW TEST:14.075 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:14:36.333: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:14:42.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6529" for this suite.
Dec 18 19:14:48.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:14:48.415: INFO: namespace namespaces-6529 deletion completed in 6.034656787s
STEP: Destroying namespace "nsdeletetest-2142" for this suite.
Dec 18 19:14:48.416: INFO: Namespace nsdeletetest-2142 was already deleted
STEP: Destroying namespace "nsdeletetest-468" for this suite.
Dec 18 19:14:54.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:14:54.451: INFO: namespace nsdeletetest-468 deletion completed in 6.035290238s

• [SLOW TEST:18.118 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:14:54.451: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-dc22305d-de1b-4e0d-af8e-a494a7526a18
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-dc22305d-de1b-4e0d-af8e-a494a7526a18
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:16:30.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1488" for this suite.
Dec 18 19:16:42.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:16:42.750: INFO: namespace projected-1488 deletion completed in 12.034544993s

• [SLOW TEST:108.299 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:16:42.750: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-0117b485-c2ba-4998-ab5b-1296fc920d59
STEP: Creating a pod to test consume secrets
Dec 18 19:16:42.767: INFO: Waiting up to 5m0s for pod "pod-secrets-7c7eadf1-a395-4c81-bb47-f84f5119a836" in namespace "secrets-2275" to be "success or failure"
Dec 18 19:16:42.767: INFO: Pod "pod-secrets-7c7eadf1-a395-4c81-bb47-f84f5119a836": Phase="Pending", Reason="", readiness=false. Elapsed: 806.664µs
Dec 18 19:16:44.770: INFO: Pod "pod-secrets-7c7eadf1-a395-4c81-bb47-f84f5119a836": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00309666s
Dec 18 19:16:46.771: INFO: Pod "pod-secrets-7c7eadf1-a395-4c81-bb47-f84f5119a836": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004657545s
Dec 18 19:16:48.773: INFO: Pod "pod-secrets-7c7eadf1-a395-4c81-bb47-f84f5119a836": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006549865s
Dec 18 19:16:50.775: INFO: Pod "pod-secrets-7c7eadf1-a395-4c81-bb47-f84f5119a836": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008042164s
STEP: Saw pod success
Dec 18 19:16:50.775: INFO: Pod "pod-secrets-7c7eadf1-a395-4c81-bb47-f84f5119a836" satisfied condition "success or failure"
Dec 18 19:16:50.776: INFO: Trying to get logs from node controller-0 pod pod-secrets-7c7eadf1-a395-4c81-bb47-f84f5119a836 container secret-volume-test: <nil>
STEP: delete the pod
Dec 18 19:16:50.789: INFO: Waiting for pod pod-secrets-7c7eadf1-a395-4c81-bb47-f84f5119a836 to disappear
Dec 18 19:16:50.790: INFO: Pod pod-secrets-7c7eadf1-a395-4c81-bb47-f84f5119a836 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:16:50.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2275" for this suite.
Dec 18 19:16:56.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:16:56.826: INFO: namespace secrets-2275 deletion completed in 6.033769449s

• [SLOW TEST:14.075 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:16:56.826: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 18 19:16:56.839: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 19:17:00.393: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:17:12.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1778" for this suite.
Dec 18 19:17:18.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:17:18.613: INFO: namespace crd-publish-openapi-1778 deletion completed in 6.035522846s

• [SLOW TEST:21.787 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:17:18.614: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-00baf562-3c4a-4030-8376-6a0ecdefe4f2
STEP: Creating a pod to test consume configMaps
Dec 18 19:17:18.630: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2c3f2520-ecd6-4763-b049-dd7f04e258f8" in namespace "projected-7093" to be "success or failure"
Dec 18 19:17:18.631: INFO: Pod "pod-projected-configmaps-2c3f2520-ecd6-4763-b049-dd7f04e258f8": Phase="Pending", Reason="", readiness=false. Elapsed: 925.37µs
Dec 18 19:17:20.633: INFO: Pod "pod-projected-configmaps-2c3f2520-ecd6-4763-b049-dd7f04e258f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003053015s
Dec 18 19:17:22.634: INFO: Pod "pod-projected-configmaps-2c3f2520-ecd6-4763-b049-dd7f04e258f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004654983s
Dec 18 19:17:24.636: INFO: Pod "pod-projected-configmaps-2c3f2520-ecd6-4763-b049-dd7f04e258f8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006876971s
Dec 18 19:17:26.638: INFO: Pod "pod-projected-configmaps-2c3f2520-ecd6-4763-b049-dd7f04e258f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008525205s
STEP: Saw pod success
Dec 18 19:17:26.638: INFO: Pod "pod-projected-configmaps-2c3f2520-ecd6-4763-b049-dd7f04e258f8" satisfied condition "success or failure"
Dec 18 19:17:26.639: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-2c3f2520-ecd6-4763-b049-dd7f04e258f8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 19:17:26.647: INFO: Waiting for pod pod-projected-configmaps-2c3f2520-ecd6-4763-b049-dd7f04e258f8 to disappear
Dec 18 19:17:26.647: INFO: Pod pod-projected-configmaps-2c3f2520-ecd6-4763-b049-dd7f04e258f8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:17:26.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7093" for this suite.
Dec 18 19:17:32.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:17:32.684: INFO: namespace projected-7093 deletion completed in 6.035353756s

• [SLOW TEST:14.071 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:17:32.685: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 18 19:17:32.698: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 18 19:17:32.703: INFO: Waiting for terminating namespaces to be deleted...
Dec 18 19:17:32.704: INFO: 
Logging pods the kubelet thinks is on node compute-0 before test
Dec 18 19:17:32.716: INFO: kube-proxy-lq2c8 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.716: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 18 19:17:32.716: INFO: kube-multus-ds-amd64-fczk7 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.716: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 19:17:32.716: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-jzbct from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 19:17:32.716: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 18 19:17:32.716: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 19:17:32.716: INFO: kube-sriov-cni-ds-amd64-nlfxr from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.716: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 19:17:32.716: INFO: calico-node-ntnk8 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.716: INFO: 	Container calico-node ready: true, restart count 1
Dec 18 19:17:32.716: INFO: sonobuoy from sonobuoy started at 2019-12-18 18:17:38 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.716: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 18 19:17:32.716: INFO: 
Logging pods the kubelet thinks is on node compute-1 before test
Dec 18 19:17:32.727: INFO: kube-sriov-cni-ds-amd64-v94kl from kube-system started at 2019-12-18 15:11:51 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.728: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 19:17:32.728: INFO: kube-proxy-w7hfz from kube-system started at 2019-12-18 15:11:52 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.728: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 18 19:17:32.728: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-49m5k from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 19:17:32.728: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 18 19:17:32.728: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 19:17:32.728: INFO: calico-node-6s2pv from kube-system started at 2019-12-18 15:11:51 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.728: INFO: 	Container calico-node ready: true, restart count 1
Dec 18 19:17:32.728: INFO: kube-multus-ds-amd64-trmzx from kube-system started at 2019-12-18 15:11:52 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.728: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 19:17:32.728: INFO: 
Logging pods the kubelet thinks is on node controller-0 before test
Dec 18 19:17:32.733: INFO: kube-controller-manager-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 18 19:17:32.733: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-q7fcg from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 18 19:17:32.733: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 19:17:32.733: INFO: kube-scheduler-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 18 19:17:32.733: INFO: kube-proxy-hs98d from kube-system started at 2019-12-18 14:17:53 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container kube-proxy ready: true, restart count 1
Dec 18 19:17:32.733: INFO: kube-sriov-cni-ds-amd64-97g65 from kube-system started at 2019-12-18 18:33:43 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 19:17:32.733: INFO: calico-kube-controllers-855577b7b5-2gktm from kube-system started at 2019-12-18 18:38:16 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 18 19:17:32.733: INFO: ceph-pools-audit-1576695900-m4n5m from kube-system started at 2019-12-18 19:05:00 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Dec 18 19:17:32.733: INFO: coredns-6bc668cd76-ks6bd from kube-system started at 2019-12-18 18:33:17 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container coredns ready: true, restart count 0
Dec 18 19:17:32.733: INFO: calico-node-7bzgw from kube-system started at 2019-12-18 14:17:53 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container calico-node ready: true, restart count 3
Dec 18 19:17:32.733: INFO: kube-multus-ds-amd64-plsvq from kube-system started at 2019-12-18 18:33:43 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 19:17:32.733: INFO: tiller-deploy-d6b59fcb-kj5hv from kube-system started at 2019-12-18 18:38:16 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container tiller ready: true, restart count 0
Dec 18 19:17:32.733: INFO: kube-apiserver-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container kube-apiserver ready: true, restart count 2
Dec 18 19:17:32.733: INFO: rbd-provisioner-7484d49cf6-858lp from kube-system started at 2019-12-18 18:33:13 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container rbd-provisioner ready: true, restart count 0
Dec 18 19:17:32.733: INFO: ceph-pools-audit-1576696200-xnsmr from kube-system started at 2019-12-18 19:10:00 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.733: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Dec 18 19:17:32.733: INFO: 
Logging pods the kubelet thinks is on node controller-1 before test
Dec 18 19:17:32.738: INFO: kube-scheduler-controller-1 from kube-system started at 2019-12-18 15:22:52 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.738: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 18 19:17:32.738: INFO: kube-multus-ds-amd64-rvvrh from kube-system started at 2019-12-18 18:38:52 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.738: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 19:17:32.738: INFO: coredns-6bc668cd76-86sgt from kube-system started at 2019-12-18 18:38:56 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.738: INFO: 	Container coredns ready: true, restart count 0
Dec 18 19:17:32.738: INFO: sonobuoy-e2e-job-8824c3aaef6e4480 from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 19:17:32.738: INFO: 	Container e2e ready: true, restart count 0
Dec 18 19:17:32.738: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 18 19:17:32.738: INFO: kube-controller-manager-controller-1 from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.738: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 18 19:17:32.738: INFO: calico-node-8q8ss from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.738: INFO: 	Container calico-node ready: true, restart count 4
Dec 18 19:17:32.738: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-rz72p from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 19:17:32.738: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 18 19:17:32.738: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 19:17:32.738: INFO: kube-sriov-cni-ds-amd64-rb5tl from kube-system started at 2019-12-18 18:38:52 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.738: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 19:17:32.738: INFO: rbd-provisioner-7484d49cf6-p98d2 from kube-system started at 2019-12-18 18:38:56 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.738: INFO: 	Container rbd-provisioner ready: true, restart count 0
Dec 18 19:17:32.738: INFO: ceph-pools-audit-1576696500-t2fvb from kube-system started at 2019-12-18 19:15:00 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.738: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Dec 18 19:17:32.738: INFO: kube-apiserver-controller-1 from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.738: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 18 19:17:32.738: INFO: kube-proxy-fds2l from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 19:17:32.738: INFO: 	Container kube-proxy ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-fa9f5886-dd9d-4e44-b5c4-18b39506b93f 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-fa9f5886-dd9d-4e44-b5c4-18b39506b93f off the node controller-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fa9f5886-dd9d-4e44-b5c4-18b39506b93f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:17:48.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5490" for this suite.
Dec 18 19:18:04.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:18:04.797: INFO: namespace sched-pred-5490 deletion completed in 16.032900788s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:32.112 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:18:04.797: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 18 19:18:04.812: INFO: Waiting up to 5m0s for pod "pod-59951973-7346-4fbf-8e7f-be0ab88395d6" in namespace "emptydir-3116" to be "success or failure"
Dec 18 19:18:04.813: INFO: Pod "pod-59951973-7346-4fbf-8e7f-be0ab88395d6": Phase="Pending", Reason="", readiness=false. Elapsed: 896.085µs
Dec 18 19:18:06.815: INFO: Pod "pod-59951973-7346-4fbf-8e7f-be0ab88395d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002371179s
Dec 18 19:18:08.816: INFO: Pod "pod-59951973-7346-4fbf-8e7f-be0ab88395d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003850015s
Dec 18 19:18:10.818: INFO: Pod "pod-59951973-7346-4fbf-8e7f-be0ab88395d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005542098s
Dec 18 19:18:12.820: INFO: Pod "pod-59951973-7346-4fbf-8e7f-be0ab88395d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007297213s
STEP: Saw pod success
Dec 18 19:18:12.820: INFO: Pod "pod-59951973-7346-4fbf-8e7f-be0ab88395d6" satisfied condition "success or failure"
Dec 18 19:18:12.820: INFO: Trying to get logs from node controller-1 pod pod-59951973-7346-4fbf-8e7f-be0ab88395d6 container test-container: <nil>
STEP: delete the pod
Dec 18 19:18:12.827: INFO: Waiting for pod pod-59951973-7346-4fbf-8e7f-be0ab88395d6 to disappear
Dec 18 19:18:12.828: INFO: Pod pod-59951973-7346-4fbf-8e7f-be0ab88395d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:18:12.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3116" for this suite.
Dec 18 19:18:18.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:18:18.864: INFO: namespace emptydir-3116 deletion completed in 6.035207035s

• [SLOW TEST:14.068 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:18:18.865: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 18 19:18:25.894: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:18:25.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4379" for this suite.
Dec 18 19:18:31.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:18:31.935: INFO: namespace container-runtime-4379 deletion completed in 6.034516594s

• [SLOW TEST:13.071 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:18:31.936: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-7d12cb2c-1232-4989-903c-61b428f3c601
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:18:31.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7838" for this suite.
Dec 18 19:18:37.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:18:37.986: INFO: namespace secrets-7838 deletion completed in 6.034593825s

• [SLOW TEST:6.051 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:18:37.986: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-2eed479e-aca5-478e-bbbf-8fb9b3219717
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-2eed479e-aca5-478e-bbbf-8fb9b3219717
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:20:14.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3866" for this suite.
Dec 18 19:20:26.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:20:26.300: INFO: namespace configmap-3866 deletion completed in 12.035047299s

• [SLOW TEST:108.314 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:20:26.300: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 18 19:20:26.322: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6442 /api/v1/namespaces/watch-6442/configmaps/e2e-watch-test-resource-version 8f8c4516-cffa-46ef-bef7-803a62cd10fc 80123 0 2019-12-18 19:20:26 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 18 19:20:26.322: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6442 /api/v1/namespaces/watch-6442/configmaps/e2e-watch-test-resource-version 8f8c4516-cffa-46ef-bef7-803a62cd10fc 80124 0 2019-12-18 19:20:26 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:20:26.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6442" for this suite.
Dec 18 19:20:32.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:20:32.358: INFO: namespace watch-6442 deletion completed in 6.033768492s

• [SLOW TEST:6.057 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:20:32.358: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Dec 18 19:20:32.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=kubectl-9870 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec 18 19:20:39.316: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec 18 19:20:39.317: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:20:41.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9870" for this suite.
Dec 18 19:20:47.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:20:47.358: INFO: namespace kubectl-9870 deletion completed in 6.03686398s

• [SLOW TEST:15.000 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:20:47.358: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 18 19:20:55.894: INFO: Successfully updated pod "labelsupdateb293f52a-6f15-425d-b965-b73df3e70fee"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:20:59.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6091" for this suite.
Dec 18 19:21:27.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:21:27.943: INFO: namespace projected-6091 deletion completed in 28.034415763s

• [SLOW TEST:40.586 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:21:27.944: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 18 19:21:36.465: INFO: Successfully updated pod "adopt-release-5jkt9"
STEP: Checking that the Job readopts the Pod
Dec 18 19:21:36.465: INFO: Waiting up to 15m0s for pod "adopt-release-5jkt9" in namespace "job-6802" to be "adopted"
Dec 18 19:21:36.466: INFO: Pod "adopt-release-5jkt9": Phase="Running", Reason="", readiness=true. Elapsed: 1.271239ms
Dec 18 19:21:38.469: INFO: Pod "adopt-release-5jkt9": Phase="Running", Reason="", readiness=true. Elapsed: 2.003500009s
Dec 18 19:21:38.469: INFO: Pod "adopt-release-5jkt9" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 18 19:21:38.972: INFO: Successfully updated pod "adopt-release-5jkt9"
STEP: Checking that the Job releases the Pod
Dec 18 19:21:38.972: INFO: Waiting up to 15m0s for pod "adopt-release-5jkt9" in namespace "job-6802" to be "released"
Dec 18 19:21:38.973: INFO: Pod "adopt-release-5jkt9": Phase="Running", Reason="", readiness=true. Elapsed: 1.681212ms
Dec 18 19:21:40.975: INFO: Pod "adopt-release-5jkt9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00332315s
Dec 18 19:21:40.975: INFO: Pod "adopt-release-5jkt9" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:21:40.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6802" for this suite.
Dec 18 19:22:24.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:22:25.011: INFO: namespace job-6802 deletion completed in 44.033876863s

• [SLOW TEST:57.067 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:22:25.011: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Dec 18 19:22:33.030: INFO: Pod pod-hostip-2bddc1f5-fc81-476b-8ae9-75f0c796e4b4 has hostIP: 192.168.204.3
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:22:33.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8377" for this suite.
Dec 18 19:22:45.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:22:45.066: INFO: namespace pods-8377 deletion completed in 12.034096038s

• [SLOW TEST:20.055 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:22:45.066: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 18 19:22:45.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-8683'
Dec 18 19:22:45.151: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 18 19:22:45.151: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: rolling-update to same image controller
Dec 18 19:22:45.155: INFO: scanned /root for discovery docs: <nil>
Dec 18 19:22:45.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-8683'
Dec 18 19:23:03.867: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 18 19:23:03.867: INFO: stdout: "Created e2e-test-httpd-rc-9a32b4adb67aeff1d48da0546ca9489e\nScaling up e2e-test-httpd-rc-9a32b4adb67aeff1d48da0546ca9489e from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-9a32b4adb67aeff1d48da0546ca9489e up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-9a32b4adb67aeff1d48da0546ca9489e to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Dec 18 19:23:03.867: INFO: stdout: "Created e2e-test-httpd-rc-9a32b4adb67aeff1d48da0546ca9489e\nScaling up e2e-test-httpd-rc-9a32b4adb67aeff1d48da0546ca9489e from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-9a32b4adb67aeff1d48da0546ca9489e up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-9a32b4adb67aeff1d48da0546ca9489e to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Dec 18 19:23:03.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-8683'
Dec 18 19:23:03.930: INFO: stderr: ""
Dec 18 19:23:03.930: INFO: stdout: "e2e-test-httpd-rc-9a32b4adb67aeff1d48da0546ca9489e-8dxnr "
Dec 18 19:23:03.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods e2e-test-httpd-rc-9a32b4adb67aeff1d48da0546ca9489e-8dxnr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8683'
Dec 18 19:23:03.991: INFO: stderr: ""
Dec 18 19:23:03.991: INFO: stdout: "true"
Dec 18 19:23:03.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods e2e-test-httpd-rc-9a32b4adb67aeff1d48da0546ca9489e-8dxnr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8683'
Dec 18 19:23:04.051: INFO: stderr: ""
Dec 18 19:23:04.051: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Dec 18 19:23:04.051: INFO: e2e-test-httpd-rc-9a32b4adb67aeff1d48da0546ca9489e-8dxnr is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Dec 18 19:23:04.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete rc e2e-test-httpd-rc --namespace=kubectl-8683'
Dec 18 19:23:04.117: INFO: stderr: ""
Dec 18 19:23:04.117: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:23:04.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8683" for this suite.
Dec 18 19:23:16.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:23:16.154: INFO: namespace kubectl-8683 deletion completed in 12.034475359s

• [SLOW TEST:31.088 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:23:16.154: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:23:25.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4634" for this suite.
Dec 18 19:23:37.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:23:37.212: INFO: namespace replication-controller-4634 deletion completed in 12.034321537s

• [SLOW TEST:21.058 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:23:37.213: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 19:23:37.228: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc7ff43e-6925-4a58-ae4f-81c169d905cb" in namespace "downward-api-6935" to be "success or failure"
Dec 18 19:23:37.230: INFO: Pod "downwardapi-volume-fc7ff43e-6925-4a58-ae4f-81c169d905cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35742ms
Dec 18 19:23:39.232: INFO: Pod "downwardapi-volume-fc7ff43e-6925-4a58-ae4f-81c169d905cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003125806s
Dec 18 19:23:41.234: INFO: Pod "downwardapi-volume-fc7ff43e-6925-4a58-ae4f-81c169d905cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005331443s
Dec 18 19:23:43.236: INFO: Pod "downwardapi-volume-fc7ff43e-6925-4a58-ae4f-81c169d905cb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00742802s
Dec 18 19:23:45.238: INFO: Pod "downwardapi-volume-fc7ff43e-6925-4a58-ae4f-81c169d905cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009115438s
STEP: Saw pod success
Dec 18 19:23:45.238: INFO: Pod "downwardapi-volume-fc7ff43e-6925-4a58-ae4f-81c169d905cb" satisfied condition "success or failure"
Dec 18 19:23:45.239: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-fc7ff43e-6925-4a58-ae4f-81c169d905cb container client-container: <nil>
STEP: delete the pod
Dec 18 19:23:45.253: INFO: Waiting for pod downwardapi-volume-fc7ff43e-6925-4a58-ae4f-81c169d905cb to disappear
Dec 18 19:23:45.254: INFO: Pod downwardapi-volume-fc7ff43e-6925-4a58-ae4f-81c169d905cb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:23:45.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6935" for this suite.
Dec 18 19:23:51.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:23:51.291: INFO: namespace downward-api-6935 deletion completed in 6.035144295s

• [SLOW TEST:14.078 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:23:51.291: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Dec 18 19:23:51.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-2999'
Dec 18 19:23:51.499: INFO: stderr: ""
Dec 18 19:23:51.499: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 18 19:23:51.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2999'
Dec 18 19:23:51.566: INFO: stderr: ""
Dec 18 19:23:51.566: INFO: stdout: "update-demo-nautilus-64t78 update-demo-nautilus-94nlv "
Dec 18 19:23:51.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-64t78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:23:51.627: INFO: stderr: ""
Dec 18 19:23:51.627: INFO: stdout: ""
Dec 18 19:23:51.627: INFO: update-demo-nautilus-64t78 is created but not running
Dec 18 19:23:56.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2999'
Dec 18 19:23:56.693: INFO: stderr: ""
Dec 18 19:23:56.693: INFO: stdout: "update-demo-nautilus-64t78 update-demo-nautilus-94nlv "
Dec 18 19:23:56.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-64t78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:23:56.756: INFO: stderr: ""
Dec 18 19:23:56.756: INFO: stdout: ""
Dec 18 19:23:56.756: INFO: update-demo-nautilus-64t78 is created but not running
Dec 18 19:24:01.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2999'
Dec 18 19:24:01.824: INFO: stderr: ""
Dec 18 19:24:01.824: INFO: stdout: "update-demo-nautilus-64t78 update-demo-nautilus-94nlv "
Dec 18 19:24:01.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-64t78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:01.888: INFO: stderr: ""
Dec 18 19:24:01.888: INFO: stdout: "true"
Dec 18 19:24:01.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-64t78 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:01.951: INFO: stderr: ""
Dec 18 19:24:01.951: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 18 19:24:01.951: INFO: validating pod update-demo-nautilus-64t78
Dec 18 19:24:01.953: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 18 19:24:01.953: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 18 19:24:01.953: INFO: update-demo-nautilus-64t78 is verified up and running
Dec 18 19:24:01.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-94nlv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:02.015: INFO: stderr: ""
Dec 18 19:24:02.015: INFO: stdout: "true"
Dec 18 19:24:02.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-94nlv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:02.075: INFO: stderr: ""
Dec 18 19:24:02.075: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 18 19:24:02.075: INFO: validating pod update-demo-nautilus-94nlv
Dec 18 19:24:02.077: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 18 19:24:02.077: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 18 19:24:02.077: INFO: update-demo-nautilus-94nlv is verified up and running
STEP: scaling down the replication controller
Dec 18 19:24:02.078: INFO: scanned /root for discovery docs: <nil>
Dec 18 19:24:02.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2999'
Dec 18 19:24:03.151: INFO: stderr: ""
Dec 18 19:24:03.151: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 18 19:24:03.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2999'
Dec 18 19:24:03.216: INFO: stderr: ""
Dec 18 19:24:03.216: INFO: stdout: "update-demo-nautilus-64t78 update-demo-nautilus-94nlv "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 18 19:24:08.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2999'
Dec 18 19:24:08.284: INFO: stderr: ""
Dec 18 19:24:08.284: INFO: stdout: "update-demo-nautilus-64t78 "
Dec 18 19:24:08.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-64t78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:08.347: INFO: stderr: ""
Dec 18 19:24:08.347: INFO: stdout: "true"
Dec 18 19:24:08.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-64t78 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:08.410: INFO: stderr: ""
Dec 18 19:24:08.410: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 18 19:24:08.410: INFO: validating pod update-demo-nautilus-64t78
Dec 18 19:24:08.411: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 18 19:24:08.411: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 18 19:24:08.411: INFO: update-demo-nautilus-64t78 is verified up and running
STEP: scaling up the replication controller
Dec 18 19:24:08.413: INFO: scanned /root for discovery docs: <nil>
Dec 18 19:24:08.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2999'
Dec 18 19:24:09.484: INFO: stderr: ""
Dec 18 19:24:09.484: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 18 19:24:09.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2999'
Dec 18 19:24:09.553: INFO: stderr: ""
Dec 18 19:24:09.553: INFO: stdout: "update-demo-nautilus-64t78 update-demo-nautilus-6trhm "
Dec 18 19:24:09.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-64t78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:09.614: INFO: stderr: ""
Dec 18 19:24:09.614: INFO: stdout: "true"
Dec 18 19:24:09.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-64t78 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:09.676: INFO: stderr: ""
Dec 18 19:24:09.676: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 18 19:24:09.676: INFO: validating pod update-demo-nautilus-64t78
Dec 18 19:24:09.678: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 18 19:24:09.678: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 18 19:24:09.678: INFO: update-demo-nautilus-64t78 is verified up and running
Dec 18 19:24:09.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-6trhm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:09.739: INFO: stderr: ""
Dec 18 19:24:09.739: INFO: stdout: ""
Dec 18 19:24:09.739: INFO: update-demo-nautilus-6trhm is created but not running
Dec 18 19:24:14.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2999'
Dec 18 19:24:14.807: INFO: stderr: ""
Dec 18 19:24:14.807: INFO: stdout: "update-demo-nautilus-64t78 update-demo-nautilus-6trhm "
Dec 18 19:24:14.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-64t78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:14.874: INFO: stderr: ""
Dec 18 19:24:14.875: INFO: stdout: "true"
Dec 18 19:24:14.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-64t78 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:14.940: INFO: stderr: ""
Dec 18 19:24:14.940: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 18 19:24:14.940: INFO: validating pod update-demo-nautilus-64t78
Dec 18 19:24:14.942: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 18 19:24:14.942: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 18 19:24:14.942: INFO: update-demo-nautilus-64t78 is verified up and running
Dec 18 19:24:14.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-6trhm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:15.005: INFO: stderr: ""
Dec 18 19:24:15.005: INFO: stdout: ""
Dec 18 19:24:15.005: INFO: update-demo-nautilus-6trhm is created but not running
Dec 18 19:24:20.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2999'
Dec 18 19:24:20.074: INFO: stderr: ""
Dec 18 19:24:20.074: INFO: stdout: "update-demo-nautilus-64t78 update-demo-nautilus-6trhm "
Dec 18 19:24:20.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-64t78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:20.135: INFO: stderr: ""
Dec 18 19:24:20.135: INFO: stdout: "true"
Dec 18 19:24:20.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-64t78 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:20.198: INFO: stderr: ""
Dec 18 19:24:20.198: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 18 19:24:20.198: INFO: validating pod update-demo-nautilus-64t78
Dec 18 19:24:20.200: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 18 19:24:20.200: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 18 19:24:20.200: INFO: update-demo-nautilus-64t78 is verified up and running
Dec 18 19:24:20.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-6trhm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:20.262: INFO: stderr: ""
Dec 18 19:24:20.262: INFO: stdout: "true"
Dec 18 19:24:20.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods update-demo-nautilus-6trhm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2999'
Dec 18 19:24:20.323: INFO: stderr: ""
Dec 18 19:24:20.323: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 18 19:24:20.323: INFO: validating pod update-demo-nautilus-6trhm
Dec 18 19:24:20.325: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 18 19:24:20.325: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 18 19:24:20.325: INFO: update-demo-nautilus-6trhm is verified up and running
STEP: using delete to clean up resources
Dec 18 19:24:20.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete --grace-period=0 --force -f - --namespace=kubectl-2999'
Dec 18 19:24:20.387: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 18 19:24:20.387: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 18 19:24:20.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2999'
Dec 18 19:24:20.451: INFO: stderr: "No resources found in kubectl-2999 namespace.\n"
Dec 18 19:24:20.451: INFO: stdout: ""
Dec 18 19:24:20.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -l name=update-demo --namespace=kubectl-2999 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 18 19:24:20.513: INFO: stderr: ""
Dec 18 19:24:20.513: INFO: stdout: "update-demo-nautilus-64t78\nupdate-demo-nautilus-6trhm\n"
Dec 18 19:24:21.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2999'
Dec 18 19:24:21.082: INFO: stderr: "No resources found in kubectl-2999 namespace.\n"
Dec 18 19:24:21.082: INFO: stdout: ""
Dec 18 19:24:21.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -l name=update-demo --namespace=kubectl-2999 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 18 19:24:21.150: INFO: stderr: ""
Dec 18 19:24:21.150: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:24:21.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2999" for this suite.
Dec 18 19:24:49.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:24:49.188: INFO: namespace kubectl-2999 deletion completed in 28.03598308s

• [SLOW TEST:57.897 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:24:49.188: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-f9ead4d2-97ff-426f-a219-99a92b4a04da
STEP: Creating a pod to test consume configMaps
Dec 18 19:24:49.205: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c215c2c7-ef32-4a7a-b350-f6844c431394" in namespace "projected-7233" to be "success or failure"
Dec 18 19:24:49.206: INFO: Pod "pod-projected-configmaps-c215c2c7-ef32-4a7a-b350-f6844c431394": Phase="Pending", Reason="", readiness=false. Elapsed: 889.549µs
Dec 18 19:24:51.208: INFO: Pod "pod-projected-configmaps-c215c2c7-ef32-4a7a-b350-f6844c431394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002404107s
Dec 18 19:24:53.209: INFO: Pod "pod-projected-configmaps-c215c2c7-ef32-4a7a-b350-f6844c431394": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004121791s
Dec 18 19:24:55.211: INFO: Pod "pod-projected-configmaps-c215c2c7-ef32-4a7a-b350-f6844c431394": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005605416s
Dec 18 19:24:57.213: INFO: Pod "pod-projected-configmaps-c215c2c7-ef32-4a7a-b350-f6844c431394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007228202s
STEP: Saw pod success
Dec 18 19:24:57.213: INFO: Pod "pod-projected-configmaps-c215c2c7-ef32-4a7a-b350-f6844c431394" satisfied condition "success or failure"
Dec 18 19:24:57.214: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-c215c2c7-ef32-4a7a-b350-f6844c431394 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 19:24:57.221: INFO: Waiting for pod pod-projected-configmaps-c215c2c7-ef32-4a7a-b350-f6844c431394 to disappear
Dec 18 19:24:57.222: INFO: Pod pod-projected-configmaps-c215c2c7-ef32-4a7a-b350-f6844c431394 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:24:57.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7233" for this suite.
Dec 18 19:25:03.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:25:03.258: INFO: namespace projected-7233 deletion completed in 6.034344296s

• [SLOW TEST:14.070 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:25:03.258: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 19:25:03.796: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 18 19:25:05.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 19:25:07.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 19:25:09.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 19:25:11.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712293903, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 19:25:14.812: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 18 19:25:22.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 attach --namespace=webhook-8424 to-be-attached-pod -i -c=container1'
Dec 18 19:25:22.901: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:25:22.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8424" for this suite.
Dec 18 19:25:50.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:25:50.940: INFO: namespace webhook-8424 deletion completed in 28.033948422s
STEP: Destroying namespace "webhook-8424-markers" for this suite.
Dec 18 19:25:56.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:25:56.974: INFO: namespace webhook-8424-markers deletion completed in 6.034461569s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:53.720 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:25:56.979: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-bbbc1e93-8797-4a75-885e-114ed1bb4aed in namespace container-probe-9146
Dec 18 19:26:04.998: INFO: Started pod busybox-bbbc1e93-8797-4a75-885e-114ed1bb4aed in namespace container-probe-9146
STEP: checking the pod's current state and verifying that restartCount is present
Dec 18 19:26:04.999: INFO: Initial restart count of pod busybox-bbbc1e93-8797-4a75-885e-114ed1bb4aed is 0
Dec 18 19:26:57.048: INFO: Restart count of pod container-probe-9146/busybox-bbbc1e93-8797-4a75-885e-114ed1bb4aed is now 1 (52.049150152s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:26:57.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9146" for this suite.
Dec 18 19:27:03.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:27:03.089: INFO: namespace container-probe-9146 deletion completed in 6.035283823s

• [SLOW TEST:66.111 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:27:03.089: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Dec 18 19:27:03.104: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-207688788 proxy --unix-socket=/tmp/kubectl-proxy-unix607996343/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:27:03.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1118" for this suite.
Dec 18 19:27:09.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:27:09.192: INFO: namespace kubectl-1118 deletion completed in 6.035739217s

• [SLOW TEST:6.103 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:27:09.192: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2035
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2035
STEP: creating replication controller externalsvc in namespace services-2035
I1218 19:27:09.218826      27 runners.go:184] Created replication controller with name: externalsvc, namespace: services-2035, replica count: 2
I1218 19:27:12.269169      27 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 19:27:15.269363      27 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 19:27:18.269553      27 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 18 19:27:18.278: INFO: Creating new exec pod
Dec 18 19:27:26.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=services-2035 execpod4q6jp -- /bin/sh -x -c nslookup clusterip-service'
Dec 18 19:27:26.483: INFO: stderr: "+ nslookup clusterip-service\n"
Dec 18 19:27:26.483: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-2035.svc.cluster.local\tcanonical name = externalsvc.services-2035.svc.cluster.local.\nName:\texternalsvc.services-2035.svc.cluster.local\nAddress: 10.106.231.226\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2035, will wait for the garbage collector to delete the pods
Dec 18 19:27:26.538: INFO: Deleting ReplicationController externalsvc took: 2.570397ms
Dec 18 19:27:26.938: INFO: Terminating ReplicationController externalsvc pods took: 400.260694ms
Dec 18 19:27:42.145: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:27:42.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2035" for this suite.
Dec 18 19:27:48.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:27:48.186: INFO: namespace services-2035 deletion completed in 6.034382426s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:38.994 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:27:48.186: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 18 19:27:48.200: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 18 19:28:01.430: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 19:28:04.978: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:28:18.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5722" for this suite.
Dec 18 19:28:24.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:28:24.677: INFO: namespace crd-publish-openapi-5722 deletion completed in 6.03566156s

• [SLOW TEST:36.491 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:28:24.677: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Dec 18 19:28:24.692: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:28:40.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2862" for this suite.
Dec 18 19:28:46.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:28:47.013: INFO: namespace crd-publish-openapi-2862 deletion completed in 6.036407308s

• [SLOW TEST:22.336 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:28:47.013: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 19:28:47.610: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 18 19:28:49.614: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712294127, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712294127, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712294127, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712294127, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 19:28:51.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712294127, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712294127, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712294127, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712294127, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 19:28:53.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712294127, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712294127, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712294127, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712294127, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 19:28:56.623: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 18 19:28:56.635: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:28:56.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3003" for this suite.
Dec 18 19:29:02.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:29:02.679: INFO: namespace webhook-3003 deletion completed in 6.036087538s
STEP: Destroying namespace "webhook-3003-markers" for this suite.
Dec 18 19:29:08.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:29:08.714: INFO: namespace webhook-3003-markers deletion completed in 6.034488646s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.705 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:29:08.718: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Dec 18 19:29:08.732: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec 18 19:29:08.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-6607'
Dec 18 19:29:08.925: INFO: stderr: ""
Dec 18 19:29:08.925: INFO: stdout: "service/redis-slave created\n"
Dec 18 19:29:08.925: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec 18 19:29:08.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-6607'
Dec 18 19:29:09.073: INFO: stderr: ""
Dec 18 19:29:09.073: INFO: stdout: "service/redis-master created\n"
Dec 18 19:29:09.073: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 18 19:29:09.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-6607'
Dec 18 19:29:09.228: INFO: stderr: ""
Dec 18 19:29:09.228: INFO: stdout: "service/frontend created\n"
Dec 18 19:29:09.228: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec 18 19:29:09.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-6607'
Dec 18 19:29:09.371: INFO: stderr: ""
Dec 18 19:29:09.371: INFO: stdout: "deployment.apps/frontend created\n"
Dec 18 19:29:09.371: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 18 19:29:09.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-6607'
Dec 18 19:29:09.514: INFO: stderr: ""
Dec 18 19:29:09.514: INFO: stdout: "deployment.apps/redis-master created\n"
Dec 18 19:29:09.515: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec 18 19:29:09.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-6607'
Dec 18 19:29:09.660: INFO: stderr: ""
Dec 18 19:29:09.660: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec 18 19:29:09.660: INFO: Waiting for all frontend pods to be Running.
Dec 18 19:29:19.711: INFO: Waiting for frontend to serve content.
Dec 18 19:29:19.720: INFO: Trying to add a new entry to the guestbook.
Dec 18 19:29:19.727: INFO: Verifying that added entry can be retrieved.
Dec 18 19:29:19.735: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 18 19:29:24.742: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 18 19:29:29.751: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 18 19:29:34.760: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 18 19:29:39.770: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 18 19:29:44.779: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 18 19:29:49.788: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 18 19:29:54.798: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 18 19:29:59.808: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 18 19:30:04.818: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 18 19:30:09.827: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 18 19:30:14.836: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Dec 18 19:30:19.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete --grace-period=0 --force -f - --namespace=kubectl-6607'
Dec 18 19:30:19.914: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 18 19:30:19.914: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec 18 19:30:19.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete --grace-period=0 --force -f - --namespace=kubectl-6607'
Dec 18 19:30:19.983: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 18 19:30:19.983: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 18 19:30:19.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete --grace-period=0 --force -f - --namespace=kubectl-6607'
Dec 18 19:30:20.053: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 18 19:30:20.053: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 18 19:30:20.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete --grace-period=0 --force -f - --namespace=kubectl-6607'
Dec 18 19:30:20.120: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 18 19:30:20.120: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 18 19:30:20.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete --grace-period=0 --force -f - --namespace=kubectl-6607'
Dec 18 19:30:20.182: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 18 19:30:20.182: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 18 19:30:20.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete --grace-period=0 --force -f - --namespace=kubectl-6607'
Dec 18 19:30:20.245: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 18 19:30:20.245: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:30:20.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6607" for this suite.
Dec 18 19:30:48.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:30:48.283: INFO: namespace kubectl-6607 deletion completed in 28.036519402s

• [SLOW TEST:99.565 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:30:48.283: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2701
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 18 19:30:48.298: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 18 19:31:22.329: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.166.189:8080/dial?request=hostName&protocol=http&host=172.16.192.112&port=8080&tries=1'] Namespace:pod-network-test-2701 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 19:31:22.330: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 19:31:22.463: INFO: Waiting for endpoints: map[]
Dec 18 19:31:22.465: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.166.189:8080/dial?request=hostName&protocol=http&host=172.16.103.169&port=8080&tries=1'] Namespace:pod-network-test-2701 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 19:31:22.465: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 19:31:22.610: INFO: Waiting for endpoints: map[]
Dec 18 19:31:22.612: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.166.189:8080/dial?request=hostName&protocol=http&host=172.16.154.54&port=8080&tries=1'] Namespace:pod-network-test-2701 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 19:31:22.612: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 19:31:22.760: INFO: Waiting for endpoints: map[]
Dec 18 19:31:22.761: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.166.189:8080/dial?request=hostName&protocol=http&host=172.16.166.188&port=8080&tries=1'] Namespace:pod-network-test-2701 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 19:31:22.761: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 19:31:22.904: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:31:22.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2701" for this suite.
Dec 18 19:31:34.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:31:34.942: INFO: namespace pod-network-test-2701 deletion completed in 12.035630729s

• [SLOW TEST:46.658 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:31:34.942: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 19:31:34.958: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 18 19:31:39.960: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 18 19:31:43.963: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 18 19:31:43.971: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-692 /apis/apps/v1/namespaces/deployment-692/deployments/test-cleanup-deployment b23e45ae-f15d-40c5-aa0b-80192d07c5d5 83576 1 2019-12-18 19:31:43 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005962848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec 18 19:31:43.973: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-692 /apis/apps/v1/namespaces/deployment-692/replicasets/test-cleanup-deployment-65db99849b ca6ed54e-5dbc-4589-9dd1-e69a8ece58e3 83578 1 2019-12-18 19:31:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b23e45ae-f15d-40c5-aa0b-80192d07c5d5 0xc005962c97 0xc005962c98}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005962cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 18 19:31:43.973: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec 18 19:31:43.973: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-692 /apis/apps/v1/namespaces/deployment-692/replicasets/test-cleanup-controller f24b2b4d-efc6-4f3f-bd93-512fbdd69f7b 83577 1 2019-12-18 19:31:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment b23e45ae-f15d-40c5-aa0b-80192d07c5d5 0xc005962bc7 0xc005962bc8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005962c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 18 19:31:43.976: INFO: Pod "test-cleanup-controller-h2tjx" is available:
&Pod{ObjectMeta:{test-cleanup-controller-h2tjx test-cleanup-controller- deployment-692 /api/v1/namespaces/deployment-692/pods/test-cleanup-controller-h2tjx d1f0319b-a43e-42de-8dc8-73fceea0b859 83568 0 2019-12-18 19:31:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:172.16.166.190/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.190"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-cleanup-controller f24b2b4d-efc6-4f3f-bd93-512fbdd69f7b 0xc005963257 0xc005963258}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jg265,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jg265,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jg265,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:31:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:31:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:31:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:31:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.3,PodIP:172.16.166.190,StartTime:2019-12-18 19:31:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-18 19:31:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://989e420a6878dae4c8efc30715d091fe8f4b3eabcab1015ac5d9498d3f46d4e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 18 19:31:43.976: INFO: Pod "test-cleanup-deployment-65db99849b-q58dz" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-q58dz test-cleanup-deployment-65db99849b- deployment-692 /api/v1/namespaces/deployment-692/pods/test-cleanup-deployment-65db99849b-q58dz 665e853d-5b4a-4067-8411-5f965fbc4a38 83579 0 2019-12-18 19:31:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b ca6ed54e-5dbc-4589-9dd1-e69a8ece58e3 0xc0059633e7 0xc0059633e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jg265,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jg265,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jg265,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:31:43.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-692" for this suite.
Dec 18 19:31:49.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:31:50.013: INFO: namespace deployment-692 deletion completed in 6.034787158s

• [SLOW TEST:15.071 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:31:50.014: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 19:31:50.027: INFO: Creating ReplicaSet my-hostname-basic-faa2e160-854a-4bc3-8847-1d5437657ac0
Dec 18 19:31:50.029: INFO: Pod name my-hostname-basic-faa2e160-854a-4bc3-8847-1d5437657ac0: Found 0 pods out of 1
Dec 18 19:31:55.031: INFO: Pod name my-hostname-basic-faa2e160-854a-4bc3-8847-1d5437657ac0: Found 1 pods out of 1
Dec 18 19:31:55.031: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-faa2e160-854a-4bc3-8847-1d5437657ac0" is running
Dec 18 19:31:59.034: INFO: Pod "my-hostname-basic-faa2e160-854a-4bc3-8847-1d5437657ac0-6bg4d" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-18 19:31:50 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-18 19:31:50 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-faa2e160-854a-4bc3-8847-1d5437657ac0]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-18 19:31:50 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-faa2e160-854a-4bc3-8847-1d5437657ac0]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-18 19:31:50 +0000 UTC Reason: Message:}])
Dec 18 19:31:59.035: INFO: Trying to dial the pod
Dec 18 19:32:04.039: INFO: Controller my-hostname-basic-faa2e160-854a-4bc3-8847-1d5437657ac0: Got expected result from replica 1 [my-hostname-basic-faa2e160-854a-4bc3-8847-1d5437657ac0-6bg4d]: "my-hostname-basic-faa2e160-854a-4bc3-8847-1d5437657ac0-6bg4d", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:32:04.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3034" for this suite.
Dec 18 19:32:10.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:32:10.075: INFO: namespace replicaset-3034 deletion completed in 6.034319573s

• [SLOW TEST:20.062 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:32:10.076: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 19:32:10.091: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d17a99ae-7614-4ca7-9b8e-3ed671150b7d" in namespace "downward-api-1002" to be "success or failure"
Dec 18 19:32:10.093: INFO: Pod "downwardapi-volume-d17a99ae-7614-4ca7-9b8e-3ed671150b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.378142ms
Dec 18 19:32:12.094: INFO: Pod "downwardapi-volume-d17a99ae-7614-4ca7-9b8e-3ed671150b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003030123s
Dec 18 19:32:14.096: INFO: Pod "downwardapi-volume-d17a99ae-7614-4ca7-9b8e-3ed671150b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004730229s
Dec 18 19:32:16.098: INFO: Pod "downwardapi-volume-d17a99ae-7614-4ca7-9b8e-3ed671150b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006417046s
Dec 18 19:32:18.099: INFO: Pod "downwardapi-volume-d17a99ae-7614-4ca7-9b8e-3ed671150b7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008259197s
STEP: Saw pod success
Dec 18 19:32:18.099: INFO: Pod "downwardapi-volume-d17a99ae-7614-4ca7-9b8e-3ed671150b7d" satisfied condition "success or failure"
Dec 18 19:32:18.101: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-d17a99ae-7614-4ca7-9b8e-3ed671150b7d container client-container: <nil>
STEP: delete the pod
Dec 18 19:32:18.116: INFO: Waiting for pod downwardapi-volume-d17a99ae-7614-4ca7-9b8e-3ed671150b7d to disappear
Dec 18 19:32:18.116: INFO: Pod downwardapi-volume-d17a99ae-7614-4ca7-9b8e-3ed671150b7d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:32:18.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1002" for this suite.
Dec 18 19:32:24.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:32:24.153: INFO: namespace downward-api-1002 deletion completed in 6.035083531s

• [SLOW TEST:14.078 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:32:24.153: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 19:32:24.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98986dbb-0e74-4fcf-a0ff-f2adc24f9bcd" in namespace "downward-api-2907" to be "success or failure"
Dec 18 19:32:24.170: INFO: Pod "downwardapi-volume-98986dbb-0e74-4fcf-a0ff-f2adc24f9bcd": Phase="Pending", Reason="", readiness=false. Elapsed: 928.365µs
Dec 18 19:32:26.171: INFO: Pod "downwardapi-volume-98986dbb-0e74-4fcf-a0ff-f2adc24f9bcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002463679s
Dec 18 19:32:28.173: INFO: Pod "downwardapi-volume-98986dbb-0e74-4fcf-a0ff-f2adc24f9bcd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004307155s
Dec 18 19:32:30.175: INFO: Pod "downwardapi-volume-98986dbb-0e74-4fcf-a0ff-f2adc24f9bcd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005959044s
Dec 18 19:32:32.177: INFO: Pod "downwardapi-volume-98986dbb-0e74-4fcf-a0ff-f2adc24f9bcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007546516s
STEP: Saw pod success
Dec 18 19:32:32.177: INFO: Pod "downwardapi-volume-98986dbb-0e74-4fcf-a0ff-f2adc24f9bcd" satisfied condition "success or failure"
Dec 18 19:32:32.178: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-98986dbb-0e74-4fcf-a0ff-f2adc24f9bcd container client-container: <nil>
STEP: delete the pod
Dec 18 19:32:32.193: INFO: Waiting for pod downwardapi-volume-98986dbb-0e74-4fcf-a0ff-f2adc24f9bcd to disappear
Dec 18 19:32:32.194: INFO: Pod downwardapi-volume-98986dbb-0e74-4fcf-a0ff-f2adc24f9bcd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:32:32.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2907" for this suite.
Dec 18 19:32:38.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:32:38.230: INFO: namespace downward-api-2907 deletion completed in 6.034081826s

• [SLOW TEST:14.076 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:32:38.230: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 18 19:32:54.257: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 18 19:32:54.258: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 18 19:32:56.258: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 18 19:32:56.260: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 18 19:32:58.258: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 18 19:32:58.260: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 18 19:33:00.258: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 18 19:33:00.260: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 18 19:33:02.258: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 18 19:33:02.260: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:33:02.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9465" for this suite.
Dec 18 19:33:30.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:33:30.301: INFO: namespace container-lifecycle-hook-9465 deletion completed in 28.035323176s

• [SLOW TEST:52.071 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:33:30.302: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 19:33:30.315: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 18 19:33:32.325: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:33:33.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5685" for this suite.
Dec 18 19:33:39.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:33:39.364: INFO: namespace replication-controller-5685 deletion completed in 6.034968702s

• [SLOW TEST:9.062 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:33:39.364: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-c574acba-2ff7-41dc-b1c3-4877d383a18b
STEP: Creating a pod to test consume secrets
Dec 18 19:33:39.380: INFO: Waiting up to 5m0s for pod "pod-secrets-e3cd7145-e354-48db-ae65-08495e7c1d8f" in namespace "secrets-2987" to be "success or failure"
Dec 18 19:33:39.381: INFO: Pod "pod-secrets-e3cd7145-e354-48db-ae65-08495e7c1d8f": Phase="Pending", Reason="", readiness=false. Elapsed: 978.898µs
Dec 18 19:33:41.383: INFO: Pod "pod-secrets-e3cd7145-e354-48db-ae65-08495e7c1d8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003211664s
Dec 18 19:33:43.384: INFO: Pod "pod-secrets-e3cd7145-e354-48db-ae65-08495e7c1d8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004704109s
Dec 18 19:33:45.386: INFO: Pod "pod-secrets-e3cd7145-e354-48db-ae65-08495e7c1d8f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006329995s
Dec 18 19:33:47.388: INFO: Pod "pod-secrets-e3cd7145-e354-48db-ae65-08495e7c1d8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007963724s
STEP: Saw pod success
Dec 18 19:33:47.388: INFO: Pod "pod-secrets-e3cd7145-e354-48db-ae65-08495e7c1d8f" satisfied condition "success or failure"
Dec 18 19:33:47.389: INFO: Trying to get logs from node controller-0 pod pod-secrets-e3cd7145-e354-48db-ae65-08495e7c1d8f container secret-volume-test: <nil>
STEP: delete the pod
Dec 18 19:33:47.395: INFO: Waiting for pod pod-secrets-e3cd7145-e354-48db-ae65-08495e7c1d8f to disappear
Dec 18 19:33:47.397: INFO: Pod pod-secrets-e3cd7145-e354-48db-ae65-08495e7c1d8f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:33:47.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2987" for this suite.
Dec 18 19:33:53.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:33:53.435: INFO: namespace secrets-2987 deletion completed in 6.036342544s

• [SLOW TEST:14.070 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:33:53.435: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7987
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-7987
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7987
Dec 18 19:33:53.451: INFO: Found 0 stateful pods, waiting for 1
Dec 18 19:34:03.453: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 18 19:34:03.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-7987 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 18 19:34:03.708: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 18 19:34:03.708: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 18 19:34:03.708: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 18 19:34:03.710: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 18 19:34:13.713: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 18 19:34:13.713: INFO: Waiting for statefulset status.replicas updated to 0
Dec 18 19:34:13.718: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Dec 18 19:34:13.718: INFO: ss-0  controller-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:33:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:33:53 +0000 UTC  }]
Dec 18 19:34:13.718: INFO: 
Dec 18 19:34:13.718: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 18 19:34:14.720: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998712795s
Dec 18 19:34:15.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.9965943s
Dec 18 19:34:16.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.99460911s
Dec 18 19:34:17.726: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.99269957s
Dec 18 19:34:18.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.990559391s
Dec 18 19:34:19.730: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.988613935s
Dec 18 19:34:20.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.986692797s
Dec 18 19:34:21.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.984772271s
Dec 18 19:34:22.736: INFO: Verifying statefulset ss doesn't scale past 3 for another 982.273706ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7987
Dec 18 19:34:23.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-7987 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 18 19:34:23.933: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 18 19:34:23.933: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 18 19:34:23.933: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 18 19:34:23.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-7987 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 18 19:34:24.133: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 18 19:34:24.133: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 18 19:34:24.133: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 18 19:34:24.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-7987 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 18 19:34:24.364: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 18 19:34:24.364: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 18 19:34:24.364: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 18 19:34:24.366: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec 18 19:34:34.368: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 19:34:34.368: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 19:34:34.368: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 18 19:34:34.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-7987 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 18 19:34:34.564: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 18 19:34:34.564: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 18 19:34:34.564: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 18 19:34:34.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-7987 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 18 19:34:34.759: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 18 19:34:34.759: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 18 19:34:34.759: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 18 19:34:34.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-7987 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 18 19:34:34.994: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 18 19:34:34.994: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 18 19:34:34.994: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 18 19:34:34.994: INFO: Waiting for statefulset status.replicas updated to 0
Dec 18 19:34:34.995: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec 18 19:34:44.999: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 18 19:34:44.999: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 18 19:34:44.999: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 18 19:34:45.002: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Dec 18 19:34:45.002: INFO: ss-0  controller-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:33:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:33:53 +0000 UTC  }]
Dec 18 19:34:45.002: INFO: ss-1  controller-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:45.002: INFO: ss-2  compute-1     Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:45.002: INFO: 
Dec 18 19:34:45.002: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 18 19:34:46.004: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Dec 18 19:34:46.004: INFO: ss-0  controller-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:33:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:33:53 +0000 UTC  }]
Dec 18 19:34:46.004: INFO: ss-1  controller-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:46.004: INFO: ss-2  compute-1     Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:46.004: INFO: 
Dec 18 19:34:46.004: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 18 19:34:47.006: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Dec 18 19:34:47.007: INFO: ss-0  controller-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:33:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:33:53 +0000 UTC  }]
Dec 18 19:34:47.007: INFO: ss-1  controller-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:47.007: INFO: ss-2  compute-1     Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:47.007: INFO: 
Dec 18 19:34:47.007: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 18 19:34:48.009: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Dec 18 19:34:48.009: INFO: ss-1  controller-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:48.010: INFO: ss-2  compute-1     Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:48.010: INFO: 
Dec 18 19:34:48.010: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 18 19:34:49.012: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Dec 18 19:34:49.012: INFO: ss-1  controller-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:49.012: INFO: ss-2  compute-1     Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:49.012: INFO: 
Dec 18 19:34:49.012: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 18 19:34:50.014: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Dec 18 19:34:50.015: INFO: ss-1  controller-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:50.015: INFO: ss-2  compute-1     Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:50.015: INFO: 
Dec 18 19:34:50.015: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 18 19:34:51.016: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Dec 18 19:34:51.016: INFO: ss-1  controller-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:51.017: INFO: ss-2  compute-1     Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:51.017: INFO: 
Dec 18 19:34:51.017: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 18 19:34:52.018: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Dec 18 19:34:52.019: INFO: ss-1  controller-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-18 19:34:13 +0000 UTC  }]
Dec 18 19:34:52.019: INFO: 
Dec 18 19:34:52.019: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 18 19:34:53.021: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.982349921s
Dec 18 19:34:54.022: INFO: Verifying statefulset ss doesn't scale past 0 for another 980.291769ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7987
Dec 18 19:34:55.024: INFO: Scaling statefulset ss to 0
Dec 18 19:34:55.027: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 18 19:34:55.028: INFO: Deleting all statefulset in ns statefulset-7987
Dec 18 19:34:55.030: INFO: Scaling statefulset ss to 0
Dec 18 19:34:55.033: INFO: Waiting for statefulset status.replicas updated to 0
Dec 18 19:34:55.034: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:34:55.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7987" for this suite.
Dec 18 19:35:01.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:35:01.074: INFO: namespace statefulset-7987 deletion completed in 6.035228202s

• [SLOW TEST:67.639 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:35:01.074: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-0e647990-06e2-488f-9863-314966b94c16
STEP: Creating a pod to test consume configMaps
Dec 18 19:35:01.090: INFO: Waiting up to 5m0s for pod "pod-configmaps-13792f4c-d488-4baf-87c5-7912845a75c4" in namespace "configmap-1152" to be "success or failure"
Dec 18 19:35:01.092: INFO: Pod "pod-configmaps-13792f4c-d488-4baf-87c5-7912845a75c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.382989ms
Dec 18 19:35:03.094: INFO: Pod "pod-configmaps-13792f4c-d488-4baf-87c5-7912845a75c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003048551s
Dec 18 19:35:05.095: INFO: Pod "pod-configmaps-13792f4c-d488-4baf-87c5-7912845a75c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004738112s
Dec 18 19:35:07.097: INFO: Pod "pod-configmaps-13792f4c-d488-4baf-87c5-7912845a75c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006829899s
Dec 18 19:35:09.099: INFO: Pod "pod-configmaps-13792f4c-d488-4baf-87c5-7912845a75c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008196185s
STEP: Saw pod success
Dec 18 19:35:09.099: INFO: Pod "pod-configmaps-13792f4c-d488-4baf-87c5-7912845a75c4" satisfied condition "success or failure"
Dec 18 19:35:09.100: INFO: Trying to get logs from node controller-1 pod pod-configmaps-13792f4c-d488-4baf-87c5-7912845a75c4 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 19:35:09.115: INFO: Waiting for pod pod-configmaps-13792f4c-d488-4baf-87c5-7912845a75c4 to disappear
Dec 18 19:35:09.116: INFO: Pod pod-configmaps-13792f4c-d488-4baf-87c5-7912845a75c4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:35:09.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1152" for this suite.
Dec 18 19:35:15.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:35:15.151: INFO: namespace configmap-1152 deletion completed in 6.033388119s

• [SLOW TEST:14.076 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:35:15.151: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 18 19:35:15.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-5376'
Dec 18 19:35:15.237: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 18 19:35:15.237: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Dec 18 19:35:15.239: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-m4llf]
Dec 18 19:35:15.239: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-m4llf" in namespace "kubectl-5376" to be "running and ready"
Dec 18 19:35:15.242: INFO: Pod "e2e-test-httpd-rc-m4llf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.720346ms
Dec 18 19:35:17.244: INFO: Pod "e2e-test-httpd-rc-m4llf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004731854s
Dec 18 19:35:19.245: INFO: Pod "e2e-test-httpd-rc-m4llf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006618892s
Dec 18 19:35:21.247: INFO: Pod "e2e-test-httpd-rc-m4llf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00832406s
Dec 18 19:35:23.249: INFO: Pod "e2e-test-httpd-rc-m4llf": Phase="Running", Reason="", readiness=true. Elapsed: 8.010275411s
Dec 18 19:35:23.249: INFO: Pod "e2e-test-httpd-rc-m4llf" satisfied condition "running and ready"
Dec 18 19:35:23.249: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-m4llf]
Dec 18 19:35:23.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 logs rc/e2e-test-httpd-rc --namespace=kubectl-5376'
Dec 18 19:35:23.334: INFO: stderr: ""
Dec 18 19:35:23.334: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.16.192.118. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.16.192.118. Set the 'ServerName' directive globally to suppress this message\n[Wed Dec 18 19:35:21.837765 2019] [mpm_event:notice] [pid 1:tid 140690183818088] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Wed Dec 18 19:35:21.837793 2019] [core:notice] [pid 1:tid 140690183818088] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Dec 18 19:35:23.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete rc e2e-test-httpd-rc --namespace=kubectl-5376'
Dec 18 19:35:23.400: INFO: stderr: ""
Dec 18 19:35:23.400: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:35:23.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5376" for this suite.
Dec 18 19:35:29.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:35:29.437: INFO: namespace kubectl-5376 deletion completed in 6.034730017s

• [SLOW TEST:14.286 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:35:29.438: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Dec 18 19:35:29.460: INFO: Waiting up to 5m0s for pod "client-containers-7f878b6f-bd81-4363-8a4b-41536303354c" in namespace "containers-3998" to be "success or failure"
Dec 18 19:35:29.461: INFO: Pod "client-containers-7f878b6f-bd81-4363-8a4b-41536303354c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.395623ms
Dec 18 19:35:31.463: INFO: Pod "client-containers-7f878b6f-bd81-4363-8a4b-41536303354c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002924871s
Dec 18 19:35:33.465: INFO: Pod "client-containers-7f878b6f-bd81-4363-8a4b-41536303354c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004915052s
Dec 18 19:35:35.467: INFO: Pod "client-containers-7f878b6f-bd81-4363-8a4b-41536303354c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007052778s
Dec 18 19:35:37.469: INFO: Pod "client-containers-7f878b6f-bd81-4363-8a4b-41536303354c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008853303s
STEP: Saw pod success
Dec 18 19:35:37.469: INFO: Pod "client-containers-7f878b6f-bd81-4363-8a4b-41536303354c" satisfied condition "success or failure"
Dec 18 19:35:37.470: INFO: Trying to get logs from node controller-0 pod client-containers-7f878b6f-bd81-4363-8a4b-41536303354c container test-container: <nil>
STEP: delete the pod
Dec 18 19:35:37.478: INFO: Waiting for pod client-containers-7f878b6f-bd81-4363-8a4b-41536303354c to disappear
Dec 18 19:35:37.479: INFO: Pod client-containers-7f878b6f-bd81-4363-8a4b-41536303354c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:35:37.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3998" for this suite.
Dec 18 19:35:43.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:35:43.516: INFO: namespace containers-3998 deletion completed in 6.035789572s

• [SLOW TEST:14.079 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:35:43.516: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-30e021b3-65ba-427c-ad93-5793c877019f in namespace container-probe-3772
Dec 18 19:35:51.535: INFO: Started pod liveness-30e021b3-65ba-427c-ad93-5793c877019f in namespace container-probe-3772
STEP: checking the pod's current state and verifying that restartCount is present
Dec 18 19:35:51.536: INFO: Initial restart count of pod liveness-30e021b3-65ba-427c-ad93-5793c877019f is 0
Dec 18 19:36:03.548: INFO: Restart count of pod container-probe-3772/liveness-30e021b3-65ba-427c-ad93-5793c877019f is now 1 (12.011778637s elapsed)
Dec 18 19:36:25.568: INFO: Restart count of pod container-probe-3772/liveness-30e021b3-65ba-427c-ad93-5793c877019f is now 2 (34.032035872s elapsed)
Dec 18 19:36:45.586: INFO: Restart count of pod container-probe-3772/liveness-30e021b3-65ba-427c-ad93-5793c877019f is now 3 (54.050191184s elapsed)
Dec 18 19:37:05.605: INFO: Restart count of pod container-probe-3772/liveness-30e021b3-65ba-427c-ad93-5793c877019f is now 4 (1m14.069465945s elapsed)
Dec 18 19:38:13.668: INFO: Restart count of pod container-probe-3772/liveness-30e021b3-65ba-427c-ad93-5793c877019f is now 5 (2m22.13241045s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:38:13.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3772" for this suite.
Dec 18 19:38:19.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:38:19.709: INFO: namespace container-probe-3772 deletion completed in 6.034221481s

• [SLOW TEST:156.193 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:38:19.709: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-ea9a31bc-dce7-4851-afec-c1a88fc14bad
STEP: Creating a pod to test consume secrets
Dec 18 19:38:19.726: INFO: Waiting up to 5m0s for pod "pod-secrets-a0ae5c3b-0e85-4f67-8430-8c0281e29765" in namespace "secrets-7225" to be "success or failure"
Dec 18 19:38:19.728: INFO: Pod "pod-secrets-a0ae5c3b-0e85-4f67-8430-8c0281e29765": Phase="Pending", Reason="", readiness=false. Elapsed: 1.348861ms
Dec 18 19:38:21.729: INFO: Pod "pod-secrets-a0ae5c3b-0e85-4f67-8430-8c0281e29765": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002797469s
Dec 18 19:38:23.731: INFO: Pod "pod-secrets-a0ae5c3b-0e85-4f67-8430-8c0281e29765": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004440944s
Dec 18 19:38:25.732: INFO: Pod "pod-secrets-a0ae5c3b-0e85-4f67-8430-8c0281e29765": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006065795s
Dec 18 19:38:27.734: INFO: Pod "pod-secrets-a0ae5c3b-0e85-4f67-8430-8c0281e29765": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008074505s
STEP: Saw pod success
Dec 18 19:38:27.734: INFO: Pod "pod-secrets-a0ae5c3b-0e85-4f67-8430-8c0281e29765" satisfied condition "success or failure"
Dec 18 19:38:27.735: INFO: Trying to get logs from node controller-0 pod pod-secrets-a0ae5c3b-0e85-4f67-8430-8c0281e29765 container secret-volume-test: <nil>
STEP: delete the pod
Dec 18 19:38:27.750: INFO: Waiting for pod pod-secrets-a0ae5c3b-0e85-4f67-8430-8c0281e29765 to disappear
Dec 18 19:38:27.751: INFO: Pod pod-secrets-a0ae5c3b-0e85-4f67-8430-8c0281e29765 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:38:27.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7225" for this suite.
Dec 18 19:38:33.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:38:33.792: INFO: namespace secrets-7225 deletion completed in 6.03912363s

• [SLOW TEST:14.083 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:38:33.792: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Dec 18 19:38:33.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-5801'
Dec 18 19:38:33.952: INFO: stderr: ""
Dec 18 19:38:33.952: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 18 19:38:34.954: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:38:34.954: INFO: Found 0 / 1
Dec 18 19:38:35.954: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:38:35.954: INFO: Found 0 / 1
Dec 18 19:38:36.954: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:38:36.954: INFO: Found 0 / 1
Dec 18 19:38:37.955: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:38:37.955: INFO: Found 0 / 1
Dec 18 19:38:38.955: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:38:38.955: INFO: Found 0 / 1
Dec 18 19:38:39.954: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:38:39.954: INFO: Found 0 / 1
Dec 18 19:38:40.954: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:38:40.954: INFO: Found 0 / 1
Dec 18 19:38:41.954: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:38:41.954: INFO: Found 1 / 1
Dec 18 19:38:41.954: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 18 19:38:41.956: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:38:41.956: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 18 19:38:41.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 patch pod redis-master-t45ld --namespace=kubectl-5801 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 18 19:38:42.022: INFO: stderr: ""
Dec 18 19:38:42.022: INFO: stdout: "pod/redis-master-t45ld patched\n"
STEP: checking annotations
Dec 18 19:38:42.024: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:38:42.024: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:38:42.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5801" for this suite.
Dec 18 19:38:54.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:38:54.058: INFO: namespace kubectl-5801 deletion completed in 12.03321232s

• [SLOW TEST:20.266 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:38:54.059: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 19:38:54.071: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:38:54.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5293" for this suite.
Dec 18 19:39:00.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:39:00.622: INFO: namespace custom-resource-definition-5293 deletion completed in 6.03501661s

• [SLOW TEST:6.564 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:39:00.622: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 18 19:39:00.635: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:39:09.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4570" for this suite.
Dec 18 19:39:37.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:39:37.764: INFO: namespace init-container-4570 deletion completed in 28.03426272s

• [SLOW TEST:37.142 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:39:37.764: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 18 19:39:37.780: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 18 19:39:50.802: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:39:50.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8580" for this suite.
Dec 18 19:39:56.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:39:56.838: INFO: namespace pods-8580 deletion completed in 6.03354234s

• [SLOW TEST:19.074 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:39:56.838: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-6430
STEP: creating replication controller nodeport-test in namespace services-6430
I1218 19:39:56.859351      27 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-6430, replica count: 2
I1218 19:39:59.909701      27 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 19:40:02.909964      27 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 18 19:40:05.910: INFO: Creating new exec pod
I1218 19:40:05.910184      27 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 18 19:40:14.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=services-6430 execpodr24s4 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Dec 18 19:40:15.109: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 18 19:40:15.109: INFO: stdout: ""
Dec 18 19:40:15.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=services-6430 execpodr24s4 -- /bin/sh -x -c nc -zv -t -w 2 10.109.206.181 80'
Dec 18 19:40:15.310: INFO: stderr: "+ nc -zv -t -w 2 10.109.206.181 80\nConnection to 10.109.206.181 80 port [tcp/http] succeeded!\n"
Dec 18 19:40:15.310: INFO: stdout: ""
Dec 18 19:40:15.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=services-6430 execpodr24s4 -- /bin/sh -x -c nc -zv -t -w 2 192.168.204.40 32120'
Dec 18 19:40:15.506: INFO: stderr: "+ nc -zv -t -w 2 192.168.204.40 32120\nConnection to 192.168.204.40 32120 port [tcp/32120] succeeded!\n"
Dec 18 19:40:15.506: INFO: stdout: ""
Dec 18 19:40:15.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=services-6430 execpodr24s4 -- /bin/sh -x -c nc -zv -t -w 2 192.168.204.183 32120'
Dec 18 19:40:15.704: INFO: stderr: "+ nc -zv -t -w 2 192.168.204.183 32120\nConnection to 192.168.204.183 32120 port [tcp/32120] succeeded!\n"
Dec 18 19:40:15.704: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:40:15.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6430" for this suite.
Dec 18 19:40:21.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:40:21.741: INFO: namespace services-6430 deletion completed in 6.034725964s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:24.903 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:40:21.741: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 18 19:40:21.757: INFO: Waiting up to 5m0s for pod "pod-5fd24fbb-0495-48b5-9050-a60fea364edc" in namespace "emptydir-7796" to be "success or failure"
Dec 18 19:40:21.759: INFO: Pod "pod-5fd24fbb-0495-48b5-9050-a60fea364edc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084481ms
Dec 18 19:40:23.761: INFO: Pod "pod-5fd24fbb-0495-48b5-9050-a60fea364edc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003722834s
Dec 18 19:40:25.763: INFO: Pod "pod-5fd24fbb-0495-48b5-9050-a60fea364edc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005722923s
Dec 18 19:40:27.765: INFO: Pod "pod-5fd24fbb-0495-48b5-9050-a60fea364edc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007624341s
Dec 18 19:40:29.766: INFO: Pod "pod-5fd24fbb-0495-48b5-9050-a60fea364edc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009514584s
STEP: Saw pod success
Dec 18 19:40:29.766: INFO: Pod "pod-5fd24fbb-0495-48b5-9050-a60fea364edc" satisfied condition "success or failure"
Dec 18 19:40:29.768: INFO: Trying to get logs from node controller-0 pod pod-5fd24fbb-0495-48b5-9050-a60fea364edc container test-container: <nil>
STEP: delete the pod
Dec 18 19:40:29.783: INFO: Waiting for pod pod-5fd24fbb-0495-48b5-9050-a60fea364edc to disappear
Dec 18 19:40:29.784: INFO: Pod pod-5fd24fbb-0495-48b5-9050-a60fea364edc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:40:29.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7796" for this suite.
Dec 18 19:40:35.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:40:35.819: INFO: namespace emptydir-7796 deletion completed in 6.034123237s

• [SLOW TEST:14.078 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:40:35.820: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:40:51.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1126" for this suite.
Dec 18 19:40:57.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:40:57.885: INFO: namespace resourcequota-1126 deletion completed in 6.034203149s

• [SLOW TEST:22.065 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:40:57.885: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 18 19:40:57.901: INFO: Waiting up to 5m0s for pod "downward-api-73429390-0066-4b93-9c9a-55a178417556" in namespace "downward-api-7351" to be "success or failure"
Dec 18 19:40:57.902: INFO: Pod "downward-api-73429390-0066-4b93-9c9a-55a178417556": Phase="Pending", Reason="", readiness=false. Elapsed: 855.569µs
Dec 18 19:40:59.903: INFO: Pod "downward-api-73429390-0066-4b93-9c9a-55a178417556": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002755955s
Dec 18 19:41:01.905: INFO: Pod "downward-api-73429390-0066-4b93-9c9a-55a178417556": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004300636s
Dec 18 19:41:03.907: INFO: Pod "downward-api-73429390-0066-4b93-9c9a-55a178417556": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006123057s
Dec 18 19:41:05.908: INFO: Pod "downward-api-73429390-0066-4b93-9c9a-55a178417556": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007770852s
STEP: Saw pod success
Dec 18 19:41:05.908: INFO: Pod "downward-api-73429390-0066-4b93-9c9a-55a178417556" satisfied condition "success or failure"
Dec 18 19:41:05.909: INFO: Trying to get logs from node controller-1 pod downward-api-73429390-0066-4b93-9c9a-55a178417556 container dapi-container: <nil>
STEP: delete the pod
Dec 18 19:41:05.917: INFO: Waiting for pod downward-api-73429390-0066-4b93-9c9a-55a178417556 to disappear
Dec 18 19:41:05.918: INFO: Pod downward-api-73429390-0066-4b93-9c9a-55a178417556 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:41:05.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7351" for this suite.
Dec 18 19:41:11.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:41:11.955: INFO: namespace downward-api-7351 deletion completed in 6.035207711s

• [SLOW TEST:14.070 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:41:11.955: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:41:27.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7087" for this suite.
Dec 18 19:41:34.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:41:34.036: INFO: namespace resourcequota-7087 deletion completed in 6.035710942s

• [SLOW TEST:22.081 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:41:34.036: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 19:41:34.051: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b5237c3-8e87-4997-b496-48854d8aaf04" in namespace "projected-3899" to be "success or failure"
Dec 18 19:41:34.052: INFO: Pod "downwardapi-volume-2b5237c3-8e87-4997-b496-48854d8aaf04": Phase="Pending", Reason="", readiness=false. Elapsed: 938.557µs
Dec 18 19:41:36.054: INFO: Pod "downwardapi-volume-2b5237c3-8e87-4997-b496-48854d8aaf04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003015839s
Dec 18 19:41:38.056: INFO: Pod "downwardapi-volume-2b5237c3-8e87-4997-b496-48854d8aaf04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004464658s
Dec 18 19:41:40.057: INFO: Pod "downwardapi-volume-2b5237c3-8e87-4997-b496-48854d8aaf04": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005852634s
Dec 18 19:41:42.059: INFO: Pod "downwardapi-volume-2b5237c3-8e87-4997-b496-48854d8aaf04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007583634s
STEP: Saw pod success
Dec 18 19:41:42.059: INFO: Pod "downwardapi-volume-2b5237c3-8e87-4997-b496-48854d8aaf04" satisfied condition "success or failure"
Dec 18 19:41:42.060: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-2b5237c3-8e87-4997-b496-48854d8aaf04 container client-container: <nil>
STEP: delete the pod
Dec 18 19:41:42.068: INFO: Waiting for pod downwardapi-volume-2b5237c3-8e87-4997-b496-48854d8aaf04 to disappear
Dec 18 19:41:42.069: INFO: Pod downwardapi-volume-2b5237c3-8e87-4997-b496-48854d8aaf04 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:41:42.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3899" for this suite.
Dec 18 19:41:48.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:41:48.105: INFO: namespace projected-3899 deletion completed in 6.034246614s

• [SLOW TEST:14.069 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:41:48.105: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Dec 18 19:41:48.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 api-versions'
Dec 18 19:41:48.180: INFO: stderr: ""
Dec 18 19:41:48.180: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\narmada.process/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nk8s.cni.cncf.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:41:48.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9382" for this suite.
Dec 18 19:41:54.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:41:54.216: INFO: namespace kubectl-9382 deletion completed in 6.034590073s

• [SLOW TEST:6.111 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:41:54.216: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 18 19:41:54.229: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 18 19:41:54.234: INFO: Waiting for terminating namespaces to be deleted...
Dec 18 19:41:54.235: INFO: 
Logging pods the kubelet thinks is on node compute-0 before test
Dec 18 19:41:54.247: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-jzbct from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 19:41:54.247: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 18 19:41:54.247: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 19:41:54.247: INFO: kube-proxy-lq2c8 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.247: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 18 19:41:54.247: INFO: kube-multus-ds-amd64-fczk7 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.247: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 19:41:54.247: INFO: sonobuoy from sonobuoy started at 2019-12-18 18:17:38 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.247: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 18 19:41:54.247: INFO: kube-sriov-cni-ds-amd64-nlfxr from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.247: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 19:41:54.247: INFO: calico-node-ntnk8 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.247: INFO: 	Container calico-node ready: true, restart count 1
Dec 18 19:41:54.247: INFO: 
Logging pods the kubelet thinks is on node compute-1 before test
Dec 18 19:41:54.258: INFO: calico-node-6s2pv from kube-system started at 2019-12-18 15:11:51 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.258: INFO: 	Container calico-node ready: true, restart count 1
Dec 18 19:41:54.258: INFO: kube-multus-ds-amd64-trmzx from kube-system started at 2019-12-18 15:11:52 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.258: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 19:41:54.258: INFO: kube-sriov-cni-ds-amd64-v94kl from kube-system started at 2019-12-18 15:11:51 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.258: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 19:41:54.258: INFO: kube-proxy-w7hfz from kube-system started at 2019-12-18 15:11:52 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.258: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 18 19:41:54.258: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-49m5k from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 19:41:54.258: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 18 19:41:54.258: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 19:41:54.258: INFO: 
Logging pods the kubelet thinks is on node controller-0 before test
Dec 18 19:41:54.262: INFO: kube-scheduler-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.262: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 18 19:41:54.262: INFO: kube-proxy-hs98d from kube-system started at 2019-12-18 14:17:53 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.262: INFO: 	Container kube-proxy ready: true, restart count 1
Dec 18 19:41:54.262: INFO: kube-sriov-cni-ds-amd64-97g65 from kube-system started at 2019-12-18 18:33:43 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.262: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 19:41:54.262: INFO: calico-kube-controllers-855577b7b5-2gktm from kube-system started at 2019-12-18 18:38:16 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.262: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 18 19:41:54.262: INFO: coredns-6bc668cd76-ks6bd from kube-system started at 2019-12-18 18:33:17 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.262: INFO: 	Container coredns ready: true, restart count 0
Dec 18 19:41:54.262: INFO: calico-node-7bzgw from kube-system started at 2019-12-18 14:17:53 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.263: INFO: 	Container calico-node ready: true, restart count 3
Dec 18 19:41:54.263: INFO: kube-multus-ds-amd64-plsvq from kube-system started at 2019-12-18 18:33:43 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.263: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 19:41:54.263: INFO: tiller-deploy-d6b59fcb-kj5hv from kube-system started at 2019-12-18 18:38:16 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.263: INFO: 	Container tiller ready: true, restart count 0
Dec 18 19:41:54.263: INFO: kube-apiserver-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.263: INFO: 	Container kube-apiserver ready: true, restart count 2
Dec 18 19:41:54.263: INFO: rbd-provisioner-7484d49cf6-858lp from kube-system started at 2019-12-18 18:33:13 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.263: INFO: 	Container rbd-provisioner ready: true, restart count 0
Dec 18 19:41:54.263: INFO: kube-controller-manager-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.263: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 18 19:41:54.263: INFO: ceph-pools-audit-1576697400-56l9f from kube-system started at 2019-12-18 19:30:01 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.263: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Dec 18 19:41:54.263: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-q7fcg from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 19:41:54.263: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 18 19:41:54.263: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 19:41:54.263: INFO: 
Logging pods the kubelet thinks is on node controller-1 before test
Dec 18 19:41:54.268: INFO: ceph-pools-audit-1576697700-s5xkc from kube-system started at 2019-12-18 19:35:01 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.268: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Dec 18 19:41:54.268: INFO: kube-apiserver-controller-1 from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.268: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 18 19:41:54.268: INFO: kube-proxy-fds2l from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.268: INFO: 	Container kube-proxy ready: true, restart count 1
Dec 18 19:41:54.268: INFO: coredns-6bc668cd76-86sgt from kube-system started at 2019-12-18 18:38:56 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.268: INFO: 	Container coredns ready: true, restart count 0
Dec 18 19:41:54.268: INFO: sonobuoy-e2e-job-8824c3aaef6e4480 from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 19:41:54.268: INFO: 	Container e2e ready: true, restart count 0
Dec 18 19:41:54.268: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 18 19:41:54.268: INFO: kube-scheduler-controller-1 from kube-system started at 2019-12-18 15:22:52 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.268: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 18 19:41:54.268: INFO: kube-multus-ds-amd64-rvvrh from kube-system started at 2019-12-18 18:38:52 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.268: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 19:41:54.268: INFO: kube-controller-manager-controller-1 from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.268: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 18 19:41:54.268: INFO: kube-sriov-cni-ds-amd64-rb5tl from kube-system started at 2019-12-18 18:38:52 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.268: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 19:41:54.268: INFO: rbd-provisioner-7484d49cf6-p98d2 from kube-system started at 2019-12-18 18:38:56 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.268: INFO: 	Container rbd-provisioner ready: true, restart count 0
Dec 18 19:41:54.268: INFO: ceph-pools-audit-1576698000-cpvd7 from kube-system started at 2019-12-18 19:40:01 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.268: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Dec 18 19:41:54.268: INFO: calico-node-8q8ss from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 19:41:54.268: INFO: 	Container calico-node ready: true, restart count 4
Dec 18 19:41:54.268: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-rz72p from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 19:41:54.268: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 18 19:41:54.268: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-58278d32-7768-4af8-8bad-96e8cfbc2c5b 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-58278d32-7768-4af8-8bad-96e8cfbc2c5b off the node controller-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-58278d32-7768-4af8-8bad-96e8cfbc2c5b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:42:26.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3088" for this suite.
Dec 18 19:42:38.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:42:38.337: INFO: namespace sched-pred-3088 deletion completed in 12.034050487s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:44.121 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:42:38.338: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 18 19:42:38.353: INFO: Waiting up to 5m0s for pod "pod-bec46f21-d9e4-490f-a0ef-8ba5397e863f" in namespace "emptydir-93" to be "success or failure"
Dec 18 19:42:38.354: INFO: Pod "pod-bec46f21-d9e4-490f-a0ef-8ba5397e863f": Phase="Pending", Reason="", readiness=false. Elapsed: 892.426µs
Dec 18 19:42:40.356: INFO: Pod "pod-bec46f21-d9e4-490f-a0ef-8ba5397e863f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002806158s
Dec 18 19:42:42.357: INFO: Pod "pod-bec46f21-d9e4-490f-a0ef-8ba5397e863f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004409379s
Dec 18 19:42:44.359: INFO: Pod "pod-bec46f21-d9e4-490f-a0ef-8ba5397e863f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005787951s
Dec 18 19:42:46.360: INFO: Pod "pod-bec46f21-d9e4-490f-a0ef-8ba5397e863f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007406098s
STEP: Saw pod success
Dec 18 19:42:46.360: INFO: Pod "pod-bec46f21-d9e4-490f-a0ef-8ba5397e863f" satisfied condition "success or failure"
Dec 18 19:42:46.361: INFO: Trying to get logs from node controller-0 pod pod-bec46f21-d9e4-490f-a0ef-8ba5397e863f container test-container: <nil>
STEP: delete the pod
Dec 18 19:42:46.369: INFO: Waiting for pod pod-bec46f21-d9e4-490f-a0ef-8ba5397e863f to disappear
Dec 18 19:42:46.370: INFO: Pod pod-bec46f21-d9e4-490f-a0ef-8ba5397e863f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:42:46.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-93" for this suite.
Dec 18 19:42:52.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:42:52.406: INFO: namespace emptydir-93 deletion completed in 6.033911505s

• [SLOW TEST:14.068 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:42:52.406: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 18 19:42:52.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7036'
Dec 18 19:42:52.490: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 18 19:42:52.490: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Dec 18 19:42:52.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete jobs e2e-test-httpd-job --namespace=kubectl-7036'
Dec 18 19:42:52.590: INFO: stderr: ""
Dec 18 19:42:52.590: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:42:52.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7036" for this suite.
Dec 18 19:42:58.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:42:58.626: INFO: namespace kubectl-7036 deletion completed in 6.034232513s

• [SLOW TEST:6.220 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:42:58.626: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-6951
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6951 to expose endpoints map[]
Dec 18 19:42:58.645: INFO: Get endpoints failed (832.407µs elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 18 19:42:59.646: INFO: successfully validated that service endpoint-test2 in namespace services-6951 exposes endpoints map[] (1.002322127s elapsed)
STEP: Creating pod pod1 in namespace services-6951
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6951 to expose endpoints map[pod1:[80]]
Dec 18 19:43:03.662: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.013228194s elapsed, will retry)
Dec 18 19:43:06.671: INFO: successfully validated that service endpoint-test2 in namespace services-6951 exposes endpoints map[pod1:[80]] (7.022137596s elapsed)
STEP: Creating pod pod2 in namespace services-6951
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6951 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 18 19:43:10.691: INFO: Unexpected endpoints: found map[b2c6a09f-4d43-4209-b258-53667bd64921:[80]], expected map[pod1:[80] pod2:[80]] (4.018368048s elapsed, will retry)
Dec 18 19:43:14.706: INFO: successfully validated that service endpoint-test2 in namespace services-6951 exposes endpoints map[pod1:[80] pod2:[80]] (8.033081672s elapsed)
STEP: Deleting pod pod1 in namespace services-6951
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6951 to expose endpoints map[pod2:[80]]
Dec 18 19:43:15.713: INFO: successfully validated that service endpoint-test2 in namespace services-6951 exposes endpoints map[pod2:[80]] (1.005184195s elapsed)
STEP: Deleting pod pod2 in namespace services-6951
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6951 to expose endpoints map[]
Dec 18 19:43:16.717: INFO: successfully validated that service endpoint-test2 in namespace services-6951 exposes endpoints map[] (1.002252716s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:43:16.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6951" for this suite.
Dec 18 19:43:28.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:43:28.762: INFO: namespace services-6951 deletion completed in 12.034658936s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:30.136 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:43:28.762: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Dec 18 19:43:28.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-8818'
Dec 18 19:43:28.971: INFO: stderr: ""
Dec 18 19:43:28.971: INFO: stdout: "pod/pause created\n"
Dec 18 19:43:28.971: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 18 19:43:28.971: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8818" to be "running and ready"
Dec 18 19:43:28.972: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.174009ms
Dec 18 19:43:30.974: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002884101s
Dec 18 19:43:32.975: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004319739s
Dec 18 19:43:34.977: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005843151s
Dec 18 19:43:36.978: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 8.007269123s
Dec 18 19:43:36.978: INFO: Pod "pause" satisfied condition "running and ready"
Dec 18 19:43:36.978: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 18 19:43:36.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 label pods pause testing-label=testing-label-value --namespace=kubectl-8818'
Dec 18 19:43:37.044: INFO: stderr: ""
Dec 18 19:43:37.044: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 18 19:43:37.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pod pause -L testing-label --namespace=kubectl-8818'
Dec 18 19:43:37.109: INFO: stderr: ""
Dec 18 19:43:37.109: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          9s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 18 19:43:37.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 label pods pause testing-label- --namespace=kubectl-8818'
Dec 18 19:43:37.174: INFO: stderr: ""
Dec 18 19:43:37.174: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 18 19:43:37.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pod pause -L testing-label --namespace=kubectl-8818'
Dec 18 19:43:37.235: INFO: stderr: ""
Dec 18 19:43:37.235: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          9s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Dec 18 19:43:37.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete --grace-period=0 --force -f - --namespace=kubectl-8818'
Dec 18 19:43:37.298: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 18 19:43:37.298: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 18 19:43:37.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get rc,svc -l name=pause --no-headers --namespace=kubectl-8818'
Dec 18 19:43:37.364: INFO: stderr: "No resources found in kubectl-8818 namespace.\n"
Dec 18 19:43:37.364: INFO: stdout: ""
Dec 18 19:43:37.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 get pods -l name=pause --namespace=kubectl-8818 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 18 19:43:37.424: INFO: stderr: ""
Dec 18 19:43:37.424: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:43:37.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8818" for this suite.
Dec 18 19:43:43.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:43:43.460: INFO: namespace kubectl-8818 deletion completed in 6.034162142s

• [SLOW TEST:14.698 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:43:43.461: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 18 19:43:43.476: INFO: Waiting up to 5m0s for pod "pod-077def87-eebd-4261-8681-58d4284589d4" in namespace "emptydir-2871" to be "success or failure"
Dec 18 19:43:43.477: INFO: Pod "pod-077def87-eebd-4261-8681-58d4284589d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.456105ms
Dec 18 19:43:45.479: INFO: Pod "pod-077def87-eebd-4261-8681-58d4284589d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003281047s
Dec 18 19:43:47.481: INFO: Pod "pod-077def87-eebd-4261-8681-58d4284589d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005126954s
Dec 18 19:43:49.483: INFO: Pod "pod-077def87-eebd-4261-8681-58d4284589d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006680904s
Dec 18 19:43:51.484: INFO: Pod "pod-077def87-eebd-4261-8681-58d4284589d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008370088s
STEP: Saw pod success
Dec 18 19:43:51.484: INFO: Pod "pod-077def87-eebd-4261-8681-58d4284589d4" satisfied condition "success or failure"
Dec 18 19:43:51.485: INFO: Trying to get logs from node controller-1 pod pod-077def87-eebd-4261-8681-58d4284589d4 container test-container: <nil>
STEP: delete the pod
Dec 18 19:43:51.499: INFO: Waiting for pod pod-077def87-eebd-4261-8681-58d4284589d4 to disappear
Dec 18 19:43:51.499: INFO: Pod pod-077def87-eebd-4261-8681-58d4284589d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:43:51.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2871" for this suite.
Dec 18 19:43:57.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:43:57.535: INFO: namespace emptydir-2871 deletion completed in 6.034182311s

• [SLOW TEST:14.075 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:43:57.535: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 19:43:57.551: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c5c4e66e-ccf1-46a4-b604-ee7bb186f3d2" in namespace "security-context-test-582" to be "success or failure"
Dec 18 19:43:57.551: INFO: Pod "alpine-nnp-false-c5c4e66e-ccf1-46a4-b604-ee7bb186f3d2": Phase="Pending", Reason="", readiness=false. Elapsed: 870.558µs
Dec 18 19:43:59.553: INFO: Pod "alpine-nnp-false-c5c4e66e-ccf1-46a4-b604-ee7bb186f3d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002603013s
Dec 18 19:44:01.555: INFO: Pod "alpine-nnp-false-c5c4e66e-ccf1-46a4-b604-ee7bb186f3d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004475993s
Dec 18 19:44:03.558: INFO: Pod "alpine-nnp-false-c5c4e66e-ccf1-46a4-b604-ee7bb186f3d2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007065038s
Dec 18 19:44:05.560: INFO: Pod "alpine-nnp-false-c5c4e66e-ccf1-46a4-b604-ee7bb186f3d2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0089176s
Dec 18 19:44:07.562: INFO: Pod "alpine-nnp-false-c5c4e66e-ccf1-46a4-b604-ee7bb186f3d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.010992073s
Dec 18 19:44:07.562: INFO: Pod "alpine-nnp-false-c5c4e66e-ccf1-46a4-b604-ee7bb186f3d2" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:44:07.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-582" for this suite.
Dec 18 19:44:13.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:44:13.604: INFO: namespace security-context-test-582 deletion completed in 6.036118756s

• [SLOW TEST:16.068 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:44:13.604: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8953.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8953.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8953.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8953.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 219.125.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.125.219_udp@PTR;check="$$(dig +tcp +noall +answer +search 219.125.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.125.219_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8953.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8953.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8953.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8953.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 219.125.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.125.219_udp@PTR;check="$$(dig +tcp +noall +answer +search 219.125.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.125.219_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 18 19:44:21.635: INFO: Unable to read wheezy_udp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:21.636: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:21.639: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:21.648: INFO: Unable to read jessie_udp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:21.649: INFO: Unable to read jessie_tcp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:21.660: INFO: Lookups using dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9 failed for: [wheezy_udp@dns-test-service.dns-8953.svc.cluster.local wheezy_tcp@dns-test-service.dns-8953.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8953.svc.cluster.local jessie_udp@dns-test-service.dns-8953.svc.cluster.local jessie_tcp@dns-test-service.dns-8953.svc.cluster.local]

Dec 18 19:44:26.662: INFO: Unable to read wheezy_udp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:26.664: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:26.675: INFO: Unable to read jessie_udp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:26.677: INFO: Unable to read jessie_tcp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:26.687: INFO: Lookups using dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9 failed for: [wheezy_udp@dns-test-service.dns-8953.svc.cluster.local wheezy_tcp@dns-test-service.dns-8953.svc.cluster.local jessie_udp@dns-test-service.dns-8953.svc.cluster.local jessie_tcp@dns-test-service.dns-8953.svc.cluster.local]

Dec 18 19:44:31.662: INFO: Unable to read wheezy_udp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:31.664: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:31.676: INFO: Unable to read jessie_udp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:31.677: INFO: Unable to read jessie_tcp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:31.687: INFO: Lookups using dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9 failed for: [wheezy_udp@dns-test-service.dns-8953.svc.cluster.local wheezy_tcp@dns-test-service.dns-8953.svc.cluster.local jessie_udp@dns-test-service.dns-8953.svc.cluster.local jessie_tcp@dns-test-service.dns-8953.svc.cluster.local]

Dec 18 19:44:36.662: INFO: Unable to read wheezy_udp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:36.664: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:36.675: INFO: Unable to read jessie_udp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:36.677: INFO: Unable to read jessie_tcp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:36.686: INFO: Lookups using dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9 failed for: [wheezy_udp@dns-test-service.dns-8953.svc.cluster.local wheezy_tcp@dns-test-service.dns-8953.svc.cluster.local jessie_udp@dns-test-service.dns-8953.svc.cluster.local jessie_tcp@dns-test-service.dns-8953.svc.cluster.local]

Dec 18 19:44:41.663: INFO: Unable to read wheezy_udp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:41.664: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:41.676: INFO: Unable to read jessie_udp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:41.677: INFO: Unable to read jessie_tcp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:41.687: INFO: Lookups using dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9 failed for: [wheezy_udp@dns-test-service.dns-8953.svc.cluster.local wheezy_tcp@dns-test-service.dns-8953.svc.cluster.local jessie_udp@dns-test-service.dns-8953.svc.cluster.local jessie_tcp@dns-test-service.dns-8953.svc.cluster.local]

Dec 18 19:44:46.664: INFO: Unable to read wheezy_udp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:46.666: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:46.677: INFO: Unable to read jessie_udp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:46.679: INFO: Unable to read jessie_tcp@dns-test-service.dns-8953.svc.cluster.local from pod dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9: the server could not find the requested resource (get pods dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9)
Dec 18 19:44:46.689: INFO: Lookups using dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9 failed for: [wheezy_udp@dns-test-service.dns-8953.svc.cluster.local wheezy_tcp@dns-test-service.dns-8953.svc.cluster.local jessie_udp@dns-test-service.dns-8953.svc.cluster.local jessie_tcp@dns-test-service.dns-8953.svc.cluster.local]

Dec 18 19:44:51.688: INFO: DNS probes using dns-8953/dns-test-90555ca7-b0f3-4f38-8b85-10a835e192f9 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:44:51.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8953" for this suite.
Dec 18 19:44:57.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:44:57.739: INFO: namespace dns-8953 deletion completed in 6.033738966s

• [SLOW TEST:44.136 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:44:57.740: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6673
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Dec 18 19:44:57.757: INFO: Found 0 stateful pods, waiting for 3
Dec 18 19:45:07.759: INFO: Found 2 stateful pods, waiting for 3
Dec 18 19:45:17.759: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 19:45:17.759: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 19:45:17.759: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 18 19:45:27.759: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 19:45:27.759: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 19:45:27.759: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 19:45:27.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-6673 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 18 19:45:28.016: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 18 19:45:28.016: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 18 19:45:28.016: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 18 19:45:38.036: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 18 19:45:48.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-6673 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 18 19:45:48.247: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 18 19:45:48.247: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 18 19:45:48.247: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 18 19:45:58.256: INFO: Waiting for StatefulSet statefulset-6673/ss2 to complete update
Dec 18 19:45:58.256: INFO: Waiting for Pod statefulset-6673/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 18 19:45:58.256: INFO: Waiting for Pod statefulset-6673/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 18 19:45:58.256: INFO: Waiting for Pod statefulset-6673/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 18 19:46:08.259: INFO: Waiting for StatefulSet statefulset-6673/ss2 to complete update
Dec 18 19:46:08.259: INFO: Waiting for Pod statefulset-6673/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 18 19:46:08.259: INFO: Waiting for Pod statefulset-6673/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 18 19:46:18.260: INFO: Waiting for StatefulSet statefulset-6673/ss2 to complete update
Dec 18 19:46:18.260: INFO: Waiting for Pod statefulset-6673/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 18 19:46:28.259: INFO: Waiting for StatefulSet statefulset-6673/ss2 to complete update
Dec 18 19:46:28.259: INFO: Waiting for Pod statefulset-6673/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 18 19:46:38.260: INFO: Waiting for StatefulSet statefulset-6673/ss2 to complete update
STEP: Rolling back to a previous revision
Dec 18 19:46:48.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-6673 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 18 19:46:48.456: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 18 19:46:48.456: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 18 19:46:48.456: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 18 19:46:58.476: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 18 19:47:08.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-6673 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 18 19:47:08.680: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 18 19:47:08.680: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 18 19:47:08.680: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 18 19:47:18.689: INFO: Waiting for StatefulSet statefulset-6673/ss2 to complete update
Dec 18 19:47:18.689: INFO: Waiting for Pod statefulset-6673/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 18 19:47:18.689: INFO: Waiting for Pod statefulset-6673/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 18 19:47:28.692: INFO: Waiting for StatefulSet statefulset-6673/ss2 to complete update
Dec 18 19:47:28.692: INFO: Waiting for Pod statefulset-6673/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 18 19:47:38.693: INFO: Waiting for StatefulSet statefulset-6673/ss2 to complete update
Dec 18 19:47:38.693: INFO: Waiting for Pod statefulset-6673/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 18 19:47:48.692: INFO: Waiting for StatefulSet statefulset-6673/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 18 19:47:58.692: INFO: Deleting all statefulset in ns statefulset-6673
Dec 18 19:47:58.693: INFO: Scaling statefulset ss2 to 0
Dec 18 19:48:28.699: INFO: Waiting for statefulset status.replicas updated to 0
Dec 18 19:48:28.700: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:48:28.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6673" for this suite.
Dec 18 19:48:34.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:48:34.741: INFO: namespace statefulset-6673 deletion completed in 6.035812354s

• [SLOW TEST:217.002 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:48:34.741: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 18 19:48:34.757: INFO: Waiting up to 5m0s for pod "pod-7d1917e9-416e-4c72-9873-4d77cb3a9963" in namespace "emptydir-5295" to be "success or failure"
Dec 18 19:48:34.758: INFO: Pod "pod-7d1917e9-416e-4c72-9873-4d77cb3a9963": Phase="Pending", Reason="", readiness=false. Elapsed: 823.395µs
Dec 18 19:48:36.760: INFO: Pod "pod-7d1917e9-416e-4c72-9873-4d77cb3a9963": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002343213s
Dec 18 19:48:38.762: INFO: Pod "pod-7d1917e9-416e-4c72-9873-4d77cb3a9963": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00438916s
Dec 18 19:48:40.763: INFO: Pod "pod-7d1917e9-416e-4c72-9873-4d77cb3a9963": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005957152s
Dec 18 19:48:42.765: INFO: Pod "pod-7d1917e9-416e-4c72-9873-4d77cb3a9963": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007436436s
STEP: Saw pod success
Dec 18 19:48:42.765: INFO: Pod "pod-7d1917e9-416e-4c72-9873-4d77cb3a9963" satisfied condition "success or failure"
Dec 18 19:48:42.766: INFO: Trying to get logs from node controller-0 pod pod-7d1917e9-416e-4c72-9873-4d77cb3a9963 container test-container: <nil>
STEP: delete the pod
Dec 18 19:48:42.780: INFO: Waiting for pod pod-7d1917e9-416e-4c72-9873-4d77cb3a9963 to disappear
Dec 18 19:48:42.781: INFO: Pod pod-7d1917e9-416e-4c72-9873-4d77cb3a9963 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:48:42.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5295" for this suite.
Dec 18 19:48:48.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:48:48.831: INFO: namespace emptydir-5295 deletion completed in 6.047816699s

• [SLOW TEST:14.089 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:48:48.831: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 19:48:48.849: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8172981-c92c-4803-abb5-26d28b0d1fba" in namespace "projected-2593" to be "success or failure"
Dec 18 19:48:48.850: INFO: Pod "downwardapi-volume-e8172981-c92c-4803-abb5-26d28b0d1fba": Phase="Pending", Reason="", readiness=false. Elapsed: 918.561µs
Dec 18 19:48:50.852: INFO: Pod "downwardapi-volume-e8172981-c92c-4803-abb5-26d28b0d1fba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002743726s
Dec 18 19:48:52.854: INFO: Pod "downwardapi-volume-e8172981-c92c-4803-abb5-26d28b0d1fba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004452485s
Dec 18 19:48:54.856: INFO: Pod "downwardapi-volume-e8172981-c92c-4803-abb5-26d28b0d1fba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006461009s
Dec 18 19:48:56.857: INFO: Pod "downwardapi-volume-e8172981-c92c-4803-abb5-26d28b0d1fba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008172932s
STEP: Saw pod success
Dec 18 19:48:56.857: INFO: Pod "downwardapi-volume-e8172981-c92c-4803-abb5-26d28b0d1fba" satisfied condition "success or failure"
Dec 18 19:48:56.859: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-e8172981-c92c-4803-abb5-26d28b0d1fba container client-container: <nil>
STEP: delete the pod
Dec 18 19:48:56.866: INFO: Waiting for pod downwardapi-volume-e8172981-c92c-4803-abb5-26d28b0d1fba to disappear
Dec 18 19:48:56.867: INFO: Pod downwardapi-volume-e8172981-c92c-4803-abb5-26d28b0d1fba no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:48:56.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2593" for this suite.
Dec 18 19:49:02.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:49:02.902: INFO: namespace projected-2593 deletion completed in 6.033452771s

• [SLOW TEST:14.071 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:49:02.902: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 19:49:02.923: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 18 19:49:02.926: INFO: Number of nodes with available pods: 0
Dec 18 19:49:02.926: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 18 19:49:02.931: INFO: Number of nodes with available pods: 0
Dec 18 19:49:02.931: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:03.933: INFO: Number of nodes with available pods: 0
Dec 18 19:49:03.933: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:04.934: INFO: Number of nodes with available pods: 0
Dec 18 19:49:04.934: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:05.933: INFO: Number of nodes with available pods: 0
Dec 18 19:49:05.933: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:06.932: INFO: Number of nodes with available pods: 0
Dec 18 19:49:06.932: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:07.933: INFO: Number of nodes with available pods: 0
Dec 18 19:49:07.933: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:08.933: INFO: Number of nodes with available pods: 0
Dec 18 19:49:08.933: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:09.933: INFO: Number of nodes with available pods: 0
Dec 18 19:49:09.933: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:10.933: INFO: Number of nodes with available pods: 1
Dec 18 19:49:10.933: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 18 19:49:10.939: INFO: Number of nodes with available pods: 1
Dec 18 19:49:10.939: INFO: Number of running nodes: 0, number of available pods: 1
Dec 18 19:49:11.941: INFO: Number of nodes with available pods: 0
Dec 18 19:49:11.941: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 18 19:49:11.945: INFO: Number of nodes with available pods: 0
Dec 18 19:49:11.945: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:12.946: INFO: Number of nodes with available pods: 0
Dec 18 19:49:12.946: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:13.947: INFO: Number of nodes with available pods: 0
Dec 18 19:49:13.947: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:14.946: INFO: Number of nodes with available pods: 0
Dec 18 19:49:14.946: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:15.946: INFO: Number of nodes with available pods: 0
Dec 18 19:49:15.946: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:16.946: INFO: Number of nodes with available pods: 0
Dec 18 19:49:16.946: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:17.946: INFO: Number of nodes with available pods: 0
Dec 18 19:49:17.946: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:18.946: INFO: Number of nodes with available pods: 0
Dec 18 19:49:18.946: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:19.946: INFO: Number of nodes with available pods: 0
Dec 18 19:49:19.946: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:20.946: INFO: Number of nodes with available pods: 0
Dec 18 19:49:20.946: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:49:21.947: INFO: Number of nodes with available pods: 1
Dec 18 19:49:21.947: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2838, will wait for the garbage collector to delete the pods
Dec 18 19:49:22.003: INFO: Deleting DaemonSet.extensions daemon-set took: 2.521593ms
Dec 18 19:49:22.403: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.158781ms
Dec 18 19:49:33.904: INFO: Number of nodes with available pods: 0
Dec 18 19:49:33.904: INFO: Number of running nodes: 0, number of available pods: 0
Dec 18 19:49:33.905: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2838/daemonsets","resourceVersion":"89042"},"items":null}

Dec 18 19:49:33.906: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2838/pods","resourceVersion":"89042"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:49:33.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2838" for this suite.
Dec 18 19:49:39.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:49:39.951: INFO: namespace daemonsets-2838 deletion completed in 6.035689908s

• [SLOW TEST:37.049 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:49:39.951: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-cd1ac75f-1531-439c-a643-b6898e95a28f
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:49:39.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5909" for this suite.
Dec 18 19:49:45.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:49:46.002: INFO: namespace configmap-5909 deletion completed in 6.034224521s

• [SLOW TEST:6.050 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:49:46.002: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 19:49:46.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-4614'
Dec 18 19:49:46.158: INFO: stderr: ""
Dec 18 19:49:46.158: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec 18 19:49:46.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 create -f - --namespace=kubectl-4614'
Dec 18 19:49:46.308: INFO: stderr: ""
Dec 18 19:49:46.308: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 18 19:49:47.311: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:49:47.311: INFO: Found 0 / 1
Dec 18 19:49:48.310: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:49:48.310: INFO: Found 0 / 1
Dec 18 19:49:49.311: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:49:49.311: INFO: Found 0 / 1
Dec 18 19:49:50.310: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:49:50.310: INFO: Found 0 / 1
Dec 18 19:49:51.310: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:49:51.310: INFO: Found 0 / 1
Dec 18 19:49:52.310: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:49:52.310: INFO: Found 0 / 1
Dec 18 19:49:53.310: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:49:53.310: INFO: Found 1 / 1
Dec 18 19:49:53.310: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 18 19:49:53.312: INFO: Selector matched 1 pods for map[app:redis]
Dec 18 19:49:53.312: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 18 19:49:53.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 describe pod redis-master-qc4r8 --namespace=kubectl-4614'
Dec 18 19:49:53.385: INFO: stderr: ""
Dec 18 19:49:53.386: INFO: stdout: "Name:         redis-master-qc4r8\nNamespace:    kubectl-4614\nPriority:     0\nNode:         controller-1/192.168.204.3\nStart Time:   Wed, 18 Dec 2019 19:49:46 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.16.166.149/32\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"chain\",\n                    \"ips\": [\n                        \"172.16.166.149\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\nStatus:       Running\nIP:           172.16.166.149\nIPs:\n  IP:           172.16.166.149\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://bd658549edf088d545e7011031c012232501468768592f6b180b547979637541\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 18 Dec 2019 19:49:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-h829s (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-h829s:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-h829s\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 30s\n                 node.kubernetes.io/unreachable:NoExecute for 30s\nEvents:\n  Type    Reason     Age        From                   Message\n  ----    ------     ----       ----                   -------\n  Normal  Scheduled  <unknown>  default-scheduler      Successfully assigned kubectl-4614/redis-master-qc4r8 to controller-1\n  Normal  Pulled     1s         kubelet, controller-1  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, controller-1  Created container redis-master\n  Normal  Started    1s         kubelet, controller-1  Started container redis-master\n"
Dec 18 19:49:53.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 describe rc redis-master --namespace=kubectl-4614'
Dec 18 19:49:53.463: INFO: stderr: ""
Dec 18 19:49:53.463: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4614\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  7s    replication-controller  Created pod: redis-master-qc4r8\n"
Dec 18 19:49:53.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 describe service redis-master --namespace=kubectl-4614'
Dec 18 19:49:53.533: INFO: stderr: ""
Dec 18 19:49:53.533: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4614\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.111.36.151\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.16.166.149:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 18 19:49:53.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 describe node compute-0'
Dec 18 19:49:53.629: INFO: stderr: ""
Dec 18 19:49:53.629: INFO: stdout: "Name:               compute-0\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=compute-0\n                    kubernetes.io/os=linux\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.205.97/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.16.154.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 18 Dec 2019 15:12:04 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 18 Dec 2019 15:12:42 +0000   Wed, 18 Dec 2019 15:12:42 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 18 Dec 2019 19:49:53 +0000   Wed, 18 Dec 2019 15:12:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 18 Dec 2019 19:49:53 +0000   Wed, 18 Dec 2019 15:12:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 18 Dec 2019 19:49:53 +0000   Wed, 18 Dec 2019 15:12:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 18 Dec 2019 19:49:53 +0000   Wed, 18 Dec 2019 15:12:37 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.204.40\n  Hostname:    compute-0\nCapacity:\n cpu:                20\n ephemeral-storage:  10190100Ki\n hugepages-1Gi:      16Gi\n hugepages-2Mi:      0\n memory:             32731588Ki\n pods:               110\nAllocatable:\n cpu:                19\n ephemeral-storage:  9391196145\n hugepages-1Gi:      16Gi\n hugepages-2Mi:      0\n memory:             7659972Ki\n pods:               110\nSystem Info:\n Machine ID:                 7c820e5626c742fcb19619d61fba45b9\n System UUID:                F644095C-73CD-E111-BE39-001E67515708\n Boot ID:                    5c55da0b-e3db-49a6-b36c-0601e4c148a2\n Kernel Version:             3.10.0-957.21.3.el7.2.tis.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.6\n Kubelet Version:            v1.16.2\n Kube-Proxy Version:         v1.16.2\nPodCIDR:                     172.16.3.0/24\nPodCIDRs:                    172.16.3.0/24\nNon-terminated Pods:         (6 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                calico-node-ntnk8                                          250m (1%)     0 (0%)      0 (0%)           0 (0%)         4h37m\n  kube-system                kube-multus-ds-amd64-fczk7                                 100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      4h37m\n  kube-system                kube-proxy-lq2c8                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h37m\n  kube-system                kube-sriov-cni-ds-amd64-nlfxr                              100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      4h37m\n  sonobuoy                   sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-c91b178dff064717-jzbct    0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                450m (2%)   200m (1%)\n  memory             100Mi (1%)  100Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                    From                   Message\n  ----    ------                   ----                   ----                   -------\n  Normal  Starting                 4h37m                  kubelet, compute-0     Starting kubelet.\n  Normal  NodeHasSufficientMemory  4h37m (x2 over 4h37m)  kubelet, compute-0     Node compute-0 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    4h37m (x2 over 4h37m)  kubelet, compute-0     Node compute-0 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     4h37m (x2 over 4h37m)  kubelet, compute-0     Node compute-0 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  4h37m                  kubelet, compute-0     Updated Node Allocatable limit across pods\n  Normal  Starting                 4h37m                  kube-proxy, compute-0  Starting kube-proxy.\n  Normal  NodeReady                4h37m                  kubelet, compute-0     Node compute-0 status is now: NodeReady\n"
Dec 18 19:49:53.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 describe namespace kubectl-4614'
Dec 18 19:49:53.694: INFO: stderr: ""
Dec 18 19:49:53.695: INFO: stdout: "Name:         kubectl-4614\nLabels:       e2e-framework=kubectl\n              e2e-run=0c59fe68-2173-4430-b06c-f27acbe8d08d\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:49:53.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4614" for this suite.
Dec 18 19:50:05.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:50:05.729: INFO: namespace kubectl-4614 deletion completed in 12.033155813s

• [SLOW TEST:19.727 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:50:05.730: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:50:22.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6074" for this suite.
Dec 18 19:50:28.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:50:28.797: INFO: namespace resourcequota-6074 deletion completed in 6.034891975s

• [SLOW TEST:23.067 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:50:28.798: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-8041
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8041 to expose endpoints map[]
Dec 18 19:50:28.817: INFO: Get endpoints failed (773.168µs elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec 18 19:50:29.818: INFO: successfully validated that service multi-endpoint-test in namespace services-8041 exposes endpoints map[] (1.002105183s elapsed)
STEP: Creating pod pod1 in namespace services-8041
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8041 to expose endpoints map[pod1:[100]]
Dec 18 19:50:33.832: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.011410183s elapsed, will retry)
Dec 18 19:50:36.840: INFO: successfully validated that service multi-endpoint-test in namespace services-8041 exposes endpoints map[pod1:[100]] (7.018795417s elapsed)
STEP: Creating pod pod2 in namespace services-8041
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8041 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 18 19:50:40.861: INFO: Unexpected endpoints: found map[82e6a931-d9e0-4980-af62-02bd7608c53a:[100]], expected map[pod1:[100] pod2:[101]] (4.019621348s elapsed, will retry)
Dec 18 19:50:43.873: INFO: successfully validated that service multi-endpoint-test in namespace services-8041 exposes endpoints map[pod1:[100] pod2:[101]] (7.031117631s elapsed)
STEP: Deleting pod pod1 in namespace services-8041
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8041 to expose endpoints map[pod2:[101]]
Dec 18 19:50:44.880: INFO: successfully validated that service multi-endpoint-test in namespace services-8041 exposes endpoints map[pod2:[101]] (1.005657968s elapsed)
STEP: Deleting pod pod2 in namespace services-8041
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8041 to expose endpoints map[]
Dec 18 19:50:45.884: INFO: successfully validated that service multi-endpoint-test in namespace services-8041 exposes endpoints map[] (1.002248631s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:50:45.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8041" for this suite.
Dec 18 19:50:57.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:50:57.928: INFO: namespace services-8041 deletion completed in 12.033631248s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:29.131 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:50:57.929: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 18 19:51:13.967: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 18 19:51:13.968: INFO: Pod pod-with-poststart-http-hook still exists
Dec 18 19:51:15.969: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 18 19:51:15.970: INFO: Pod pod-with-poststart-http-hook still exists
Dec 18 19:51:17.969: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 18 19:51:17.970: INFO: Pod pod-with-poststart-http-hook still exists
Dec 18 19:51:19.969: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 18 19:51:19.970: INFO: Pod pod-with-poststart-http-hook still exists
Dec 18 19:51:21.969: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 18 19:51:21.970: INFO: Pod pod-with-poststart-http-hook still exists
Dec 18 19:51:23.969: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 18 19:51:23.970: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:51:23.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-844" for this suite.
Dec 18 19:51:35.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:51:36.008: INFO: namespace container-lifecycle-hook-844 deletion completed in 12.035853879s

• [SLOW TEST:38.079 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:51:36.008: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Dec 18 19:51:36.024: INFO: Waiting up to 5m0s for pod "client-containers-d98e5182-4d92-45ed-99c7-e6cca7ca8be7" in namespace "containers-5866" to be "success or failure"
Dec 18 19:51:36.024: INFO: Pod "client-containers-d98e5182-4d92-45ed-99c7-e6cca7ca8be7": Phase="Pending", Reason="", readiness=false. Elapsed: 825.995µs
Dec 18 19:51:38.027: INFO: Pod "client-containers-d98e5182-4d92-45ed-99c7-e6cca7ca8be7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003015077s
Dec 18 19:51:40.028: INFO: Pod "client-containers-d98e5182-4d92-45ed-99c7-e6cca7ca8be7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004903283s
Dec 18 19:51:42.030: INFO: Pod "client-containers-d98e5182-4d92-45ed-99c7-e6cca7ca8be7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006631343s
Dec 18 19:51:44.032: INFO: Pod "client-containers-d98e5182-4d92-45ed-99c7-e6cca7ca8be7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008791641s
STEP: Saw pod success
Dec 18 19:51:44.032: INFO: Pod "client-containers-d98e5182-4d92-45ed-99c7-e6cca7ca8be7" satisfied condition "success or failure"
Dec 18 19:51:44.033: INFO: Trying to get logs from node controller-1 pod client-containers-d98e5182-4d92-45ed-99c7-e6cca7ca8be7 container test-container: <nil>
STEP: delete the pod
Dec 18 19:51:44.055: INFO: Waiting for pod client-containers-d98e5182-4d92-45ed-99c7-e6cca7ca8be7 to disappear
Dec 18 19:51:44.056: INFO: Pod client-containers-d98e5182-4d92-45ed-99c7-e6cca7ca8be7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:51:44.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5866" for this suite.
Dec 18 19:51:50.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:51:50.094: INFO: namespace containers-5866 deletion completed in 6.035798089s

• [SLOW TEST:14.085 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:51:50.094: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Dec 18 19:51:50.108: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:52:09.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-294" for this suite.
Dec 18 19:52:15.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:52:15.439: INFO: namespace crd-publish-openapi-294 deletion completed in 6.034201954s

• [SLOW TEST:25.345 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:52:15.439: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec 18 19:52:25.477: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1218 19:52:25.477939      27 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:52:25.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-66" for this suite.
Dec 18 19:52:31.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:52:31.513: INFO: namespace gc-66 deletion completed in 6.034243751s

• [SLOW TEST:16.074 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:52:31.513: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 18 19:52:47.545: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 18 19:52:47.546: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 18 19:52:49.547: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 18 19:52:49.549: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 18 19:52:51.546: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 18 19:52:51.548: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:52:51.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6333" for this suite.
Dec 18 19:53:03.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:53:03.584: INFO: namespace container-lifecycle-hook-6333 deletion completed in 12.034263687s

• [SLOW TEST:32.071 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:53:03.585: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 18 19:53:03.604: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7645 /api/v1/namespaces/watch-7645/configmaps/e2e-watch-test-label-changed 79b5be4f-55ba-48b4-8b8a-c5a816064c00 90275 0 2019-12-18 19:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 18 19:53:03.604: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7645 /api/v1/namespaces/watch-7645/configmaps/e2e-watch-test-label-changed 79b5be4f-55ba-48b4-8b8a-c5a816064c00 90276 0 2019-12-18 19:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 18 19:53:03.604: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7645 /api/v1/namespaces/watch-7645/configmaps/e2e-watch-test-label-changed 79b5be4f-55ba-48b4-8b8a-c5a816064c00 90277 0 2019-12-18 19:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 18 19:53:13.613: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7645 /api/v1/namespaces/watch-7645/configmaps/e2e-watch-test-label-changed 79b5be4f-55ba-48b4-8b8a-c5a816064c00 90310 0 2019-12-18 19:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 18 19:53:13.613: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7645 /api/v1/namespaces/watch-7645/configmaps/e2e-watch-test-label-changed 79b5be4f-55ba-48b4-8b8a-c5a816064c00 90311 0 2019-12-18 19:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec 18 19:53:13.613: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7645 /api/v1/namespaces/watch-7645/configmaps/e2e-watch-test-label-changed 79b5be4f-55ba-48b4-8b8a-c5a816064c00 90312 0 2019-12-18 19:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:53:13.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7645" for this suite.
Dec 18 19:53:19.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:53:19.649: INFO: namespace watch-7645 deletion completed in 6.034396087s

• [SLOW TEST:16.064 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:53:19.649: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 18 19:53:19.664: INFO: Waiting up to 5m0s for pod "pod-4c31a0de-f8e0-4aea-867f-cb68fb7239a9" in namespace "emptydir-6276" to be "success or failure"
Dec 18 19:53:19.665: INFO: Pod "pod-4c31a0de-f8e0-4aea-867f-cb68fb7239a9": Phase="Pending", Reason="", readiness=false. Elapsed: 881.881µs
Dec 18 19:53:21.667: INFO: Pod "pod-4c31a0de-f8e0-4aea-867f-cb68fb7239a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002427311s
Dec 18 19:53:23.668: INFO: Pod "pod-4c31a0de-f8e0-4aea-867f-cb68fb7239a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004121505s
Dec 18 19:53:25.670: INFO: Pod "pod-4c31a0de-f8e0-4aea-867f-cb68fb7239a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005997218s
Dec 18 19:53:27.672: INFO: Pod "pod-4c31a0de-f8e0-4aea-867f-cb68fb7239a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007812723s
STEP: Saw pod success
Dec 18 19:53:27.672: INFO: Pod "pod-4c31a0de-f8e0-4aea-867f-cb68fb7239a9" satisfied condition "success or failure"
Dec 18 19:53:27.673: INFO: Trying to get logs from node controller-1 pod pod-4c31a0de-f8e0-4aea-867f-cb68fb7239a9 container test-container: <nil>
STEP: delete the pod
Dec 18 19:53:27.681: INFO: Waiting for pod pod-4c31a0de-f8e0-4aea-867f-cb68fb7239a9 to disappear
Dec 18 19:53:27.682: INFO: Pod pod-4c31a0de-f8e0-4aea-867f-cb68fb7239a9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:53:27.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6276" for this suite.
Dec 18 19:53:33.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:53:33.718: INFO: namespace emptydir-6276 deletion completed in 6.034791577s

• [SLOW TEST:14.069 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:53:33.718: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1547, will wait for the garbage collector to delete the pods
Dec 18 19:53:41.788: INFO: Deleting Job.batch foo took: 2.296756ms
Dec 18 19:53:42.188: INFO: Terminating Job.batch foo pods took: 400.178089ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:54:14.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1547" for this suite.
Dec 18 19:54:20.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:54:21.026: INFO: namespace job-1547 deletion completed in 6.033323537s

• [SLOW TEST:47.308 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:54:21.026: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 18 19:54:29.048: INFO: &Pod{ObjectMeta:{send-events-ead1945f-6715-4711-821f-84c19fb5c99a  events-8278 /api/v1/namespaces/events-8278/pods/send-events-ead1945f-6715-4711-821f-84c19fb5c99a 20b5a3bb-7895-4bf8-b133-d78ac57e6ffb 90673 0 2019-12-18 19:54:21 +0000 UTC <nil> <nil> map[name:foo time:40175968] map[cni.projectcalico.org/podIP:172.16.192.85/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.85"
    ],
    "default": true,
    "dns": {}
}]] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pn5rc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pn5rc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pn5rc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:54:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:54:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:54:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-18 19:54:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.204.2,PodIP:172.16.192.85,StartTime:2019-12-18 19:54:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-18 19:54:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://167ae07c41552639ff10babe4e3eca1c62ea35156f827442627fdb4e3a97ac6e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec 18 19:54:31.050: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 18 19:54:33.051: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:54:33.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8278" for this suite.
Dec 18 19:55:17.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:55:17.092: INFO: namespace events-8278 deletion completed in 44.036274125s

• [SLOW TEST:56.066 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:55:17.092: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 18 19:55:17.110: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5377 /api/v1/namespaces/watch-5377/configmaps/e2e-watch-test-configmap-a 268d5409-04db-43fe-bfdf-5db7a20cf5cd 90874 0 2019-12-18 19:55:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 18 19:55:17.110: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5377 /api/v1/namespaces/watch-5377/configmaps/e2e-watch-test-configmap-a 268d5409-04db-43fe-bfdf-5db7a20cf5cd 90874 0 2019-12-18 19:55:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 18 19:55:27.115: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5377 /api/v1/namespaces/watch-5377/configmaps/e2e-watch-test-configmap-a 268d5409-04db-43fe-bfdf-5db7a20cf5cd 90908 0 2019-12-18 19:55:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 18 19:55:27.115: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5377 /api/v1/namespaces/watch-5377/configmaps/e2e-watch-test-configmap-a 268d5409-04db-43fe-bfdf-5db7a20cf5cd 90908 0 2019-12-18 19:55:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 18 19:55:37.119: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5377 /api/v1/namespaces/watch-5377/configmaps/e2e-watch-test-configmap-a 268d5409-04db-43fe-bfdf-5db7a20cf5cd 90939 0 2019-12-18 19:55:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 18 19:55:37.119: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5377 /api/v1/namespaces/watch-5377/configmaps/e2e-watch-test-configmap-a 268d5409-04db-43fe-bfdf-5db7a20cf5cd 90939 0 2019-12-18 19:55:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 18 19:55:47.122: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5377 /api/v1/namespaces/watch-5377/configmaps/e2e-watch-test-configmap-a 268d5409-04db-43fe-bfdf-5db7a20cf5cd 90972 0 2019-12-18 19:55:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 18 19:55:47.122: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5377 /api/v1/namespaces/watch-5377/configmaps/e2e-watch-test-configmap-a 268d5409-04db-43fe-bfdf-5db7a20cf5cd 90972 0 2019-12-18 19:55:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 18 19:55:57.126: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5377 /api/v1/namespaces/watch-5377/configmaps/e2e-watch-test-configmap-b 54571dac-6706-44dc-b031-d5e0d0a2168f 91003 0 2019-12-18 19:55:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 18 19:55:57.126: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5377 /api/v1/namespaces/watch-5377/configmaps/e2e-watch-test-configmap-b 54571dac-6706-44dc-b031-d5e0d0a2168f 91003 0 2019-12-18 19:55:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 18 19:56:07.130: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5377 /api/v1/namespaces/watch-5377/configmaps/e2e-watch-test-configmap-b 54571dac-6706-44dc-b031-d5e0d0a2168f 91036 0 2019-12-18 19:55:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 18 19:56:07.130: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5377 /api/v1/namespaces/watch-5377/configmaps/e2e-watch-test-configmap-b 54571dac-6706-44dc-b031-d5e0d0a2168f 91036 0 2019-12-18 19:55:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:56:17.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5377" for this suite.
Dec 18 19:56:23.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:56:23.169: INFO: namespace watch-5377 deletion completed in 6.035750328s

• [SLOW TEST:66.077 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:56:23.169: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-8841/configmap-test-91915b3c-e9b0-4ca3-a63d-c008a81c4e0a
STEP: Creating a pod to test consume configMaps
Dec 18 19:56:23.188: INFO: Waiting up to 5m0s for pod "pod-configmaps-9996433a-bcbc-487b-aa5b-3e089cf38b1d" in namespace "configmap-8841" to be "success or failure"
Dec 18 19:56:23.189: INFO: Pod "pod-configmaps-9996433a-bcbc-487b-aa5b-3e089cf38b1d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.118445ms
Dec 18 19:56:25.190: INFO: Pod "pod-configmaps-9996433a-bcbc-487b-aa5b-3e089cf38b1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002873799s
Dec 18 19:56:27.193: INFO: Pod "pod-configmaps-9996433a-bcbc-487b-aa5b-3e089cf38b1d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005361651s
Dec 18 19:56:29.195: INFO: Pod "pod-configmaps-9996433a-bcbc-487b-aa5b-3e089cf38b1d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00695809s
Dec 18 19:56:31.196: INFO: Pod "pod-configmaps-9996433a-bcbc-487b-aa5b-3e089cf38b1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008460155s
STEP: Saw pod success
Dec 18 19:56:31.196: INFO: Pod "pod-configmaps-9996433a-bcbc-487b-aa5b-3e089cf38b1d" satisfied condition "success or failure"
Dec 18 19:56:31.197: INFO: Trying to get logs from node controller-1 pod pod-configmaps-9996433a-bcbc-487b-aa5b-3e089cf38b1d container env-test: <nil>
STEP: delete the pod
Dec 18 19:56:31.211: INFO: Waiting for pod pod-configmaps-9996433a-bcbc-487b-aa5b-3e089cf38b1d to disappear
Dec 18 19:56:31.212: INFO: Pod pod-configmaps-9996433a-bcbc-487b-aa5b-3e089cf38b1d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:56:31.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8841" for this suite.
Dec 18 19:56:37.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:56:37.251: INFO: namespace configmap-8841 deletion completed in 6.036799604s

• [SLOW TEST:14.082 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:56:37.251: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-417f08b4-bdaa-48e0-b9d0-701ed33c3ee4
STEP: Creating a pod to test consume secrets
Dec 18 19:56:37.269: INFO: Waiting up to 5m0s for pod "pod-secrets-9c74b195-f0b1-43cb-a9d0-6fcefd9e4370" in namespace "secrets-4948" to be "success or failure"
Dec 18 19:56:37.270: INFO: Pod "pod-secrets-9c74b195-f0b1-43cb-a9d0-6fcefd9e4370": Phase="Pending", Reason="", readiness=false. Elapsed: 1.399283ms
Dec 18 19:56:39.272: INFO: Pod "pod-secrets-9c74b195-f0b1-43cb-a9d0-6fcefd9e4370": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003059877s
Dec 18 19:56:41.274: INFO: Pod "pod-secrets-9c74b195-f0b1-43cb-a9d0-6fcefd9e4370": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004749832s
Dec 18 19:56:43.275: INFO: Pod "pod-secrets-9c74b195-f0b1-43cb-a9d0-6fcefd9e4370": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006471541s
Dec 18 19:56:45.277: INFO: Pod "pod-secrets-9c74b195-f0b1-43cb-a9d0-6fcefd9e4370": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008060421s
STEP: Saw pod success
Dec 18 19:56:45.277: INFO: Pod "pod-secrets-9c74b195-f0b1-43cb-a9d0-6fcefd9e4370" satisfied condition "success or failure"
Dec 18 19:56:45.278: INFO: Trying to get logs from node controller-1 pod pod-secrets-9c74b195-f0b1-43cb-a9d0-6fcefd9e4370 container secret-volume-test: <nil>
STEP: delete the pod
Dec 18 19:56:45.285: INFO: Waiting for pod pod-secrets-9c74b195-f0b1-43cb-a9d0-6fcefd9e4370 to disappear
Dec 18 19:56:45.287: INFO: Pod pod-secrets-9c74b195-f0b1-43cb-a9d0-6fcefd9e4370 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:56:45.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4948" for this suite.
Dec 18 19:56:51.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:56:51.323: INFO: namespace secrets-4948 deletion completed in 6.03425929s

• [SLOW TEST:14.072 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:56:51.323: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-bcbd5373-539d-4951-a4bc-9b10f4392750
STEP: Creating a pod to test consume configMaps
Dec 18 19:56:51.340: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8923ae0c-16d9-48ac-b1e7-07c9ca0f1178" in namespace "projected-1817" to be "success or failure"
Dec 18 19:56:51.341: INFO: Pod "pod-projected-configmaps-8923ae0c-16d9-48ac-b1e7-07c9ca0f1178": Phase="Pending", Reason="", readiness=false. Elapsed: 876.853µs
Dec 18 19:56:53.343: INFO: Pod "pod-projected-configmaps-8923ae0c-16d9-48ac-b1e7-07c9ca0f1178": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002986204s
Dec 18 19:56:55.345: INFO: Pod "pod-projected-configmaps-8923ae0c-16d9-48ac-b1e7-07c9ca0f1178": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004866925s
Dec 18 19:56:57.348: INFO: Pod "pod-projected-configmaps-8923ae0c-16d9-48ac-b1e7-07c9ca0f1178": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007423125s
Dec 18 19:56:59.350: INFO: Pod "pod-projected-configmaps-8923ae0c-16d9-48ac-b1e7-07c9ca0f1178": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009891888s
STEP: Saw pod success
Dec 18 19:56:59.350: INFO: Pod "pod-projected-configmaps-8923ae0c-16d9-48ac-b1e7-07c9ca0f1178" satisfied condition "success or failure"
Dec 18 19:56:59.351: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-8923ae0c-16d9-48ac-b1e7-07c9ca0f1178 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 19:56:59.360: INFO: Waiting for pod pod-projected-configmaps-8923ae0c-16d9-48ac-b1e7-07c9ca0f1178 to disappear
Dec 18 19:56:59.360: INFO: Pod pod-projected-configmaps-8923ae0c-16d9-48ac-b1e7-07c9ca0f1178 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:56:59.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1817" for this suite.
Dec 18 19:57:05.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:57:05.397: INFO: namespace projected-1817 deletion completed in 6.034487004s

• [SLOW TEST:14.074 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:57:05.397: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:57:13.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7816" for this suite.
Dec 18 19:57:57.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:57:57.459: INFO: namespace kubelet-test-7816 deletion completed in 44.035557398s

• [SLOW TEST:52.062 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:57:57.459: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:58:10.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4961" for this suite.
Dec 18 19:58:16.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:58:16.531: INFO: namespace resourcequota-4961 deletion completed in 6.034053348s

• [SLOW TEST:19.071 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:58:16.531: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 19:58:16.544: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:58:24.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-504" for this suite.
Dec 18 19:59:12.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:59:12.593: INFO: namespace pods-504 deletion completed in 48.033647845s

• [SLOW TEST:56.063 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:59:12.594: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 18 19:59:12.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-4257'
Dec 18 19:59:12.746: INFO: stderr: ""
Dec 18 19:59:12.746: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Dec 18 19:59:12.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 delete pods e2e-test-httpd-pod --namespace=kubectl-4257'
Dec 18 19:59:22.105: INFO: stderr: ""
Dec 18 19:59:22.105: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:59:22.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4257" for this suite.
Dec 18 19:59:28.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:59:28.142: INFO: namespace kubectl-4257 deletion completed in 6.034632635s

• [SLOW TEST:15.548 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:59:28.142: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-b2e5c837-de7c-4aa5-9639-59c93965b0f9
STEP: Creating a pod to test consume secrets
Dec 18 19:59:28.158: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-87471b53-c1ec-4e88-a7e2-fb8e9e7e3468" in namespace "projected-3832" to be "success or failure"
Dec 18 19:59:28.159: INFO: Pod "pod-projected-secrets-87471b53-c1ec-4e88-a7e2-fb8e9e7e3468": Phase="Pending", Reason="", readiness=false. Elapsed: 889.745µs
Dec 18 19:59:30.162: INFO: Pod "pod-projected-secrets-87471b53-c1ec-4e88-a7e2-fb8e9e7e3468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003405479s
Dec 18 19:59:32.163: INFO: Pod "pod-projected-secrets-87471b53-c1ec-4e88-a7e2-fb8e9e7e3468": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005138986s
Dec 18 19:59:34.165: INFO: Pod "pod-projected-secrets-87471b53-c1ec-4e88-a7e2-fb8e9e7e3468": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006904523s
Dec 18 19:59:36.167: INFO: Pod "pod-projected-secrets-87471b53-c1ec-4e88-a7e2-fb8e9e7e3468": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008535311s
STEP: Saw pod success
Dec 18 19:59:36.167: INFO: Pod "pod-projected-secrets-87471b53-c1ec-4e88-a7e2-fb8e9e7e3468" satisfied condition "success or failure"
Dec 18 19:59:36.168: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-87471b53-c1ec-4e88-a7e2-fb8e9e7e3468 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 18 19:59:36.175: INFO: Waiting for pod pod-projected-secrets-87471b53-c1ec-4e88-a7e2-fb8e9e7e3468 to disappear
Dec 18 19:59:36.176: INFO: Pod pod-projected-secrets-87471b53-c1ec-4e88-a7e2-fb8e9e7e3468 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:59:36.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3832" for this suite.
Dec 18 19:59:42.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:59:42.212: INFO: namespace projected-3832 deletion completed in 6.034071718s

• [SLOW TEST:14.070 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:59:42.212: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-31110e3c-47d6-4ebb-9d1a-68550154355e
STEP: Creating a pod to test consume configMaps
Dec 18 19:59:42.229: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-599831f2-7195-4659-b30a-9968c395ab49" in namespace "projected-3897" to be "success or failure"
Dec 18 19:59:42.229: INFO: Pod "pod-projected-configmaps-599831f2-7195-4659-b30a-9968c395ab49": Phase="Pending", Reason="", readiness=false. Elapsed: 874.485µs
Dec 18 19:59:44.231: INFO: Pod "pod-projected-configmaps-599831f2-7195-4659-b30a-9968c395ab49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002419917s
Dec 18 19:59:46.233: INFO: Pod "pod-projected-configmaps-599831f2-7195-4659-b30a-9968c395ab49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00422175s
Dec 18 19:59:48.235: INFO: Pod "pod-projected-configmaps-599831f2-7195-4659-b30a-9968c395ab49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006274754s
Dec 18 19:59:50.237: INFO: Pod "pod-projected-configmaps-599831f2-7195-4659-b30a-9968c395ab49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00815975s
STEP: Saw pod success
Dec 18 19:59:50.237: INFO: Pod "pod-projected-configmaps-599831f2-7195-4659-b30a-9968c395ab49" satisfied condition "success or failure"
Dec 18 19:59:50.238: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-599831f2-7195-4659-b30a-9968c395ab49 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 19:59:50.253: INFO: Waiting for pod pod-projected-configmaps-599831f2-7195-4659-b30a-9968c395ab49 to disappear
Dec 18 19:59:50.254: INFO: Pod pod-projected-configmaps-599831f2-7195-4659-b30a-9968c395ab49 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 19:59:50.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3897" for this suite.
Dec 18 19:59:56.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 19:59:56.290: INFO: namespace projected-3897 deletion completed in 6.034301839s

• [SLOW TEST:14.078 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 19:59:56.290: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 18 19:59:56.315: INFO: Number of nodes with available pods: 0
Dec 18 19:59:56.315: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:59:57.319: INFO: Number of nodes with available pods: 0
Dec 18 19:59:57.319: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:59:58.319: INFO: Number of nodes with available pods: 0
Dec 18 19:59:58.319: INFO: Node compute-0 is running more than one daemon pod
Dec 18 19:59:59.319: INFO: Number of nodes with available pods: 0
Dec 18 19:59:59.319: INFO: Node compute-0 is running more than one daemon pod
Dec 18 20:00:00.320: INFO: Number of nodes with available pods: 0
Dec 18 20:00:00.320: INFO: Node compute-0 is running more than one daemon pod
Dec 18 20:00:01.319: INFO: Number of nodes with available pods: 0
Dec 18 20:00:01.319: INFO: Node compute-0 is running more than one daemon pod
Dec 18 20:00:02.319: INFO: Number of nodes with available pods: 0
Dec 18 20:00:02.319: INFO: Node compute-0 is running more than one daemon pod
Dec 18 20:00:03.319: INFO: Number of nodes with available pods: 2
Dec 18 20:00:03.319: INFO: Node compute-1 is running more than one daemon pod
Dec 18 20:00:04.319: INFO: Number of nodes with available pods: 4
Dec 18 20:00:04.319: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 18 20:00:04.327: INFO: Number of nodes with available pods: 3
Dec 18 20:00:04.327: INFO: Node controller-0 is running more than one daemon pod
Dec 18 20:00:05.331: INFO: Number of nodes with available pods: 3
Dec 18 20:00:05.331: INFO: Node controller-0 is running more than one daemon pod
Dec 18 20:00:06.331: INFO: Number of nodes with available pods: 3
Dec 18 20:00:06.331: INFO: Node controller-0 is running more than one daemon pod
Dec 18 20:00:07.330: INFO: Number of nodes with available pods: 3
Dec 18 20:00:07.330: INFO: Node controller-0 is running more than one daemon pod
Dec 18 20:00:08.331: INFO: Number of nodes with available pods: 3
Dec 18 20:00:08.331: INFO: Node controller-0 is running more than one daemon pod
Dec 18 20:00:09.330: INFO: Number of nodes with available pods: 3
Dec 18 20:00:09.330: INFO: Node controller-0 is running more than one daemon pod
Dec 18 20:00:10.331: INFO: Number of nodes with available pods: 3
Dec 18 20:00:10.331: INFO: Node controller-0 is running more than one daemon pod
Dec 18 20:00:11.332: INFO: Number of nodes with available pods: 4
Dec 18 20:00:11.332: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1581, will wait for the garbage collector to delete the pods
Dec 18 20:00:11.387: INFO: Deleting DaemonSet.extensions daemon-set took: 2.112486ms
Dec 18 20:00:11.787: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.210686ms
Dec 18 20:00:23.389: INFO: Number of nodes with available pods: 0
Dec 18 20:00:23.389: INFO: Number of running nodes: 0, number of available pods: 0
Dec 18 20:00:23.390: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1581/daemonsets","resourceVersion":"92272"},"items":null}

Dec 18 20:00:23.391: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1581/pods","resourceVersion":"92272"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:00:23.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1581" for this suite.
Dec 18 20:00:29.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:00:29.434: INFO: namespace daemonsets-1581 deletion completed in 6.035661382s

• [SLOW TEST:33.144 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:00:29.434: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 20:00:29.450: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b49440a-55ec-4ee0-a02b-69e5cf82ec79" in namespace "projected-7684" to be "success or failure"
Dec 18 20:00:29.451: INFO: Pod "downwardapi-volume-9b49440a-55ec-4ee0-a02b-69e5cf82ec79": Phase="Pending", Reason="", readiness=false. Elapsed: 958.426µs
Dec 18 20:00:31.453: INFO: Pod "downwardapi-volume-9b49440a-55ec-4ee0-a02b-69e5cf82ec79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002668008s
Dec 18 20:00:33.454: INFO: Pod "downwardapi-volume-9b49440a-55ec-4ee0-a02b-69e5cf82ec79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004240628s
Dec 18 20:00:35.456: INFO: Pod "downwardapi-volume-9b49440a-55ec-4ee0-a02b-69e5cf82ec79": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005783554s
Dec 18 20:00:37.457: INFO: Pod "downwardapi-volume-9b49440a-55ec-4ee0-a02b-69e5cf82ec79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007360217s
STEP: Saw pod success
Dec 18 20:00:37.457: INFO: Pod "downwardapi-volume-9b49440a-55ec-4ee0-a02b-69e5cf82ec79" satisfied condition "success or failure"
Dec 18 20:00:37.458: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-9b49440a-55ec-4ee0-a02b-69e5cf82ec79 container client-container: <nil>
STEP: delete the pod
Dec 18 20:00:37.466: INFO: Waiting for pod downwardapi-volume-9b49440a-55ec-4ee0-a02b-69e5cf82ec79 to disappear
Dec 18 20:00:37.467: INFO: Pod downwardapi-volume-9b49440a-55ec-4ee0-a02b-69e5cf82ec79 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:00:37.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7684" for this suite.
Dec 18 20:00:43.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:00:43.503: INFO: namespace projected-7684 deletion completed in 6.034265907s

• [SLOW TEST:14.069 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:00:43.503: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:01:01.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2504" for this suite.
Dec 18 20:01:07.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:01:07.557: INFO: namespace job-2504 deletion completed in 6.034359738s

• [SLOW TEST:24.054 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:01:07.557: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 20:01:07.571: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:01:15.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6349" for this suite.
Dec 18 20:02:13.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:02:13.744: INFO: namespace pods-6349 deletion completed in 58.035112137s

• [SLOW TEST:66.187 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:02:13.744: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec 18 20:02:21.766: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-207688788 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 18 20:02:26.839: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:02:26.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6413" for this suite.
Dec 18 20:02:32.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:02:32.876: INFO: namespace pods-6413 deletion completed in 6.033657365s

• [SLOW TEST:19.132 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:02:32.876: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-c69e148b-26be-4bd9-a281-4b1d28c7083e
STEP: Creating a pod to test consume secrets
Dec 18 20:02:32.893: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a6a47bb-e5a7-46e8-8497-aa31ed3475d4" in namespace "projected-3263" to be "success or failure"
Dec 18 20:02:32.894: INFO: Pod "pod-projected-secrets-5a6a47bb-e5a7-46e8-8497-aa31ed3475d4": Phase="Pending", Reason="", readiness=false. Elapsed: 860.026µs
Dec 18 20:02:34.895: INFO: Pod "pod-projected-secrets-5a6a47bb-e5a7-46e8-8497-aa31ed3475d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002570742s
Dec 18 20:02:36.897: INFO: Pod "pod-projected-secrets-5a6a47bb-e5a7-46e8-8497-aa31ed3475d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004222121s
Dec 18 20:02:38.899: INFO: Pod "pod-projected-secrets-5a6a47bb-e5a7-46e8-8497-aa31ed3475d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005820117s
Dec 18 20:02:40.900: INFO: Pod "pod-projected-secrets-5a6a47bb-e5a7-46e8-8497-aa31ed3475d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007539838s
STEP: Saw pod success
Dec 18 20:02:40.900: INFO: Pod "pod-projected-secrets-5a6a47bb-e5a7-46e8-8497-aa31ed3475d4" satisfied condition "success or failure"
Dec 18 20:02:40.901: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-5a6a47bb-e5a7-46e8-8497-aa31ed3475d4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 18 20:02:40.910: INFO: Waiting for pod pod-projected-secrets-5a6a47bb-e5a7-46e8-8497-aa31ed3475d4 to disappear
Dec 18 20:02:40.911: INFO: Pod pod-projected-secrets-5a6a47bb-e5a7-46e8-8497-aa31ed3475d4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:02:40.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3263" for this suite.
Dec 18 20:02:46.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:02:46.948: INFO: namespace projected-3263 deletion completed in 6.034037452s

• [SLOW TEST:14.072 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:02:46.948: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 18 20:02:46.961: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 18 20:02:46.966: INFO: Waiting for terminating namespaces to be deleted...
Dec 18 20:02:46.967: INFO: 
Logging pods the kubelet thinks is on node compute-0 before test
Dec 18 20:02:46.979: INFO: kube-proxy-lq2c8 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:46.979: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 18 20:02:46.979: INFO: kube-multus-ds-amd64-fczk7 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:46.979: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 20:02:46.979: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-jzbct from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 20:02:46.979: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 18 20:02:46.979: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 20:02:46.979: INFO: kube-sriov-cni-ds-amd64-nlfxr from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:46.979: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 20:02:46.979: INFO: calico-node-ntnk8 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:46.979: INFO: 	Container calico-node ready: true, restart count 1
Dec 18 20:02:46.979: INFO: sonobuoy from sonobuoy started at 2019-12-18 18:17:38 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:46.979: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 18 20:02:46.979: INFO: 
Logging pods the kubelet thinks is on node compute-1 before test
Dec 18 20:02:46.990: INFO: kube-multus-ds-amd64-trmzx from kube-system started at 2019-12-18 15:11:52 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:46.990: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 20:02:46.990: INFO: calico-node-6s2pv from kube-system started at 2019-12-18 15:11:51 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:46.990: INFO: 	Container calico-node ready: true, restart count 1
Dec 18 20:02:46.990: INFO: kube-proxy-w7hfz from kube-system started at 2019-12-18 15:11:52 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:46.990: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 18 20:02:46.990: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-49m5k from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 20:02:46.990: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 18 20:02:46.990: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 20:02:46.990: INFO: kube-sriov-cni-ds-amd64-v94kl from kube-system started at 2019-12-18 15:11:51 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:46.990: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 20:02:46.990: INFO: 
Logging pods the kubelet thinks is on node controller-0 before test
Dec 18 20:02:47.001: INFO: kube-apiserver-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container kube-apiserver ready: true, restart count 2
Dec 18 20:02:47.001: INFO: rbd-provisioner-7484d49cf6-858lp from kube-system started at 2019-12-18 18:33:13 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container rbd-provisioner ready: true, restart count 0
Dec 18 20:02:47.001: INFO: kube-controller-manager-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 18 20:02:47.001: INFO: ceph-pools-audit-1576698600-8z6pb from kube-system started at 2019-12-18 19:50:02 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Dec 18 20:02:47.001: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-q7fcg from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 18 20:02:47.001: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 20:02:47.001: INFO: kube-scheduler-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 18 20:02:47.001: INFO: kube-proxy-hs98d from kube-system started at 2019-12-18 14:17:53 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container kube-proxy ready: true, restart count 1
Dec 18 20:02:47.001: INFO: kube-sriov-cni-ds-amd64-97g65 from kube-system started at 2019-12-18 18:33:43 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 20:02:47.001: INFO: calico-kube-controllers-855577b7b5-2gktm from kube-system started at 2019-12-18 18:38:16 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 18 20:02:47.001: INFO: ceph-pools-audit-1576698900-s5xcf from kube-system started at 2019-12-18 19:55:02 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Dec 18 20:02:47.001: INFO: coredns-6bc668cd76-ks6bd from kube-system started at 2019-12-18 18:33:17 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container coredns ready: true, restart count 0
Dec 18 20:02:47.001: INFO: calico-node-7bzgw from kube-system started at 2019-12-18 14:17:53 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container calico-node ready: true, restart count 3
Dec 18 20:02:47.001: INFO: kube-multus-ds-amd64-plsvq from kube-system started at 2019-12-18 18:33:43 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 20:02:47.001: INFO: tiller-deploy-d6b59fcb-kj5hv from kube-system started at 2019-12-18 18:38:16 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.001: INFO: 	Container tiller ready: true, restart count 0
Dec 18 20:02:47.001: INFO: 
Logging pods the kubelet thinks is on node controller-1 before test
Dec 18 20:02:47.006: INFO: ceph-pools-audit-1576699200-wh2cs from kube-system started at 2019-12-18 20:00:02 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.006: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Dec 18 20:02:47.006: INFO: kube-controller-manager-controller-1 from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.006: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 18 20:02:47.006: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-rz72p from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 20:02:47.006: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 18 20:02:47.006: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 20:02:47.006: INFO: kube-sriov-cni-ds-amd64-rb5tl from kube-system started at 2019-12-18 18:38:52 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.006: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 20:02:47.006: INFO: rbd-provisioner-7484d49cf6-p98d2 from kube-system started at 2019-12-18 18:38:56 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.006: INFO: 	Container rbd-provisioner ready: true, restart count 0
Dec 18 20:02:47.006: INFO: calico-node-8q8ss from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.006: INFO: 	Container calico-node ready: true, restart count 4
Dec 18 20:02:47.006: INFO: kube-proxy-fds2l from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.006: INFO: 	Container kube-proxy ready: true, restart count 1
Dec 18 20:02:47.006: INFO: kube-apiserver-controller-1 from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.006: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 18 20:02:47.006: INFO: kube-multus-ds-amd64-rvvrh from kube-system started at 2019-12-18 18:38:52 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.006: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 20:02:47.006: INFO: coredns-6bc668cd76-86sgt from kube-system started at 2019-12-18 18:38:56 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.006: INFO: 	Container coredns ready: true, restart count 0
Dec 18 20:02:47.006: INFO: sonobuoy-e2e-job-8824c3aaef6e4480 from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 20:02:47.006: INFO: 	Container e2e ready: true, restart count 0
Dec 18 20:02:47.006: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 18 20:02:47.006: INFO: kube-scheduler-controller-1 from kube-system started at 2019-12-18 15:22:52 +0000 UTC (1 container statuses recorded)
Dec 18 20:02:47.006: INFO: 	Container kube-scheduler ready: true, restart count 1
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e54806df-43c9-4d34-828e-2639c30ed669 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-e54806df-43c9-4d34-828e-2639c30ed669 off the node controller-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e54806df-43c9-4d34-828e-2639c30ed669
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:08:03.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4674" for this suite.
Dec 18 20:08:15.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:08:15.072: INFO: namespace sched-pred-4674 deletion completed in 12.035211452s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:328.125 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:08:15.073: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 20:08:15.789: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 18 20:08:17.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296495, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296495, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296495, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296495, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:08:19.795: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296495, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296495, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296495, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296495, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:08:21.796: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296495, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296495, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296495, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296495, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 20:08:24.802: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:08:24.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5688" for this suite.
Dec 18 20:08:30.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:08:30.841: INFO: namespace webhook-5688 deletion completed in 6.033657657s
STEP: Destroying namespace "webhook-5688-markers" for this suite.
Dec 18 20:08:36.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:08:36.877: INFO: namespace webhook-5688-markers deletion completed in 6.035829533s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.809 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:08:36.881: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-91be9aeb-2a55-433a-8b59-c60508db4188
STEP: Creating a pod to test consume configMaps
Dec 18 20:08:36.898: INFO: Waiting up to 5m0s for pod "pod-configmaps-1c03c61e-1638-41e2-a301-a2943ebcf81d" in namespace "configmap-1982" to be "success or failure"
Dec 18 20:08:36.899: INFO: Pod "pod-configmaps-1c03c61e-1638-41e2-a301-a2943ebcf81d": Phase="Pending", Reason="", readiness=false. Elapsed: 921.681µs
Dec 18 20:08:38.901: INFO: Pod "pod-configmaps-1c03c61e-1638-41e2-a301-a2943ebcf81d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00262979s
Dec 18 20:08:40.902: INFO: Pod "pod-configmaps-1c03c61e-1638-41e2-a301-a2943ebcf81d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004127748s
Dec 18 20:08:42.904: INFO: Pod "pod-configmaps-1c03c61e-1638-41e2-a301-a2943ebcf81d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0056182s
Dec 18 20:08:44.906: INFO: Pod "pod-configmaps-1c03c61e-1638-41e2-a301-a2943ebcf81d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007185554s
STEP: Saw pod success
Dec 18 20:08:44.906: INFO: Pod "pod-configmaps-1c03c61e-1638-41e2-a301-a2943ebcf81d" satisfied condition "success or failure"
Dec 18 20:08:44.907: INFO: Trying to get logs from node controller-0 pod pod-configmaps-1c03c61e-1638-41e2-a301-a2943ebcf81d container configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 20:08:44.921: INFO: Waiting for pod pod-configmaps-1c03c61e-1638-41e2-a301-a2943ebcf81d to disappear
Dec 18 20:08:44.922: INFO: Pod pod-configmaps-1c03c61e-1638-41e2-a301-a2943ebcf81d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:08:44.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1982" for this suite.
Dec 18 20:08:50.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:08:50.959: INFO: namespace configmap-1982 deletion completed in 6.035018765s

• [SLOW TEST:14.077 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:08:50.959: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Dec 18 20:09:30.985: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W1218 20:09:30.985368      27 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 18 20:09:30.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1213" for this suite.
Dec 18 20:09:36.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:09:37.022: INFO: namespace gc-1213 deletion completed in 6.035752443s

• [SLOW TEST:46.063 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:09:37.022: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 20:09:37.501: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 18 20:09:39.506: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296577, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296577, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296577, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296577, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:09:41.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296577, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296577, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296577, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296577, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:09:43.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296577, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296577, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296577, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296577, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 20:09:46.514: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 20:09:46.515: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:09:47.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3385" for this suite.
Dec 18 20:09:53.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:09:53.792: INFO: namespace webhook-3385 deletion completed in 6.035553304s
STEP: Destroying namespace "webhook-3385-markers" for this suite.
Dec 18 20:09:59.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:09:59.827: INFO: namespace webhook-3385-markers deletion completed in 6.034988836s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:22.809 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:09:59.832: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-2378
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2378
STEP: Deleting pre-stop pod
Dec 18 20:10:20.861: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:10:20.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2378" for this suite.
Dec 18 20:11:04.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:11:04.901: INFO: namespace prestop-2378 deletion completed in 44.034391937s

• [SLOW TEST:65.069 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:11:04.901: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 18 20:11:04.915: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 20:11:08.479: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:11:22.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5506" for this suite.
Dec 18 20:11:28.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:11:28.210: INFO: namespace crd-publish-openapi-5506 deletion completed in 6.037176116s

• [SLOW TEST:23.309 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:11:28.210: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 18 20:11:35.241: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:11:35.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9429" for this suite.
Dec 18 20:11:41.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:11:41.283: INFO: namespace container-runtime-9429 deletion completed in 6.03615371s

• [SLOW TEST:13.073 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:11:41.283: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-6987
STEP: Creating a pod to test atomic-volume-subpath
Dec 18 20:11:41.301: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6987" in namespace "subpath-8827" to be "success or failure"
Dec 18 20:11:41.303: INFO: Pod "pod-subpath-test-projected-6987": Phase="Pending", Reason="", readiness=false. Elapsed: 1.202999ms
Dec 18 20:11:43.304: INFO: Pod "pod-subpath-test-projected-6987": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002786847s
Dec 18 20:11:45.306: INFO: Pod "pod-subpath-test-projected-6987": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004640883s
Dec 18 20:11:47.308: INFO: Pod "pod-subpath-test-projected-6987": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006419949s
Dec 18 20:11:49.309: INFO: Pod "pod-subpath-test-projected-6987": Phase="Running", Reason="", readiness=true. Elapsed: 8.008129015s
Dec 18 20:11:51.311: INFO: Pod "pod-subpath-test-projected-6987": Phase="Running", Reason="", readiness=true. Elapsed: 10.009770972s
Dec 18 20:11:53.313: INFO: Pod "pod-subpath-test-projected-6987": Phase="Running", Reason="", readiness=true. Elapsed: 12.011761721s
Dec 18 20:11:55.315: INFO: Pod "pod-subpath-test-projected-6987": Phase="Running", Reason="", readiness=true. Elapsed: 14.013207705s
Dec 18 20:11:57.316: INFO: Pod "pod-subpath-test-projected-6987": Phase="Running", Reason="", readiness=true. Elapsed: 16.014753226s
Dec 18 20:11:59.318: INFO: Pod "pod-subpath-test-projected-6987": Phase="Running", Reason="", readiness=true. Elapsed: 18.016395716s
Dec 18 20:12:01.320: INFO: Pod "pod-subpath-test-projected-6987": Phase="Running", Reason="", readiness=true. Elapsed: 20.018254414s
Dec 18 20:12:03.321: INFO: Pod "pod-subpath-test-projected-6987": Phase="Running", Reason="", readiness=true. Elapsed: 22.02012226s
Dec 18 20:12:05.323: INFO: Pod "pod-subpath-test-projected-6987": Phase="Running", Reason="", readiness=true. Elapsed: 24.02167055s
Dec 18 20:12:07.325: INFO: Pod "pod-subpath-test-projected-6987": Phase="Running", Reason="", readiness=true. Elapsed: 26.023461151s
Dec 18 20:12:09.327: INFO: Pod "pod-subpath-test-projected-6987": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.025640436s
STEP: Saw pod success
Dec 18 20:12:09.327: INFO: Pod "pod-subpath-test-projected-6987" satisfied condition "success or failure"
Dec 18 20:12:09.328: INFO: Trying to get logs from node controller-0 pod pod-subpath-test-projected-6987 container test-container-subpath-projected-6987: <nil>
STEP: delete the pod
Dec 18 20:12:09.343: INFO: Waiting for pod pod-subpath-test-projected-6987 to disappear
Dec 18 20:12:09.344: INFO: Pod pod-subpath-test-projected-6987 no longer exists
STEP: Deleting pod pod-subpath-test-projected-6987
Dec 18 20:12:09.344: INFO: Deleting pod "pod-subpath-test-projected-6987" in namespace "subpath-8827"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:12:09.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8827" for this suite.
Dec 18 20:12:15.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:12:15.381: INFO: namespace subpath-8827 deletion completed in 6.03480199s

• [SLOW TEST:34.098 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:12:15.381: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 18 20:12:16.142: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 18 20:12:18.146: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:12:20.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:12:22.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:12:24.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296736, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 20:12:27.155: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 20:12:27.156: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4323-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:12:28.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2510" for this suite.
Dec 18 20:12:34.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:12:34.291: INFO: namespace webhook-2510 deletion completed in 6.036069287s
STEP: Destroying namespace "webhook-2510-markers" for this suite.
Dec 18 20:12:40.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:12:40.326: INFO: namespace webhook-2510-markers deletion completed in 6.034778214s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.949 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:12:40.330: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:12:47.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2065" for this suite.
Dec 18 20:12:53.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:12:53.385: INFO: namespace resourcequota-2065 deletion completed in 6.034927251s

• [SLOW TEST:13.055 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:12:53.385: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 18 20:13:03.411: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-117 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 20:13:03.411: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 20:13:03.540: INFO: Exec stderr: ""
Dec 18 20:13:03.541: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-117 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 20:13:03.541: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 20:13:03.687: INFO: Exec stderr: ""
Dec 18 20:13:03.687: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-117 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 20:13:03.687: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 20:13:03.814: INFO: Exec stderr: ""
Dec 18 20:13:03.815: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-117 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 20:13:03.815: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 20:13:03.945: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 18 20:13:03.945: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-117 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 20:13:03.945: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 20:13:04.070: INFO: Exec stderr: ""
Dec 18 20:13:04.070: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-117 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 20:13:04.070: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 20:13:04.215: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 18 20:13:04.215: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-117 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 20:13:04.215: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 20:13:04.340: INFO: Exec stderr: ""
Dec 18 20:13:04.340: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-117 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 20:13:04.340: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 20:13:04.479: INFO: Exec stderr: ""
Dec 18 20:13:04.479: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-117 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 20:13:04.479: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 20:13:04.598: INFO: Exec stderr: ""
Dec 18 20:13:04.598: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-117 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 18 20:13:04.598: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
Dec 18 20:13:04.749: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:13:04.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-117" for this suite.
Dec 18 20:13:52.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:13:52.785: INFO: namespace e2e-kubelet-etc-hosts-117 deletion completed in 48.034825966s

• [SLOW TEST:59.400 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:13:52.786: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-1d8e7587-eaa8-4dcb-b740-d3988323bebd
STEP: Creating secret with name secret-projected-all-test-volume-892e3228-6349-4577-b1ce-2b7b17764d23
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 18 20:13:52.804: INFO: Waiting up to 5m0s for pod "projected-volume-ada095e4-f4ea-40d9-9669-3df77cb4b491" in namespace "projected-4567" to be "success or failure"
Dec 18 20:13:52.805: INFO: Pod "projected-volume-ada095e4-f4ea-40d9-9669-3df77cb4b491": Phase="Pending", Reason="", readiness=false. Elapsed: 838.39µs
Dec 18 20:13:54.807: INFO: Pod "projected-volume-ada095e4-f4ea-40d9-9669-3df77cb4b491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002653847s
Dec 18 20:13:56.808: INFO: Pod "projected-volume-ada095e4-f4ea-40d9-9669-3df77cb4b491": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004193308s
Dec 18 20:13:58.810: INFO: Pod "projected-volume-ada095e4-f4ea-40d9-9669-3df77cb4b491": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006232623s
Dec 18 20:14:00.812: INFO: Pod "projected-volume-ada095e4-f4ea-40d9-9669-3df77cb4b491": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008106207s
STEP: Saw pod success
Dec 18 20:14:00.812: INFO: Pod "projected-volume-ada095e4-f4ea-40d9-9669-3df77cb4b491" satisfied condition "success or failure"
Dec 18 20:14:00.813: INFO: Trying to get logs from node controller-0 pod projected-volume-ada095e4-f4ea-40d9-9669-3df77cb4b491 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 18 20:14:00.828: INFO: Waiting for pod projected-volume-ada095e4-f4ea-40d9-9669-3df77cb4b491 to disappear
Dec 18 20:14:00.829: INFO: Pod projected-volume-ada095e4-f4ea-40d9-9669-3df77cb4b491 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:14:00.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4567" for this suite.
Dec 18 20:14:06.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:14:06.865: INFO: namespace projected-4567 deletion completed in 6.033702536s

• [SLOW TEST:14.079 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:14:06.865: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-fecddf47-844b-4f00-aff5-d2f23bd0f6ef
STEP: Creating a pod to test consume secrets
Dec 18 20:14:06.882: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-62b2024f-85c0-4a00-b726-bdb0970f4d73" in namespace "projected-1779" to be "success or failure"
Dec 18 20:14:06.883: INFO: Pod "pod-projected-secrets-62b2024f-85c0-4a00-b726-bdb0970f4d73": Phase="Pending", Reason="", readiness=false. Elapsed: 899.874µs
Dec 18 20:14:08.885: INFO: Pod "pod-projected-secrets-62b2024f-85c0-4a00-b726-bdb0970f4d73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002377554s
Dec 18 20:14:10.887: INFO: Pod "pod-projected-secrets-62b2024f-85c0-4a00-b726-bdb0970f4d73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00411963s
Dec 18 20:14:12.888: INFO: Pod "pod-projected-secrets-62b2024f-85c0-4a00-b726-bdb0970f4d73": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005851619s
Dec 18 20:14:14.890: INFO: Pod "pod-projected-secrets-62b2024f-85c0-4a00-b726-bdb0970f4d73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007732083s
STEP: Saw pod success
Dec 18 20:14:14.890: INFO: Pod "pod-projected-secrets-62b2024f-85c0-4a00-b726-bdb0970f4d73" satisfied condition "success or failure"
Dec 18 20:14:14.891: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-62b2024f-85c0-4a00-b726-bdb0970f4d73 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 18 20:14:14.908: INFO: Waiting for pod pod-projected-secrets-62b2024f-85c0-4a00-b726-bdb0970f4d73 to disappear
Dec 18 20:14:14.909: INFO: Pod pod-projected-secrets-62b2024f-85c0-4a00-b726-bdb0970f4d73 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:14:14.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1779" for this suite.
Dec 18 20:14:20.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:14:20.944: INFO: namespace projected-1779 deletion completed in 6.033938682s

• [SLOW TEST:14.079 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:14:20.944: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 20:14:20.958: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:14:27.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4643" for this suite.
Dec 18 20:14:33.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:14:33.086: INFO: namespace custom-resource-definition-4643 deletion completed in 6.035861856s

• [SLOW TEST:12.141 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:14:33.086: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 18 20:14:33.575: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 18 20:14:35.580: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296873, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296873, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296873, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296873, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:14:37.581: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296873, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296873, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296873, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296873, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:14:39.582: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296873, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296873, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296873, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712296873, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 20:14:42.587: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 20:14:42.589: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:14:43.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7782" for this suite.
Dec 18 20:14:49.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:14:49.689: INFO: namespace crd-webhook-7782 deletion completed in 6.035285232s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:16.608 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:14:49.694: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:15:00.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6457" for this suite.
Dec 18 20:15:06.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:15:06.759: INFO: namespace resourcequota-6457 deletion completed in 6.035582563s

• [SLOW TEST:17.066 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:15:06.759: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 18 20:15:15.284: INFO: Successfully updated pod "pod-update-2d89f113-c26c-4e6a-aa07-0830e475c363"
STEP: verifying the updated pod is in kubernetes
Dec 18 20:15:15.286: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:15:15.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8256" for this suite.
Dec 18 20:15:43.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:15:43.321: INFO: namespace pods-8256 deletion completed in 28.033309211s

• [SLOW TEST:36.562 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:15:43.321: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 20:15:43.335: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 18 20:15:46.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-9327 create -f -'
Dec 18 20:15:47.166: INFO: stderr: ""
Dec 18 20:15:47.166: INFO: stdout: "e2e-test-crd-publish-openapi-3946-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 18 20:15:47.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-9327 delete e2e-test-crd-publish-openapi-3946-crds test-cr'
Dec 18 20:15:47.235: INFO: stderr: ""
Dec 18 20:15:47.235: INFO: stdout: "e2e-test-crd-publish-openapi-3946-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 18 20:15:47.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-9327 apply -f -'
Dec 18 20:15:47.380: INFO: stderr: ""
Dec 18 20:15:47.380: INFO: stdout: "e2e-test-crd-publish-openapi-3946-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 18 20:15:47.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 --namespace=crd-publish-openapi-9327 delete e2e-test-crd-publish-openapi-3946-crds test-cr'
Dec 18 20:15:47.445: INFO: stderr: ""
Dec 18 20:15:47.445: INFO: stdout: "e2e-test-crd-publish-openapi-3946-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 18 20:15:47.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 explain e2e-test-crd-publish-openapi-3946-crds'
Dec 18 20:15:47.589: INFO: stderr: ""
Dec 18 20:15:47.589: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3946-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:15:51.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9327" for this suite.
Dec 18 20:15:57.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:15:57.149: INFO: namespace crd-publish-openapi-9327 deletion completed in 6.036154872s

• [SLOW TEST:13.828 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:15:57.149: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-4856/secret-test-321c02c6-cb0c-4b30-8edf-b7dc0b171fbb
STEP: Creating a pod to test consume secrets
Dec 18 20:15:57.167: INFO: Waiting up to 5m0s for pod "pod-configmaps-2d17f3fd-764c-479d-938b-cab8542ac2e5" in namespace "secrets-4856" to be "success or failure"
Dec 18 20:15:57.168: INFO: Pod "pod-configmaps-2d17f3fd-764c-479d-938b-cab8542ac2e5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.406112ms
Dec 18 20:15:59.171: INFO: Pod "pod-configmaps-2d17f3fd-764c-479d-938b-cab8542ac2e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003522841s
Dec 18 20:16:01.172: INFO: Pod "pod-configmaps-2d17f3fd-764c-479d-938b-cab8542ac2e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005193897s
Dec 18 20:16:03.174: INFO: Pod "pod-configmaps-2d17f3fd-764c-479d-938b-cab8542ac2e5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006704967s
Dec 18 20:16:05.175: INFO: Pod "pod-configmaps-2d17f3fd-764c-479d-938b-cab8542ac2e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008452224s
STEP: Saw pod success
Dec 18 20:16:05.176: INFO: Pod "pod-configmaps-2d17f3fd-764c-479d-938b-cab8542ac2e5" satisfied condition "success or failure"
Dec 18 20:16:05.177: INFO: Trying to get logs from node controller-1 pod pod-configmaps-2d17f3fd-764c-479d-938b-cab8542ac2e5 container env-test: <nil>
STEP: delete the pod
Dec 18 20:16:05.192: INFO: Waiting for pod pod-configmaps-2d17f3fd-764c-479d-938b-cab8542ac2e5 to disappear
Dec 18 20:16:05.193: INFO: Pod pod-configmaps-2d17f3fd-764c-479d-938b-cab8542ac2e5 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:16:05.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4856" for this suite.
Dec 18 20:16:11.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:16:11.229: INFO: namespace secrets-4856 deletion completed in 6.034584911s

• [SLOW TEST:14.080 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:16:11.229: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-8dc0f7e2-4bd9-4b9b-9292-b0697bd53ddb
STEP: Creating a pod to test consume configMaps
Dec 18 20:16:11.246: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa46b133-de9b-4da1-bdfa-dfb4d9aa2033" in namespace "configmap-8874" to be "success or failure"
Dec 18 20:16:11.247: INFO: Pod "pod-configmaps-fa46b133-de9b-4da1-bdfa-dfb4d9aa2033": Phase="Pending", Reason="", readiness=false. Elapsed: 942.22µs
Dec 18 20:16:13.248: INFO: Pod "pod-configmaps-fa46b133-de9b-4da1-bdfa-dfb4d9aa2033": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002521593s
Dec 18 20:16:15.250: INFO: Pod "pod-configmaps-fa46b133-de9b-4da1-bdfa-dfb4d9aa2033": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004369263s
Dec 18 20:16:17.252: INFO: Pod "pod-configmaps-fa46b133-de9b-4da1-bdfa-dfb4d9aa2033": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006185417s
Dec 18 20:16:19.254: INFO: Pod "pod-configmaps-fa46b133-de9b-4da1-bdfa-dfb4d9aa2033": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007967182s
STEP: Saw pod success
Dec 18 20:16:19.254: INFO: Pod "pod-configmaps-fa46b133-de9b-4da1-bdfa-dfb4d9aa2033" satisfied condition "success or failure"
Dec 18 20:16:19.255: INFO: Trying to get logs from node controller-1 pod pod-configmaps-fa46b133-de9b-4da1-bdfa-dfb4d9aa2033 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 20:16:19.264: INFO: Waiting for pod pod-configmaps-fa46b133-de9b-4da1-bdfa-dfb4d9aa2033 to disappear
Dec 18 20:16:19.265: INFO: Pod pod-configmaps-fa46b133-de9b-4da1-bdfa-dfb4d9aa2033 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:16:19.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8874" for this suite.
Dec 18 20:16:25.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:16:25.302: INFO: namespace configmap-8874 deletion completed in 6.034737416s

• [SLOW TEST:14.072 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:16:25.302: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-8e7676f2-28ad-4f7b-8de1-80b8bd10a370 in namespace container-probe-7294
Dec 18 20:16:33.320: INFO: Started pod busybox-8e7676f2-28ad-4f7b-8de1-80b8bd10a370 in namespace container-probe-7294
STEP: checking the pod's current state and verifying that restartCount is present
Dec 18 20:16:33.321: INFO: Initial restart count of pod busybox-8e7676f2-28ad-4f7b-8de1-80b8bd10a370 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:20:33.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7294" for this suite.
Dec 18 20:20:39.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:20:39.574: INFO: namespace container-probe-7294 deletion completed in 6.034395498s

• [SLOW TEST:254.272 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:20:39.574: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-fd2ddfec-7988-4989-97d3-23672e99ef97
STEP: Creating a pod to test consume secrets
Dec 18 20:20:39.592: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d69a5a37-e98e-40ab-b50d-3ebf44b4bc5c" in namespace "projected-1885" to be "success or failure"
Dec 18 20:20:39.593: INFO: Pod "pod-projected-secrets-d69a5a37-e98e-40ab-b50d-3ebf44b4bc5c": Phase="Pending", Reason="", readiness=false. Elapsed: 906.128µs
Dec 18 20:20:41.594: INFO: Pod "pod-projected-secrets-d69a5a37-e98e-40ab-b50d-3ebf44b4bc5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00257852s
Dec 18 20:20:43.596: INFO: Pod "pod-projected-secrets-d69a5a37-e98e-40ab-b50d-3ebf44b4bc5c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004570226s
Dec 18 20:20:45.598: INFO: Pod "pod-projected-secrets-d69a5a37-e98e-40ab-b50d-3ebf44b4bc5c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005956581s
Dec 18 20:20:47.600: INFO: Pod "pod-projected-secrets-d69a5a37-e98e-40ab-b50d-3ebf44b4bc5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008327836s
STEP: Saw pod success
Dec 18 20:20:47.600: INFO: Pod "pod-projected-secrets-d69a5a37-e98e-40ab-b50d-3ebf44b4bc5c" satisfied condition "success or failure"
Dec 18 20:20:47.601: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-d69a5a37-e98e-40ab-b50d-3ebf44b4bc5c container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 18 20:20:47.616: INFO: Waiting for pod pod-projected-secrets-d69a5a37-e98e-40ab-b50d-3ebf44b4bc5c to disappear
Dec 18 20:20:47.617: INFO: Pod pod-projected-secrets-d69a5a37-e98e-40ab-b50d-3ebf44b4bc5c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:20:47.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1885" for this suite.
Dec 18 20:20:53.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:20:53.652: INFO: namespace projected-1885 deletion completed in 6.033279648s

• [SLOW TEST:14.078 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:20:53.652: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 18 20:20:53.669: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7619 /api/v1/namespaces/watch-7619/configmaps/e2e-watch-test-watch-closed 56213690-ee1c-497d-873f-6448dcf4fda2 97797 0 2019-12-18 20:20:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 18 20:20:53.669: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7619 /api/v1/namespaces/watch-7619/configmaps/e2e-watch-test-watch-closed 56213690-ee1c-497d-873f-6448dcf4fda2 97798 0 2019-12-18 20:20:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 18 20:20:53.673: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7619 /api/v1/namespaces/watch-7619/configmaps/e2e-watch-test-watch-closed 56213690-ee1c-497d-873f-6448dcf4fda2 97799 0 2019-12-18 20:20:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 18 20:20:53.673: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7619 /api/v1/namespaces/watch-7619/configmaps/e2e-watch-test-watch-closed 56213690-ee1c-497d-873f-6448dcf4fda2 97800 0 2019-12-18 20:20:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:20:53.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7619" for this suite.
Dec 18 20:20:59.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:20:59.709: INFO: namespace watch-7619 deletion completed in 6.0341912s

• [SLOW TEST:6.057 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:20:59.709: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 20:20:59.725: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f96e5186-a53c-4709-9e2f-ccfd58fa54e1" in namespace "projected-2009" to be "success or failure"
Dec 18 20:20:59.727: INFO: Pod "downwardapi-volume-f96e5186-a53c-4709-9e2f-ccfd58fa54e1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.386936ms
Dec 18 20:21:01.728: INFO: Pod "downwardapi-volume-f96e5186-a53c-4709-9e2f-ccfd58fa54e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003108482s
Dec 18 20:21:03.730: INFO: Pod "downwardapi-volume-f96e5186-a53c-4709-9e2f-ccfd58fa54e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004709498s
Dec 18 20:21:05.732: INFO: Pod "downwardapi-volume-f96e5186-a53c-4709-9e2f-ccfd58fa54e1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006655757s
Dec 18 20:21:07.734: INFO: Pod "downwardapi-volume-f96e5186-a53c-4709-9e2f-ccfd58fa54e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009047435s
STEP: Saw pod success
Dec 18 20:21:07.734: INFO: Pod "downwardapi-volume-f96e5186-a53c-4709-9e2f-ccfd58fa54e1" satisfied condition "success or failure"
Dec 18 20:21:07.736: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-f96e5186-a53c-4709-9e2f-ccfd58fa54e1 container client-container: <nil>
STEP: delete the pod
Dec 18 20:21:07.744: INFO: Waiting for pod downwardapi-volume-f96e5186-a53c-4709-9e2f-ccfd58fa54e1 to disappear
Dec 18 20:21:07.744: INFO: Pod downwardapi-volume-f96e5186-a53c-4709-9e2f-ccfd58fa54e1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:21:07.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2009" for this suite.
Dec 18 20:21:13.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:21:13.781: INFO: namespace projected-2009 deletion completed in 6.034861234s

• [SLOW TEST:14.072 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:21:13.781: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 18 20:21:14.409: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 18 20:21:16.413: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297274, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297274, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297274, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297274, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:21:18.415: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297274, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297274, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297274, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297274, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:21:20.415: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297274, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297274, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297274, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297274, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 18 20:21:23.422: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 20:21:23.423: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:21:24.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2356" for this suite.
Dec 18 20:21:30.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:21:30.563: INFO: namespace crd-webhook-2356 deletion completed in 6.033316195s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:16.787 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:21:30.568: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 18 20:21:30.581: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 18 20:21:30.586: INFO: Waiting for terminating namespaces to be deleted...
Dec 18 20:21:30.587: INFO: 
Logging pods the kubelet thinks is on node compute-0 before test
Dec 18 20:21:30.598: INFO: kube-proxy-lq2c8 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.598: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 18 20:21:30.598: INFO: kube-multus-ds-amd64-fczk7 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.598: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 20:21:30.598: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-jzbct from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 20:21:30.598: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Dec 18 20:21:30.598: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 20:21:30.598: INFO: kube-sriov-cni-ds-amd64-nlfxr from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.598: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 20:21:30.598: INFO: calico-node-ntnk8 from kube-system started at 2019-12-18 15:12:04 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.598: INFO: 	Container calico-node ready: true, restart count 1
Dec 18 20:21:30.598: INFO: sonobuoy from sonobuoy started at 2019-12-18 18:17:38 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.598: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 18 20:21:30.598: INFO: 
Logging pods the kubelet thinks is on node compute-1 before test
Dec 18 20:21:30.610: INFO: kube-sriov-cni-ds-amd64-v94kl from kube-system started at 2019-12-18 15:11:51 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.610: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 20:21:30.610: INFO: kube-proxy-w7hfz from kube-system started at 2019-12-18 15:11:52 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.610: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 18 20:21:30.610: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-49m5k from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 20:21:30.610: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Dec 18 20:21:30.610: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 20:21:30.610: INFO: calico-node-6s2pv from kube-system started at 2019-12-18 15:11:51 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.610: INFO: 	Container calico-node ready: true, restart count 1
Dec 18 20:21:30.610: INFO: kube-multus-ds-amd64-trmzx from kube-system started at 2019-12-18 15:11:52 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.610: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 20:21:30.610: INFO: 
Logging pods the kubelet thinks is on node controller-0 before test
Dec 18 20:21:30.621: INFO: kube-apiserver-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.621: INFO: 	Container kube-apiserver ready: true, restart count 2
Dec 18 20:21:30.621: INFO: rbd-provisioner-7484d49cf6-858lp from kube-system started at 2019-12-18 18:33:13 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.621: INFO: 	Container rbd-provisioner ready: true, restart count 0
Dec 18 20:21:30.621: INFO: kube-controller-manager-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.621: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 18 20:21:30.621: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-q7fcg from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 20:21:30.621: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Dec 18 20:21:30.621: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 20:21:30.621: INFO: kube-scheduler-controller-0 from kube-system started at 2019-12-18 14:35:13 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.621: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 18 20:21:30.621: INFO: kube-proxy-hs98d from kube-system started at 2019-12-18 14:17:53 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.621: INFO: 	Container kube-proxy ready: true, restart count 1
Dec 18 20:21:30.621: INFO: kube-sriov-cni-ds-amd64-97g65 from kube-system started at 2019-12-18 18:33:43 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.621: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 20:21:30.621: INFO: calico-kube-controllers-855577b7b5-2gktm from kube-system started at 2019-12-18 18:38:16 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.621: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 18 20:21:30.621: INFO: coredns-6bc668cd76-ks6bd from kube-system started at 2019-12-18 18:33:17 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.621: INFO: 	Container coredns ready: true, restart count 0
Dec 18 20:21:30.621: INFO: calico-node-7bzgw from kube-system started at 2019-12-18 14:17:53 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.621: INFO: 	Container calico-node ready: true, restart count 3
Dec 18 20:21:30.621: INFO: kube-multus-ds-amd64-plsvq from kube-system started at 2019-12-18 18:33:43 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.621: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 20:21:30.621: INFO: tiller-deploy-d6b59fcb-kj5hv from kube-system started at 2019-12-18 18:38:16 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.621: INFO: 	Container tiller ready: true, restart count 0
Dec 18 20:21:30.621: INFO: 
Logging pods the kubelet thinks is on node controller-1 before test
Dec 18 20:21:30.626: INFO: calico-node-8q8ss from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container calico-node ready: true, restart count 4
Dec 18 20:21:30.626: INFO: sonobuoy-systemd-logs-daemon-set-c91b178dff064717-rz72p from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Dec 18 20:21:30.626: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 18 20:21:30.626: INFO: kube-sriov-cni-ds-amd64-rb5tl from kube-system started at 2019-12-18 18:38:52 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Dec 18 20:21:30.626: INFO: rbd-provisioner-7484d49cf6-p98d2 from kube-system started at 2019-12-18 18:38:56 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container rbd-provisioner ready: true, restart count 0
Dec 18 20:21:30.626: INFO: ceph-pools-audit-1576700400-dtjlw from kube-system started at 2019-12-18 20:20:03 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Dec 18 20:21:30.626: INFO: kube-apiserver-controller-1 from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 18 20:21:30.626: INFO: kube-proxy-fds2l from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container kube-proxy ready: true, restart count 1
Dec 18 20:21:30.626: INFO: ceph-pools-audit-1576699800-wlknp from kube-system started at 2019-12-18 20:10:02 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Dec 18 20:21:30.626: INFO: kube-scheduler-controller-1 from kube-system started at 2019-12-18 15:22:52 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 18 20:21:30.626: INFO: kube-multus-ds-amd64-rvvrh from kube-system started at 2019-12-18 18:38:52 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container kube-multus ready: true, restart count 0
Dec 18 20:21:30.626: INFO: coredns-6bc668cd76-86sgt from kube-system started at 2019-12-18 18:38:56 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container coredns ready: true, restart count 0
Dec 18 20:21:30.626: INFO: sonobuoy-e2e-job-8824c3aaef6e4480 from sonobuoy started at 2019-12-18 18:17:50 +0000 UTC (2 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container e2e ready: true, restart count 0
Dec 18 20:21:30.626: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 18 20:21:30.626: INFO: kube-controller-manager-controller-1 from kube-system started at 2019-12-18 14:57:01 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 18 20:21:30.626: INFO: ceph-pools-audit-1576700100-f2qmm from kube-system started at 2019-12-18 20:15:03 +0000 UTC (1 container statuses recorded)
Dec 18 20:21:30.626: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15e190be15dd2fe5], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:21:31.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5625" for this suite.
Dec 18 20:21:37.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:21:37.677: INFO: namespace sched-pred-5625 deletion completed in 6.035845544s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.109 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:21:37.677: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec 18 20:21:43.710: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W1218 20:21:43.710825      27 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 18 20:21:43.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8300" for this suite.
Dec 18 20:21:49.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:21:49.757: INFO: namespace gc-8300 deletion completed in 6.045207039s

• [SLOW TEST:12.080 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:21:49.757: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-516fafc5-2c15-49df-9b64-80d3cff1f105
STEP: Creating a pod to test consume configMaps
Dec 18 20:21:49.775: INFO: Waiting up to 5m0s for pod "pod-configmaps-d14637e9-8124-496e-b8fa-1cc41c461c16" in namespace "configmap-9403" to be "success or failure"
Dec 18 20:21:49.775: INFO: Pod "pod-configmaps-d14637e9-8124-496e-b8fa-1cc41c461c16": Phase="Pending", Reason="", readiness=false. Elapsed: 899.557µs
Dec 18 20:21:51.777: INFO: Pod "pod-configmaps-d14637e9-8124-496e-b8fa-1cc41c461c16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002698084s
Dec 18 20:21:53.779: INFO: Pod "pod-configmaps-d14637e9-8124-496e-b8fa-1cc41c461c16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004456114s
Dec 18 20:21:55.781: INFO: Pod "pod-configmaps-d14637e9-8124-496e-b8fa-1cc41c461c16": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006728631s
Dec 18 20:21:57.783: INFO: Pod "pod-configmaps-d14637e9-8124-496e-b8fa-1cc41c461c16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008758877s
STEP: Saw pod success
Dec 18 20:21:57.783: INFO: Pod "pod-configmaps-d14637e9-8124-496e-b8fa-1cc41c461c16" satisfied condition "success or failure"
Dec 18 20:21:57.784: INFO: Trying to get logs from node controller-1 pod pod-configmaps-d14637e9-8124-496e-b8fa-1cc41c461c16 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 20:21:57.791: INFO: Waiting for pod pod-configmaps-d14637e9-8124-496e-b8fa-1cc41c461c16 to disappear
Dec 18 20:21:57.793: INFO: Pod pod-configmaps-d14637e9-8124-496e-b8fa-1cc41c461c16 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:21:57.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9403" for this suite.
Dec 18 20:22:03.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:22:03.829: INFO: namespace configmap-9403 deletion completed in 6.034576884s

• [SLOW TEST:14.072 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:22:03.829: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Dec 18 20:22:03.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 cluster-info'
Dec 18 20:22:03.909: INFO: stderr: ""
Dec 18 20:22:03.909: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:22:03.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5535" for this suite.
Dec 18 20:22:09.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:22:09.944: INFO: namespace kubectl-5535 deletion completed in 6.03384246s

• [SLOW TEST:6.115 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:22:09.945: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:22:17.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-605" for this suite.
Dec 18 20:23:03.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:23:04.004: INFO: namespace kubelet-test-605 deletion completed in 46.035656689s

• [SLOW TEST:54.059 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:23:04.004: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-0e76ef8c-1d8a-45d9-856d-f5f793470d44
STEP: Creating a pod to test consume configMaps
Dec 18 20:23:04.021: INFO: Waiting up to 5m0s for pod "pod-configmaps-5b99d49c-f8fa-4a28-bb86-ca057ae1d53b" in namespace "configmap-9303" to be "success or failure"
Dec 18 20:23:04.022: INFO: Pod "pod-configmaps-5b99d49c-f8fa-4a28-bb86-ca057ae1d53b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.776064ms
Dec 18 20:23:06.024: INFO: Pod "pod-configmaps-5b99d49c-f8fa-4a28-bb86-ca057ae1d53b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003611418s
Dec 18 20:23:08.026: INFO: Pod "pod-configmaps-5b99d49c-f8fa-4a28-bb86-ca057ae1d53b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00560545s
Dec 18 20:23:10.028: INFO: Pod "pod-configmaps-5b99d49c-f8fa-4a28-bb86-ca057ae1d53b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007416141s
Dec 18 20:23:12.030: INFO: Pod "pod-configmaps-5b99d49c-f8fa-4a28-bb86-ca057ae1d53b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00952202s
STEP: Saw pod success
Dec 18 20:23:12.030: INFO: Pod "pod-configmaps-5b99d49c-f8fa-4a28-bb86-ca057ae1d53b" satisfied condition "success or failure"
Dec 18 20:23:12.031: INFO: Trying to get logs from node controller-0 pod pod-configmaps-5b99d49c-f8fa-4a28-bb86-ca057ae1d53b container configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 20:23:12.045: INFO: Waiting for pod pod-configmaps-5b99d49c-f8fa-4a28-bb86-ca057ae1d53b to disappear
Dec 18 20:23:12.046: INFO: Pod pod-configmaps-5b99d49c-f8fa-4a28-bb86-ca057ae1d53b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:23:12.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9303" for this suite.
Dec 18 20:23:18.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:23:18.082: INFO: namespace configmap-9303 deletion completed in 6.034348442s

• [SLOW TEST:14.078 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:23:18.082: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1534
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1534
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1534
Dec 18 20:23:18.100: INFO: Found 0 stateful pods, waiting for 1
Dec 18 20:23:28.102: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 18 20:23:28.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-1534 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 18 20:23:28.297: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 18 20:23:28.297: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 18 20:23:28.297: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 18 20:23:28.299: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 18 20:23:38.301: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 18 20:23:38.301: INFO: Waiting for statefulset status.replicas updated to 0
Dec 18 20:23:38.306: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999756s
Dec 18 20:23:39.309: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998613151s
Dec 18 20:23:40.311: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.996163994s
Dec 18 20:23:41.313: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.994052845s
Dec 18 20:23:42.315: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.991994652s
Dec 18 20:23:43.317: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.989881879s
Dec 18 20:23:44.319: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.987975138s
Dec 18 20:23:45.321: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.985974522s
Dec 18 20:23:46.322: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.984215472s
Dec 18 20:23:47.325: INFO: Verifying statefulset ss doesn't scale past 1 for another 982.562216ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1534
Dec 18 20:23:48.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-1534 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 18 20:23:48.530: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 18 20:23:48.530: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 18 20:23:48.530: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 18 20:23:48.532: INFO: Found 1 stateful pods, waiting for 3
Dec 18 20:23:58.534: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 20:23:58.534: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 20:23:58.534: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 18 20:24:08.534: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 20:24:08.534: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 18 20:24:08.534: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 18 20:24:08.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-1534 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 18 20:24:08.736: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 18 20:24:08.736: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 18 20:24:08.736: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 18 20:24:08.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-1534 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 18 20:24:08.935: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 18 20:24:08.935: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 18 20:24:08.935: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 18 20:24:08.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-1534 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 18 20:24:09.165: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 18 20:24:09.165: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 18 20:24:09.165: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 18 20:24:09.165: INFO: Waiting for statefulset status.replicas updated to 0
Dec 18 20:24:09.167: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 18 20:24:19.171: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 18 20:24:19.171: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 18 20:24:19.171: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 18 20:24:19.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999771s
Dec 18 20:24:20.177: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998431875s
Dec 18 20:24:21.179: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.996095205s
Dec 18 20:24:22.182: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.993808745s
Dec 18 20:24:23.184: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.991629365s
Dec 18 20:24:24.186: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.989592046s
Dec 18 20:24:25.188: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.987352083s
Dec 18 20:24:26.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.985289516s
Dec 18 20:24:27.192: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.98320013s
Dec 18 20:24:28.194: INFO: Verifying statefulset ss doesn't scale past 3 for another 981.176402ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1534
Dec 18 20:24:29.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-1534 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 18 20:24:29.396: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 18 20:24:29.396: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 18 20:24:29.396: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 18 20:24:29.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-1534 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 18 20:24:29.591: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 18 20:24:29.591: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 18 20:24:29.591: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 18 20:24:29.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=statefulset-1534 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 18 20:24:29.850: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 18 20:24:29.850: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 18 20:24:29.850: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 18 20:24:29.850: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 18 20:24:49.857: INFO: Deleting all statefulset in ns statefulset-1534
Dec 18 20:24:49.859: INFO: Scaling statefulset ss to 0
Dec 18 20:24:49.862: INFO: Waiting for statefulset status.replicas updated to 0
Dec 18 20:24:49.863: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:24:49.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1534" for this suite.
Dec 18 20:24:55.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:24:55.903: INFO: namespace statefulset-1534 deletion completed in 6.034758248s

• [SLOW TEST:97.821 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:24:55.904: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 18 20:24:55.920: INFO: Waiting up to 5m0s for pod "pod-89cda04e-d23b-4399-86b9-a6e75c20d581" in namespace "emptydir-2752" to be "success or failure"
Dec 18 20:24:55.921: INFO: Pod "pod-89cda04e-d23b-4399-86b9-a6e75c20d581": Phase="Pending", Reason="", readiness=false. Elapsed: 1.013459ms
Dec 18 20:24:57.923: INFO: Pod "pod-89cda04e-d23b-4399-86b9-a6e75c20d581": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002872891s
Dec 18 20:24:59.926: INFO: Pod "pod-89cda04e-d23b-4399-86b9-a6e75c20d581": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005244518s
Dec 18 20:25:01.927: INFO: Pod "pod-89cda04e-d23b-4399-86b9-a6e75c20d581": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00711183s
Dec 18 20:25:03.929: INFO: Pod "pod-89cda04e-d23b-4399-86b9-a6e75c20d581": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008680051s
STEP: Saw pod success
Dec 18 20:25:03.929: INFO: Pod "pod-89cda04e-d23b-4399-86b9-a6e75c20d581" satisfied condition "success or failure"
Dec 18 20:25:03.930: INFO: Trying to get logs from node controller-1 pod pod-89cda04e-d23b-4399-86b9-a6e75c20d581 container test-container: <nil>
STEP: delete the pod
Dec 18 20:25:03.944: INFO: Waiting for pod pod-89cda04e-d23b-4399-86b9-a6e75c20d581 to disappear
Dec 18 20:25:03.945: INFO: Pod pod-89cda04e-d23b-4399-86b9-a6e75c20d581 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:25:03.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2752" for this suite.
Dec 18 20:25:09.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:25:09.981: INFO: namespace emptydir-2752 deletion completed in 6.034856399s

• [SLOW TEST:14.078 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:25:09.982: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 18 20:25:09.998: INFO: Waiting up to 5m0s for pod "pod-38c93085-33e2-408d-98ad-d5c4288515dd" in namespace "emptydir-6984" to be "success or failure"
Dec 18 20:25:10.000: INFO: Pod "pod-38c93085-33e2-408d-98ad-d5c4288515dd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.259189ms
Dec 18 20:25:12.001: INFO: Pod "pod-38c93085-33e2-408d-98ad-d5c4288515dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002724235s
Dec 18 20:25:14.003: INFO: Pod "pod-38c93085-33e2-408d-98ad-d5c4288515dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004901999s
Dec 18 20:25:16.005: INFO: Pod "pod-38c93085-33e2-408d-98ad-d5c4288515dd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006482426s
Dec 18 20:25:18.007: INFO: Pod "pod-38c93085-33e2-408d-98ad-d5c4288515dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008274935s
STEP: Saw pod success
Dec 18 20:25:18.007: INFO: Pod "pod-38c93085-33e2-408d-98ad-d5c4288515dd" satisfied condition "success or failure"
Dec 18 20:25:18.008: INFO: Trying to get logs from node controller-1 pod pod-38c93085-33e2-408d-98ad-d5c4288515dd container test-container: <nil>
STEP: delete the pod
Dec 18 20:25:18.016: INFO: Waiting for pod pod-38c93085-33e2-408d-98ad-d5c4288515dd to disappear
Dec 18 20:25:18.016: INFO: Pod pod-38c93085-33e2-408d-98ad-d5c4288515dd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:25:18.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6984" for this suite.
Dec 18 20:25:24.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:25:24.054: INFO: namespace emptydir-6984 deletion completed in 6.035713936s

• [SLOW TEST:14.072 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:25:24.054: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 18 20:25:24.070: INFO: Waiting up to 5m0s for pod "downward-api-3947e927-c04b-4f83-a3da-c81d06d2a890" in namespace "downward-api-4062" to be "success or failure"
Dec 18 20:25:24.072: INFO: Pod "downward-api-3947e927-c04b-4f83-a3da-c81d06d2a890": Phase="Pending", Reason="", readiness=false. Elapsed: 1.531986ms
Dec 18 20:25:26.074: INFO: Pod "downward-api-3947e927-c04b-4f83-a3da-c81d06d2a890": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003226223s
Dec 18 20:25:28.075: INFO: Pod "downward-api-3947e927-c04b-4f83-a3da-c81d06d2a890": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005092859s
Dec 18 20:25:30.077: INFO: Pod "downward-api-3947e927-c04b-4f83-a3da-c81d06d2a890": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006782393s
Dec 18 20:25:32.079: INFO: Pod "downward-api-3947e927-c04b-4f83-a3da-c81d06d2a890": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008739668s
STEP: Saw pod success
Dec 18 20:25:32.079: INFO: Pod "downward-api-3947e927-c04b-4f83-a3da-c81d06d2a890" satisfied condition "success or failure"
Dec 18 20:25:32.080: INFO: Trying to get logs from node controller-0 pod downward-api-3947e927-c04b-4f83-a3da-c81d06d2a890 container dapi-container: <nil>
STEP: delete the pod
Dec 18 20:25:32.096: INFO: Waiting for pod downward-api-3947e927-c04b-4f83-a3da-c81d06d2a890 to disappear
Dec 18 20:25:32.096: INFO: Pod downward-api-3947e927-c04b-4f83-a3da-c81d06d2a890 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:25:32.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4062" for this suite.
Dec 18 20:25:38.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:25:38.134: INFO: namespace downward-api-4062 deletion completed in 6.036087203s

• [SLOW TEST:14.080 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:25:38.134: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:25:46.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6582" for this suite.
Dec 18 20:26:30.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:26:30.197: INFO: namespace kubelet-test-6582 deletion completed in 44.035771379s

• [SLOW TEST:52.062 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:26:30.197: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:26:30.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1250" for this suite.
Dec 18 20:26:52.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:26:52.250: INFO: namespace pods-1250 deletion completed in 22.034080008s

• [SLOW TEST:22.053 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:26:52.250: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4970
I1218 20:26:52.264832      27 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4970, replica count: 1
I1218 20:26:53.315233      27 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 20:26:54.315502      27 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 20:26:55.315770      27 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 20:26:56.316049      27 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 20:26:57.316317      27 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 20:26:58.316505      27 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 20:26:59.316697      27 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 18 20:26:59.422: INFO: Created: latency-svc-7twnm
Dec 18 20:26:59.425: INFO: Got endpoints: latency-svc-7twnm [8.13762ms]
Dec 18 20:26:59.430: INFO: Created: latency-svc-mhc6q
Dec 18 20:26:59.432: INFO: Got endpoints: latency-svc-mhc6q [7.057819ms]
Dec 18 20:26:59.435: INFO: Created: latency-svc-hkndd
Dec 18 20:26:59.436: INFO: Got endpoints: latency-svc-hkndd [10.937729ms]
Dec 18 20:26:59.439: INFO: Created: latency-svc-vb6df
Dec 18 20:26:59.444: INFO: Got endpoints: latency-svc-vb6df [18.731451ms]
Dec 18 20:26:59.444: INFO: Created: latency-svc-snfjc
Dec 18 20:26:59.445: INFO: Got endpoints: latency-svc-snfjc [20.194457ms]
Dec 18 20:26:59.449: INFO: Created: latency-svc-w2wtt
Dec 18 20:26:59.450: INFO: Got endpoints: latency-svc-w2wtt [25.097839ms]
Dec 18 20:26:59.455: INFO: Created: latency-svc-zrg2g
Dec 18 20:26:59.456: INFO: Got endpoints: latency-svc-zrg2g [30.94623ms]
Dec 18 20:26:59.459: INFO: Created: latency-svc-kh2fd
Dec 18 20:26:59.461: INFO: Got endpoints: latency-svc-kh2fd [35.999801ms]
Dec 18 20:26:59.464: INFO: Created: latency-svc-lxdcc
Dec 18 20:26:59.466: INFO: Got endpoints: latency-svc-lxdcc [41.322458ms]
Dec 18 20:26:59.469: INFO: Created: latency-svc-wwr78
Dec 18 20:26:59.470: INFO: Got endpoints: latency-svc-wwr78 [9.265625ms]
Dec 18 20:26:59.474: INFO: Created: latency-svc-xrgb5
Dec 18 20:26:59.475: INFO: Got endpoints: latency-svc-xrgb5 [50.170673ms]
Dec 18 20:26:59.478: INFO: Created: latency-svc-xjt6s
Dec 18 20:26:59.480: INFO: Got endpoints: latency-svc-xjt6s [55.049855ms]
Dec 18 20:26:59.483: INFO: Created: latency-svc-zx5js
Dec 18 20:26:59.502: INFO: Got endpoints: latency-svc-zx5js [76.972127ms]
Dec 18 20:26:59.503: INFO: Created: latency-svc-lg6wq
Dec 18 20:26:59.504: INFO: Got endpoints: latency-svc-lg6wq [79.151957ms]
Dec 18 20:26:59.508: INFO: Created: latency-svc-89svt
Dec 18 20:26:59.509: INFO: Got endpoints: latency-svc-89svt [84.594871ms]
Dec 18 20:26:59.514: INFO: Created: latency-svc-fkmkk
Dec 18 20:26:59.519: INFO: Created: latency-svc-tdt6d
Dec 18 20:26:59.519: INFO: Got endpoints: latency-svc-fkmkk [94.372191ms]
Dec 18 20:26:59.520: INFO: Got endpoints: latency-svc-tdt6d [95.647536ms]
Dec 18 20:26:59.524: INFO: Created: latency-svc-djdmn
Dec 18 20:26:59.526: INFO: Got endpoints: latency-svc-djdmn [94.029118ms]
Dec 18 20:26:59.529: INFO: Created: latency-svc-pxmw6
Dec 18 20:26:59.531: INFO: Got endpoints: latency-svc-pxmw6 [94.977633ms]
Dec 18 20:26:59.535: INFO: Created: latency-svc-n2jsv
Dec 18 20:26:59.536: INFO: Got endpoints: latency-svc-n2jsv [92.380163ms]
Dec 18 20:26:59.540: INFO: Created: latency-svc-xhtx4
Dec 18 20:26:59.541: INFO: Got endpoints: latency-svc-xhtx4 [96.089675ms]
Dec 18 20:26:59.545: INFO: Created: latency-svc-v9hmk
Dec 18 20:26:59.547: INFO: Got endpoints: latency-svc-v9hmk [96.672489ms]
Dec 18 20:26:59.550: INFO: Created: latency-svc-9lc29
Dec 18 20:26:59.552: INFO: Got endpoints: latency-svc-9lc29 [95.993555ms]
Dec 18 20:26:59.556: INFO: Created: latency-svc-qd6s2
Dec 18 20:26:59.557: INFO: Got endpoints: latency-svc-qd6s2 [90.959032ms]
Dec 18 20:26:59.561: INFO: Created: latency-svc-7r46g
Dec 18 20:26:59.562: INFO: Got endpoints: latency-svc-7r46g [92.452992ms]
Dec 18 20:26:59.567: INFO: Created: latency-svc-d7w9f
Dec 18 20:26:59.568: INFO: Got endpoints: latency-svc-d7w9f [92.94516ms]
Dec 18 20:26:59.572: INFO: Created: latency-svc-lfk9v
Dec 18 20:26:59.573: INFO: Got endpoints: latency-svc-lfk9v [93.164758ms]
Dec 18 20:26:59.577: INFO: Created: latency-svc-4b8fz
Dec 18 20:26:59.578: INFO: Got endpoints: latency-svc-4b8fz [76.707431ms]
Dec 18 20:26:59.583: INFO: Created: latency-svc-7mgj8
Dec 18 20:26:59.584: INFO: Got endpoints: latency-svc-7mgj8 [79.874949ms]
Dec 18 20:26:59.589: INFO: Created: latency-svc-sf9p5
Dec 18 20:26:59.590: INFO: Got endpoints: latency-svc-sf9p5 [80.639096ms]
Dec 18 20:26:59.594: INFO: Created: latency-svc-fzpx8
Dec 18 20:26:59.595: INFO: Got endpoints: latency-svc-fzpx8 [75.973151ms]
Dec 18 20:26:59.599: INFO: Created: latency-svc-xj457
Dec 18 20:26:59.601: INFO: Got endpoints: latency-svc-xj457 [80.487501ms]
Dec 18 20:26:59.631: INFO: Created: latency-svc-mqg5s
Dec 18 20:26:59.632: INFO: Got endpoints: latency-svc-mqg5s [106.598683ms]
Dec 18 20:26:59.637: INFO: Created: latency-svc-k65lf
Dec 18 20:26:59.638: INFO: Got endpoints: latency-svc-k65lf [107.44227ms]
Dec 18 20:26:59.643: INFO: Created: latency-svc-2r4ds
Dec 18 20:26:59.648: INFO: Created: latency-svc-nnqbt
Dec 18 20:26:59.654: INFO: Created: latency-svc-bf7c4
Dec 18 20:26:59.659: INFO: Created: latency-svc-j6krm
Dec 18 20:26:59.665: INFO: Created: latency-svc-7rlg2
Dec 18 20:26:59.670: INFO: Created: latency-svc-lq274
Dec 18 20:26:59.675: INFO: Created: latency-svc-hj599
Dec 18 20:26:59.676: INFO: Got endpoints: latency-svc-2r4ds [43.122623ms]
Dec 18 20:26:59.681: INFO: Created: latency-svc-cz779
Dec 18 20:26:59.687: INFO: Created: latency-svc-25zrm
Dec 18 20:26:59.692: INFO: Created: latency-svc-6v5gz
Dec 18 20:26:59.698: INFO: Created: latency-svc-rw9z6
Dec 18 20:26:59.703: INFO: Created: latency-svc-hvpp9
Dec 18 20:26:59.709: INFO: Created: latency-svc-pz7mj
Dec 18 20:26:59.715: INFO: Created: latency-svc-psxrt
Dec 18 20:26:59.720: INFO: Created: latency-svc-8lw2x
Dec 18 20:26:59.726: INFO: Got endpoints: latency-svc-nnqbt [184.887104ms]
Dec 18 20:26:59.726: INFO: Created: latency-svc-z5dmj
Dec 18 20:26:59.759: INFO: Created: latency-svc-bll2f
Dec 18 20:26:59.775: INFO: Got endpoints: latency-svc-bf7c4 [228.069135ms]
Dec 18 20:26:59.782: INFO: Created: latency-svc-xnwx6
Dec 18 20:26:59.825: INFO: Got endpoints: latency-svc-j6krm [272.876573ms]
Dec 18 20:26:59.831: INFO: Created: latency-svc-gdqwr
Dec 18 20:26:59.874: INFO: Got endpoints: latency-svc-7rlg2 [317.378181ms]
Dec 18 20:26:59.881: INFO: Created: latency-svc-scfbj
Dec 18 20:26:59.924: INFO: Got endpoints: latency-svc-lq274 [361.81897ms]
Dec 18 20:26:59.931: INFO: Created: latency-svc-2rmk2
Dec 18 20:26:59.975: INFO: Got endpoints: latency-svc-hj599 [407.328949ms]
Dec 18 20:26:59.982: INFO: Created: latency-svc-pt9zp
Dec 18 20:27:00.025: INFO: Got endpoints: latency-svc-cz779 [451.787916ms]
Dec 18 20:27:00.032: INFO: Created: latency-svc-j2kf8
Dec 18 20:27:00.078: INFO: Got endpoints: latency-svc-25zrm [499.233654ms]
Dec 18 20:27:00.084: INFO: Created: latency-svc-5pwh8
Dec 18 20:27:00.125: INFO: Got endpoints: latency-svc-6v5gz [540.687451ms]
Dec 18 20:27:00.131: INFO: Created: latency-svc-56qrq
Dec 18 20:27:00.174: INFO: Got endpoints: latency-svc-rw9z6 [584.321742ms]
Dec 18 20:27:00.186: INFO: Created: latency-svc-nt4lr
Dec 18 20:27:00.225: INFO: Got endpoints: latency-svc-hvpp9 [630.174957ms]
Dec 18 20:27:00.232: INFO: Created: latency-svc-dpdm9
Dec 18 20:27:00.275: INFO: Got endpoints: latency-svc-pz7mj [674.087944ms]
Dec 18 20:27:00.282: INFO: Created: latency-svc-jvdgk
Dec 18 20:27:00.325: INFO: Got endpoints: latency-svc-psxrt [788.672197ms]
Dec 18 20:27:00.331: INFO: Created: latency-svc-cj6vv
Dec 18 20:27:00.375: INFO: Got endpoints: latency-svc-8lw2x [736.812412ms]
Dec 18 20:27:00.382: INFO: Created: latency-svc-rnpbc
Dec 18 20:27:00.424: INFO: Got endpoints: latency-svc-z5dmj [748.897693ms]
Dec 18 20:27:00.431: INFO: Created: latency-svc-gndj9
Dec 18 20:27:00.475: INFO: Got endpoints: latency-svc-bll2f [748.742148ms]
Dec 18 20:27:00.481: INFO: Created: latency-svc-q75dp
Dec 18 20:27:00.525: INFO: Got endpoints: latency-svc-xnwx6 [749.88017ms]
Dec 18 20:27:00.531: INFO: Created: latency-svc-7s6tf
Dec 18 20:27:00.574: INFO: Got endpoints: latency-svc-gdqwr [749.603972ms]
Dec 18 20:27:00.581: INFO: Created: latency-svc-pwvvr
Dec 18 20:27:00.625: INFO: Got endpoints: latency-svc-scfbj [750.374068ms]
Dec 18 20:27:00.632: INFO: Created: latency-svc-tz8x5
Dec 18 20:27:00.675: INFO: Got endpoints: latency-svc-2rmk2 [750.461396ms]
Dec 18 20:27:00.681: INFO: Created: latency-svc-fcppv
Dec 18 20:27:00.725: INFO: Got endpoints: latency-svc-pt9zp [749.910089ms]
Dec 18 20:27:00.732: INFO: Created: latency-svc-pks5n
Dec 18 20:27:00.775: INFO: Got endpoints: latency-svc-j2kf8 [750.048901ms]
Dec 18 20:27:00.781: INFO: Created: latency-svc-xr84j
Dec 18 20:27:00.824: INFO: Got endpoints: latency-svc-5pwh8 [746.642169ms]
Dec 18 20:27:00.831: INFO: Created: latency-svc-mpbbz
Dec 18 20:27:00.874: INFO: Got endpoints: latency-svc-56qrq [749.814064ms]
Dec 18 20:27:00.881: INFO: Created: latency-svc-mlrjp
Dec 18 20:27:00.928: INFO: Got endpoints: latency-svc-nt4lr [753.931691ms]
Dec 18 20:27:00.935: INFO: Created: latency-svc-258hp
Dec 18 20:27:00.974: INFO: Got endpoints: latency-svc-dpdm9 [748.976881ms]
Dec 18 20:27:00.981: INFO: Created: latency-svc-p6bm8
Dec 18 20:27:01.025: INFO: Got endpoints: latency-svc-jvdgk [749.663117ms]
Dec 18 20:27:01.036: INFO: Created: latency-svc-8md5n
Dec 18 20:27:01.075: INFO: Got endpoints: latency-svc-cj6vv [749.892394ms]
Dec 18 20:27:01.081: INFO: Created: latency-svc-fswkk
Dec 18 20:27:01.125: INFO: Got endpoints: latency-svc-rnpbc [749.899123ms]
Dec 18 20:27:01.132: INFO: Created: latency-svc-bz6gm
Dec 18 20:27:01.175: INFO: Got endpoints: latency-svc-gndj9 [750.228664ms]
Dec 18 20:27:01.182: INFO: Created: latency-svc-h64qn
Dec 18 20:27:01.224: INFO: Got endpoints: latency-svc-q75dp [749.718822ms]
Dec 18 20:27:01.231: INFO: Created: latency-svc-5bnlq
Dec 18 20:27:01.275: INFO: Got endpoints: latency-svc-7s6tf [749.910055ms]
Dec 18 20:27:01.281: INFO: Created: latency-svc-52dl7
Dec 18 20:27:01.324: INFO: Got endpoints: latency-svc-pwvvr [750.199829ms]
Dec 18 20:27:01.332: INFO: Created: latency-svc-pp7th
Dec 18 20:27:01.374: INFO: Got endpoints: latency-svc-tz8x5 [749.550953ms]
Dec 18 20:27:01.381: INFO: Created: latency-svc-c5sfs
Dec 18 20:27:01.425: INFO: Got endpoints: latency-svc-fcppv [749.720819ms]
Dec 18 20:27:01.431: INFO: Created: latency-svc-x6lqv
Dec 18 20:27:01.475: INFO: Got endpoints: latency-svc-pks5n [749.755136ms]
Dec 18 20:27:01.482: INFO: Created: latency-svc-szw9s
Dec 18 20:27:01.525: INFO: Got endpoints: latency-svc-xr84j [749.780841ms]
Dec 18 20:27:01.531: INFO: Created: latency-svc-zzkkt
Dec 18 20:27:01.575: INFO: Got endpoints: latency-svc-mpbbz [750.379677ms]
Dec 18 20:27:01.582: INFO: Created: latency-svc-j7kbq
Dec 18 20:27:01.624: INFO: Got endpoints: latency-svc-mlrjp [750.091516ms]
Dec 18 20:27:01.631: INFO: Created: latency-svc-tlnvj
Dec 18 20:27:01.675: INFO: Got endpoints: latency-svc-258hp [746.407874ms]
Dec 18 20:27:01.682: INFO: Created: latency-svc-dkd4x
Dec 18 20:27:01.725: INFO: Got endpoints: latency-svc-p6bm8 [750.224753ms]
Dec 18 20:27:01.731: INFO: Created: latency-svc-jtfbt
Dec 18 20:27:01.779: INFO: Got endpoints: latency-svc-8md5n [754.191037ms]
Dec 18 20:27:01.786: INFO: Created: latency-svc-kcgrz
Dec 18 20:27:01.824: INFO: Got endpoints: latency-svc-fswkk [749.925288ms]
Dec 18 20:27:01.831: INFO: Created: latency-svc-rxzwl
Dec 18 20:27:01.875: INFO: Got endpoints: latency-svc-bz6gm [749.841557ms]
Dec 18 20:27:01.887: INFO: Created: latency-svc-96wl2
Dec 18 20:27:01.925: INFO: Got endpoints: latency-svc-h64qn [750.127808ms]
Dec 18 20:27:01.932: INFO: Created: latency-svc-x6rbc
Dec 18 20:27:01.975: INFO: Got endpoints: latency-svc-5bnlq [750.035974ms]
Dec 18 20:27:01.981: INFO: Created: latency-svc-xprjb
Dec 18 20:27:02.024: INFO: Got endpoints: latency-svc-52dl7 [749.834455ms]
Dec 18 20:27:02.032: INFO: Created: latency-svc-8qgdj
Dec 18 20:27:02.075: INFO: Got endpoints: latency-svc-pp7th [750.012459ms]
Dec 18 20:27:02.081: INFO: Created: latency-svc-np6tm
Dec 18 20:27:02.125: INFO: Got endpoints: latency-svc-c5sfs [750.724678ms]
Dec 18 20:27:02.132: INFO: Created: latency-svc-8nfcd
Dec 18 20:27:02.175: INFO: Got endpoints: latency-svc-x6lqv [749.978928ms]
Dec 18 20:27:02.181: INFO: Created: latency-svc-zl9v8
Dec 18 20:27:02.224: INFO: Got endpoints: latency-svc-szw9s [749.438349ms]
Dec 18 20:27:02.231: INFO: Created: latency-svc-nmtk2
Dec 18 20:27:02.275: INFO: Got endpoints: latency-svc-zzkkt [750.151192ms]
Dec 18 20:27:02.282: INFO: Created: latency-svc-55kp6
Dec 18 20:27:02.325: INFO: Got endpoints: latency-svc-j7kbq [749.682875ms]
Dec 18 20:27:02.331: INFO: Created: latency-svc-dgg6x
Dec 18 20:27:02.374: INFO: Got endpoints: latency-svc-tlnvj [749.85702ms]
Dec 18 20:27:02.381: INFO: Created: latency-svc-mnk86
Dec 18 20:27:02.424: INFO: Got endpoints: latency-svc-dkd4x [749.665122ms]
Dec 18 20:27:02.431: INFO: Created: latency-svc-rprxc
Dec 18 20:27:02.474: INFO: Got endpoints: latency-svc-jtfbt [749.870191ms]
Dec 18 20:27:02.481: INFO: Created: latency-svc-9kgf8
Dec 18 20:27:02.526: INFO: Got endpoints: latency-svc-kcgrz [747.246738ms]
Dec 18 20:27:02.533: INFO: Created: latency-svc-qrcqr
Dec 18 20:27:02.574: INFO: Got endpoints: latency-svc-rxzwl [749.963756ms]
Dec 18 20:27:02.581: INFO: Created: latency-svc-pw5xp
Dec 18 20:27:02.625: INFO: Got endpoints: latency-svc-96wl2 [749.987893ms]
Dec 18 20:27:02.633: INFO: Created: latency-svc-gr8vl
Dec 18 20:27:02.674: INFO: Got endpoints: latency-svc-x6rbc [749.453009ms]
Dec 18 20:27:02.681: INFO: Created: latency-svc-5f8k2
Dec 18 20:27:02.725: INFO: Got endpoints: latency-svc-xprjb [750.075086ms]
Dec 18 20:27:02.732: INFO: Created: latency-svc-2cfcs
Dec 18 20:27:02.775: INFO: Got endpoints: latency-svc-8qgdj [750.097909ms]
Dec 18 20:27:02.781: INFO: Created: latency-svc-xsxjw
Dec 18 20:27:02.825: INFO: Got endpoints: latency-svc-np6tm [750.071387ms]
Dec 18 20:27:02.831: INFO: Created: latency-svc-7c2pt
Dec 18 20:27:02.875: INFO: Got endpoints: latency-svc-8nfcd [749.450581ms]
Dec 18 20:27:02.881: INFO: Created: latency-svc-mjcsn
Dec 18 20:27:02.925: INFO: Got endpoints: latency-svc-zl9v8 [750.103571ms]
Dec 18 20:27:02.931: INFO: Created: latency-svc-27qq9
Dec 18 20:27:02.975: INFO: Got endpoints: latency-svc-nmtk2 [750.315583ms]
Dec 18 20:27:02.983: INFO: Created: latency-svc-8sn7b
Dec 18 20:27:03.026: INFO: Got endpoints: latency-svc-55kp6 [750.690305ms]
Dec 18 20:27:03.032: INFO: Created: latency-svc-kngbr
Dec 18 20:27:03.075: INFO: Got endpoints: latency-svc-dgg6x [750.148607ms]
Dec 18 20:27:03.081: INFO: Created: latency-svc-hg2r6
Dec 18 20:27:03.125: INFO: Got endpoints: latency-svc-mnk86 [750.187431ms]
Dec 18 20:27:03.131: INFO: Created: latency-svc-2tpwr
Dec 18 20:27:03.175: INFO: Got endpoints: latency-svc-rprxc [750.387996ms]
Dec 18 20:27:03.181: INFO: Created: latency-svc-khb8x
Dec 18 20:27:03.224: INFO: Got endpoints: latency-svc-9kgf8 [749.973709ms]
Dec 18 20:27:03.231: INFO: Created: latency-svc-tjxqv
Dec 18 20:27:03.275: INFO: Got endpoints: latency-svc-qrcqr [748.73493ms]
Dec 18 20:27:03.281: INFO: Created: latency-svc-f565b
Dec 18 20:27:03.325: INFO: Got endpoints: latency-svc-pw5xp [750.589427ms]
Dec 18 20:27:03.331: INFO: Created: latency-svc-mlsdh
Dec 18 20:27:03.377: INFO: Got endpoints: latency-svc-gr8vl [751.804016ms]
Dec 18 20:27:03.383: INFO: Created: latency-svc-z5mgz
Dec 18 20:27:03.425: INFO: Got endpoints: latency-svc-5f8k2 [750.535909ms]
Dec 18 20:27:03.431: INFO: Created: latency-svc-7qlp8
Dec 18 20:27:03.475: INFO: Got endpoints: latency-svc-2cfcs [750.039653ms]
Dec 18 20:27:03.484: INFO: Created: latency-svc-st8j9
Dec 18 20:27:03.525: INFO: Got endpoints: latency-svc-xsxjw [750.363409ms]
Dec 18 20:27:03.531: INFO: Created: latency-svc-vvbpf
Dec 18 20:27:03.575: INFO: Got endpoints: latency-svc-7c2pt [750.174941ms]
Dec 18 20:27:03.582: INFO: Created: latency-svc-262f4
Dec 18 20:27:03.625: INFO: Got endpoints: latency-svc-mjcsn [750.508013ms]
Dec 18 20:27:03.632: INFO: Created: latency-svc-25qbh
Dec 18 20:27:03.674: INFO: Got endpoints: latency-svc-27qq9 [749.728304ms]
Dec 18 20:27:03.681: INFO: Created: latency-svc-pf9x8
Dec 18 20:27:03.725: INFO: Got endpoints: latency-svc-8sn7b [749.998103ms]
Dec 18 20:27:03.731: INFO: Created: latency-svc-ss596
Dec 18 20:27:03.775: INFO: Got endpoints: latency-svc-kngbr [749.103514ms]
Dec 18 20:27:03.781: INFO: Created: latency-svc-49llw
Dec 18 20:27:03.825: INFO: Got endpoints: latency-svc-hg2r6 [750.001317ms]
Dec 18 20:27:03.831: INFO: Created: latency-svc-pcfj2
Dec 18 20:27:03.875: INFO: Got endpoints: latency-svc-2tpwr [750.033386ms]
Dec 18 20:27:03.881: INFO: Created: latency-svc-bc292
Dec 18 20:27:03.925: INFO: Got endpoints: latency-svc-khb8x [749.714934ms]
Dec 18 20:27:03.931: INFO: Created: latency-svc-zjwz9
Dec 18 20:27:03.975: INFO: Got endpoints: latency-svc-tjxqv [750.267231ms]
Dec 18 20:27:03.982: INFO: Created: latency-svc-7zbv2
Dec 18 20:27:04.025: INFO: Got endpoints: latency-svc-f565b [749.824185ms]
Dec 18 20:27:04.031: INFO: Created: latency-svc-587d4
Dec 18 20:27:04.074: INFO: Got endpoints: latency-svc-mlsdh [749.288064ms]
Dec 18 20:27:04.081: INFO: Created: latency-svc-557pz
Dec 18 20:27:04.125: INFO: Got endpoints: latency-svc-z5mgz [748.045651ms]
Dec 18 20:27:04.131: INFO: Created: latency-svc-mcjlc
Dec 18 20:27:04.175: INFO: Got endpoints: latency-svc-7qlp8 [749.864354ms]
Dec 18 20:27:04.181: INFO: Created: latency-svc-mdc4c
Dec 18 20:27:04.228: INFO: Got endpoints: latency-svc-st8j9 [752.846099ms]
Dec 18 20:27:04.234: INFO: Created: latency-svc-q5rn5
Dec 18 20:27:04.275: INFO: Got endpoints: latency-svc-vvbpf [749.628707ms]
Dec 18 20:27:04.281: INFO: Created: latency-svc-fc8p6
Dec 18 20:27:04.325: INFO: Got endpoints: latency-svc-262f4 [749.67914ms]
Dec 18 20:27:04.336: INFO: Created: latency-svc-rk77j
Dec 18 20:27:04.375: INFO: Got endpoints: latency-svc-25qbh [749.439391ms]
Dec 18 20:27:04.381: INFO: Created: latency-svc-stvkg
Dec 18 20:27:04.425: INFO: Got endpoints: latency-svc-pf9x8 [750.136724ms]
Dec 18 20:27:04.434: INFO: Created: latency-svc-gpl22
Dec 18 20:27:04.475: INFO: Got endpoints: latency-svc-ss596 [749.96579ms]
Dec 18 20:27:04.481: INFO: Created: latency-svc-mqvmt
Dec 18 20:27:04.525: INFO: Got endpoints: latency-svc-49llw [750.435341ms]
Dec 18 20:27:04.532: INFO: Created: latency-svc-db5n6
Dec 18 20:27:04.575: INFO: Got endpoints: latency-svc-pcfj2 [750.279861ms]
Dec 18 20:27:04.582: INFO: Created: latency-svc-7vpgx
Dec 18 20:27:04.625: INFO: Got endpoints: latency-svc-bc292 [750.763281ms]
Dec 18 20:27:04.632: INFO: Created: latency-svc-rt8hq
Dec 18 20:27:04.675: INFO: Got endpoints: latency-svc-zjwz9 [750.378749ms]
Dec 18 20:27:04.682: INFO: Created: latency-svc-gzwrp
Dec 18 20:27:04.725: INFO: Got endpoints: latency-svc-7zbv2 [749.956919ms]
Dec 18 20:27:04.731: INFO: Created: latency-svc-f2vvr
Dec 18 20:27:04.774: INFO: Got endpoints: latency-svc-587d4 [749.676342ms]
Dec 18 20:27:04.781: INFO: Created: latency-svc-npflw
Dec 18 20:27:04.824: INFO: Got endpoints: latency-svc-557pz [749.982159ms]
Dec 18 20:27:04.831: INFO: Created: latency-svc-bfkrf
Dec 18 20:27:04.875: INFO: Got endpoints: latency-svc-mcjlc [750.164588ms]
Dec 18 20:27:04.881: INFO: Created: latency-svc-zjqzk
Dec 18 20:27:04.925: INFO: Got endpoints: latency-svc-mdc4c [750.223712ms]
Dec 18 20:27:04.931: INFO: Created: latency-svc-dxsx5
Dec 18 20:27:04.975: INFO: Got endpoints: latency-svc-q5rn5 [747.146707ms]
Dec 18 20:27:04.981: INFO: Created: latency-svc-2sl2p
Dec 18 20:27:05.025: INFO: Got endpoints: latency-svc-fc8p6 [750.115898ms]
Dec 18 20:27:05.031: INFO: Created: latency-svc-cjl95
Dec 18 20:27:05.079: INFO: Got endpoints: latency-svc-rk77j [754.037753ms]
Dec 18 20:27:05.085: INFO: Created: latency-svc-bdcpz
Dec 18 20:27:05.125: INFO: Got endpoints: latency-svc-stvkg [749.891211ms]
Dec 18 20:27:05.131: INFO: Created: latency-svc-rmtrc
Dec 18 20:27:05.175: INFO: Got endpoints: latency-svc-gpl22 [749.88788ms]
Dec 18 20:27:05.187: INFO: Created: latency-svc-fqjjn
Dec 18 20:27:05.224: INFO: Got endpoints: latency-svc-mqvmt [749.646687ms]
Dec 18 20:27:05.231: INFO: Created: latency-svc-h6xgj
Dec 18 20:27:05.276: INFO: Got endpoints: latency-svc-db5n6 [750.341851ms]
Dec 18 20:27:05.282: INFO: Created: latency-svc-c97dx
Dec 18 20:27:05.325: INFO: Got endpoints: latency-svc-7vpgx [749.996281ms]
Dec 18 20:27:05.331: INFO: Created: latency-svc-w57bf
Dec 18 20:27:05.375: INFO: Got endpoints: latency-svc-rt8hq [749.412066ms]
Dec 18 20:27:05.381: INFO: Created: latency-svc-ljvfn
Dec 18 20:27:05.425: INFO: Got endpoints: latency-svc-gzwrp [750.426396ms]
Dec 18 20:27:05.432: INFO: Created: latency-svc-l5dsd
Dec 18 20:27:05.475: INFO: Got endpoints: latency-svc-f2vvr [750.141608ms]
Dec 18 20:27:05.481: INFO: Created: latency-svc-zxdjf
Dec 18 20:27:05.525: INFO: Got endpoints: latency-svc-npflw [750.662335ms]
Dec 18 20:27:05.532: INFO: Created: latency-svc-pt4qj
Dec 18 20:27:05.575: INFO: Got endpoints: latency-svc-bfkrf [750.170671ms]
Dec 18 20:27:05.581: INFO: Created: latency-svc-62dl5
Dec 18 20:27:05.625: INFO: Got endpoints: latency-svc-zjqzk [750.122149ms]
Dec 18 20:27:05.631: INFO: Created: latency-svc-gtcrk
Dec 18 20:27:05.675: INFO: Got endpoints: latency-svc-dxsx5 [749.723635ms]
Dec 18 20:27:05.682: INFO: Created: latency-svc-5d9xh
Dec 18 20:27:05.725: INFO: Got endpoints: latency-svc-2sl2p [750.365098ms]
Dec 18 20:27:05.732: INFO: Created: latency-svc-k8hpf
Dec 18 20:27:05.775: INFO: Got endpoints: latency-svc-cjl95 [750.1121ms]
Dec 18 20:27:05.781: INFO: Created: latency-svc-kwhhr
Dec 18 20:27:05.825: INFO: Got endpoints: latency-svc-bdcpz [746.854941ms]
Dec 18 20:27:05.833: INFO: Created: latency-svc-jzx4c
Dec 18 20:27:05.875: INFO: Got endpoints: latency-svc-rmtrc [750.105978ms]
Dec 18 20:27:05.883: INFO: Created: latency-svc-fdhf9
Dec 18 20:27:05.925: INFO: Got endpoints: latency-svc-fqjjn [750.379112ms]
Dec 18 20:27:05.937: INFO: Created: latency-svc-g554p
Dec 18 20:27:05.974: INFO: Got endpoints: latency-svc-h6xgj [750.079513ms]
Dec 18 20:27:05.981: INFO: Created: latency-svc-wnpk5
Dec 18 20:27:06.025: INFO: Got endpoints: latency-svc-c97dx [749.073312ms]
Dec 18 20:27:06.031: INFO: Created: latency-svc-lrd9x
Dec 18 20:27:06.075: INFO: Got endpoints: latency-svc-w57bf [749.779187ms]
Dec 18 20:27:06.082: INFO: Created: latency-svc-nvxfj
Dec 18 20:27:06.125: INFO: Got endpoints: latency-svc-ljvfn [749.887083ms]
Dec 18 20:27:06.131: INFO: Created: latency-svc-2rj9g
Dec 18 20:27:06.175: INFO: Got endpoints: latency-svc-l5dsd [749.821246ms]
Dec 18 20:27:06.182: INFO: Created: latency-svc-2nmth
Dec 18 20:27:06.225: INFO: Got endpoints: latency-svc-zxdjf [749.995834ms]
Dec 18 20:27:06.231: INFO: Created: latency-svc-lz9t8
Dec 18 20:27:06.275: INFO: Got endpoints: latency-svc-pt4qj [750.225183ms]
Dec 18 20:27:06.282: INFO: Created: latency-svc-ctfhh
Dec 18 20:27:06.325: INFO: Got endpoints: latency-svc-62dl5 [750.521623ms]
Dec 18 20:27:06.332: INFO: Created: latency-svc-hcj7c
Dec 18 20:27:06.375: INFO: Got endpoints: latency-svc-gtcrk [750.227844ms]
Dec 18 20:27:06.382: INFO: Created: latency-svc-jkbdq
Dec 18 20:27:06.425: INFO: Got endpoints: latency-svc-5d9xh [750.2377ms]
Dec 18 20:27:06.432: INFO: Created: latency-svc-5d28j
Dec 18 20:27:06.475: INFO: Got endpoints: latency-svc-k8hpf [749.504428ms]
Dec 18 20:27:06.481: INFO: Created: latency-svc-nk5nh
Dec 18 20:27:06.525: INFO: Got endpoints: latency-svc-kwhhr [750.052255ms]
Dec 18 20:27:06.532: INFO: Created: latency-svc-v85nx
Dec 18 20:27:06.575: INFO: Got endpoints: latency-svc-jzx4c [749.421974ms]
Dec 18 20:27:06.583: INFO: Created: latency-svc-rt5gg
Dec 18 20:27:06.624: INFO: Got endpoints: latency-svc-fdhf9 [749.705061ms]
Dec 18 20:27:06.631: INFO: Created: latency-svc-9b2tr
Dec 18 20:27:06.678: INFO: Got endpoints: latency-svc-g554p [752.916139ms]
Dec 18 20:27:06.684: INFO: Created: latency-svc-6vb5b
Dec 18 20:27:06.725: INFO: Got endpoints: latency-svc-wnpk5 [750.098132ms]
Dec 18 20:27:06.731: INFO: Created: latency-svc-flx8n
Dec 18 20:27:06.775: INFO: Got endpoints: latency-svc-lrd9x [750.001514ms]
Dec 18 20:27:06.786: INFO: Created: latency-svc-rgwlq
Dec 18 20:27:06.824: INFO: Got endpoints: latency-svc-nvxfj [749.517659ms]
Dec 18 20:27:06.831: INFO: Created: latency-svc-wpkkq
Dec 18 20:27:06.874: INFO: Got endpoints: latency-svc-2rj9g [749.546508ms]
Dec 18 20:27:06.883: INFO: Created: latency-svc-sxtmq
Dec 18 20:27:06.925: INFO: Got endpoints: latency-svc-2nmth [749.342566ms]
Dec 18 20:27:06.931: INFO: Created: latency-svc-hrlk6
Dec 18 20:27:06.975: INFO: Got endpoints: latency-svc-lz9t8 [749.540532ms]
Dec 18 20:27:06.981: INFO: Created: latency-svc-h8qpp
Dec 18 20:27:07.025: INFO: Got endpoints: latency-svc-ctfhh [749.799559ms]
Dec 18 20:27:07.031: INFO: Created: latency-svc-jzhql
Dec 18 20:27:07.075: INFO: Got endpoints: latency-svc-hcj7c [750.101536ms]
Dec 18 20:27:07.082: INFO: Created: latency-svc-shmlr
Dec 18 20:27:07.126: INFO: Got endpoints: latency-svc-jkbdq [750.326908ms]
Dec 18 20:27:07.133: INFO: Created: latency-svc-4p5kx
Dec 18 20:27:07.176: INFO: Got endpoints: latency-svc-5d28j [750.454346ms]
Dec 18 20:27:07.182: INFO: Created: latency-svc-pfpvc
Dec 18 20:27:07.224: INFO: Got endpoints: latency-svc-nk5nh [749.70401ms]
Dec 18 20:27:07.231: INFO: Created: latency-svc-rhnz5
Dec 18 20:27:07.275: INFO: Got endpoints: latency-svc-v85nx [749.884492ms]
Dec 18 20:27:07.325: INFO: Got endpoints: latency-svc-rt5gg [749.778527ms]
Dec 18 20:27:07.375: INFO: Got endpoints: latency-svc-9b2tr [750.371489ms]
Dec 18 20:27:07.424: INFO: Got endpoints: latency-svc-6vb5b [746.552078ms]
Dec 18 20:27:07.475: INFO: Got endpoints: latency-svc-flx8n [749.832273ms]
Dec 18 20:27:07.525: INFO: Got endpoints: latency-svc-rgwlq [750.037838ms]
Dec 18 20:27:07.574: INFO: Got endpoints: latency-svc-wpkkq [750.035727ms]
Dec 18 20:27:07.625: INFO: Got endpoints: latency-svc-sxtmq [750.476039ms]
Dec 18 20:27:07.675: INFO: Got endpoints: latency-svc-hrlk6 [750.090014ms]
Dec 18 20:27:07.725: INFO: Got endpoints: latency-svc-h8qpp [750.225208ms]
Dec 18 20:27:07.775: INFO: Got endpoints: latency-svc-jzhql [749.828159ms]
Dec 18 20:27:07.825: INFO: Got endpoints: latency-svc-shmlr [749.883546ms]
Dec 18 20:27:07.875: INFO: Got endpoints: latency-svc-4p5kx [749.037006ms]
Dec 18 20:27:07.925: INFO: Got endpoints: latency-svc-pfpvc [749.592782ms]
Dec 18 20:27:07.975: INFO: Got endpoints: latency-svc-rhnz5 [750.891853ms]
Dec 18 20:27:07.975: INFO: Latencies: [7.057819ms 9.265625ms 10.937729ms 18.731451ms 20.194457ms 25.097839ms 30.94623ms 35.999801ms 41.322458ms 43.122623ms 50.170673ms 55.049855ms 75.973151ms 76.707431ms 76.972127ms 79.151957ms 79.874949ms 80.487501ms 80.639096ms 84.594871ms 90.959032ms 92.380163ms 92.452992ms 92.94516ms 93.164758ms 94.029118ms 94.372191ms 94.977633ms 95.647536ms 95.993555ms 96.089675ms 96.672489ms 106.598683ms 107.44227ms 184.887104ms 228.069135ms 272.876573ms 317.378181ms 361.81897ms 407.328949ms 451.787916ms 499.233654ms 540.687451ms 584.321742ms 630.174957ms 674.087944ms 736.812412ms 746.407874ms 746.552078ms 746.642169ms 746.854941ms 747.146707ms 747.246738ms 748.045651ms 748.73493ms 748.742148ms 748.897693ms 748.976881ms 749.037006ms 749.073312ms 749.103514ms 749.288064ms 749.342566ms 749.412066ms 749.421974ms 749.438349ms 749.439391ms 749.450581ms 749.453009ms 749.504428ms 749.517659ms 749.540532ms 749.546508ms 749.550953ms 749.592782ms 749.603972ms 749.628707ms 749.646687ms 749.663117ms 749.665122ms 749.676342ms 749.67914ms 749.682875ms 749.70401ms 749.705061ms 749.714934ms 749.718822ms 749.720819ms 749.723635ms 749.728304ms 749.755136ms 749.778527ms 749.779187ms 749.780841ms 749.799559ms 749.814064ms 749.821246ms 749.824185ms 749.828159ms 749.832273ms 749.834455ms 749.841557ms 749.85702ms 749.864354ms 749.870191ms 749.88017ms 749.883546ms 749.884492ms 749.887083ms 749.88788ms 749.891211ms 749.892394ms 749.899123ms 749.910055ms 749.910089ms 749.925288ms 749.956919ms 749.963756ms 749.96579ms 749.973709ms 749.978928ms 749.982159ms 749.987893ms 749.995834ms 749.996281ms 749.998103ms 750.001317ms 750.001514ms 750.012459ms 750.033386ms 750.035727ms 750.035974ms 750.037838ms 750.039653ms 750.048901ms 750.052255ms 750.071387ms 750.075086ms 750.079513ms 750.090014ms 750.091516ms 750.097909ms 750.098132ms 750.101536ms 750.103571ms 750.105978ms 750.1121ms 750.115898ms 750.122149ms 750.127808ms 750.136724ms 750.141608ms 750.148607ms 750.151192ms 750.164588ms 750.170671ms 750.174941ms 750.187431ms 750.199829ms 750.223712ms 750.224753ms 750.225183ms 750.225208ms 750.227844ms 750.228664ms 750.2377ms 750.267231ms 750.279861ms 750.315583ms 750.326908ms 750.341851ms 750.363409ms 750.365098ms 750.371489ms 750.374068ms 750.378749ms 750.379112ms 750.379677ms 750.387996ms 750.426396ms 750.435341ms 750.454346ms 750.461396ms 750.476039ms 750.508013ms 750.521623ms 750.535909ms 750.589427ms 750.662335ms 750.690305ms 750.724678ms 750.763281ms 750.891853ms 751.804016ms 752.846099ms 752.916139ms 753.931691ms 754.037753ms 754.191037ms 788.672197ms]
Dec 18 20:27:07.975: INFO: 50 %ile: 749.834455ms
Dec 18 20:27:07.975: INFO: 90 %ile: 750.435341ms
Dec 18 20:27:07.976: INFO: 99 %ile: 754.191037ms
Dec 18 20:27:07.976: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:27:07.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4970" for this suite.
Dec 18 20:27:25.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:27:26.012: INFO: namespace svc-latency-4970 deletion completed in 18.033627988s

• [SLOW TEST:33.761 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:27:26.012: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Dec 18 20:27:26.027: INFO: Waiting up to 5m0s for pod "var-expansion-10460b88-1380-4d32-8a6c-474447706d49" in namespace "var-expansion-1496" to be "success or failure"
Dec 18 20:27:26.028: INFO: Pod "var-expansion-10460b88-1380-4d32-8a6c-474447706d49": Phase="Pending", Reason="", readiness=false. Elapsed: 841.74µs
Dec 18 20:27:28.030: INFO: Pod "var-expansion-10460b88-1380-4d32-8a6c-474447706d49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002993306s
Dec 18 20:27:30.032: INFO: Pod "var-expansion-10460b88-1380-4d32-8a6c-474447706d49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005047678s
Dec 18 20:27:32.034: INFO: Pod "var-expansion-10460b88-1380-4d32-8a6c-474447706d49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006636613s
Dec 18 20:27:34.036: INFO: Pod "var-expansion-10460b88-1380-4d32-8a6c-474447706d49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008697638s
STEP: Saw pod success
Dec 18 20:27:34.036: INFO: Pod "var-expansion-10460b88-1380-4d32-8a6c-474447706d49" satisfied condition "success or failure"
Dec 18 20:27:34.037: INFO: Trying to get logs from node controller-0 pod var-expansion-10460b88-1380-4d32-8a6c-474447706d49 container dapi-container: <nil>
STEP: delete the pod
Dec 18 20:27:34.053: INFO: Waiting for pod var-expansion-10460b88-1380-4d32-8a6c-474447706d49 to disappear
Dec 18 20:27:34.053: INFO: Pod var-expansion-10460b88-1380-4d32-8a6c-474447706d49 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:27:34.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1496" for this suite.
Dec 18 20:27:40.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:27:40.089: INFO: namespace var-expansion-1496 deletion completed in 6.034088161s

• [SLOW TEST:14.077 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:27:40.089: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-c905bbb2-cc0c-4733-a172-c86f907aef3a
STEP: Creating a pod to test consume secrets
Dec 18 20:27:40.106: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9d0e11b3-ec57-434c-b9d3-64e668c91466" in namespace "projected-327" to be "success or failure"
Dec 18 20:27:40.107: INFO: Pod "pod-projected-secrets-9d0e11b3-ec57-434c-b9d3-64e668c91466": Phase="Pending", Reason="", readiness=false. Elapsed: 909.345µs
Dec 18 20:27:42.109: INFO: Pod "pod-projected-secrets-9d0e11b3-ec57-434c-b9d3-64e668c91466": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00242889s
Dec 18 20:27:44.110: INFO: Pod "pod-projected-secrets-9d0e11b3-ec57-434c-b9d3-64e668c91466": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004189135s
Dec 18 20:27:46.112: INFO: Pod "pod-projected-secrets-9d0e11b3-ec57-434c-b9d3-64e668c91466": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006066036s
Dec 18 20:27:48.114: INFO: Pod "pod-projected-secrets-9d0e11b3-ec57-434c-b9d3-64e668c91466": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007823596s
STEP: Saw pod success
Dec 18 20:27:48.114: INFO: Pod "pod-projected-secrets-9d0e11b3-ec57-434c-b9d3-64e668c91466" satisfied condition "success or failure"
Dec 18 20:27:48.115: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-9d0e11b3-ec57-434c-b9d3-64e668c91466 container secret-volume-test: <nil>
STEP: delete the pod
Dec 18 20:27:48.131: INFO: Waiting for pod pod-projected-secrets-9d0e11b3-ec57-434c-b9d3-64e668c91466 to disappear
Dec 18 20:27:48.132: INFO: Pod pod-projected-secrets-9d0e11b3-ec57-434c-b9d3-64e668c91466 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:27:48.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-327" for this suite.
Dec 18 20:27:54.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:27:54.169: INFO: namespace projected-327 deletion completed in 6.035049382s

• [SLOW TEST:14.079 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:27:54.169: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-43853b1e-a2ac-4413-b3c2-c289b19d154a
STEP: Creating a pod to test consume configMaps
Dec 18 20:27:54.186: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-536ebe47-33de-46d4-9d56-33598f79ef15" in namespace "projected-6382" to be "success or failure"
Dec 18 20:27:54.187: INFO: Pod "pod-projected-configmaps-536ebe47-33de-46d4-9d56-33598f79ef15": Phase="Pending", Reason="", readiness=false. Elapsed: 932.303µs
Dec 18 20:27:56.189: INFO: Pod "pod-projected-configmaps-536ebe47-33de-46d4-9d56-33598f79ef15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00286194s
Dec 18 20:27:58.191: INFO: Pod "pod-projected-configmaps-536ebe47-33de-46d4-9d56-33598f79ef15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004892245s
Dec 18 20:28:00.193: INFO: Pod "pod-projected-configmaps-536ebe47-33de-46d4-9d56-33598f79ef15": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006730412s
Dec 18 20:28:02.195: INFO: Pod "pod-projected-configmaps-536ebe47-33de-46d4-9d56-33598f79ef15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008942497s
STEP: Saw pod success
Dec 18 20:28:02.195: INFO: Pod "pod-projected-configmaps-536ebe47-33de-46d4-9d56-33598f79ef15" satisfied condition "success or failure"
Dec 18 20:28:02.196: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-536ebe47-33de-46d4-9d56-33598f79ef15 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 20:28:02.204: INFO: Waiting for pod pod-projected-configmaps-536ebe47-33de-46d4-9d56-33598f79ef15 to disappear
Dec 18 20:28:02.205: INFO: Pod pod-projected-configmaps-536ebe47-33de-46d4-9d56-33598f79ef15 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:28:02.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6382" for this suite.
Dec 18 20:28:08.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:28:08.241: INFO: namespace projected-6382 deletion completed in 6.034949867s

• [SLOW TEST:14.072 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:28:08.241: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:28:08.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5831" for this suite.
Dec 18 20:28:14.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:28:14.292: INFO: namespace tables-5831 deletion completed in 6.035144719s

• [SLOW TEST:6.051 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:28:14.293: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 20:28:14.310: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a42dabc1-2b86-45b7-9bbb-26062786e65c" in namespace "projected-1352" to be "success or failure"
Dec 18 20:28:14.311: INFO: Pod "downwardapi-volume-a42dabc1-2b86-45b7-9bbb-26062786e65c": Phase="Pending", Reason="", readiness=false. Elapsed: 947.81µs
Dec 18 20:28:16.313: INFO: Pod "downwardapi-volume-a42dabc1-2b86-45b7-9bbb-26062786e65c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002989178s
Dec 18 20:28:18.315: INFO: Pod "downwardapi-volume-a42dabc1-2b86-45b7-9bbb-26062786e65c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004618236s
Dec 18 20:28:20.316: INFO: Pod "downwardapi-volume-a42dabc1-2b86-45b7-9bbb-26062786e65c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006517027s
Dec 18 20:28:22.318: INFO: Pod "downwardapi-volume-a42dabc1-2b86-45b7-9bbb-26062786e65c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008108555s
STEP: Saw pod success
Dec 18 20:28:22.318: INFO: Pod "downwardapi-volume-a42dabc1-2b86-45b7-9bbb-26062786e65c" satisfied condition "success or failure"
Dec 18 20:28:22.319: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-a42dabc1-2b86-45b7-9bbb-26062786e65c container client-container: <nil>
STEP: delete the pod
Dec 18 20:28:22.327: INFO: Waiting for pod downwardapi-volume-a42dabc1-2b86-45b7-9bbb-26062786e65c to disappear
Dec 18 20:28:22.328: INFO: Pod downwardapi-volume-a42dabc1-2b86-45b7-9bbb-26062786e65c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:28:22.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1352" for this suite.
Dec 18 20:28:28.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:28:28.363: INFO: namespace projected-1352 deletion completed in 6.033763426s

• [SLOW TEST:14.071 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:28:28.363: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8108
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8108
I1218 20:28:28.388647      27 runners.go:184] Created replication controller with name: externalname-service, namespace: services-8108, replica count: 2
I1218 20:28:31.439021      27 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 20:28:34.439252      27 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 18 20:28:37.439: INFO: Creating new exec pod
I1218 20:28:37.439471      27 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 18 20:28:46.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=services-8108 execpodvr5jm -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 18 20:28:46.702: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 18 20:28:46.702: INFO: stdout: ""
Dec 18 20:28:46.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=services-8108 execpodvr5jm -- /bin/sh -x -c nc -zv -t -w 2 10.109.36.82 80'
Dec 18 20:28:46.898: INFO: stderr: "+ nc -zv -t -w 2 10.109.36.82 80\nConnection to 10.109.36.82 80 port [tcp/http] succeeded!\n"
Dec 18 20:28:46.898: INFO: stdout: ""
Dec 18 20:28:46.898: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:28:46.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8108" for this suite.
Dec 18 20:28:52.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:28:52.942: INFO: namespace services-8108 deletion completed in 6.034319327s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:24.579 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:28:52.943: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 18 20:28:52.956: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:28:53.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2707" for this suite.
Dec 18 20:28:59.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:29:00.005: INFO: namespace custom-resource-definition-2707 deletion completed in 6.03824394s

• [SLOW TEST:7.063 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:29:00.005: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Dec 18 20:29:00.018: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Dec 18 20:29:00.509: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 18 20:29:02.525: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297740, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297740, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297740, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297740, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:29:04.527: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297740, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297740, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297740, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297740, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:29:06.527: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297740, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297740, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297740, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712297740, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 18 20:29:09.237: INFO: Waited 706.934317ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:29:09.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5215" for this suite.
Dec 18 20:29:15.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:29:15.970: INFO: namespace aggregator-5215 deletion completed in 6.131166047s

• [SLOW TEST:15.965 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:29:15.970: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-5b4b5e69-f246-4a18-aaee-95578b3ef0f3
STEP: Creating configMap with name cm-test-opt-upd-ca6e6b40-a07c-404e-898f-627f72a91271
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-5b4b5e69-f246-4a18-aaee-95578b3ef0f3
STEP: Updating configmap cm-test-opt-upd-ca6e6b40-a07c-404e-898f-627f72a91271
STEP: Creating configMap with name cm-test-opt-create-fa71f665-dc15-49e2-aa62-4257bfb57c0b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:29:28.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5175" for this suite.
Dec 18 20:29:40.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:29:40.068: INFO: namespace projected-5175 deletion completed in 12.034452203s

• [SLOW TEST:24.097 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:29:40.068: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 18 20:29:40.086: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7125ffaa-2548-462e-84ba-23ecae93e611" in namespace "projected-1258" to be "success or failure"
Dec 18 20:29:40.087: INFO: Pod "downwardapi-volume-7125ffaa-2548-462e-84ba-23ecae93e611": Phase="Pending", Reason="", readiness=false. Elapsed: 970.166µs
Dec 18 20:29:42.089: INFO: Pod "downwardapi-volume-7125ffaa-2548-462e-84ba-23ecae93e611": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003115036s
Dec 18 20:29:44.092: INFO: Pod "downwardapi-volume-7125ffaa-2548-462e-84ba-23ecae93e611": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00555067s
Dec 18 20:29:46.094: INFO: Pod "downwardapi-volume-7125ffaa-2548-462e-84ba-23ecae93e611": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007576185s
Dec 18 20:29:48.096: INFO: Pod "downwardapi-volume-7125ffaa-2548-462e-84ba-23ecae93e611": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009835092s
STEP: Saw pod success
Dec 18 20:29:48.096: INFO: Pod "downwardapi-volume-7125ffaa-2548-462e-84ba-23ecae93e611" satisfied condition "success or failure"
Dec 18 20:29:48.097: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-7125ffaa-2548-462e-84ba-23ecae93e611 container client-container: <nil>
STEP: delete the pod
Dec 18 20:29:48.105: INFO: Waiting for pod downwardapi-volume-7125ffaa-2548-462e-84ba-23ecae93e611 to disappear
Dec 18 20:29:48.106: INFO: Pod downwardapi-volume-7125ffaa-2548-462e-84ba-23ecae93e611 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:29:48.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1258" for this suite.
Dec 18 20:29:54.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:29:54.142: INFO: namespace projected-1258 deletion completed in 6.033798712s

• [SLOW TEST:14.075 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:29:54.143: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-bbrk
STEP: Creating a pod to test atomic-volume-subpath
Dec 18 20:29:54.161: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-bbrk" in namespace "subpath-8329" to be "success or failure"
Dec 18 20:29:54.162: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Pending", Reason="", readiness=false. Elapsed: 892.755µs
Dec 18 20:29:56.164: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002769926s
Dec 18 20:29:58.165: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004345974s
Dec 18 20:30:00.167: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006249159s
Dec 18 20:30:02.169: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Running", Reason="", readiness=true. Elapsed: 8.007895218s
Dec 18 20:30:04.171: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Running", Reason="", readiness=true. Elapsed: 10.010379431s
Dec 18 20:30:06.173: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Running", Reason="", readiness=true. Elapsed: 12.012530706s
Dec 18 20:30:08.175: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Running", Reason="", readiness=true. Elapsed: 14.014589586s
Dec 18 20:30:10.178: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Running", Reason="", readiness=true. Elapsed: 16.016741266s
Dec 18 20:30:12.179: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Running", Reason="", readiness=true. Elapsed: 18.018452609s
Dec 18 20:30:14.182: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Running", Reason="", readiness=true. Elapsed: 20.020856918s
Dec 18 20:30:16.184: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Running", Reason="", readiness=true. Elapsed: 22.023059527s
Dec 18 20:30:18.186: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Running", Reason="", readiness=true. Elapsed: 24.024768151s
Dec 18 20:30:20.188: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Running", Reason="", readiness=true. Elapsed: 26.026982455s
Dec 18 20:30:22.190: INFO: Pod "pod-subpath-test-secret-bbrk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.028698209s
STEP: Saw pod success
Dec 18 20:30:22.190: INFO: Pod "pod-subpath-test-secret-bbrk" satisfied condition "success or failure"
Dec 18 20:30:22.191: INFO: Trying to get logs from node controller-0 pod pod-subpath-test-secret-bbrk container test-container-subpath-secret-bbrk: <nil>
STEP: delete the pod
Dec 18 20:30:22.205: INFO: Waiting for pod pod-subpath-test-secret-bbrk to disappear
Dec 18 20:30:22.206: INFO: Pod pod-subpath-test-secret-bbrk no longer exists
STEP: Deleting pod pod-subpath-test-secret-bbrk
Dec 18 20:30:22.206: INFO: Deleting pod "pod-subpath-test-secret-bbrk" in namespace "subpath-8329"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:30:22.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8329" for this suite.
Dec 18 20:30:28.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:30:28.243: INFO: namespace subpath-8329 deletion completed in 6.034555974s

• [SLOW TEST:34.100 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:30:28.243: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 18 20:30:36.302: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:30:36.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6100" for this suite.
Dec 18 20:30:42.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:30:42.343: INFO: namespace container-runtime-6100 deletion completed in 6.03429509s

• [SLOW TEST:14.100 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:30:42.343: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:31:42.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7310" for this suite.
Dec 18 20:31:54.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:31:54.398: INFO: namespace container-probe-7310 deletion completed in 12.034807755s

• [SLOW TEST:72.055 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:31:54.398: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 18 20:31:54.413: INFO: Waiting up to 5m0s for pod "pod-a09126b1-00f9-4195-a9c4-e418fcbacecb" in namespace "emptydir-9842" to be "success or failure"
Dec 18 20:31:54.414: INFO: Pod "pod-a09126b1-00f9-4195-a9c4-e418fcbacecb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.065948ms
Dec 18 20:31:56.416: INFO: Pod "pod-a09126b1-00f9-4195-a9c4-e418fcbacecb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002779132s
Dec 18 20:31:58.418: INFO: Pod "pod-a09126b1-00f9-4195-a9c4-e418fcbacecb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004211335s
Dec 18 20:32:00.420: INFO: Pod "pod-a09126b1-00f9-4195-a9c4-e418fcbacecb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006078744s
Dec 18 20:32:02.422: INFO: Pod "pod-a09126b1-00f9-4195-a9c4-e418fcbacecb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008267495s
STEP: Saw pod success
Dec 18 20:32:02.422: INFO: Pod "pod-a09126b1-00f9-4195-a9c4-e418fcbacecb" satisfied condition "success or failure"
Dec 18 20:32:02.423: INFO: Trying to get logs from node controller-1 pod pod-a09126b1-00f9-4195-a9c4-e418fcbacecb container test-container: <nil>
STEP: delete the pod
Dec 18 20:32:02.437: INFO: Waiting for pod pod-a09126b1-00f9-4195-a9c4-e418fcbacecb to disappear
Dec 18 20:32:02.438: INFO: Pod pod-a09126b1-00f9-4195-a9c4-e418fcbacecb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:32:02.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9842" for this suite.
Dec 18 20:32:08.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:32:08.474: INFO: namespace emptydir-9842 deletion completed in 6.034161786s

• [SLOW TEST:14.076 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:32:08.474: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 18 20:32:08.728: INFO: Pod name wrapped-volume-race-bcd9e8ef-a521-4fa1-926e-b6f3be011d22: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-bcd9e8ef-a521-4fa1-926e-b6f3be011d22 in namespace emptydir-wrapper-642, will wait for the garbage collector to delete the pods
Dec 18 20:32:30.837: INFO: Deleting ReplicationController wrapped-volume-race-bcd9e8ef-a521-4fa1-926e-b6f3be011d22 took: 2.256942ms
Dec 18 20:32:31.237: INFO: Terminating ReplicationController wrapped-volume-race-bcd9e8ef-a521-4fa1-926e-b6f3be011d22 pods took: 400.208193ms
STEP: Creating RC which spawns configmap-volume pods
Dec 18 20:33:07.844: INFO: Pod name wrapped-volume-race-f31eeacb-56d0-4c56-9c09-d61b9e571c9e: Found 0 pods out of 5
Dec 18 20:33:12.848: INFO: Pod name wrapped-volume-race-f31eeacb-56d0-4c56-9c09-d61b9e571c9e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f31eeacb-56d0-4c56-9c09-d61b9e571c9e in namespace emptydir-wrapper-642, will wait for the garbage collector to delete the pods
Dec 18 20:33:30.910: INFO: Deleting ReplicationController wrapped-volume-race-f31eeacb-56d0-4c56-9c09-d61b9e571c9e took: 2.373668ms
Dec 18 20:33:31.310: INFO: Terminating ReplicationController wrapped-volume-race-f31eeacb-56d0-4c56-9c09-d61b9e571c9e pods took: 400.23219ms
STEP: Creating RC which spawns configmap-volume pods
Dec 18 20:34:13.916: INFO: Pod name wrapped-volume-race-550434ff-a07e-4feb-8e60-ac8827f80b18: Found 0 pods out of 5
Dec 18 20:34:18.920: INFO: Pod name wrapped-volume-race-550434ff-a07e-4feb-8e60-ac8827f80b18: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-550434ff-a07e-4feb-8e60-ac8827f80b18 in namespace emptydir-wrapper-642, will wait for the garbage collector to delete the pods
Dec 18 20:34:36.981: INFO: Deleting ReplicationController wrapped-volume-race-550434ff-a07e-4feb-8e60-ac8827f80b18 took: 2.328285ms
Dec 18 20:34:37.381: INFO: Terminating ReplicationController wrapped-volume-race-550434ff-a07e-4feb-8e60-ac8827f80b18 pods took: 400.201046ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:35:24.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-642" for this suite.
Dec 18 20:35:30.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:35:30.466: INFO: namespace emptydir-wrapper-642 deletion completed in 6.101681812s

• [SLOW TEST:201.992 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:35:30.466: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7095
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7095
STEP: creating replication controller externalsvc in namespace services-7095
I1218 20:35:30.492978      27 runners.go:184] Created replication controller with name: externalsvc, namespace: services-7095, replica count: 2
I1218 20:35:33.543384      27 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 20:35:36.543594      27 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1218 20:35:39.543894      27 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 18 20:35:39.553: INFO: Creating new exec pod
Dec 18 20:35:47.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-207688788 exec --namespace=services-7095 execpodk85rf -- /bin/sh -x -c nslookup nodeport-service'
Dec 18 20:35:47.756: INFO: stderr: "+ nslookup nodeport-service\n"
Dec 18 20:35:47.756: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-7095.svc.cluster.local\tcanonical name = externalsvc.services-7095.svc.cluster.local.\nName:\texternalsvc.services-7095.svc.cluster.local\nAddress: 10.107.62.2\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7095, will wait for the garbage collector to delete the pods
Dec 18 20:35:47.810: INFO: Deleting ReplicationController externalsvc took: 2.274576ms
Dec 18 20:35:48.210: INFO: Terminating ReplicationController externalsvc pods took: 400.249623ms
Dec 18 20:36:03.317: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:36:03.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7095" for this suite.
Dec 18 20:36:09.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:36:09.358: INFO: namespace services-7095 deletion completed in 6.033955299s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:38.893 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:36:09.358: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Dec 18 20:36:09.374: INFO: Waiting up to 5m0s for pod "var-expansion-0858c2eb-1afa-4f22-8f25-4d2dfcbe7246" in namespace "var-expansion-7553" to be "success or failure"
Dec 18 20:36:09.375: INFO: Pod "var-expansion-0858c2eb-1afa-4f22-8f25-4d2dfcbe7246": Phase="Pending", Reason="", readiness=false. Elapsed: 1.559252ms
Dec 18 20:36:11.377: INFO: Pod "var-expansion-0858c2eb-1afa-4f22-8f25-4d2dfcbe7246": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003402564s
Dec 18 20:36:13.379: INFO: Pod "var-expansion-0858c2eb-1afa-4f22-8f25-4d2dfcbe7246": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005016887s
Dec 18 20:36:15.381: INFO: Pod "var-expansion-0858c2eb-1afa-4f22-8f25-4d2dfcbe7246": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006935101s
Dec 18 20:36:17.382: INFO: Pod "var-expansion-0858c2eb-1afa-4f22-8f25-4d2dfcbe7246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008917684s
STEP: Saw pod success
Dec 18 20:36:17.383: INFO: Pod "var-expansion-0858c2eb-1afa-4f22-8f25-4d2dfcbe7246" satisfied condition "success or failure"
Dec 18 20:36:17.384: INFO: Trying to get logs from node controller-0 pod var-expansion-0858c2eb-1afa-4f22-8f25-4d2dfcbe7246 container dapi-container: <nil>
STEP: delete the pod
Dec 18 20:36:17.398: INFO: Waiting for pod var-expansion-0858c2eb-1afa-4f22-8f25-4d2dfcbe7246 to disappear
Dec 18 20:36:17.398: INFO: Pod var-expansion-0858c2eb-1afa-4f22-8f25-4d2dfcbe7246 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:36:17.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7553" for this suite.
Dec 18 20:36:23.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:36:23.436: INFO: namespace var-expansion-7553 deletion completed in 6.035663025s

• [SLOW TEST:14.077 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 18 20:36:23.436: INFO: >>> kubeConfig: /tmp/kubeconfig-207688788
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-c87fbc89-3cd4-4f26-9f69-b4f87bb3ba16
STEP: Creating a pod to test consume configMaps
Dec 18 20:36:23.454: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-89da74d9-c29b-45ca-96a9-56e4daf575ff" in namespace "projected-2272" to be "success or failure"
Dec 18 20:36:23.454: INFO: Pod "pod-projected-configmaps-89da74d9-c29b-45ca-96a9-56e4daf575ff": Phase="Pending", Reason="", readiness=false. Elapsed: 928.303µs
Dec 18 20:36:25.456: INFO: Pod "pod-projected-configmaps-89da74d9-c29b-45ca-96a9-56e4daf575ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002417389s
Dec 18 20:36:27.458: INFO: Pod "pod-projected-configmaps-89da74d9-c29b-45ca-96a9-56e4daf575ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004100362s
Dec 18 20:36:29.459: INFO: Pod "pod-projected-configmaps-89da74d9-c29b-45ca-96a9-56e4daf575ff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005764373s
Dec 18 20:36:31.461: INFO: Pod "pod-projected-configmaps-89da74d9-c29b-45ca-96a9-56e4daf575ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007535978s
STEP: Saw pod success
Dec 18 20:36:31.461: INFO: Pod "pod-projected-configmaps-89da74d9-c29b-45ca-96a9-56e4daf575ff" satisfied condition "success or failure"
Dec 18 20:36:31.462: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-89da74d9-c29b-45ca-96a9-56e4daf575ff container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 18 20:36:31.478: INFO: Waiting for pod pod-projected-configmaps-89da74d9-c29b-45ca-96a9-56e4daf575ff to disappear
Dec 18 20:36:31.479: INFO: Pod pod-projected-configmaps-89da74d9-c29b-45ca-96a9-56e4daf575ff no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 18 20:36:31.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2272" for this suite.
Dec 18 20:36:37.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 18 20:36:37.515: INFO: namespace projected-2272 deletion completed in 6.034549281s

• [SLOW TEST:14.079 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSDec 18 20:36:37.515: INFO: Running AfterSuite actions on all nodes
Dec 18 20:36:37.515: INFO: Running AfterSuite actions on node 1
Dec 18 20:36:37.515: INFO: Skipping dumping logs from cluster

Ran 276 of 4897 Specs in 8315.294 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4621 Skipped
PASS

Ginkgo ran 1 suite in 2h18m36.495035063s
Test Suite Passed
