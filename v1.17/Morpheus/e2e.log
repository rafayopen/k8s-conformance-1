I0323 18:50:26.765380      23 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-244672999
I0323 18:50:26.765488      23 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0323 18:50:26.765624      23 e2e.go:109] Starting e2e run "52a5c7fd-fc5b-4f20-ad22-3e3036cabc16" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1584989423 - Will randomize all specs
Will run 280 of 4842 specs

Mar 23 18:50:26.801: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 18:50:26.808: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 23 18:50:26.838: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 23 18:50:26.897: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 23 18:50:26.897: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Mar 23 18:50:26.897: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 23 18:50:26.917: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar 23 18:50:26.917: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Mar 23 18:50:26.917: INFO: e2e test version: v1.17.4
Mar 23 18:50:26.919: INFO: kube-apiserver version: v1.17.4
Mar 23 18:50:26.919: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 18:50:26.931: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:50:26.939: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename crd-publish-openapi
Mar 23 18:50:57.525: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 18:50:57.527: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 23 18:51:02.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-2301 create -f -'
Mar 23 18:51:03.723: INFO: stderr: ""
Mar 23 18:51:03.723: INFO: stdout: "e2e-test-crd-publish-openapi-9560-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 23 18:51:03.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-2301 delete e2e-test-crd-publish-openapi-9560-crds test-cr'
Mar 23 18:51:03.940: INFO: stderr: ""
Mar 23 18:51:03.940: INFO: stdout: "e2e-test-crd-publish-openapi-9560-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 23 18:51:03.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-2301 apply -f -'
Mar 23 18:51:04.279: INFO: stderr: ""
Mar 23 18:51:04.279: INFO: stdout: "e2e-test-crd-publish-openapi-9560-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 23 18:51:04.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-2301 delete e2e-test-crd-publish-openapi-9560-crds test-cr'
Mar 23 18:51:04.404: INFO: stderr: ""
Mar 23 18:51:04.404: INFO: stdout: "e2e-test-crd-publish-openapi-9560-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 23 18:51:04.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 explain e2e-test-crd-publish-openapi-9560-crds'
Mar 23 18:51:04.693: INFO: stderr: ""
Mar 23 18:51:04.693: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9560-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:51:09.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2301" for this suite.

• [SLOW TEST:42.847 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":1,"skipped":32,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:51:09.785: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-4451
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 23 18:51:09.861: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 23 18:51:40.631: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.0.4:8080/dial?request=hostname&protocol=udp&host=10.44.0.3&port=8081&tries=1'] Namespace:pod-network-test-4451 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 18:51:40.631: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 18:51:40.873: INFO: Waiting for responses: map[]
Mar 23 18:51:40.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.0.4:8080/dial?request=hostname&protocol=udp&host=10.42.0.3&port=8081&tries=1'] Namespace:pod-network-test-4451 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 18:51:40.879: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 18:51:41.132: INFO: Waiting for responses: map[]
Mar 23 18:51:41.137: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.0.4:8080/dial?request=hostname&protocol=udp&host=10.36.0.5&port=8081&tries=1'] Namespace:pod-network-test-4451 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 18:51:41.137: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 18:51:41.371: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:51:41.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4451" for this suite.

• [SLOW TEST:31.605 seconds]
[sig-network] Networking
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":2,"skipped":45,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:51:41.391: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 18:51:41.491: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 23 18:51:41.519: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:41.546: INFO: Number of nodes with available pods: 0
Mar 23 18:51:41.546: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 18:51:42.554: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:42.560: INFO: Number of nodes with available pods: 0
Mar 23 18:51:42.560: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 18:51:43.561: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:43.576: INFO: Number of nodes with available pods: 0
Mar 23 18:51:43.576: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 18:51:44.553: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:44.558: INFO: Number of nodes with available pods: 0
Mar 23 18:51:44.558: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 18:51:45.555: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:45.562: INFO: Number of nodes with available pods: 0
Mar 23 18:51:45.562: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 18:51:46.557: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:46.567: INFO: Number of nodes with available pods: 0
Mar 23 18:51:46.568: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 18:51:47.555: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:47.559: INFO: Number of nodes with available pods: 0
Mar 23 18:51:47.560: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 18:51:48.554: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:48.560: INFO: Number of nodes with available pods: 0
Mar 23 18:51:48.560: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 18:51:49.557: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:49.579: INFO: Number of nodes with available pods: 0
Mar 23 18:51:49.579: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 18:51:50.553: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:50.558: INFO: Number of nodes with available pods: 0
Mar 23 18:51:50.558: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 18:51:51.628: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:51.639: INFO: Number of nodes with available pods: 0
Mar 23 18:51:51.640: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 18:51:52.555: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:52.561: INFO: Number of nodes with available pods: 0
Mar 23 18:51:52.562: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 18:51:53.554: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:53.559: INFO: Number of nodes with available pods: 3
Mar 23 18:51:53.560: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 23 18:51:53.613: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:53.613: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:53.613: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:53.620: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:54.627: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:54.627: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:54.627: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:54.634: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:55.627: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:55.627: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:55.627: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:55.634: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:56.627: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:56.627: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:56.627: INFO: Pod daemon-set-dpftc is not available
Mar 23 18:51:56.627: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:56.632: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:57.627: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:57.627: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:57.627: INFO: Pod daemon-set-dpftc is not available
Mar 23 18:51:57.627: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:57.634: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:58.628: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:58.628: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:58.628: INFO: Pod daemon-set-dpftc is not available
Mar 23 18:51:58.628: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:58.635: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:51:59.627: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:59.627: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:59.627: INFO: Pod daemon-set-dpftc is not available
Mar 23 18:51:59.628: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:51:59.635: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:00.666: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:00.666: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:00.666: INFO: Pod daemon-set-dpftc is not available
Mar 23 18:52:00.666: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:00.680: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:01.626: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:01.626: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:01.626: INFO: Pod daemon-set-dpftc is not available
Mar 23 18:52:01.626: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:01.633: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:02.629: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:02.629: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:02.629: INFO: Pod daemon-set-dpftc is not available
Mar 23 18:52:02.629: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:02.637: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:03.626: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:03.626: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:03.626: INFO: Pod daemon-set-dpftc is not available
Mar 23 18:52:03.626: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:03.632: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:04.627: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:04.627: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:04.627: INFO: Pod daemon-set-dpftc is not available
Mar 23 18:52:04.627: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:04.634: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:05.627: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:05.627: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:05.627: INFO: Pod daemon-set-dpftc is not available
Mar 23 18:52:05.627: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:05.636: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:06.975: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:06.975: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:06.975: INFO: Pod daemon-set-dpftc is not available
Mar 23 18:52:06.975: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:06.994: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:07.627: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:07.627: INFO: Wrong image for pod: daemon-set-dpftc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:07.627: INFO: Pod daemon-set-dpftc is not available
Mar 23 18:52:07.628: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:07.634: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:08.627: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:08.627: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:08.627: INFO: Pod daemon-set-xlfgp is not available
Mar 23 18:52:08.634: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:09.629: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:09.629: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:09.629: INFO: Pod daemon-set-xlfgp is not available
Mar 23 18:52:09.637: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:10.627: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:10.627: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:10.627: INFO: Pod daemon-set-xlfgp is not available
Mar 23 18:52:10.633: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:11.627: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:11.627: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:11.634: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:12.627: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:12.628: INFO: Pod daemon-set-2qpb4 is not available
Mar 23 18:52:12.628: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:12.635: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:13.626: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:13.627: INFO: Pod daemon-set-2qpb4 is not available
Mar 23 18:52:13.627: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:13.656: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:14.627: INFO: Wrong image for pod: daemon-set-2qpb4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:14.627: INFO: Pod daemon-set-2qpb4 is not available
Mar 23 18:52:14.627: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:14.634: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:15.635: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:15.642: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:16.628: INFO: Pod daemon-set-p2vvg is not available
Mar 23 18:52:16.628: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:16.635: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:17.627: INFO: Pod daemon-set-p2vvg is not available
Mar 23 18:52:17.627: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:17.634: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:18.629: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:18.636: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:19.626: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:19.632: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:20.625: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:20.625: INFO: Pod daemon-set-rbm5h is not available
Mar 23 18:52:20.630: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:22.290: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:22.290: INFO: Pod daemon-set-rbm5h is not available
Mar 23 18:52:22.443: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:22.655: INFO: Wrong image for pod: daemon-set-rbm5h. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 18:52:22.656: INFO: Pod daemon-set-rbm5h is not available
Mar 23 18:52:22.870: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:23.627: INFO: Pod daemon-set-whmpx is not available
Mar 23 18:52:23.633: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 23 18:52:23.641: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:23.647: INFO: Number of nodes with available pods: 2
Mar 23 18:52:23.647: INFO: Node kube17-worker-2 is running more than one daemon pod
Mar 23 18:52:24.657: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:24.664: INFO: Number of nodes with available pods: 2
Mar 23 18:52:24.664: INFO: Node kube17-worker-2 is running more than one daemon pod
Mar 23 18:52:25.655: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:25.662: INFO: Number of nodes with available pods: 2
Mar 23 18:52:25.662: INFO: Node kube17-worker-2 is running more than one daemon pod
Mar 23 18:52:26.655: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:26.662: INFO: Number of nodes with available pods: 2
Mar 23 18:52:26.662: INFO: Node kube17-worker-2 is running more than one daemon pod
Mar 23 18:52:27.657: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:52:27.664: INFO: Number of nodes with available pods: 3
Mar 23 18:52:27.664: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4704, will wait for the garbage collector to delete the pods
Mar 23 18:52:27.764: INFO: Deleting DaemonSet.extensions daemon-set took: 15.702303ms
Mar 23 18:52:28.265: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.586396ms
Mar 23 18:52:38.369: INFO: Number of nodes with available pods: 0
Mar 23 18:52:38.369: INFO: Number of running nodes: 0, number of available pods: 0
Mar 23 18:52:38.373: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4704/daemonsets","resourceVersion":"7750"},"items":null}

Mar 23 18:52:38.377: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4704/pods","resourceVersion":"7750"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:52:38.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4704" for this suite.

• [SLOW TEST:57.022 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":3,"skipped":51,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:52:38.413: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 18:52:38.478: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:52:39.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4526" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":4,"skipped":64,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:52:39.848: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-1248
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 23 18:52:39.948: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 23 18:53:08.298: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.44.0.4:8080/dial?request=hostname&protocol=http&host=10.44.0.3&port=8080&tries=1'] Namespace:pod-network-test-1248 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 18:53:08.298: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 18:53:08.488: INFO: Waiting for responses: map[]
Mar 23 18:53:08.493: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.44.0.4:8080/dial?request=hostname&protocol=http&host=10.42.0.3&port=8080&tries=1'] Namespace:pod-network-test-1248 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 18:53:08.493: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 18:53:08.701: INFO: Waiting for responses: map[]
Mar 23 18:53:08.706: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.44.0.4:8080/dial?request=hostname&protocol=http&host=10.36.0.5&port=8080&tries=1'] Namespace:pod-network-test-1248 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 18:53:08.706: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 18:53:08.935: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:53:08.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1248" for this suite.

• [SLOW TEST:29.104 seconds]
[sig-network] Networking
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":5,"skipped":74,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:53:08.953: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:53:27.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2045" for this suite.

• [SLOW TEST:18.601 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":6,"skipped":89,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:53:27.564: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 18:53:27.627: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:53:27.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9803" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":7,"skipped":107,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:53:27.704: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1681
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 18:53:27.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8420'
Mar 23 18:53:28.019: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 23 18:53:28.020: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
Mar 23 18:53:28.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete jobs e2e-test-httpd-job --namespace=kubectl-8420'
Mar 23 18:53:28.252: INFO: stderr: ""
Mar 23 18:53:28.253: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:53:28.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8420" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":280,"completed":8,"skipped":121,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:53:28.275: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 23 18:53:35.140: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6a559f72-b40e-44f9-b8a1-7d1bce617928"
Mar 23 18:53:35.141: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6a559f72-b40e-44f9-b8a1-7d1bce617928" in namespace "pods-1998" to be "terminated due to deadline exceeded"
Mar 23 18:53:35.145: INFO: Pod "pod-update-activedeadlineseconds-6a559f72-b40e-44f9-b8a1-7d1bce617928": Phase="Running", Reason="", readiness=true. Elapsed: 4.161416ms
Mar 23 18:53:37.150: INFO: Pod "pod-update-activedeadlineseconds-6a559f72-b40e-44f9-b8a1-7d1bce617928": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.009775878s
Mar 23 18:53:37.150: INFO: Pod "pod-update-activedeadlineseconds-6a559f72-b40e-44f9-b8a1-7d1bce617928" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:53:37.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1998" for this suite.

• [SLOW TEST:8.891 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":9,"skipped":124,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:53:37.168: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Mar 23 18:53:42.976: INFO: Successfully updated pod "annotationupdate889f5c9d-8b28-47b8-af69-d7b39a2689ed"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:53:45.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3568" for this suite.

• [SLOW TEST:7.897 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":10,"skipped":154,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:53:45.066: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-1758de56-ae03-4f89-91c7-712628f962a1
STEP: Creating configMap with name cm-test-opt-upd-8d480327-5e27-4341-9b1c-617b85c041a2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1758de56-ae03-4f89-91c7-712628f962a1
STEP: Updating configmap cm-test-opt-upd-8d480327-5e27-4341-9b1c-617b85c041a2
STEP: Creating configMap with name cm-test-opt-create-1600cc1f-106f-4f33-8c65-b44e8cccf14a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:53:51.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7291" for this suite.

• [SLOW TEST:6.491 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":11,"skipped":155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:53:51.559: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 18:53:51.635: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9537
I0323 18:53:51.673373      23 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9537, replica count: 1
I0323 18:53:52.724504      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 18:53:53.725038      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 18:53:53.858: INFO: Created: latency-svc-4rwp2
Mar 23 18:53:53.863: INFO: Got endpoints: latency-svc-4rwp2 [38.20661ms]
Mar 23 18:53:53.970: INFO: Created: latency-svc-fllk2
Mar 23 18:53:54.002: INFO: Got endpoints: latency-svc-fllk2 [137.928093ms]
Mar 23 18:53:54.026: INFO: Created: latency-svc-4qgj4
Mar 23 18:53:54.038: INFO: Got endpoints: latency-svc-4qgj4 [172.662667ms]
Mar 23 18:53:54.052: INFO: Created: latency-svc-svcgx
Mar 23 18:53:54.101: INFO: Got endpoints: latency-svc-svcgx [236.106378ms]
Mar 23 18:53:54.131: INFO: Created: latency-svc-ps2rw
Mar 23 18:53:54.132: INFO: Got endpoints: latency-svc-ps2rw [93.582831ms]
Mar 23 18:53:54.147: INFO: Created: latency-svc-8vf9d
Mar 23 18:53:54.151: INFO: Got endpoints: latency-svc-8vf9d [285.997858ms]
Mar 23 18:53:54.173: INFO: Created: latency-svc-zgrq7
Mar 23 18:53:54.182: INFO: Got endpoints: latency-svc-zgrq7 [316.523885ms]
Mar 23 18:53:54.206: INFO: Created: latency-svc-hfpk4
Mar 23 18:53:54.211: INFO: Got endpoints: latency-svc-hfpk4 [346.788593ms]
Mar 23 18:53:54.228: INFO: Created: latency-svc-q67jp
Mar 23 18:53:54.230: INFO: Got endpoints: latency-svc-q67jp [364.791136ms]
Mar 23 18:53:54.248: INFO: Created: latency-svc-pdxk9
Mar 23 18:53:54.253: INFO: Created: latency-svc-dsqpn
Mar 23 18:53:54.260: INFO: Got endpoints: latency-svc-pdxk9 [395.350677ms]
Mar 23 18:53:54.261: INFO: Got endpoints: latency-svc-dsqpn [395.25367ms]
Mar 23 18:53:54.274: INFO: Created: latency-svc-txt24
Mar 23 18:53:54.278: INFO: Got endpoints: latency-svc-txt24 [412.748081ms]
Mar 23 18:53:54.288: INFO: Created: latency-svc-r58sd
Mar 23 18:53:54.324: INFO: Created: latency-svc-8kxxf
Mar 23 18:53:54.326: INFO: Got endpoints: latency-svc-r58sd [460.422243ms]
Mar 23 18:53:54.335: INFO: Got endpoints: latency-svc-8kxxf [470.048217ms]
Mar 23 18:53:54.347: INFO: Created: latency-svc-pvslz
Mar 23 18:53:54.358: INFO: Got endpoints: latency-svc-pvslz [492.449502ms]
Mar 23 18:53:54.365: INFO: Created: latency-svc-2bwwv
Mar 23 18:53:54.374: INFO: Got endpoints: latency-svc-2bwwv [510.235806ms]
Mar 23 18:53:54.381: INFO: Created: latency-svc-wpv4j
Mar 23 18:53:54.391: INFO: Got endpoints: latency-svc-wpv4j [526.885047ms]
Mar 23 18:53:54.405: INFO: Created: latency-svc-bl2sm
Mar 23 18:53:54.422: INFO: Got endpoints: latency-svc-bl2sm [420.166791ms]
Mar 23 18:53:54.456: INFO: Created: latency-svc-hlbpg
Mar 23 18:53:54.458: INFO: Got endpoints: latency-svc-hlbpg [354.000944ms]
Mar 23 18:53:54.471: INFO: Created: latency-svc-mklsg
Mar 23 18:53:54.481: INFO: Got endpoints: latency-svc-mklsg [348.650651ms]
Mar 23 18:53:54.487: INFO: Created: latency-svc-sg2wq
Mar 23 18:53:54.496: INFO: Got endpoints: latency-svc-sg2wq [344.685544ms]
Mar 23 18:53:54.507: INFO: Created: latency-svc-h4pwt
Mar 23 18:53:54.510: INFO: Got endpoints: latency-svc-h4pwt [327.172884ms]
Mar 23 18:53:54.523: INFO: Created: latency-svc-8kcpw
Mar 23 18:53:54.535: INFO: Got endpoints: latency-svc-8kcpw [324.228559ms]
Mar 23 18:53:54.542: INFO: Created: latency-svc-2lxlh
Mar 23 18:53:54.545: INFO: Got endpoints: latency-svc-2lxlh [314.65451ms]
Mar 23 18:53:54.568: INFO: Created: latency-svc-p4kwl
Mar 23 18:53:54.581: INFO: Got endpoints: latency-svc-p4kwl [320.759041ms]
Mar 23 18:53:54.604: INFO: Created: latency-svc-xhrff
Mar 23 18:53:54.604: INFO: Got endpoints: latency-svc-xhrff [343.482968ms]
Mar 23 18:53:54.619: INFO: Created: latency-svc-4rz4b
Mar 23 18:53:54.619: INFO: Got endpoints: latency-svc-4rz4b [340.608848ms]
Mar 23 18:53:54.631: INFO: Created: latency-svc-pq2h9
Mar 23 18:53:54.634: INFO: Got endpoints: latency-svc-pq2h9 [307.129519ms]
Mar 23 18:53:54.657: INFO: Created: latency-svc-ppcrr
Mar 23 18:53:54.658: INFO: Got endpoints: latency-svc-ppcrr [322.786897ms]
Mar 23 18:53:54.706: INFO: Created: latency-svc-d84jn
Mar 23 18:53:54.711: INFO: Got endpoints: latency-svc-d84jn [352.972709ms]
Mar 23 18:53:54.733: INFO: Created: latency-svc-vzfzl
Mar 23 18:53:54.737: INFO: Got endpoints: latency-svc-vzfzl [362.597419ms]
Mar 23 18:53:54.747: INFO: Created: latency-svc-rj72p
Mar 23 18:53:54.753: INFO: Got endpoints: latency-svc-rj72p [361.306993ms]
Mar 23 18:53:54.772: INFO: Created: latency-svc-mr82t
Mar 23 18:53:54.775: INFO: Got endpoints: latency-svc-mr82t [353.103828ms]
Mar 23 18:53:54.783: INFO: Created: latency-svc-9r5b5
Mar 23 18:53:54.808: INFO: Got endpoints: latency-svc-9r5b5 [349.95964ms]
Mar 23 18:53:54.819: INFO: Created: latency-svc-76v5h
Mar 23 18:53:54.819: INFO: Got endpoints: latency-svc-76v5h [337.96644ms]
Mar 23 18:53:54.834: INFO: Created: latency-svc-phkt8
Mar 23 18:53:54.838: INFO: Got endpoints: latency-svc-phkt8 [341.493828ms]
Mar 23 18:53:54.852: INFO: Created: latency-svc-f972r
Mar 23 18:53:54.857: INFO: Got endpoints: latency-svc-f972r [346.728171ms]
Mar 23 18:53:54.869: INFO: Created: latency-svc-sh9lk
Mar 23 18:53:54.869: INFO: Got endpoints: latency-svc-sh9lk [334.261912ms]
Mar 23 18:53:54.878: INFO: Created: latency-svc-rw9c5
Mar 23 18:53:54.878: INFO: Got endpoints: latency-svc-rw9c5 [333.472155ms]
Mar 23 18:53:54.897: INFO: Created: latency-svc-5b9z6
Mar 23 18:53:54.901: INFO: Got endpoints: latency-svc-5b9z6 [318.906026ms]
Mar 23 18:53:54.925: INFO: Created: latency-svc-88tk2
Mar 23 18:53:54.929: INFO: Got endpoints: latency-svc-88tk2 [324.52287ms]
Mar 23 18:53:54.950: INFO: Created: latency-svc-5qmkz
Mar 23 18:53:54.960: INFO: Got endpoints: latency-svc-5qmkz [341.129125ms]
Mar 23 18:53:55.037: INFO: Created: latency-svc-9jscj
Mar 23 18:53:55.038: INFO: Got endpoints: latency-svc-9jscj [403.8584ms]
Mar 23 18:53:55.044: INFO: Created: latency-svc-9klkp
Mar 23 18:53:55.055: INFO: Got endpoints: latency-svc-9klkp [397.238372ms]
Mar 23 18:53:55.067: INFO: Created: latency-svc-wc6zx
Mar 23 18:53:55.081: INFO: Got endpoints: latency-svc-wc6zx [369.320709ms]
Mar 23 18:53:55.089: INFO: Created: latency-svc-spq8s
Mar 23 18:53:55.093: INFO: Got endpoints: latency-svc-spq8s [354.985617ms]
Mar 23 18:53:55.118: INFO: Created: latency-svc-x8z65
Mar 23 18:53:55.118: INFO: Got endpoints: latency-svc-x8z65 [364.791343ms]
Mar 23 18:53:55.137: INFO: Created: latency-svc-cqsg7
Mar 23 18:53:55.153: INFO: Got endpoints: latency-svc-cqsg7 [377.707381ms]
Mar 23 18:53:55.172: INFO: Created: latency-svc-cdkcz
Mar 23 18:53:55.172: INFO: Got endpoints: latency-svc-cdkcz [364.046254ms]
Mar 23 18:53:55.175: INFO: Created: latency-svc-9sq7d
Mar 23 18:53:55.190: INFO: Got endpoints: latency-svc-9sq7d [370.999292ms]
Mar 23 18:53:55.201: INFO: Created: latency-svc-vgpr8
Mar 23 18:53:55.202: INFO: Got endpoints: latency-svc-vgpr8 [364.126963ms]
Mar 23 18:53:55.227: INFO: Created: latency-svc-hzvcl
Mar 23 18:53:55.228: INFO: Got endpoints: latency-svc-hzvcl [371.017764ms]
Mar 23 18:53:55.248: INFO: Created: latency-svc-rsdjs
Mar 23 18:53:55.251: INFO: Got endpoints: latency-svc-rsdjs [381.185622ms]
Mar 23 18:53:55.273: INFO: Created: latency-svc-lpd42
Mar 23 18:53:55.279: INFO: Got endpoints: latency-svc-lpd42 [400.203449ms]
Mar 23 18:53:55.325: INFO: Created: latency-svc-qt7fm
Mar 23 18:53:55.325: INFO: Got endpoints: latency-svc-qt7fm [423.86757ms]
Mar 23 18:53:55.353: INFO: Created: latency-svc-5cv5f
Mar 23 18:53:55.357: INFO: Got endpoints: latency-svc-5cv5f [428.192514ms]
Mar 23 18:53:55.371: INFO: Created: latency-svc-9fnwv
Mar 23 18:53:55.383: INFO: Got endpoints: latency-svc-9fnwv [422.181698ms]
Mar 23 18:53:55.398: INFO: Created: latency-svc-55pkg
Mar 23 18:53:55.433: INFO: Got endpoints: latency-svc-55pkg [394.328732ms]
Mar 23 18:53:55.445: INFO: Created: latency-svc-zppcb
Mar 23 18:53:55.452: INFO: Got endpoints: latency-svc-zppcb [396.73614ms]
Mar 23 18:53:55.467: INFO: Created: latency-svc-q8nl5
Mar 23 18:53:55.474: INFO: Got endpoints: latency-svc-q8nl5 [392.721396ms]
Mar 23 18:53:55.486: INFO: Created: latency-svc-n95rs
Mar 23 18:53:55.499: INFO: Got endpoints: latency-svc-n95rs [405.670557ms]
Mar 23 18:53:55.511: INFO: Created: latency-svc-lsgnp
Mar 23 18:53:55.523: INFO: Created: latency-svc-bxkb7
Mar 23 18:53:55.536: INFO: Got endpoints: latency-svc-lsgnp [417.524165ms]
Mar 23 18:53:55.577: INFO: Created: latency-svc-v2pzs
Mar 23 18:53:55.587: INFO: Got endpoints: latency-svc-bxkb7 [432.867091ms]
Mar 23 18:53:55.595: INFO: Created: latency-svc-j5d88
Mar 23 18:53:55.609: INFO: Created: latency-svc-n2hvq
Mar 23 18:53:55.676: INFO: Got endpoints: latency-svc-v2pzs [503.045178ms]
Mar 23 18:53:55.698: INFO: Created: latency-svc-8kmpc
Mar 23 18:53:55.709: INFO: Got endpoints: latency-svc-j5d88 [517.957227ms]
Mar 23 18:53:55.719: INFO: Created: latency-svc-c2t89
Mar 23 18:53:55.739: INFO: Got endpoints: latency-svc-n2hvq [535.958849ms]
Mar 23 18:53:55.742: INFO: Created: latency-svc-b5f9g
Mar 23 18:53:55.763: INFO: Created: latency-svc-q5lt5
Mar 23 18:53:55.775: INFO: Created: latency-svc-dqtr6
Mar 23 18:53:55.785: INFO: Got endpoints: latency-svc-8kmpc [557.23948ms]
Mar 23 18:53:55.796: INFO: Created: latency-svc-lwhlp
Mar 23 18:53:55.825: INFO: Created: latency-svc-dvk6m
Mar 23 18:53:55.844: INFO: Got endpoints: latency-svc-c2t89 [592.854401ms]
Mar 23 18:53:55.856: INFO: Created: latency-svc-7rxxn
Mar 23 18:53:55.878: INFO: Created: latency-svc-q685b
Mar 23 18:53:55.884: INFO: Got endpoints: latency-svc-b5f9g [604.999266ms]
Mar 23 18:53:55.908: INFO: Created: latency-svc-9zrgq
Mar 23 18:53:55.924: INFO: Created: latency-svc-c6fcf
Mar 23 18:53:55.933: INFO: Got endpoints: latency-svc-q5lt5 [608.673369ms]
Mar 23 18:53:55.936: INFO: Created: latency-svc-kt5th
Mar 23 18:53:55.964: INFO: Created: latency-svc-f6rgw
Mar 23 18:53:55.993: INFO: Got endpoints: latency-svc-dqtr6 [635.447276ms]
Mar 23 18:53:56.008: INFO: Created: latency-svc-8vcp9
Mar 23 18:53:56.032: INFO: Created: latency-svc-6k7zt
Mar 23 18:53:56.038: INFO: Got endpoints: latency-svc-lwhlp [654.353612ms]
Mar 23 18:53:56.061: INFO: Created: latency-svc-9jnmx
Mar 23 18:53:56.211: INFO: Got endpoints: latency-svc-7rxxn [758.298902ms]
Mar 23 18:53:56.217: INFO: Got endpoints: latency-svc-dvk6m [783.907218ms]
Mar 23 18:53:56.218: INFO: Got endpoints: latency-svc-q685b [744.244042ms]
Mar 23 18:53:56.253: INFO: Created: latency-svc-khzbk
Mar 23 18:53:56.270: INFO: Got endpoints: latency-svc-9zrgq [771.54378ms]
Mar 23 18:53:56.287: INFO: Created: latency-svc-crtp4
Mar 23 18:53:56.297: INFO: Got endpoints: latency-svc-c6fcf [760.394937ms]
Mar 23 18:53:56.334: INFO: Got endpoints: latency-svc-kt5th [747.331533ms]
Mar 23 18:53:56.336: INFO: Created: latency-svc-7tlnp
Mar 23 18:53:56.347: INFO: Created: latency-svc-5v6vm
Mar 23 18:53:56.388: INFO: Created: latency-svc-vmmmd
Mar 23 18:53:56.393: INFO: Got endpoints: latency-svc-f6rgw [716.125582ms]
Mar 23 18:53:56.396: INFO: Created: latency-svc-54mfc
Mar 23 18:53:56.451: INFO: Got endpoints: latency-svc-8vcp9 [741.812696ms]
Mar 23 18:53:56.451: INFO: Created: latency-svc-8pwft
Mar 23 18:53:56.457: INFO: Created: latency-svc-c4d5b
Mar 23 18:53:56.478: INFO: Created: latency-svc-njxs4
Mar 23 18:53:56.487: INFO: Got endpoints: latency-svc-6k7zt [748.457943ms]
Mar 23 18:53:56.516: INFO: Created: latency-svc-nbmld
Mar 23 18:53:57.299: INFO: Got endpoints: latency-svc-9jnmx [1.5132139s]
Mar 23 18:53:57.317: INFO: Got endpoints: latency-svc-khzbk [1.473075196s]
Mar 23 18:53:57.394: INFO: Got endpoints: latency-svc-crtp4 [1.510282509s]
Mar 23 18:53:57.395: INFO: Got endpoints: latency-svc-7tlnp [1.461010277s]
Mar 23 18:53:57.398: INFO: Got endpoints: latency-svc-5v6vm [1.405185627s]
Mar 23 18:53:57.419: INFO: Got endpoints: latency-svc-vmmmd [1.380028513s]
Mar 23 18:53:57.431: INFO: Got endpoints: latency-svc-54mfc [1.219726887s]
Mar 23 18:53:57.431: INFO: Created: latency-svc-jzl44
Mar 23 18:53:57.450: INFO: Got endpoints: latency-svc-8pwft [1.232143449s]
Mar 23 18:53:57.462: INFO: Got endpoints: latency-svc-c4d5b [1.241459478s]
Mar 23 18:53:57.480: INFO: Got endpoints: latency-svc-njxs4 [1.209911956s]
Mar 23 18:53:57.497: INFO: Got endpoints: latency-svc-nbmld [1.199816078s]
Mar 23 18:53:57.501: INFO: Got endpoints: latency-svc-jzl44 [1.165870972s]
Mar 23 18:53:57.515: INFO: Created: latency-svc-7bdvz
Mar 23 18:53:57.527: INFO: Got endpoints: latency-svc-7bdvz [1.133235679s]
Mar 23 18:53:57.532: INFO: Created: latency-svc-4wl9n
Mar 23 18:53:57.534: INFO: Got endpoints: latency-svc-4wl9n [1.082724612s]
Mar 23 18:53:57.554: INFO: Created: latency-svc-28rjz
Mar 23 18:53:57.555: INFO: Got endpoints: latency-svc-28rjz [1.067696662s]
Mar 23 18:53:57.601: INFO: Created: latency-svc-k9vnt
Mar 23 18:53:57.612: INFO: Got endpoints: latency-svc-k9vnt [312.594385ms]
Mar 23 18:53:57.628: INFO: Created: latency-svc-cw7p9
Mar 23 18:53:57.659: INFO: Got endpoints: latency-svc-cw7p9 [342.054158ms]
Mar 23 18:53:57.680: INFO: Created: latency-svc-7flzk
Mar 23 18:53:57.695: INFO: Got endpoints: latency-svc-7flzk [298.022755ms]
Mar 23 18:53:57.715: INFO: Created: latency-svc-s2n2r
Mar 23 18:53:57.722: INFO: Got endpoints: latency-svc-s2n2r [322.62221ms]
Mar 23 18:53:57.767: INFO: Created: latency-svc-bkvkq
Mar 23 18:53:57.776: INFO: Got endpoints: latency-svc-bkvkq [379.916185ms]
Mar 23 18:53:57.786: INFO: Created: latency-svc-l8cts
Mar 23 18:53:57.791: INFO: Got endpoints: latency-svc-l8cts [359.571098ms]
Mar 23 18:53:57.806: INFO: Created: latency-svc-mtrhg
Mar 23 18:53:57.855: INFO: Got endpoints: latency-svc-mtrhg [420.08975ms]
Mar 23 18:53:57.865: INFO: Created: latency-svc-ph2sl
Mar 23 18:53:57.878: INFO: Got endpoints: latency-svc-ph2sl [427.748626ms]
Mar 23 18:53:57.887: INFO: Created: latency-svc-gshcx
Mar 23 18:53:57.896: INFO: Created: latency-svc-ncsds
Mar 23 18:53:57.896: INFO: Got endpoints: latency-svc-gshcx [434.142574ms]
Mar 23 18:53:57.908: INFO: Got endpoints: latency-svc-ncsds [427.049714ms]
Mar 23 18:53:57.922: INFO: Created: latency-svc-jl9jz
Mar 23 18:53:57.927: INFO: Got endpoints: latency-svc-jl9jz [430.464138ms]
Mar 23 18:53:57.941: INFO: Created: latency-svc-mnxz9
Mar 23 18:53:57.943: INFO: Got endpoints: latency-svc-mnxz9 [441.450748ms]
Mar 23 18:53:57.980: INFO: Created: latency-svc-2shls
Mar 23 18:53:57.981: INFO: Got endpoints: latency-svc-2shls [453.541407ms]
Mar 23 18:53:58.001: INFO: Created: latency-svc-x4rzt
Mar 23 18:53:58.013: INFO: Got endpoints: latency-svc-x4rzt [478.983133ms]
Mar 23 18:53:58.041: INFO: Created: latency-svc-djxj6
Mar 23 18:53:58.042: INFO: Got endpoints: latency-svc-djxj6 [486.138595ms]
Mar 23 18:53:58.050: INFO: Created: latency-svc-2ktjg
Mar 23 18:53:58.052: INFO: Got endpoints: latency-svc-2ktjg [439.737433ms]
Mar 23 18:53:58.073: INFO: Created: latency-svc-2zltm
Mar 23 18:53:58.098: INFO: Got endpoints: latency-svc-2zltm [438.307173ms]
Mar 23 18:53:58.104: INFO: Created: latency-svc-5rwlr
Mar 23 18:53:58.117: INFO: Created: latency-svc-4wplh
Mar 23 18:53:58.141: INFO: Created: latency-svc-z6kgb
Mar 23 18:53:58.142: INFO: Got endpoints: latency-svc-5rwlr [446.917485ms]
Mar 23 18:53:58.169: INFO: Created: latency-svc-7wbdc
Mar 23 18:53:58.194: INFO: Got endpoints: latency-svc-4wplh [471.326287ms]
Mar 23 18:53:58.218: INFO: Created: latency-svc-pf84c
Mar 23 18:53:58.237: INFO: Created: latency-svc-t7cbr
Mar 23 18:53:58.240: INFO: Got endpoints: latency-svc-z6kgb [464.532412ms]
Mar 23 18:53:58.259: INFO: Created: latency-svc-f928g
Mar 23 18:53:58.273: INFO: Created: latency-svc-9jdqc
Mar 23 18:53:58.291: INFO: Created: latency-svc-swgng
Mar 23 18:53:58.295: INFO: Got endpoints: latency-svc-7wbdc [504.165152ms]
Mar 23 18:53:58.317: INFO: Created: latency-svc-spljx
Mar 23 18:53:58.340: INFO: Got endpoints: latency-svc-pf84c [485.135019ms]
Mar 23 18:53:58.367: INFO: Created: latency-svc-b9z5w
Mar 23 18:53:58.397: INFO: Got endpoints: latency-svc-t7cbr [518.95868ms]
Mar 23 18:53:58.397: INFO: Created: latency-svc-s76r6
Mar 23 18:53:58.423: INFO: Created: latency-svc-r5jcg
Mar 23 18:53:58.435: INFO: Got endpoints: latency-svc-f928g [538.466067ms]
Mar 23 18:53:58.481: INFO: Created: latency-svc-khdk5
Mar 23 18:53:58.485: INFO: Got endpoints: latency-svc-9jdqc [577.703135ms]
Mar 23 18:53:58.505: INFO: Created: latency-svc-dzpl6
Mar 23 18:53:58.529: INFO: Created: latency-svc-7wvbr
Mar 23 18:53:58.538: INFO: Got endpoints: latency-svc-swgng [609.978404ms]
Mar 23 18:53:58.543: INFO: Created: latency-svc-5vnr7
Mar 23 18:53:58.596: INFO: Got endpoints: latency-svc-spljx [653.109211ms]
Mar 23 18:53:58.608: INFO: Created: latency-svc-g9r68
Mar 23 18:53:58.608: INFO: Created: latency-svc-jqcl4
Mar 23 18:53:58.612: INFO: Created: latency-svc-2ntpx
Mar 23 18:53:58.629: INFO: Created: latency-svc-f5tp8
Mar 23 18:53:58.639: INFO: Got endpoints: latency-svc-b9z5w [658.681129ms]
Mar 23 18:53:58.685: INFO: Got endpoints: latency-svc-s76r6 [671.335406ms]
Mar 23 18:53:58.703: INFO: Created: latency-svc-cjvgm
Mar 23 18:53:58.733: INFO: Created: latency-svc-4ghcw
Mar 23 18:53:58.740: INFO: Got endpoints: latency-svc-r5jcg [697.71202ms]
Mar 23 18:53:58.751: INFO: Created: latency-svc-k7sjv
Mar 23 18:53:58.784: INFO: Got endpoints: latency-svc-khdk5 [731.54141ms]
Mar 23 18:53:58.801: INFO: Created: latency-svc-5lqpl
Mar 23 18:53:58.823: INFO: Created: latency-svc-77vdq
Mar 23 18:53:58.842: INFO: Got endpoints: latency-svc-dzpl6 [742.902822ms]
Mar 23 18:53:58.856: INFO: Created: latency-svc-bd7pf
Mar 23 18:53:58.886: INFO: Got endpoints: latency-svc-7wvbr [743.719205ms]
Mar 23 18:53:58.906: INFO: Created: latency-svc-r4fqq
Mar 23 18:53:58.906: INFO: Created: latency-svc-7rr4t
Mar 23 18:53:58.915: INFO: Created: latency-svc-9674p
Mar 23 18:53:58.931: INFO: Created: latency-svc-75pp9
Mar 23 18:53:58.934: INFO: Got endpoints: latency-svc-5vnr7 [740.549423ms]
Mar 23 18:53:58.954: INFO: Created: latency-svc-wqsj4
Mar 23 18:53:58.985: INFO: Got endpoints: latency-svc-jqcl4 [744.271624ms]
Mar 23 18:53:59.009: INFO: Created: latency-svc-6c56c
Mar 23 18:53:59.036: INFO: Got endpoints: latency-svc-g9r68 [740.886936ms]
Mar 23 18:53:59.058: INFO: Created: latency-svc-wl2qw
Mar 23 18:53:59.085: INFO: Got endpoints: latency-svc-2ntpx [744.107242ms]
Mar 23 18:53:59.104: INFO: Created: latency-svc-ccwpm
Mar 23 18:53:59.134: INFO: Got endpoints: latency-svc-f5tp8 [736.876665ms]
Mar 23 18:53:59.166: INFO: Created: latency-svc-lzmvk
Mar 23 18:53:59.184: INFO: Got endpoints: latency-svc-cjvgm [748.775371ms]
Mar 23 18:53:59.211: INFO: Created: latency-svc-748fh
Mar 23 18:53:59.234: INFO: Got endpoints: latency-svc-4ghcw [748.409374ms]
Mar 23 18:53:59.271: INFO: Created: latency-svc-zgkdx
Mar 23 18:53:59.287: INFO: Got endpoints: latency-svc-k7sjv [748.836039ms]
Mar 23 18:53:59.338: INFO: Got endpoints: latency-svc-5lqpl [741.642209ms]
Mar 23 18:53:59.343: INFO: Created: latency-svc-hkvhb
Mar 23 18:53:59.393: INFO: Created: latency-svc-t8h9g
Mar 23 18:53:59.395: INFO: Got endpoints: latency-svc-77vdq [755.211921ms]
Mar 23 18:53:59.422: INFO: Created: latency-svc-tsx67
Mar 23 18:53:59.434: INFO: Got endpoints: latency-svc-bd7pf [748.383115ms]
Mar 23 18:54:00.097: INFO: Got endpoints: latency-svc-r4fqq [1.356843025s]
Mar 23 18:54:00.105: INFO: Got endpoints: latency-svc-7rr4t [1.320592371s]
Mar 23 18:54:00.109: INFO: Created: latency-svc-bwfq4
Mar 23 18:54:00.116: INFO: Got endpoints: latency-svc-9674p [1.274592983s]
Mar 23 18:54:00.133: INFO: Got endpoints: latency-svc-wqsj4 [1.198523894s]
Mar 23 18:54:00.134: INFO: Got endpoints: latency-svc-75pp9 [1.247475984s]
Mar 23 18:54:00.157: INFO: Got endpoints: latency-svc-6c56c [1.172279323s]
Mar 23 18:54:00.480: INFO: Got endpoints: latency-svc-lzmvk [1.345418789s]
Mar 23 18:54:00.482: INFO: Got endpoints: latency-svc-748fh [1.297646378s]
Mar 23 18:54:00.503: INFO: Got endpoints: latency-svc-zgkdx [1.268633075s]
Mar 23 18:54:00.504: INFO: Got endpoints: latency-svc-wl2qw [1.467224234s]
Mar 23 18:54:00.504: INFO: Got endpoints: latency-svc-ccwpm [1.419235449s]
Mar 23 18:54:00.547: INFO: Created: latency-svc-szfs5
Mar 23 18:54:00.555: INFO: Got endpoints: latency-svc-hkvhb [1.268215408s]
Mar 23 18:54:00.579: INFO: Got endpoints: latency-svc-tsx67 [1.18385181s]
Mar 23 18:54:00.589: INFO: Got endpoints: latency-svc-bwfq4 [1.155192838s]
Mar 23 18:54:00.601: INFO: Got endpoints: latency-svc-t8h9g [1.262516204s]
Mar 23 18:54:00.609: INFO: Got endpoints: latency-svc-szfs5 [511.841891ms]
Mar 23 18:54:00.642: INFO: Created: latency-svc-xqsw7
Mar 23 18:54:00.691: INFO: Got endpoints: latency-svc-xqsw7 [586.369982ms]
Mar 23 18:54:00.797: INFO: Created: latency-svc-phpv6
Mar 23 18:54:00.798: INFO: Got endpoints: latency-svc-phpv6 [681.229085ms]
Mar 23 18:54:00.809: INFO: Created: latency-svc-22pxd
Mar 23 18:54:00.822: INFO: Got endpoints: latency-svc-22pxd [688.743752ms]
Mar 23 18:54:00.832: INFO: Created: latency-svc-4r2dv
Mar 23 18:54:00.834: INFO: Got endpoints: latency-svc-4r2dv [700.301512ms]
Mar 23 18:54:00.858: INFO: Created: latency-svc-ww9db
Mar 23 18:54:00.867: INFO: Got endpoints: latency-svc-ww9db [709.869365ms]
Mar 23 18:54:00.897: INFO: Created: latency-svc-2ptg7
Mar 23 18:54:00.910: INFO: Got endpoints: latency-svc-2ptg7 [430.083126ms]
Mar 23 18:54:00.923: INFO: Created: latency-svc-z9v5m
Mar 23 18:54:00.927: INFO: Got endpoints: latency-svc-z9v5m [445.32566ms]
Mar 23 18:54:00.939: INFO: Created: latency-svc-5ssjq
Mar 23 18:54:00.950: INFO: Got endpoints: latency-svc-5ssjq [447.197124ms]
Mar 23 18:54:00.962: INFO: Created: latency-svc-ffm8z
Mar 23 18:54:00.973: INFO: Got endpoints: latency-svc-ffm8z [468.907918ms]
Mar 23 18:54:00.982: INFO: Created: latency-svc-25n77
Mar 23 18:54:00.987: INFO: Got endpoints: latency-svc-25n77 [482.579157ms]
Mar 23 18:54:01.007: INFO: Created: latency-svc-2sxtr
Mar 23 18:54:01.023: INFO: Got endpoints: latency-svc-2sxtr [468.330262ms]
Mar 23 18:54:01.034: INFO: Created: latency-svc-4k52w
Mar 23 18:54:01.055: INFO: Got endpoints: latency-svc-4k52w [476.454658ms]
Mar 23 18:54:01.081: INFO: Created: latency-svc-kbcz7
Mar 23 18:54:01.081: INFO: Got endpoints: latency-svc-kbcz7 [490.625395ms]
Mar 23 18:54:01.105: INFO: Created: latency-svc-xlrvt
Mar 23 18:54:01.109: INFO: Got endpoints: latency-svc-xlrvt [507.284568ms]
Mar 23 18:54:01.117: INFO: Created: latency-svc-w44pf
Mar 23 18:54:01.123: INFO: Got endpoints: latency-svc-w44pf [513.841064ms]
Mar 23 18:54:01.145: INFO: Created: latency-svc-mj5gl
Mar 23 18:54:01.155: INFO: Got endpoints: latency-svc-mj5gl [463.301848ms]
Mar 23 18:54:01.156: INFO: Created: latency-svc-zrtmx
Mar 23 18:54:01.167: INFO: Got endpoints: latency-svc-zrtmx [368.854225ms]
Mar 23 18:54:01.183: INFO: Created: latency-svc-w6m58
Mar 23 18:54:01.183: INFO: Got endpoints: latency-svc-w6m58 [361.160101ms]
Mar 23 18:54:01.199: INFO: Created: latency-svc-jgr7l
Mar 23 18:54:01.204: INFO: Got endpoints: latency-svc-jgr7l [368.990356ms]
Mar 23 18:54:01.219: INFO: Created: latency-svc-g8mjh
Mar 23 18:54:01.234: INFO: Created: latency-svc-f6lf7
Mar 23 18:54:01.253: INFO: Got endpoints: latency-svc-g8mjh [384.954387ms]
Mar 23 18:54:01.259: INFO: Created: latency-svc-8kxct
Mar 23 18:54:01.276: INFO: Created: latency-svc-7pftf
Mar 23 18:54:01.287: INFO: Got endpoints: latency-svc-f6lf7 [376.237373ms]
Mar 23 18:54:01.295: INFO: Created: latency-svc-gc4rs
Mar 23 18:54:01.309: INFO: Created: latency-svc-45bfs
Mar 23 18:54:01.335: INFO: Got endpoints: latency-svc-8kxct [407.222545ms]
Mar 23 18:54:01.345: INFO: Created: latency-svc-f5mkc
Mar 23 18:54:01.356: INFO: Created: latency-svc-fks7j
Mar 23 18:54:01.389: INFO: Created: latency-svc-rc6kk
Mar 23 18:54:01.392: INFO: Got endpoints: latency-svc-7pftf [441.115051ms]
Mar 23 18:54:01.407: INFO: Created: latency-svc-6lvwx
Mar 23 18:54:01.422: INFO: Created: latency-svc-m9kkr
Mar 23 18:54:01.435: INFO: Created: latency-svc-67m7d
Mar 23 18:54:01.436: INFO: Got endpoints: latency-svc-gc4rs [462.795293ms]
Mar 23 18:54:01.449: INFO: Created: latency-svc-b4xn5
Mar 23 18:54:01.501: INFO: Got endpoints: latency-svc-45bfs [213.302295ms]
Mar 23 18:54:01.506: INFO: Created: latency-svc-n82g4
Mar 23 18:54:01.520: INFO: Created: latency-svc-625pp
Mar 23 18:54:01.539: INFO: Created: latency-svc-4c2lg
Mar 23 18:54:01.541: INFO: Got endpoints: latency-svc-f5mkc [554.395133ms]
Mar 23 18:54:01.559: INFO: Created: latency-svc-4pnmx
Mar 23 18:54:01.578: INFO: Created: latency-svc-k5ng9
Mar 23 18:54:01.582: INFO: Got endpoints: latency-svc-fks7j [558.474097ms]
Mar 23 18:54:01.619: INFO: Created: latency-svc-6qllf
Mar 23 18:54:01.629: INFO: Created: latency-svc-nvjjs
Mar 23 18:54:01.642: INFO: Got endpoints: latency-svc-rc6kk [586.59189ms]
Mar 23 18:54:01.646: INFO: Created: latency-svc-49sr6
Mar 23 18:54:01.662: INFO: Created: latency-svc-zzlg7
Mar 23 18:54:01.679: INFO: Created: latency-svc-l9tpx
Mar 23 18:54:01.686: INFO: Got endpoints: latency-svc-6lvwx [605.228852ms]
Mar 23 18:54:01.697: INFO: Created: latency-svc-wx2n7
Mar 23 18:54:01.724: INFO: Created: latency-svc-rdc8v
Mar 23 18:54:01.734: INFO: Got endpoints: latency-svc-m9kkr [624.894355ms]
Mar 23 18:54:01.756: INFO: Created: latency-svc-9ch5c
Mar 23 18:54:01.784: INFO: Got endpoints: latency-svc-67m7d [661.165624ms]
Mar 23 18:54:01.834: INFO: Got endpoints: latency-svc-b4xn5 [678.676069ms]
Mar 23 18:54:01.884: INFO: Got endpoints: latency-svc-n82g4 [716.343832ms]
Mar 23 18:54:01.934: INFO: Got endpoints: latency-svc-625pp [750.773385ms]
Mar 23 18:54:01.984: INFO: Got endpoints: latency-svc-4c2lg [780.394539ms]
Mar 23 18:54:02.034: INFO: Got endpoints: latency-svc-4pnmx [780.984977ms]
Mar 23 18:54:02.089: INFO: Got endpoints: latency-svc-k5ng9 [754.163572ms]
Mar 23 18:54:02.135: INFO: Got endpoints: latency-svc-6qllf [742.954307ms]
Mar 23 18:54:02.189: INFO: Got endpoints: latency-svc-nvjjs [752.212177ms]
Mar 23 18:54:02.237: INFO: Got endpoints: latency-svc-49sr6 [736.448464ms]
Mar 23 18:54:02.289: INFO: Got endpoints: latency-svc-zzlg7 [747.926728ms]
Mar 23 18:54:02.335: INFO: Got endpoints: latency-svc-l9tpx [752.516269ms]
Mar 23 18:54:02.384: INFO: Got endpoints: latency-svc-wx2n7 [740.812077ms]
Mar 23 18:54:02.450: INFO: Got endpoints: latency-svc-rdc8v [763.798973ms]
Mar 23 18:54:02.487: INFO: Got endpoints: latency-svc-9ch5c [751.908153ms]
Mar 23 18:54:02.487: INFO: Latencies: [93.582831ms 137.928093ms 172.662667ms 213.302295ms 236.106378ms 285.997858ms 298.022755ms 307.129519ms 312.594385ms 314.65451ms 316.523885ms 318.906026ms 320.759041ms 322.62221ms 322.786897ms 324.228559ms 324.52287ms 327.172884ms 333.472155ms 334.261912ms 337.96644ms 340.608848ms 341.129125ms 341.493828ms 342.054158ms 343.482968ms 344.685544ms 346.728171ms 346.788593ms 348.650651ms 349.95964ms 352.972709ms 353.103828ms 354.000944ms 354.985617ms 359.571098ms 361.160101ms 361.306993ms 362.597419ms 364.046254ms 364.126963ms 364.791136ms 364.791343ms 368.854225ms 368.990356ms 369.320709ms 370.999292ms 371.017764ms 376.237373ms 377.707381ms 379.916185ms 381.185622ms 384.954387ms 392.721396ms 394.328732ms 395.25367ms 395.350677ms 396.73614ms 397.238372ms 400.203449ms 403.8584ms 405.670557ms 407.222545ms 412.748081ms 417.524165ms 420.08975ms 420.166791ms 422.181698ms 423.86757ms 427.049714ms 427.748626ms 428.192514ms 430.083126ms 430.464138ms 432.867091ms 434.142574ms 438.307173ms 439.737433ms 441.115051ms 441.450748ms 445.32566ms 446.917485ms 447.197124ms 453.541407ms 460.422243ms 462.795293ms 463.301848ms 464.532412ms 468.330262ms 468.907918ms 470.048217ms 471.326287ms 476.454658ms 478.983133ms 482.579157ms 485.135019ms 486.138595ms 490.625395ms 492.449502ms 503.045178ms 504.165152ms 507.284568ms 510.235806ms 511.841891ms 513.841064ms 517.957227ms 518.95868ms 526.885047ms 535.958849ms 538.466067ms 554.395133ms 557.23948ms 558.474097ms 577.703135ms 586.369982ms 586.59189ms 592.854401ms 604.999266ms 605.228852ms 608.673369ms 609.978404ms 624.894355ms 635.447276ms 653.109211ms 654.353612ms 658.681129ms 661.165624ms 671.335406ms 678.676069ms 681.229085ms 688.743752ms 697.71202ms 700.301512ms 709.869365ms 716.125582ms 716.343832ms 731.54141ms 736.448464ms 736.876665ms 740.549423ms 740.812077ms 740.886936ms 741.642209ms 741.812696ms 742.902822ms 742.954307ms 743.719205ms 744.107242ms 744.244042ms 744.271624ms 747.331533ms 747.926728ms 748.383115ms 748.409374ms 748.457943ms 748.775371ms 748.836039ms 750.773385ms 751.908153ms 752.212177ms 752.516269ms 754.163572ms 755.211921ms 758.298902ms 760.394937ms 763.798973ms 771.54378ms 780.394539ms 780.984977ms 783.907218ms 1.067696662s 1.082724612s 1.133235679s 1.155192838s 1.165870972s 1.172279323s 1.18385181s 1.198523894s 1.199816078s 1.209911956s 1.219726887s 1.232143449s 1.241459478s 1.247475984s 1.262516204s 1.268215408s 1.268633075s 1.274592983s 1.297646378s 1.320592371s 1.345418789s 1.356843025s 1.380028513s 1.405185627s 1.419235449s 1.461010277s 1.467224234s 1.473075196s 1.510282509s 1.5132139s]
Mar 23 18:54:02.487: INFO: 50 %ile: 504.165152ms
Mar 23 18:54:02.487: INFO: 90 %ile: 1.219726887s
Mar 23 18:54:02.487: INFO: 99 %ile: 1.510282509s
Mar 23 18:54:02.487: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:54:02.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9537" for this suite.

• [SLOW TEST:10.970 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":12,"skipped":183,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:54:02.530: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-3664
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-3664
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3664
Mar 23 18:54:02.677: INFO: Found 0 stateful pods, waiting for 1
Mar 23 18:54:12.757: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 23 18:54:12.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-3664 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 18:54:13.198: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 18:54:13.199: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 18:54:13.199: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 18:54:13.205: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 23 18:54:23.213: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 18:54:23.213: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 18:54:23.240: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar 23 18:54:23.240: INFO: ss-0  kube17-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:02 +0000 UTC  }]
Mar 23 18:54:23.240: INFO: 
Mar 23 18:54:23.240: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 23 18:54:24.248: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993479077s
Mar 23 18:54:25.259: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985224673s
Mar 23 18:54:26.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.974454364s
Mar 23 18:54:27.966: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.967967017s
Mar 23 18:54:28.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.267990941s
Mar 23 18:54:29.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.260057362s
Mar 23 18:54:30.986: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.253386472s
Mar 23 18:54:31.995: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.247263269s
Mar 23 18:54:33.001: INFO: Verifying statefulset ss doesn't scale past 3 for another 238.512944ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3664
Mar 23 18:54:34.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-3664 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 18:54:34.507: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 18:54:34.507: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 18:54:34.507: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 18:54:34.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-3664 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 18:54:34.925: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 23 18:54:34.925: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 18:54:34.925: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 18:54:34.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-3664 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 18:54:35.418: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 23 18:54:35.418: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 18:54:35.418: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 18:54:35.425: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:54:35.425: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:54:35.425: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 23 18:54:35.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-3664 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 18:54:35.930: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 18:54:35.930: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 18:54:35.930: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 18:54:35.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-3664 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 18:54:36.419: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 18:54:36.419: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 18:54:36.419: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 18:54:36.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-3664 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 18:54:36.905: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 18:54:36.905: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 18:54:36.905: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 18:54:36.905: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 18:54:36.935: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 18:54:36.935: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 18:54:36.935: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 18:54:36.953: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar 23 18:54:36.953: INFO: ss-0  kube17-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:02 +0000 UTC  }]
Mar 23 18:54:36.953: INFO: ss-1  kube17-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  }]
Mar 23 18:54:36.953: INFO: ss-2  kube17-worker-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  }]
Mar 23 18:54:36.953: INFO: 
Mar 23 18:54:36.953: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 23 18:54:37.960: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar 23 18:54:37.960: INFO: ss-0  kube17-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:02 +0000 UTC  }]
Mar 23 18:54:37.961: INFO: ss-1  kube17-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  }]
Mar 23 18:54:37.961: INFO: ss-2  kube17-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  }]
Mar 23 18:54:37.961: INFO: 
Mar 23 18:54:37.961: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 23 18:54:38.979: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar 23 18:54:38.979: INFO: ss-1  kube17-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  }]
Mar 23 18:54:38.979: INFO: ss-2  kube17-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  }]
Mar 23 18:54:38.979: INFO: 
Mar 23 18:54:38.979: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 23 18:54:40.024: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar 23 18:54:40.024: INFO: ss-2  kube17-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  }]
Mar 23 18:54:40.024: INFO: 
Mar 23 18:54:40.024: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 23 18:54:41.031: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar 23 18:54:41.031: INFO: ss-2  kube17-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  }]
Mar 23 18:54:41.031: INFO: 
Mar 23 18:54:41.031: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 23 18:54:42.038: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar 23 18:54:42.039: INFO: ss-2  kube17-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  }]
Mar 23 18:54:42.039: INFO: 
Mar 23 18:54:42.039: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 23 18:54:44.598: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar 23 18:54:44.599: INFO: ss-2  kube17-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  }]
Mar 23 18:54:44.599: INFO: 
Mar 23 18:54:44.599: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 23 18:54:48.027: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar 23 18:54:48.027: INFO: ss-2  kube17-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 18:54:23 +0000 UTC  }]
Mar 23 18:54:48.027: INFO: 
Mar 23 18:54:48.027: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3664
Mar 23 18:54:49.033: INFO: Scaling statefulset ss to 0
Mar 23 18:54:49.048: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar 23 18:54:49.053: INFO: Deleting all statefulset in ns statefulset-3664
Mar 23 18:54:49.059: INFO: Scaling statefulset ss to 0
Mar 23 18:54:49.072: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 18:54:49.077: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:54:49.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3664" for this suite.

• [SLOW TEST:46.611 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":13,"skipped":192,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:54:49.144: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1585
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 18:54:49.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-1746'
Mar 23 18:54:49.468: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 23 18:54:49.468: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Mar 23 18:54:49.497: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Mar 23 18:54:49.500: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar 23 18:54:49.511: INFO: scanned /root for discovery docs: <nil>
Mar 23 18:54:49.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1746'
Mar 23 18:55:05.509: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 23 18:55:05.509: INFO: stdout: "Created e2e-test-httpd-rc-6717fffda1f3d0c48bc5bf3ca15e24fe\nScaling up e2e-test-httpd-rc-6717fffda1f3d0c48bc5bf3ca15e24fe from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-6717fffda1f3d0c48bc5bf3ca15e24fe up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-6717fffda1f3d0c48bc5bf3ca15e24fe to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Mar 23 18:55:05.509: INFO: stdout: "Created e2e-test-httpd-rc-6717fffda1f3d0c48bc5bf3ca15e24fe\nScaling up e2e-test-httpd-rc-6717fffda1f3d0c48bc5bf3ca15e24fe from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-6717fffda1f3d0c48bc5bf3ca15e24fe up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-6717fffda1f3d0c48bc5bf3ca15e24fe to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Mar 23 18:55:05.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-1746'
Mar 23 18:55:05.734: INFO: stderr: ""
Mar 23 18:55:05.734: INFO: stdout: "e2e-test-httpd-rc-6717fffda1f3d0c48bc5bf3ca15e24fe-8w2rh "
Mar 23 18:55:05.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods e2e-test-httpd-rc-6717fffda1f3d0c48bc5bf3ca15e24fe-8w2rh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1746'
Mar 23 18:55:05.914: INFO: stderr: ""
Mar 23 18:55:05.914: INFO: stdout: "true"
Mar 23 18:55:05.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods e2e-test-httpd-rc-6717fffda1f3d0c48bc5bf3ca15e24fe-8w2rh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1746'
Mar 23 18:55:06.107: INFO: stderr: ""
Mar 23 18:55:06.107: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Mar 23 18:55:06.107: INFO: e2e-test-httpd-rc-6717fffda1f3d0c48bc5bf3ca15e24fe-8w2rh is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
Mar 23 18:55:06.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete rc e2e-test-httpd-rc --namespace=kubectl-1746'
Mar 23 18:55:06.347: INFO: stderr: ""
Mar 23 18:55:06.347: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:55:06.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1746" for this suite.

• [SLOW TEST:17.233 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1580
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":280,"completed":14,"skipped":205,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:55:06.377: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-3036fb23-0496-4db9-84c2-4f6445538d81
STEP: Creating a pod to test consume configMaps
Mar 23 18:55:06.493: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ec283f19-edb7-4e58-8f63-8f3fa747b647" in namespace "projected-4442" to be "success or failure"
Mar 23 18:55:06.502: INFO: Pod "pod-projected-configmaps-ec283f19-edb7-4e58-8f63-8f3fa747b647": Phase="Pending", Reason="", readiness=false. Elapsed: 8.344075ms
Mar 23 18:55:08.506: INFO: Pod "pod-projected-configmaps-ec283f19-edb7-4e58-8f63-8f3fa747b647": Phase="Running", Reason="", readiness=true. Elapsed: 2.013015878s
Mar 23 18:55:10.514: INFO: Pod "pod-projected-configmaps-ec283f19-edb7-4e58-8f63-8f3fa747b647": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020689096s
STEP: Saw pod success
Mar 23 18:55:10.514: INFO: Pod "pod-projected-configmaps-ec283f19-edb7-4e58-8f63-8f3fa747b647" satisfied condition "success or failure"
Mar 23 18:55:10.522: INFO: Trying to get logs from node kube17-worker-1 pod pod-projected-configmaps-ec283f19-edb7-4e58-8f63-8f3fa747b647 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 18:55:10.566: INFO: Waiting for pod pod-projected-configmaps-ec283f19-edb7-4e58-8f63-8f3fa747b647 to disappear
Mar 23 18:55:10.570: INFO: Pod pod-projected-configmaps-ec283f19-edb7-4e58-8f63-8f3fa747b647 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:55:10.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4442" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":15,"skipped":211,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:55:10.625: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:55:26.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9689" for this suite.
STEP: Destroying namespace "nsdeletetest-5895" for this suite.
Mar 23 18:55:26.959: INFO: Namespace nsdeletetest-5895 was already deleted
STEP: Destroying namespace "nsdeletetest-9913" for this suite.

• [SLOW TEST:16.341 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":16,"skipped":230,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:55:26.969: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar 23 18:55:27.039: INFO: Waiting up to 5m0s for pod "downward-api-5256f5ca-4de9-4854-bd8c-4a0cdbb1f448" in namespace "downward-api-6204" to be "success or failure"
Mar 23 18:55:27.046: INFO: Pod "downward-api-5256f5ca-4de9-4854-bd8c-4a0cdbb1f448": Phase="Pending", Reason="", readiness=false. Elapsed: 7.413754ms
Mar 23 18:55:29.052: INFO: Pod "downward-api-5256f5ca-4de9-4854-bd8c-4a0cdbb1f448": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0128367s
Mar 23 18:55:31.058: INFO: Pod "downward-api-5256f5ca-4de9-4854-bd8c-4a0cdbb1f448": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019200963s
Mar 23 18:55:33.064: INFO: Pod "downward-api-5256f5ca-4de9-4854-bd8c-4a0cdbb1f448": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025376359s
STEP: Saw pod success
Mar 23 18:55:33.065: INFO: Pod "downward-api-5256f5ca-4de9-4854-bd8c-4a0cdbb1f448" satisfied condition "success or failure"
Mar 23 18:55:33.069: INFO: Trying to get logs from node kube17-worker-1 pod downward-api-5256f5ca-4de9-4854-bd8c-4a0cdbb1f448 container dapi-container: <nil>
STEP: delete the pod
Mar 23 18:55:33.119: INFO: Waiting for pod downward-api-5256f5ca-4de9-4854-bd8c-4a0cdbb1f448 to disappear
Mar 23 18:55:33.124: INFO: Pod downward-api-5256f5ca-4de9-4854-bd8c-4a0cdbb1f448 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:55:33.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6204" for this suite.

• [SLOW TEST:6.169 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":17,"skipped":304,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:55:33.140: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7847
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7847
STEP: creating replication controller externalsvc in namespace services-7847
I0323 18:55:33.961956      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-7847, replica count: 2
I0323 18:55:37.014641      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 18:55:40.014989      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 23 18:55:40.056: INFO: Creating new exec pod
Mar 23 18:55:42.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=services-7847 execpod9qxrz -- /bin/sh -x -c nslookup nodeport-service'
Mar 23 18:55:42.533: INFO: stderr: "+ nslookup nodeport-service\n"
Mar 23 18:55:42.533: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-7847.svc.cluster.local\tcanonical name = externalsvc.services-7847.svc.cluster.local.\nName:\texternalsvc.services-7847.svc.cluster.local\nAddress: 10.101.197.110\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7847, will wait for the garbage collector to delete the pods
Mar 23 18:55:42.609: INFO: Deleting ReplicationController externalsvc took: 21.167832ms
Mar 23 18:55:44.910: INFO: Terminating ReplicationController externalsvc pods took: 2.301025032s
Mar 23 18:55:58.389: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:55:58.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7847" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:25.366 seconds]
[sig-network] Services
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":18,"skipped":314,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:55:58.507: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 23 18:55:58.614: INFO: Waiting up to 5m0s for pod "pod-3b5743f8-416f-4431-9546-18700addcbfc" in namespace "emptydir-827" to be "success or failure"
Mar 23 18:55:58.627: INFO: Pod "pod-3b5743f8-416f-4431-9546-18700addcbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.046659ms
Mar 23 18:56:00.634: INFO: Pod "pod-3b5743f8-416f-4431-9546-18700addcbfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.019530151s
Mar 23 18:56:02.640: INFO: Pod "pod-3b5743f8-416f-4431-9546-18700addcbfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02551934s
STEP: Saw pod success
Mar 23 18:56:02.640: INFO: Pod "pod-3b5743f8-416f-4431-9546-18700addcbfc" satisfied condition "success or failure"
Mar 23 18:56:02.644: INFO: Trying to get logs from node kube17-worker-2 pod pod-3b5743f8-416f-4431-9546-18700addcbfc container test-container: <nil>
STEP: delete the pod
Mar 23 18:56:02.933: INFO: Waiting for pod pod-3b5743f8-416f-4431-9546-18700addcbfc to disappear
Mar 23 18:56:02.942: INFO: Pod pod-3b5743f8-416f-4431-9546-18700addcbfc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:56:02.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-827" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":19,"skipped":334,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:56:02.977: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-j886
STEP: Creating a pod to test atomic-volume-subpath
Mar 23 18:56:03.050: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j886" in namespace "subpath-787" to be "success or failure"
Mar 23 18:56:03.057: INFO: Pod "pod-subpath-test-configmap-j886": Phase="Pending", Reason="", readiness=false. Elapsed: 6.873497ms
Mar 23 18:56:06.086: INFO: Pod "pod-subpath-test-configmap-j886": Phase="Pending", Reason="", readiness=false. Elapsed: 3.036675427s
Mar 23 18:56:08.096: INFO: Pod "pod-subpath-test-configmap-j886": Phase="Running", Reason="", readiness=true. Elapsed: 5.046334694s
Mar 23 18:56:10.102: INFO: Pod "pod-subpath-test-configmap-j886": Phase="Running", Reason="", readiness=true. Elapsed: 7.051924354s
Mar 23 18:56:12.108: INFO: Pod "pod-subpath-test-configmap-j886": Phase="Running", Reason="", readiness=true. Elapsed: 9.058169194s
Mar 23 18:56:14.128: INFO: Pod "pod-subpath-test-configmap-j886": Phase="Running", Reason="", readiness=true. Elapsed: 11.077830449s
Mar 23 18:56:16.133: INFO: Pod "pod-subpath-test-configmap-j886": Phase="Running", Reason="", readiness=true. Elapsed: 13.083538188s
Mar 23 18:56:18.139: INFO: Pod "pod-subpath-test-configmap-j886": Phase="Running", Reason="", readiness=true. Elapsed: 15.089706308s
Mar 23 18:56:20.146: INFO: Pod "pod-subpath-test-configmap-j886": Phase="Running", Reason="", readiness=true. Elapsed: 17.096319759s
Mar 23 18:56:22.153: INFO: Pod "pod-subpath-test-configmap-j886": Phase="Running", Reason="", readiness=true. Elapsed: 19.103224251s
Mar 23 18:56:24.163: INFO: Pod "pod-subpath-test-configmap-j886": Phase="Running", Reason="", readiness=true. Elapsed: 21.113626791s
Mar 23 18:56:26.169: INFO: Pod "pod-subpath-test-configmap-j886": Phase="Running", Reason="", readiness=true. Elapsed: 23.119415231s
Mar 23 18:56:28.175: INFO: Pod "pod-subpath-test-configmap-j886": Phase="Succeeded", Reason="", readiness=false. Elapsed: 25.125657525s
STEP: Saw pod success
Mar 23 18:56:28.175: INFO: Pod "pod-subpath-test-configmap-j886" satisfied condition "success or failure"
Mar 23 18:56:28.180: INFO: Trying to get logs from node kube17-worker-2 pod pod-subpath-test-configmap-j886 container test-container-subpath-configmap-j886: <nil>
STEP: delete the pod
Mar 23 18:56:28.224: INFO: Waiting for pod pod-subpath-test-configmap-j886 to disappear
Mar 23 18:56:28.230: INFO: Pod pod-subpath-test-configmap-j886 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-j886
Mar 23 18:56:28.230: INFO: Deleting pod "pod-subpath-test-configmap-j886" in namespace "subpath-787"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:56:28.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-787" for this suite.

• [SLOW TEST:25.283 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":20,"skipped":345,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:56:28.267: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 23 18:56:28.367: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 23 18:56:33.374: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:56:34.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6742" for this suite.

• [SLOW TEST:6.164 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":21,"skipped":357,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:56:34.432: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 23 18:56:42.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 18:56:42.010: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 18:56:44.010: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 18:56:44.015: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 18:56:46.010: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 18:56:46.016: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 18:56:48.010: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 18:56:48.017: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 18:56:50.010: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 18:56:50.017: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 18:56:52.010: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 18:56:52.024: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:56:52.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6298" for this suite.

• [SLOW TEST:17.627 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":22,"skipped":378,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:56:52.060: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 18:56:52.824: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 23 18:56:54.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720586612, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720586612, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720586612, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720586612, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 18:56:58.000: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:56:58.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8732" for this suite.
STEP: Destroying namespace "webhook-8732-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.353 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":23,"skipped":386,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:56:58.414: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 18:56:58.501: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:57:04.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1870" for this suite.

• [SLOW TEST:7.075 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":24,"skipped":425,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:57:05.492: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-af8952c1-764a-4e65-ba61-21de7704424b
STEP: Creating a pod to test consume configMaps
Mar 23 18:57:05.750: INFO: Waiting up to 5m0s for pod "pod-configmaps-88e6a698-4319-41d2-a7bd-09b7d8e038e1" in namespace "configmap-7629" to be "success or failure"
Mar 23 18:57:05.761: INFO: Pod "pod-configmaps-88e6a698-4319-41d2-a7bd-09b7d8e038e1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.716333ms
Mar 23 18:57:07.764: INFO: Pod "pod-configmaps-88e6a698-4319-41d2-a7bd-09b7d8e038e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013689599s
Mar 23 18:57:09.809: INFO: Pod "pod-configmaps-88e6a698-4319-41d2-a7bd-09b7d8e038e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059354182s
STEP: Saw pod success
Mar 23 18:57:09.810: INFO: Pod "pod-configmaps-88e6a698-4319-41d2-a7bd-09b7d8e038e1" satisfied condition "success or failure"
Mar 23 18:57:09.828: INFO: Trying to get logs from node kube17-worker-2 pod pod-configmaps-88e6a698-4319-41d2-a7bd-09b7d8e038e1 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 18:57:09.897: INFO: Waiting for pod pod-configmaps-88e6a698-4319-41d2-a7bd-09b7d8e038e1 to disappear
Mar 23 18:57:09.916: INFO: Pod pod-configmaps-88e6a698-4319-41d2-a7bd-09b7d8e038e1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:57:09.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7629" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":25,"skipped":444,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:57:09.938: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Mar 23 18:57:10.013: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:57:41.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4158" for this suite.

• [SLOW TEST:31.396 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":26,"skipped":458,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:57:41.338: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Mar 23 18:57:41.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-1919'
Mar 23 18:57:41.864: INFO: stderr: ""
Mar 23 18:57:41.865: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 23 18:57:41.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1919'
Mar 23 18:57:42.007: INFO: stderr: ""
Mar 23 18:57:42.007: INFO: stdout: "update-demo-nautilus-l5lg7 update-demo-nautilus-lxjc6 "
Mar 23 18:57:42.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-l5lg7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1919'
Mar 23 18:57:42.135: INFO: stderr: ""
Mar 23 18:57:42.135: INFO: stdout: ""
Mar 23 18:57:42.135: INFO: update-demo-nautilus-l5lg7 is created but not running
Mar 23 18:57:47.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1919'
Mar 23 18:57:47.333: INFO: stderr: ""
Mar 23 18:57:47.333: INFO: stdout: "update-demo-nautilus-l5lg7 update-demo-nautilus-lxjc6 "
Mar 23 18:57:47.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-l5lg7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1919'
Mar 23 18:57:47.532: INFO: stderr: ""
Mar 23 18:57:47.532: INFO: stdout: "true"
Mar 23 18:57:47.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-l5lg7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1919'
Mar 23 18:57:47.736: INFO: stderr: ""
Mar 23 18:57:47.736: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 18:57:47.736: INFO: validating pod update-demo-nautilus-l5lg7
Mar 23 18:57:47.745: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 18:57:47.746: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 18:57:47.746: INFO: update-demo-nautilus-l5lg7 is verified up and running
Mar 23 18:57:47.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-lxjc6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1919'
Mar 23 18:57:47.952: INFO: stderr: ""
Mar 23 18:57:47.952: INFO: stdout: "true"
Mar 23 18:57:47.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-lxjc6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1919'
Mar 23 18:57:48.170: INFO: stderr: ""
Mar 23 18:57:48.170: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 18:57:48.170: INFO: validating pod update-demo-nautilus-lxjc6
Mar 23 18:57:48.179: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 18:57:48.179: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 18:57:48.179: INFO: update-demo-nautilus-lxjc6 is verified up and running
STEP: scaling down the replication controller
Mar 23 18:57:48.184: INFO: scanned /root for discovery docs: <nil>
Mar 23 18:57:48.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1919'
Mar 23 18:57:49.453: INFO: stderr: ""
Mar 23 18:57:49.453: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 23 18:57:49.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1919'
Mar 23 18:57:49.663: INFO: stderr: ""
Mar 23 18:57:49.663: INFO: stdout: "update-demo-nautilus-l5lg7 update-demo-nautilus-lxjc6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 23 18:57:54.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1919'
Mar 23 18:57:54.865: INFO: stderr: ""
Mar 23 18:57:54.865: INFO: stdout: "update-demo-nautilus-l5lg7 update-demo-nautilus-lxjc6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 23 18:57:59.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1919'
Mar 23 18:58:00.006: INFO: stderr: ""
Mar 23 18:58:00.006: INFO: stdout: "update-demo-nautilus-l5lg7 update-demo-nautilus-lxjc6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 23 18:58:05.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1919'
Mar 23 18:58:05.136: INFO: stderr: ""
Mar 23 18:58:05.136: INFO: stdout: "update-demo-nautilus-l5lg7 "
Mar 23 18:58:05.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-l5lg7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1919'
Mar 23 18:58:05.259: INFO: stderr: ""
Mar 23 18:58:05.259: INFO: stdout: "true"
Mar 23 18:58:05.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-l5lg7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1919'
Mar 23 18:58:05.382: INFO: stderr: ""
Mar 23 18:58:05.382: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 18:58:05.382: INFO: validating pod update-demo-nautilus-l5lg7
Mar 23 18:58:05.387: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 18:58:05.387: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 18:58:05.387: INFO: update-demo-nautilus-l5lg7 is verified up and running
STEP: scaling up the replication controller
Mar 23 18:58:05.390: INFO: scanned /root for discovery docs: <nil>
Mar 23 18:58:05.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1919'
Mar 23 18:58:07.015: INFO: stderr: ""
Mar 23 18:58:07.015: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 23 18:58:07.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1919'
Mar 23 18:58:07.209: INFO: stderr: ""
Mar 23 18:58:07.209: INFO: stdout: "update-demo-nautilus-9c7dv update-demo-nautilus-l5lg7 "
Mar 23 18:58:07.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-9c7dv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1919'
Mar 23 18:58:07.399: INFO: stderr: ""
Mar 23 18:58:07.399: INFO: stdout: ""
Mar 23 18:58:07.399: INFO: update-demo-nautilus-9c7dv is created but not running
Mar 23 18:58:12.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1919'
Mar 23 18:58:12.562: INFO: stderr: ""
Mar 23 18:58:12.562: INFO: stdout: "update-demo-nautilus-9c7dv update-demo-nautilus-l5lg7 "
Mar 23 18:58:12.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-9c7dv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1919'
Mar 23 18:58:12.677: INFO: stderr: ""
Mar 23 18:58:12.677: INFO: stdout: "true"
Mar 23 18:58:12.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-9c7dv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1919'
Mar 23 18:58:12.808: INFO: stderr: ""
Mar 23 18:58:12.808: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 18:58:12.808: INFO: validating pod update-demo-nautilus-9c7dv
Mar 23 18:58:12.815: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 18:58:12.815: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 18:58:12.815: INFO: update-demo-nautilus-9c7dv is verified up and running
Mar 23 18:58:12.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-l5lg7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1919'
Mar 23 18:58:12.935: INFO: stderr: ""
Mar 23 18:58:12.935: INFO: stdout: "true"
Mar 23 18:58:12.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-l5lg7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1919'
Mar 23 18:58:13.053: INFO: stderr: ""
Mar 23 18:58:13.053: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 18:58:13.053: INFO: validating pod update-demo-nautilus-l5lg7
Mar 23 18:58:13.059: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 18:58:13.059: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 18:58:13.059: INFO: update-demo-nautilus-l5lg7 is verified up and running
STEP: using delete to clean up resources
Mar 23 18:58:13.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete --grace-period=0 --force -f - --namespace=kubectl-1919'
Mar 23 18:58:13.209: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 18:58:13.210: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 23 18:58:13.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1919'
Mar 23 18:58:13.354: INFO: stderr: "No resources found in kubectl-1919 namespace.\n"
Mar 23 18:58:13.354: INFO: stdout: ""
Mar 23 18:58:13.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -l name=update-demo --namespace=kubectl-1919 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 23 18:58:13.489: INFO: stderr: ""
Mar 23 18:58:13.489: INFO: stdout: "update-demo-nautilus-9c7dv\nupdate-demo-nautilus-l5lg7\n"
Mar 23 18:58:13.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1919'
Mar 23 18:58:14.150: INFO: stderr: "No resources found in kubectl-1919 namespace.\n"
Mar 23 18:58:14.150: INFO: stdout: ""
Mar 23 18:58:14.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -l name=update-demo --namespace=kubectl-1919 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 23 18:58:14.275: INFO: stderr: ""
Mar 23 18:58:14.276: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:58:14.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1919" for this suite.

• [SLOW TEST:32.953 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":27,"skipped":463,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:58:14.291: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Mar 23 18:58:14.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 api-versions'
Mar 23 18:58:14.471: INFO: stderr: ""
Mar 23 18:58:14.471: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\nopenebs.io/v1alpha1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvolumesnapshot.external-storage.k8s.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:58:14.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2484" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":28,"skipped":475,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:58:14.488: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-01f6e554-98bf-44ee-8422-bb6f639a397d
STEP: Creating a pod to test consume configMaps
Mar 23 18:58:14.577: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39264174-7cf1-454e-8157-97ff151169a5" in namespace "projected-8888" to be "success or failure"
Mar 23 18:58:14.592: INFO: Pod "pod-projected-configmaps-39264174-7cf1-454e-8157-97ff151169a5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.763687ms
Mar 23 18:58:16.597: INFO: Pod "pod-projected-configmaps-39264174-7cf1-454e-8157-97ff151169a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020355862s
Mar 23 18:58:18.608: INFO: Pod "pod-projected-configmaps-39264174-7cf1-454e-8157-97ff151169a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031361338s
STEP: Saw pod success
Mar 23 18:58:18.608: INFO: Pod "pod-projected-configmaps-39264174-7cf1-454e-8157-97ff151169a5" satisfied condition "success or failure"
Mar 23 18:58:18.618: INFO: Trying to get logs from node kube17-worker-1 pod pod-projected-configmaps-39264174-7cf1-454e-8157-97ff151169a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 18:58:18.668: INFO: Waiting for pod pod-projected-configmaps-39264174-7cf1-454e-8157-97ff151169a5 to disappear
Mar 23 18:58:18.675: INFO: Pod pod-projected-configmaps-39264174-7cf1-454e-8157-97ff151169a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:58:18.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8888" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":29,"skipped":477,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:58:18.704: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-4c7a6a64-977d-4c75-84d2-2be00d06a3cf
STEP: Creating a pod to test consume configMaps
Mar 23 18:58:18.836: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0b8da274-4906-48af-84ae-08d295ed13d9" in namespace "projected-7646" to be "success or failure"
Mar 23 18:58:18.846: INFO: Pod "pod-projected-configmaps-0b8da274-4906-48af-84ae-08d295ed13d9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.848187ms
Mar 23 18:58:21.427: INFO: Pod "pod-projected-configmaps-0b8da274-4906-48af-84ae-08d295ed13d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.590577991s
Mar 23 18:58:23.432: INFO: Pod "pod-projected-configmaps-0b8da274-4906-48af-84ae-08d295ed13d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.595663943s
STEP: Saw pod success
Mar 23 18:58:23.432: INFO: Pod "pod-projected-configmaps-0b8da274-4906-48af-84ae-08d295ed13d9" satisfied condition "success or failure"
Mar 23 18:58:23.436: INFO: Trying to get logs from node kube17-worker-1 pod pod-projected-configmaps-0b8da274-4906-48af-84ae-08d295ed13d9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 18:58:23.499: INFO: Waiting for pod pod-projected-configmaps-0b8da274-4906-48af-84ae-08d295ed13d9 to disappear
Mar 23 18:58:23.503: INFO: Pod pod-projected-configmaps-0b8da274-4906-48af-84ae-08d295ed13d9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:58:23.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7646" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":30,"skipped":496,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:58:23.534: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-6d92becc-b0f6-4046-be63-270cc5f1a858
STEP: Creating a pod to test consume configMaps
Mar 23 18:58:23.729: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-75435c29-c823-4b7c-8331-6b9c5679568d" in namespace "projected-1589" to be "success or failure"
Mar 23 18:58:23.756: INFO: Pod "pod-projected-configmaps-75435c29-c823-4b7c-8331-6b9c5679568d": Phase="Pending", Reason="", readiness=false. Elapsed: 26.664306ms
Mar 23 18:58:25.773: INFO: Pod "pod-projected-configmaps-75435c29-c823-4b7c-8331-6b9c5679568d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043609589s
STEP: Saw pod success
Mar 23 18:58:25.773: INFO: Pod "pod-projected-configmaps-75435c29-c823-4b7c-8331-6b9c5679568d" satisfied condition "success or failure"
Mar 23 18:58:25.777: INFO: Trying to get logs from node kube17-worker-1 pod pod-projected-configmaps-75435c29-c823-4b7c-8331-6b9c5679568d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 18:58:25.836: INFO: Waiting for pod pod-projected-configmaps-75435c29-c823-4b7c-8331-6b9c5679568d to disappear
Mar 23 18:58:25.844: INFO: Pod pod-projected-configmaps-75435c29-c823-4b7c-8331-6b9c5679568d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:58:25.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1589" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":31,"skipped":502,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:58:25.894: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:58:30.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9213" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":32,"skipped":538,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:58:30.055: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar 23 18:59:10.315: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:59:10.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8850" for this suite.

• [SLOW TEST:40.277 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":33,"skipped":550,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:59:10.334: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Mar 23 18:59:10.407: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 18:59:14.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9051" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":34,"skipped":552,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 18:59:14.984: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-1984
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Mar 23 18:59:15.112: INFO: Found 0 stateful pods, waiting for 3
Mar 23 18:59:25.135: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:59:25.135: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:59:25.135: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 23 18:59:35.119: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:59:35.119: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:59:35.119: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:59:35.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1984 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 18:59:37.320: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 18:59:37.320: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 18:59:37.320: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 23 18:59:47.396: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 23 19:00:00.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1984 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 19:00:00.590: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 19:00:00.590: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 19:00:00.590: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 19:00:10.639: INFO: Waiting for StatefulSet statefulset-1984/ss2 to complete update
Mar 23 19:00:10.639: INFO: Waiting for Pod statefulset-1984/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 23 19:00:10.639: INFO: Waiting for Pod statefulset-1984/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 23 19:00:10.640: INFO: Waiting for Pod statefulset-1984/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 23 19:00:22.009: INFO: Waiting for StatefulSet statefulset-1984/ss2 to complete update
Mar 23 19:00:22.009: INFO: Waiting for Pod statefulset-1984/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 23 19:00:22.009: INFO: Waiting for Pod statefulset-1984/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 23 19:00:31.810: INFO: Waiting for StatefulSet statefulset-1984/ss2 to complete update
Mar 23 19:00:31.810: INFO: Waiting for Pod statefulset-1984/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 23 19:00:31.810: INFO: Waiting for Pod statefulset-1984/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 23 19:00:40.689: INFO: Waiting for StatefulSet statefulset-1984/ss2 to complete update
Mar 23 19:00:40.689: INFO: Waiting for Pod statefulset-1984/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 23 19:00:50.650: INFO: Waiting for StatefulSet statefulset-1984/ss2 to complete update
Mar 23 19:00:50.650: INFO: Waiting for Pod statefulset-1984/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 23 19:01:00.693: INFO: Waiting for StatefulSet statefulset-1984/ss2 to complete update
Mar 23 19:01:00.693: INFO: Waiting for Pod statefulset-1984/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 23 19:01:10.651: INFO: Waiting for StatefulSet statefulset-1984/ss2 to complete update
STEP: Rolling back to a previous revision
Mar 23 19:01:20.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1984 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 19:01:22.860: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 19:01:22.860: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 19:01:22.860: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 19:01:32.905: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 23 19:01:45.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1984 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 19:01:45.890: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 19:01:45.890: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 19:01:45.890: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 19:01:45.956: INFO: Waiting for StatefulSet statefulset-1984/ss2 to complete update
Mar 23 19:01:45.956: INFO: Waiting for Pod statefulset-1984/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 23 19:01:45.956: INFO: Waiting for Pod statefulset-1984/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 23 19:01:45.956: INFO: Waiting for Pod statefulset-1984/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 23 19:01:58.008: INFO: Waiting for StatefulSet statefulset-1984/ss2 to complete update
Mar 23 19:01:58.008: INFO: Waiting for Pod statefulset-1984/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 23 19:01:58.008: INFO: Waiting for Pod statefulset-1984/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 23 19:01:58.008: INFO: Waiting for Pod statefulset-1984/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 23 19:02:05.967: INFO: Waiting for StatefulSet statefulset-1984/ss2 to complete update
Mar 23 19:02:05.967: INFO: Waiting for Pod statefulset-1984/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 23 19:02:05.967: INFO: Waiting for Pod statefulset-1984/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 23 19:02:15.967: INFO: Waiting for StatefulSet statefulset-1984/ss2 to complete update
Mar 23 19:02:15.968: INFO: Waiting for Pod statefulset-1984/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar 23 19:02:25.968: INFO: Deleting all statefulset in ns statefulset-1984
Mar 23 19:02:25.973: INFO: Scaling statefulset ss2 to 0
Mar 23 19:02:56.018: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 19:02:56.027: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:02:56.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1984" for this suite.

• [SLOW TEST:221.173 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":35,"skipped":566,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:02:56.162: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:02:56.233: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 23 19:02:56.244: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 23 19:03:01.252: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 23 19:03:01.252: INFO: Creating deployment "test-rolling-update-deployment"
Mar 23 19:03:01.263: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 23 19:03:01.277: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 23 19:03:03.304: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 23 19:03:03.310: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720586981, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720586981, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720586981, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720586981, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:03:05.317: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar 23 19:03:05.354: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-660 /apis/apps/v1/namespaces/deployment-660/deployments/test-rolling-update-deployment aa09e2d8-c4ed-4866-a0dc-28c1148024c0 13289 1 2020-03-23 19:03:01 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004b6f308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-23 19:03:01 +0000 UTC,LastTransitionTime:2020-03-23 19:03:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-03-23 19:03:03 +0000 UTC,LastTransitionTime:2020-03-23 19:03:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 23 19:03:05.365: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-660 /apis/apps/v1/namespaces/deployment-660/replicasets/test-rolling-update-deployment-67cf4f6444 9b3326c5-ab2c-4baa-8225-dd3eb5b11ec4 13278 1 2020-03-23 19:03:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment aa09e2d8-c4ed-4866-a0dc-28c1148024c0 0xc004afcce7 0xc004afcce8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004afcd58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 23 19:03:05.365: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 23 19:03:05.365: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-660 /apis/apps/v1/namespaces/deployment-660/replicasets/test-rolling-update-controller 9858e667-44d5-436f-873b-90dbffc13ec2 13288 2 2020-03-23 19:02:56 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment aa09e2d8-c4ed-4866-a0dc-28c1148024c0 0xc004afcc17 0xc004afcc18}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004afcc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 19:03:05.371: INFO: Pod "test-rolling-update-deployment-67cf4f6444-t2rzl" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-t2rzl test-rolling-update-deployment-67cf4f6444- deployment-660 /api/v1/namespaces/deployment-660/pods/test-rolling-update-deployment-67cf4f6444-t2rzl 81ff91e1-4c4b-404a-a641-381ac2a986e3 13277 0 2020-03-23 19:03:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 9b3326c5-ab2c-4baa-8225-dd3eb5b11ec4 0xc004afd227 0xc004afd228}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9lt8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9lt8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9lt8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:03:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:03:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:03:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:03:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.153,PodIP:10.44.0.3,StartTime:2020-03-23 19:03:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 19:03:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://806f2c890046a14c54939dd2f682e59297e9fee86ca028ab56fb06bd37a51a11,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.44.0.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:03:05.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-660" for this suite.

• [SLOW TEST:9.227 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":36,"skipped":600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:03:05.390: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 19:03:05.479: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83e20029-0c9a-42d6-b133-df1fa5b9911c" in namespace "downward-api-9196" to be "success or failure"
Mar 23 19:03:05.491: INFO: Pod "downwardapi-volume-83e20029-0c9a-42d6-b133-df1fa5b9911c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.553345ms
Mar 23 19:03:07.511: INFO: Pod "downwardapi-volume-83e20029-0c9a-42d6-b133-df1fa5b9911c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031011761s
STEP: Saw pod success
Mar 23 19:03:07.511: INFO: Pod "downwardapi-volume-83e20029-0c9a-42d6-b133-df1fa5b9911c" satisfied condition "success or failure"
Mar 23 19:03:07.521: INFO: Trying to get logs from node kube17-worker-2 pod downwardapi-volume-83e20029-0c9a-42d6-b133-df1fa5b9911c container client-container: <nil>
STEP: delete the pod
Mar 23 19:03:07.588: INFO: Waiting for pod downwardapi-volume-83e20029-0c9a-42d6-b133-df1fa5b9911c to disappear
Mar 23 19:03:07.600: INFO: Pod downwardapi-volume-83e20029-0c9a-42d6-b133-df1fa5b9911c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:03:07.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9196" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":37,"skipped":628,"failed":0}
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:03:07.626: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-74f96fc3-fbfc-4a9d-8ab5-9d791e40bdb9
STEP: Creating a pod to test consume secrets
Mar 23 19:03:07.753: INFO: Waiting up to 5m0s for pod "pod-secrets-23f5e2c9-f3ed-4db3-b8c9-34ea5ef373bc" in namespace "secrets-2831" to be "success or failure"
Mar 23 19:03:07.761: INFO: Pod "pod-secrets-23f5e2c9-f3ed-4db3-b8c9-34ea5ef373bc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.703568ms
Mar 23 19:03:10.384: INFO: Pod "pod-secrets-23f5e2c9-f3ed-4db3-b8c9-34ea5ef373bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.631105668s
Mar 23 19:03:12.392: INFO: Pod "pod-secrets-23f5e2c9-f3ed-4db3-b8c9-34ea5ef373bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.639036948s
STEP: Saw pod success
Mar 23 19:03:12.393: INFO: Pod "pod-secrets-23f5e2c9-f3ed-4db3-b8c9-34ea5ef373bc" satisfied condition "success or failure"
Mar 23 19:03:12.397: INFO: Trying to get logs from node kube17-worker-2 pod pod-secrets-23f5e2c9-f3ed-4db3-b8c9-34ea5ef373bc container secret-env-test: <nil>
STEP: delete the pod
Mar 23 19:03:12.450: INFO: Waiting for pod pod-secrets-23f5e2c9-f3ed-4db3-b8c9-34ea5ef373bc to disappear
Mar 23 19:03:12.458: INFO: Pod pod-secrets-23f5e2c9-f3ed-4db3-b8c9-34ea5ef373bc no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:03:12.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2831" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":38,"skipped":630,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:03:12.483: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:03:12.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-5520'
Mar 23 19:03:13.068: INFO: stderr: ""
Mar 23 19:03:13.068: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Mar 23 19:03:13.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-5520'
Mar 23 19:03:13.564: INFO: stderr: ""
Mar 23 19:03:13.564: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 23 19:03:14.600: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:03:14.600: INFO: Found 0 / 1
Mar 23 19:03:15.592: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:03:15.592: INFO: Found 1 / 1
Mar 23 19:03:15.592: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 23 19:03:15.606: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:03:15.606: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 23 19:03:15.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 describe pod agnhost-master-2p4tf --namespace=kubectl-5520'
Mar 23 19:03:15.818: INFO: stderr: ""
Mar 23 19:03:15.818: INFO: stdout: "Name:         agnhost-master-2p4tf\nNamespace:    kubectl-5520\nPriority:     0\nNode:         kube17-worker-2/10.30.20.93\nStart Time:   Mon, 23 Mar 2020 19:03:13 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           10.42.0.3\nIPs:\n  IP:           10.42.0.3\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://390547709c20c2aac824da1856618ff8291bda71a3db2579e5773c7a9d786b8d\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 23 Mar 2020 19:03:14 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tkltl (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-tkltl:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-tkltl\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                      Message\n  ----    ------     ----       ----                      -------\n  Normal  Scheduled  <unknown>  default-scheduler         Successfully assigned kubectl-5520/agnhost-master-2p4tf to kube17-worker-2\n  Normal  Pulled     1s         kubelet, kube17-worker-2  Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    1s         kubelet, kube17-worker-2  Created container agnhost-master\n  Normal  Started    1s         kubelet, kube17-worker-2  Started container agnhost-master\n"
Mar 23 19:03:15.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 describe rc agnhost-master --namespace=kubectl-5520'
Mar 23 19:03:16.047: INFO: stderr: ""
Mar 23 19:03:16.047: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-5520\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-master-2p4tf\n"
Mar 23 19:03:16.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 describe service agnhost-master --namespace=kubectl-5520'
Mar 23 19:03:16.267: INFO: stderr: ""
Mar 23 19:03:16.267: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-5520\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.105.190.101\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.42.0.3:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 23 19:03:16.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 describe node kube17-master'
Mar 23 19:03:16.743: INFO: stderr: ""
Mar 23 19:03:16.743: INFO: stdout: "Name:               kube17-master\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kube17-master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 23 Mar 2020 18:17:32 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  kube17-master\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 23 Mar 2020 19:03:15 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 23 Mar 2020 18:18:52 +0000   Mon, 23 Mar 2020 18:18:52 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Mon, 23 Mar 2020 19:00:00 +0000   Mon, 23 Mar 2020 18:17:22 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 23 Mar 2020 19:00:00 +0000   Mon, 23 Mar 2020 18:17:22 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 23 Mar 2020 19:00:00 +0000   Mon, 23 Mar 2020 18:17:22 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 23 Mar 2020 19:00:00 +0000   Mon, 23 Mar 2020 18:19:07 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.30.20.186\n  Hostname:    kube17-master\nCapacity:\n  cpu:                2\n  ephemeral-storage:  82437788Ki\n  hugepages-2Mi:      0\n  memory:             8175004Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  75974665296\n  hugepages-2Mi:      0\n  memory:             8072604Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 fdbfafba98c1c1b9cf11418e5dfa7424\n  System UUID:                421F819D-3AC7-6C5F-B471-342BD66BA9C3\n  Boot ID:                    123cd819-545b-44e7-8c23-6f46387147e0\n  Kernel Version:             4.4.0-142-generic\n  OS Image:                   Ubuntu 16.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.9.3\n  Kubelet Version:            v1.17.3\n  Kube-Proxy Version:         v1.17.3\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-6955765f44-mppws                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     45m\n  kube-system                 coredns-6955765f44-nmh4z                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     45m\n  kube-system                 etcd-kube17-master                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m\n  kube-system                 kube-apiserver-kube17-master                               250m (12%)    0 (0%)      0 (0%)           0 (0%)         45m\n  kube-system                 kube-controller-manager-kube17-master                      200m (10%)    0 (0%)      0 (0%)           0 (0%)         45m\n  kube-system                 kube-proxy-xjc9z                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m\n  kube-system                 kube-scheduler-kube17-master                               100m (5%)     0 (0%)      0 (0%)           0 (0%)         45m\n  kube-system                 weave-net-82dv5                                            20m (1%)      0 (0%)      0 (0%)           0 (0%)         44m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-vgmfm    0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                770m (38%)  0 (0%)\n  memory             140Mi (1%)  340Mi (4%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                From                       Message\n  ----    ------                   ----               ----                       -------\n  Normal  NodeHasSufficientMemory  45m (x5 over 45m)  kubelet, kube17-master     Node kube17-master status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    45m (x6 over 45m)  kubelet, kube17-master     Node kube17-master status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     45m (x5 over 45m)  kubelet, kube17-master     Node kube17-master status is now: NodeHasSufficientPID\n  Normal  Starting                 45m                kubelet, kube17-master     Starting kubelet.\n  Normal  NodeHasSufficientMemory  45m                kubelet, kube17-master     Node kube17-master status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    45m                kubelet, kube17-master     Node kube17-master status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     45m                kubelet, kube17-master     Node kube17-master status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  45m                kubelet, kube17-master     Updated Node Allocatable limit across pods\n  Normal  Starting                 45m                kube-proxy, kube17-master  Starting kube-proxy.\n  Normal  NodeReady                44m                kubelet, kube17-master     Node kube17-master status is now: NodeReady\n"
Mar 23 19:03:16.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 describe namespace kubectl-5520'
Mar 23 19:03:16.970: INFO: stderr: ""
Mar 23 19:03:16.971: INFO: stdout: "Name:         kubectl-5520\nLabels:       e2e-framework=kubectl\n              e2e-run=52a5c7fd-fc5b-4f20-ad22-3e3036cabc16\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:03:16.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5520" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":39,"skipped":723,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:03:16.989: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 23 19:03:17.054: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:03:22.037: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:03:44.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7256" for this suite.

• [SLOW TEST:27.081 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":40,"skipped":728,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:03:44.072: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 23 19:03:44.178: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2033 /api/v1/namespaces/watch-2033/configmaps/e2e-watch-test-configmap-a ebd8681d-fe43-422b-b54b-1b2a4b1b2ad2 13586 0 2020-03-23 19:03:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 23 19:03:44.179: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2033 /api/v1/namespaces/watch-2033/configmaps/e2e-watch-test-configmap-a ebd8681d-fe43-422b-b54b-1b2a4b1b2ad2 13586 0 2020-03-23 19:03:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 23 19:03:54.193: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2033 /api/v1/namespaces/watch-2033/configmaps/e2e-watch-test-configmap-a ebd8681d-fe43-422b-b54b-1b2a4b1b2ad2 13633 0 2020-03-23 19:03:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 23 19:03:54.193: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2033 /api/v1/namespaces/watch-2033/configmaps/e2e-watch-test-configmap-a ebd8681d-fe43-422b-b54b-1b2a4b1b2ad2 13633 0 2020-03-23 19:03:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 23 19:04:04.204: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2033 /api/v1/namespaces/watch-2033/configmaps/e2e-watch-test-configmap-a ebd8681d-fe43-422b-b54b-1b2a4b1b2ad2 13674 0 2020-03-23 19:03:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 23 19:04:04.204: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2033 /api/v1/namespaces/watch-2033/configmaps/e2e-watch-test-configmap-a ebd8681d-fe43-422b-b54b-1b2a4b1b2ad2 13674 0 2020-03-23 19:03:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 23 19:04:14.225: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2033 /api/v1/namespaces/watch-2033/configmaps/e2e-watch-test-configmap-a ebd8681d-fe43-422b-b54b-1b2a4b1b2ad2 13715 0 2020-03-23 19:03:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 23 19:04:14.225: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2033 /api/v1/namespaces/watch-2033/configmaps/e2e-watch-test-configmap-a ebd8681d-fe43-422b-b54b-1b2a4b1b2ad2 13715 0 2020-03-23 19:03:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 23 19:04:24.236: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2033 /api/v1/namespaces/watch-2033/configmaps/e2e-watch-test-configmap-b f7c08c08-849a-4b78-818b-917a80be5247 13756 0 2020-03-23 19:04:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 23 19:04:24.236: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2033 /api/v1/namespaces/watch-2033/configmaps/e2e-watch-test-configmap-b f7c08c08-849a-4b78-818b-917a80be5247 13756 0 2020-03-23 19:04:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 23 19:04:34.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2033 /api/v1/namespaces/watch-2033/configmaps/e2e-watch-test-configmap-b f7c08c08-849a-4b78-818b-917a80be5247 13797 0 2020-03-23 19:04:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 23 19:04:34.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2033 /api/v1/namespaces/watch-2033/configmaps/e2e-watch-test-configmap-b f7c08c08-849a-4b78-818b-917a80be5247 13797 0 2020-03-23 19:04:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:04:44.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2033" for this suite.

• [SLOW TEST:60.195 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":41,"skipped":732,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:04:44.271: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 19:04:44.395: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44a30684-767d-48eb-a143-644f4f1293d0" in namespace "downward-api-7533" to be "success or failure"
Mar 23 19:04:44.408: INFO: Pod "downwardapi-volume-44a30684-767d-48eb-a143-644f4f1293d0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.760705ms
Mar 23 19:04:46.415: INFO: Pod "downwardapi-volume-44a30684-767d-48eb-a143-644f4f1293d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020135182s
STEP: Saw pod success
Mar 23 19:04:46.416: INFO: Pod "downwardapi-volume-44a30684-767d-48eb-a143-644f4f1293d0" satisfied condition "success or failure"
Mar 23 19:04:46.424: INFO: Trying to get logs from node kube17-worker-2 pod downwardapi-volume-44a30684-767d-48eb-a143-644f4f1293d0 container client-container: <nil>
STEP: delete the pod
Mar 23 19:04:46.503: INFO: Waiting for pod downwardapi-volume-44a30684-767d-48eb-a143-644f4f1293d0 to disappear
Mar 23 19:04:46.507: INFO: Pod downwardapi-volume-44a30684-767d-48eb-a143-644f4f1293d0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:04:46.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7533" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":42,"skipped":740,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:04:46.525: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 19:04:48.375: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 19:04:50.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720587088, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720587088, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720587088, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720587088, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 19:04:53.422: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:04:53.428: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-41-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:04:54.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5830" for this suite.
STEP: Destroying namespace "webhook-5830-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.916 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":43,"skipped":750,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:04:54.446: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:04:58.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7319" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":44,"skipped":776,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:04:58.666: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-2196/configmap-test-aa97c865-e6bf-4fa1-a6d9-64b9de018c64
STEP: Creating a pod to test consume configMaps
Mar 23 19:04:58.808: INFO: Waiting up to 5m0s for pod "pod-configmaps-acbf435f-cb52-483a-83a8-a3839965b99c" in namespace "configmap-2196" to be "success or failure"
Mar 23 19:04:58.817: INFO: Pod "pod-configmaps-acbf435f-cb52-483a-83a8-a3839965b99c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.279664ms
Mar 23 19:05:00.823: INFO: Pod "pod-configmaps-acbf435f-cb52-483a-83a8-a3839965b99c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013235573s
Mar 23 19:05:02.831: INFO: Pod "pod-configmaps-acbf435f-cb52-483a-83a8-a3839965b99c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02143979s
STEP: Saw pod success
Mar 23 19:05:02.831: INFO: Pod "pod-configmaps-acbf435f-cb52-483a-83a8-a3839965b99c" satisfied condition "success or failure"
Mar 23 19:05:02.836: INFO: Trying to get logs from node kube17-worker-1 pod pod-configmaps-acbf435f-cb52-483a-83a8-a3839965b99c container env-test: <nil>
STEP: delete the pod
Mar 23 19:05:02.911: INFO: Waiting for pod pod-configmaps-acbf435f-cb52-483a-83a8-a3839965b99c to disappear
Mar 23 19:05:02.917: INFO: Pod pod-configmaps-acbf435f-cb52-483a-83a8-a3839965b99c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:05:02.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2196" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":45,"skipped":784,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:05:02.948: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-5805
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5805 to expose endpoints map[]
Mar 23 19:05:03.059: INFO: Get endpoints failed (12.86709ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar 23 19:05:04.066: INFO: successfully validated that service endpoint-test2 in namespace services-5805 exposes endpoints map[] (1.019745255s elapsed)
STEP: Creating pod pod1 in namespace services-5805
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5805 to expose endpoints map[pod1:[80]]
Mar 23 19:05:06.170: INFO: successfully validated that service endpoint-test2 in namespace services-5805 exposes endpoints map[pod1:[80]] (2.063355045s elapsed)
STEP: Creating pod pod2 in namespace services-5805
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5805 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 23 19:05:09.289: INFO: successfully validated that service endpoint-test2 in namespace services-5805 exposes endpoints map[pod1:[80] pod2:[80]] (3.106489808s elapsed)
STEP: Deleting pod pod1 in namespace services-5805
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5805 to expose endpoints map[pod2:[80]]
Mar 23 19:05:09.349: INFO: successfully validated that service endpoint-test2 in namespace services-5805 exposes endpoints map[pod2:[80]] (39.880467ms elapsed)
STEP: Deleting pod pod2 in namespace services-5805
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5805 to expose endpoints map[]
Mar 23 19:05:09.390: INFO: successfully validated that service endpoint-test2 in namespace services-5805 exposes endpoints map[] (12.025268ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:05:09.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5805" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:6.513 seconds]
[sig-network] Services
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":46,"skipped":857,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:05:09.465: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 19:05:09.544: INFO: Waiting up to 5m0s for pod "downwardapi-volume-154fcd4a-e479-4f72-853e-be90718079bb" in namespace "downward-api-402" to be "success or failure"
Mar 23 19:05:09.553: INFO: Pod "downwardapi-volume-154fcd4a-e479-4f72-853e-be90718079bb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.623791ms
Mar 23 19:05:12.163: INFO: Pod "downwardapi-volume-154fcd4a-e479-4f72-853e-be90718079bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.6189211s
Mar 23 19:05:14.168: INFO: Pod "downwardapi-volume-154fcd4a-e479-4f72-853e-be90718079bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.624811926s
STEP: Saw pod success
Mar 23 19:05:14.169: INFO: Pod "downwardapi-volume-154fcd4a-e479-4f72-853e-be90718079bb" satisfied condition "success or failure"
Mar 23 19:05:14.173: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-154fcd4a-e479-4f72-853e-be90718079bb container client-container: <nil>
STEP: delete the pod
Mar 23 19:05:14.224: INFO: Waiting for pod downwardapi-volume-154fcd4a-e479-4f72-853e-be90718079bb to disappear
Mar 23 19:05:14.231: INFO: Pod downwardapi-volume-154fcd4a-e479-4f72-853e-be90718079bb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:05:14.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-402" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":47,"skipped":870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:05:14.254: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 19:05:14.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1165be03-3df7-440a-863c-d503cc9ca20d" in namespace "projected-5815" to be "success or failure"
Mar 23 19:05:14.414: INFO: Pod "downwardapi-volume-1165be03-3df7-440a-863c-d503cc9ca20d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.46506ms
Mar 23 19:05:16.421: INFO: Pod "downwardapi-volume-1165be03-3df7-440a-863c-d503cc9ca20d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018191628s
STEP: Saw pod success
Mar 23 19:05:16.421: INFO: Pod "downwardapi-volume-1165be03-3df7-440a-863c-d503cc9ca20d" satisfied condition "success or failure"
Mar 23 19:05:16.425: INFO: Trying to get logs from node kube17-worker-2 pod downwardapi-volume-1165be03-3df7-440a-863c-d503cc9ca20d container client-container: <nil>
STEP: delete the pod
Mar 23 19:05:16.474: INFO: Waiting for pod downwardapi-volume-1165be03-3df7-440a-863c-d503cc9ca20d to disappear
Mar 23 19:05:16.479: INFO: Pod downwardapi-volume-1165be03-3df7-440a-863c-d503cc9ca20d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:05:16.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5815" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":48,"skipped":901,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:05:16.517: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:05:33.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5034" for this suite.

• [SLOW TEST:16.764 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":49,"skipped":927,"failed":0}
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:05:33.282: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Mar 23 19:05:33.410: INFO: Waiting up to 5m0s for pod "client-containers-3223d508-9383-4013-8fe2-cc37a2a92d5a" in namespace "containers-3255" to be "success or failure"
Mar 23 19:05:33.438: INFO: Pod "client-containers-3223d508-9383-4013-8fe2-cc37a2a92d5a": Phase="Pending", Reason="", readiness=false. Elapsed: 27.772836ms
Mar 23 19:05:35.444: INFO: Pod "client-containers-3223d508-9383-4013-8fe2-cc37a2a92d5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034350781s
STEP: Saw pod success
Mar 23 19:05:35.445: INFO: Pod "client-containers-3223d508-9383-4013-8fe2-cc37a2a92d5a" satisfied condition "success or failure"
Mar 23 19:05:35.449: INFO: Trying to get logs from node kube17-worker-1 pod client-containers-3223d508-9383-4013-8fe2-cc37a2a92d5a container test-container: <nil>
STEP: delete the pod
Mar 23 19:05:35.503: INFO: Waiting for pod client-containers-3223d508-9383-4013-8fe2-cc37a2a92d5a to disappear
Mar 23 19:05:35.507: INFO: Pod client-containers-3223d508-9383-4013-8fe2-cc37a2a92d5a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:05:35.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3255" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":50,"skipped":927,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:05:35.531: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Mar 23 19:05:35.600: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 23 19:06:35.638: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:06:35.644: INFO: Starting informer...
STEP: Starting pod...
Mar 23 19:06:35.878: INFO: Pod is running on kube17-worker-2. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar 23 19:06:35.920: INFO: Pod wasn't evicted. Proceeding
Mar 23 19:06:35.920: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar 23 19:07:50.966: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:07:50.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4960" for this suite.

• [SLOW TEST:135.455 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":51,"skipped":937,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:07:50.987: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 23 19:07:51.066: INFO: Waiting up to 5m0s for pod "pod-1e997d3a-1c6e-4114-ae96-6a9b89a02aef" in namespace "emptydir-3713" to be "success or failure"
Mar 23 19:07:51.071: INFO: Pod "pod-1e997d3a-1c6e-4114-ae96-6a9b89a02aef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.933539ms
Mar 23 19:07:53.078: INFO: Pod "pod-1e997d3a-1c6e-4114-ae96-6a9b89a02aef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01113181s
Mar 23 19:07:55.085: INFO: Pod "pod-1e997d3a-1c6e-4114-ae96-6a9b89a02aef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018019306s
STEP: Saw pod success
Mar 23 19:07:55.085: INFO: Pod "pod-1e997d3a-1c6e-4114-ae96-6a9b89a02aef" satisfied condition "success or failure"
Mar 23 19:07:55.089: INFO: Trying to get logs from node kube17-worker-1 pod pod-1e997d3a-1c6e-4114-ae96-6a9b89a02aef container test-container: <nil>
STEP: delete the pod
Mar 23 19:07:55.155: INFO: Waiting for pod pod-1e997d3a-1c6e-4114-ae96-6a9b89a02aef to disappear
Mar 23 19:07:55.160: INFO: Pod pod-1e997d3a-1c6e-4114-ae96-6a9b89a02aef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:07:55.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3713" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":52,"skipped":944,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:07:55.179: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-4xt5
STEP: Creating a pod to test atomic-volume-subpath
Mar 23 19:07:55.301: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4xt5" in namespace "subpath-2681" to be "success or failure"
Mar 23 19:07:55.320: INFO: Pod "pod-subpath-test-configmap-4xt5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.915712ms
Mar 23 19:07:57.328: INFO: Pod "pod-subpath-test-configmap-4xt5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026144175s
Mar 23 19:08:00.565: INFO: Pod "pod-subpath-test-configmap-4xt5": Phase="Running", Reason="", readiness=true. Elapsed: 5.263422638s
Mar 23 19:08:02.604: INFO: Pod "pod-subpath-test-configmap-4xt5": Phase="Running", Reason="", readiness=true. Elapsed: 7.302512976s
Mar 23 19:08:04.610: INFO: Pod "pod-subpath-test-configmap-4xt5": Phase="Running", Reason="", readiness=true. Elapsed: 9.309010742s
Mar 23 19:08:06.926: INFO: Pod "pod-subpath-test-configmap-4xt5": Phase="Running", Reason="", readiness=true. Elapsed: 11.624876587s
Mar 23 19:08:08.932: INFO: Pod "pod-subpath-test-configmap-4xt5": Phase="Running", Reason="", readiness=true. Elapsed: 13.631022931s
Mar 23 19:08:10.939: INFO: Pod "pod-subpath-test-configmap-4xt5": Phase="Running", Reason="", readiness=true. Elapsed: 15.637242295s
Mar 23 19:08:12.944: INFO: Pod "pod-subpath-test-configmap-4xt5": Phase="Running", Reason="", readiness=true. Elapsed: 17.642975408s
Mar 23 19:08:14.950: INFO: Pod "pod-subpath-test-configmap-4xt5": Phase="Running", Reason="", readiness=true. Elapsed: 19.64869562s
Mar 23 19:08:19.901: INFO: Pod "pod-subpath-test-configmap-4xt5": Phase="Running", Reason="", readiness=true. Elapsed: 24.599843958s
Mar 23 19:08:21.907: INFO: Pod "pod-subpath-test-configmap-4xt5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.605671977s
STEP: Saw pod success
Mar 23 19:08:21.907: INFO: Pod "pod-subpath-test-configmap-4xt5" satisfied condition "success or failure"
Mar 23 19:08:21.912: INFO: Trying to get logs from node kube17-worker-1 pod pod-subpath-test-configmap-4xt5 container test-container-subpath-configmap-4xt5: <nil>
STEP: delete the pod
Mar 23 19:08:21.949: INFO: Waiting for pod pod-subpath-test-configmap-4xt5 to disappear
Mar 23 19:08:21.967: INFO: Pod pod-subpath-test-configmap-4xt5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4xt5
Mar 23 19:08:21.967: INFO: Deleting pod "pod-subpath-test-configmap-4xt5" in namespace "subpath-2681"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:08:21.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2681" for this suite.

• [SLOW TEST:26.887 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":53,"skipped":955,"failed":0}
SSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:08:22.075: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Mar 23 19:08:22.176: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 23 19:09:22.215: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:09:22.220: INFO: Starting informer...
STEP: Starting pods...
Mar 23 19:09:22.263: INFO: Pod1 is running on kube17-worker-1. Tainting Node
Mar 23 19:09:26.506: INFO: Pod2 is running on kube17-worker-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar 23 19:09:38.673: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 23 19:09:58.278: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:09:58.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7364" for this suite.

• [SLOW TEST:96.382 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":54,"skipped":958,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:09:58.457: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Mar 23 19:09:58.534: INFO: Waiting up to 5m0s for pod "var-expansion-4627458a-f52b-40f7-9720-98157a183ba5" in namespace "var-expansion-8096" to be "success or failure"
Mar 23 19:09:58.542: INFO: Pod "var-expansion-4627458a-f52b-40f7-9720-98157a183ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.863249ms
Mar 23 19:10:00.548: INFO: Pod "var-expansion-4627458a-f52b-40f7-9720-98157a183ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013651142s
Mar 23 19:10:02.556: INFO: Pod "var-expansion-4627458a-f52b-40f7-9720-98157a183ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022017874s
Mar 23 19:10:04.563: INFO: Pod "var-expansion-4627458a-f52b-40f7-9720-98157a183ba5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028782741s
STEP: Saw pod success
Mar 23 19:10:04.563: INFO: Pod "var-expansion-4627458a-f52b-40f7-9720-98157a183ba5" satisfied condition "success or failure"
Mar 23 19:10:04.569: INFO: Trying to get logs from node kube17-worker-1 pod var-expansion-4627458a-f52b-40f7-9720-98157a183ba5 container dapi-container: <nil>
STEP: delete the pod
Mar 23 19:10:04.642: INFO: Waiting for pod var-expansion-4627458a-f52b-40f7-9720-98157a183ba5 to disappear
Mar 23 19:10:04.646: INFO: Pod var-expansion-4627458a-f52b-40f7-9720-98157a183ba5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:10:04.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8096" for this suite.

• [SLOW TEST:6.204 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":55,"skipped":979,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:10:04.662: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:10:04.772: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 23 19:10:10.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-7646 create -f -'
Mar 23 19:10:10.803: INFO: stderr: ""
Mar 23 19:10:10.803: INFO: stdout: "e2e-test-crd-publish-openapi-8862-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 23 19:10:10.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-7646 delete e2e-test-crd-publish-openapi-8862-crds test-cr'
Mar 23 19:10:11.000: INFO: stderr: ""
Mar 23 19:10:11.000: INFO: stdout: "e2e-test-crd-publish-openapi-8862-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 23 19:10:11.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-7646 apply -f -'
Mar 23 19:10:11.457: INFO: stderr: ""
Mar 23 19:10:11.457: INFO: stdout: "e2e-test-crd-publish-openapi-8862-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 23 19:10:11.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-7646 delete e2e-test-crd-publish-openapi-8862-crds test-cr'
Mar 23 19:10:11.659: INFO: stderr: ""
Mar 23 19:10:11.660: INFO: stdout: "e2e-test-crd-publish-openapi-8862-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 23 19:10:11.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 explain e2e-test-crd-publish-openapi-8862-crds'
Mar 23 19:10:12.112: INFO: stderr: ""
Mar 23 19:10:12.112: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8862-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:10:18.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7646" for this suite.

• [SLOW TEST:13.912 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":56,"skipped":979,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:10:18.574: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 23 19:10:18.682: INFO: Waiting up to 5m0s for pod "pod-740b8ff6-9f6d-4e6d-8b1b-3540967e8ac8" in namespace "emptydir-9619" to be "success or failure"
Mar 23 19:10:18.707: INFO: Pod "pod-740b8ff6-9f6d-4e6d-8b1b-3540967e8ac8": Phase="Pending", Reason="", readiness=false. Elapsed: 25.181244ms
Mar 23 19:10:20.713: INFO: Pod "pod-740b8ff6-9f6d-4e6d-8b1b-3540967e8ac8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03117741s
Mar 23 19:10:22.719: INFO: Pod "pod-740b8ff6-9f6d-4e6d-8b1b-3540967e8ac8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036833311s
STEP: Saw pod success
Mar 23 19:10:22.719: INFO: Pod "pod-740b8ff6-9f6d-4e6d-8b1b-3540967e8ac8" satisfied condition "success or failure"
Mar 23 19:10:22.724: INFO: Trying to get logs from node kube17-worker-1 pod pod-740b8ff6-9f6d-4e6d-8b1b-3540967e8ac8 container test-container: <nil>
STEP: delete the pod
Mar 23 19:10:22.774: INFO: Waiting for pod pod-740b8ff6-9f6d-4e6d-8b1b-3540967e8ac8 to disappear
Mar 23 19:10:22.779: INFO: Pod pod-740b8ff6-9f6d-4e6d-8b1b-3540967e8ac8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:10:22.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9619" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":57,"skipped":994,"failed":0}
SSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:10:22.815: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:10:30.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2047" for this suite.

• [SLOW TEST:8.114 seconds]
[sig-apps] Job
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":58,"skipped":1000,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:10:30.929: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Mar 23 19:10:31.024: INFO: Waiting up to 5m0s for pod "var-expansion-3c520c66-bd0e-4b58-b4f6-a566989d1a98" in namespace "var-expansion-185" to be "success or failure"
Mar 23 19:10:31.043: INFO: Pod "var-expansion-3c520c66-bd0e-4b58-b4f6-a566989d1a98": Phase="Pending", Reason="", readiness=false. Elapsed: 18.750823ms
Mar 23 19:10:33.049: INFO: Pod "var-expansion-3c520c66-bd0e-4b58-b4f6-a566989d1a98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024684309s
Mar 23 19:10:35.055: INFO: Pod "var-expansion-3c520c66-bd0e-4b58-b4f6-a566989d1a98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031421699s
STEP: Saw pod success
Mar 23 19:10:35.055: INFO: Pod "var-expansion-3c520c66-bd0e-4b58-b4f6-a566989d1a98" satisfied condition "success or failure"
Mar 23 19:10:35.061: INFO: Trying to get logs from node kube17-worker-1 pod var-expansion-3c520c66-bd0e-4b58-b4f6-a566989d1a98 container dapi-container: <nil>
STEP: delete the pod
Mar 23 19:10:35.105: INFO: Waiting for pod var-expansion-3c520c66-bd0e-4b58-b4f6-a566989d1a98 to disappear
Mar 23 19:10:35.112: INFO: Pod var-expansion-3c520c66-bd0e-4b58-b4f6-a566989d1a98 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:10:35.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-185" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":59,"skipped":1005,"failed":0}

------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:10:35.140: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-9278/secret-test-71412f05-1b8e-4ed5-8652-485dddbf02ce
STEP: Creating a pod to test consume secrets
Mar 23 19:10:35.249: INFO: Waiting up to 5m0s for pod "pod-configmaps-943ab857-6ab2-4932-b102-1c693caa9331" in namespace "secrets-9278" to be "success or failure"
Mar 23 19:10:35.255: INFO: Pod "pod-configmaps-943ab857-6ab2-4932-b102-1c693caa9331": Phase="Pending", Reason="", readiness=false. Elapsed: 5.38241ms
Mar 23 19:10:38.018: INFO: Pod "pod-configmaps-943ab857-6ab2-4932-b102-1c693caa9331": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.768256359s
STEP: Saw pod success
Mar 23 19:10:38.018: INFO: Pod "pod-configmaps-943ab857-6ab2-4932-b102-1c693caa9331" satisfied condition "success or failure"
Mar 23 19:10:38.030: INFO: Trying to get logs from node kube17-worker-1 pod pod-configmaps-943ab857-6ab2-4932-b102-1c693caa9331 container env-test: <nil>
STEP: delete the pod
Mar 23 19:10:38.611: INFO: Waiting for pod pod-configmaps-943ab857-6ab2-4932-b102-1c693caa9331 to disappear
Mar 23 19:10:38.621: INFO: Pod pod-configmaps-943ab857-6ab2-4932-b102-1c693caa9331 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:10:38.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9278" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":60,"skipped":1005,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:10:38.674: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:10:41.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4068" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":61,"skipped":1016,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:10:41.845: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-2595aae5-2df3-469d-85ee-d281e524501c
STEP: Creating a pod to test consume secrets
Mar 23 19:10:41.969: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0c6ed7b9-01a6-4d18-83dc-69e010945b33" in namespace "projected-5812" to be "success or failure"
Mar 23 19:10:41.978: INFO: Pod "pod-projected-secrets-0c6ed7b9-01a6-4d18-83dc-69e010945b33": Phase="Pending", Reason="", readiness=false. Elapsed: 8.469879ms
Mar 23 19:10:43.985: INFO: Pod "pod-projected-secrets-0c6ed7b9-01a6-4d18-83dc-69e010945b33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015061958s
STEP: Saw pod success
Mar 23 19:10:43.985: INFO: Pod "pod-projected-secrets-0c6ed7b9-01a6-4d18-83dc-69e010945b33" satisfied condition "success or failure"
Mar 23 19:10:43.990: INFO: Trying to get logs from node kube17-worker-1 pod pod-projected-secrets-0c6ed7b9-01a6-4d18-83dc-69e010945b33 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 23 19:10:44.052: INFO: Waiting for pod pod-projected-secrets-0c6ed7b9-01a6-4d18-83dc-69e010945b33 to disappear
Mar 23 19:10:44.056: INFO: Pod pod-projected-secrets-0c6ed7b9-01a6-4d18-83dc-69e010945b33 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:10:44.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5812" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":62,"skipped":1026,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:10:44.106: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-038fbbda-eafb-47ba-8ab8-a74ba6cd5b63
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:10:44.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9310" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":63,"skipped":1082,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:10:44.230: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Mar 23 19:10:44.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=kubectl-324 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar 23 19:10:46.338: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar 23 19:10:46.338: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:10:48.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-324" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":280,"completed":64,"skipped":1089,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:10:48.908: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Mar 23 19:10:59.122: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:10:59.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1498" for this suite.

• [SLOW TEST:10.235 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":65,"skipped":1108,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:10:59.150: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:10:59.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1594" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":66,"skipped":1114,"failed":0}

------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:10:59.289: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:10:59.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3512" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":67,"skipped":1114,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:10:59.441: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 19:10:59.530: INFO: Waiting up to 5m0s for pod "downwardapi-volume-149fe63d-a2ed-4101-b9fa-624e57dea333" in namespace "projected-70" to be "success or failure"
Mar 23 19:10:59.537: INFO: Pod "downwardapi-volume-149fe63d-a2ed-4101-b9fa-624e57dea333": Phase="Pending", Reason="", readiness=false. Elapsed: 6.940179ms
Mar 23 19:11:01.543: INFO: Pod "downwardapi-volume-149fe63d-a2ed-4101-b9fa-624e57dea333": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013106807s
Mar 23 19:11:03.550: INFO: Pod "downwardapi-volume-149fe63d-a2ed-4101-b9fa-624e57dea333": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020482131s
STEP: Saw pod success
Mar 23 19:11:03.551: INFO: Pod "downwardapi-volume-149fe63d-a2ed-4101-b9fa-624e57dea333" satisfied condition "success or failure"
Mar 23 19:11:03.556: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-149fe63d-a2ed-4101-b9fa-624e57dea333 container client-container: <nil>
STEP: delete the pod
Mar 23 19:11:03.604: INFO: Waiting for pod downwardapi-volume-149fe63d-a2ed-4101-b9fa-624e57dea333 to disappear
Mar 23 19:11:03.623: INFO: Pod downwardapi-volume-149fe63d-a2ed-4101-b9fa-624e57dea333 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:11:03.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-70" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":68,"skipped":1141,"failed":0}

------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:11:03.648: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Mar 23 19:11:03.756: INFO: Waiting up to 5m0s for pod "var-expansion-dad9f4b3-ed6f-46c6-a84b-a703ce7dde68" in namespace "var-expansion-9071" to be "success or failure"
Mar 23 19:11:03.764: INFO: Pod "var-expansion-dad9f4b3-ed6f-46c6-a84b-a703ce7dde68": Phase="Pending", Reason="", readiness=false. Elapsed: 6.737277ms
Mar 23 19:11:05.770: INFO: Pod "var-expansion-dad9f4b3-ed6f-46c6-a84b-a703ce7dde68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013361947s
Mar 23 19:11:08.565: INFO: Pod "var-expansion-dad9f4b3-ed6f-46c6-a84b-a703ce7dde68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.807811893s
STEP: Saw pod success
Mar 23 19:11:08.565: INFO: Pod "var-expansion-dad9f4b3-ed6f-46c6-a84b-a703ce7dde68" satisfied condition "success or failure"
Mar 23 19:11:09.262: INFO: Trying to get logs from node kube17-worker-1 pod var-expansion-dad9f4b3-ed6f-46c6-a84b-a703ce7dde68 container dapi-container: <nil>
STEP: delete the pod
Mar 23 19:11:09.409: INFO: Waiting for pod var-expansion-dad9f4b3-ed6f-46c6-a84b-a703ce7dde68 to disappear
Mar 23 19:11:09.421: INFO: Pod var-expansion-dad9f4b3-ed6f-46c6-a84b-a703ce7dde68 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:11:09.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9071" for this suite.

• [SLOW TEST:5.793 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":69,"skipped":1141,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:11:09.446: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 23 19:11:09.621: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:11:09.633: INFO: Number of nodes with available pods: 0
Mar 23 19:11:09.633: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:11:10.645: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:11:10.653: INFO: Number of nodes with available pods: 0
Mar 23 19:11:10.653: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:11:11.649: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:11:11.656: INFO: Number of nodes with available pods: 2
Mar 23 19:11:11.656: INFO: Node kube17-worker-2 is running more than one daemon pod
Mar 23 19:11:12.642: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:11:12.653: INFO: Number of nodes with available pods: 3
Mar 23 19:11:12.653: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 23 19:11:12.692: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:11:12.722: INFO: Number of nodes with available pods: 2
Mar 23 19:11:12.723: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:11:13.741: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:11:13.767: INFO: Number of nodes with available pods: 2
Mar 23 19:11:13.767: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:11:14.789: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:11:14.807: INFO: Number of nodes with available pods: 2
Mar 23 19:11:14.807: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:11:15.731: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:11:15.737: INFO: Number of nodes with available pods: 3
Mar 23 19:11:15.737: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8770, will wait for the garbage collector to delete the pods
Mar 23 19:11:15.820: INFO: Deleting DaemonSet.extensions daemon-set took: 18.39396ms
Mar 23 19:11:16.320: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.338701ms
Mar 23 19:11:20.527: INFO: Number of nodes with available pods: 0
Mar 23 19:11:20.527: INFO: Number of running nodes: 0, number of available pods: 0
Mar 23 19:11:20.537: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8770/daemonsets","resourceVersion":"16500"},"items":null}

Mar 23 19:11:20.547: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8770/pods","resourceVersion":"16500"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:11:20.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8770" for this suite.

• [SLOW TEST:11.169 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":70,"skipped":1168,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:11:20.618: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-4b53f2c9-1c11-406d-ae36-8a45fcb2bb11 in namespace container-probe-9365
Mar 23 19:11:26.725: INFO: Started pod busybox-4b53f2c9-1c11-406d-ae36-8a45fcb2bb11 in namespace container-probe-9365
STEP: checking the pod's current state and verifying that restartCount is present
Mar 23 19:11:26.730: INFO: Initial restart count of pod busybox-4b53f2c9-1c11-406d-ae36-8a45fcb2bb11 is 0
Mar 23 19:12:19.004: INFO: Restart count of pod container-probe-9365/busybox-4b53f2c9-1c11-406d-ae36-8a45fcb2bb11 is now 1 (52.273804043s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:12:19.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9365" for this suite.

• [SLOW TEST:58.436 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":71,"skipped":1178,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:12:19.054: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8586.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8586.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8586.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8586.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8586.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8586.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8586.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8586.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8586.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8586.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 19:12:37.213: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:37.224: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:37.229: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:37.234: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:37.256: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:37.263: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:37.268: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:37.274: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:37.286: INFO: Lookups using dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8586.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8586.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local jessie_udp@dns-test-service-2.dns-8586.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8586.svc.cluster.local]

Mar 23 19:12:42.294: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:42.299: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:42.305: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:42.313: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:42.331: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:42.338: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:42.345: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:42.352: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:42.363: INFO: Lookups using dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8586.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8586.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local jessie_udp@dns-test-service-2.dns-8586.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8586.svc.cluster.local]

Mar 23 19:12:47.294: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:47.300: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:47.306: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:47.313: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:47.335: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:47.341: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:47.347: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:47.352: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:47.364: INFO: Lookups using dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8586.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8586.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local jessie_udp@dns-test-service-2.dns-8586.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8586.svc.cluster.local]

Mar 23 19:12:52.294: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:52.300: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:52.305: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:52.310: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:52.337: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:52.345: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:52.351: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:52.356: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8586.svc.cluster.local from pod dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54: the server could not find the requested resource (get pods dns-test-3405170b-90cd-4674-a337-af4193087e54)
Mar 23 19:12:52.369: INFO: Lookups using dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8586.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8586.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8586.svc.cluster.local jessie_udp@dns-test-service-2.dns-8586.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8586.svc.cluster.local]

Mar 23 19:12:57.423: INFO: DNS probes using dns-8586/dns-test-3405170b-90cd-4674-a337-af4193087e54 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:12:57.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8586" for this suite.

• [SLOW TEST:38.473 seconds]
[sig-network] DNS
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":72,"skipped":1191,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:12:57.528: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 23 19:12:57.586: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar 23 19:13:07.169: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:13:07.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8951" for this suite.

• [SLOW TEST:9.662 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":73,"skipped":1203,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:13:07.192: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:13:07.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 version'
Mar 23 19:13:07.486: INFO: stderr: ""
Mar 23 19:13:07.486: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.4\", GitCommit:\"8d8aa39598534325ad77120c120a22b3a990b5ea\", GitTreeState:\"clean\", BuildDate:\"2020-03-12T21:03:42Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.4\", GitCommit:\"8d8aa39598534325ad77120c120a22b3a990b5ea\", GitTreeState:\"clean\", BuildDate:\"2020-03-12T20:55:23Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:13:07.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2599" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":74,"skipped":1204,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:13:07.508: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:13:07.586: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:13:11.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-134" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":75,"skipped":1217,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:13:11.883: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 23 19:13:11.965: INFO: Waiting up to 5m0s for pod "pod-7b278f55-a4c1-4e3d-81fd-265de0ebca4f" in namespace "emptydir-8561" to be "success or failure"
Mar 23 19:13:11.975: INFO: Pod "pod-7b278f55-a4c1-4e3d-81fd-265de0ebca4f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.040471ms
Mar 23 19:13:13.980: INFO: Pod "pod-7b278f55-a4c1-4e3d-81fd-265de0ebca4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014655561s
STEP: Saw pod success
Mar 23 19:13:13.980: INFO: Pod "pod-7b278f55-a4c1-4e3d-81fd-265de0ebca4f" satisfied condition "success or failure"
Mar 23 19:13:13.985: INFO: Trying to get logs from node kube17-worker-1 pod pod-7b278f55-a4c1-4e3d-81fd-265de0ebca4f container test-container: <nil>
STEP: delete the pod
Mar 23 19:13:14.027: INFO: Waiting for pod pod-7b278f55-a4c1-4e3d-81fd-265de0ebca4f to disappear
Mar 23 19:13:14.036: INFO: Pod pod-7b278f55-a4c1-4e3d-81fd-265de0ebca4f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:13:14.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8561" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":76,"skipped":1235,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:13:14.059: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:13:14.126: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 23 19:13:19.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-8702 create -f -'
Mar 23 19:13:20.731: INFO: stderr: ""
Mar 23 19:13:20.731: INFO: stdout: "e2e-test-crd-publish-openapi-7804-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 23 19:13:20.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-8702 delete e2e-test-crd-publish-openapi-7804-crds test-cr'
Mar 23 19:13:20.936: INFO: stderr: ""
Mar 23 19:13:20.936: INFO: stdout: "e2e-test-crd-publish-openapi-7804-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 23 19:13:20.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-8702 apply -f -'
Mar 23 19:13:21.428: INFO: stderr: ""
Mar 23 19:13:21.428: INFO: stdout: "e2e-test-crd-publish-openapi-7804-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 23 19:13:21.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-8702 delete e2e-test-crd-publish-openapi-7804-crds test-cr'
Mar 23 19:13:21.645: INFO: stderr: ""
Mar 23 19:13:21.645: INFO: stdout: "e2e-test-crd-publish-openapi-7804-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 23 19:13:21.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 explain e2e-test-crd-publish-openapi-7804-crds'
Mar 23 19:13:22.048: INFO: stderr: ""
Mar 23 19:13:22.048: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7804-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:13:27.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8702" for this suite.

• [SLOW TEST:13.663 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":77,"skipped":1256,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:13:27.724: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:13:27.795: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:13:29.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3348" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":78,"skipped":1297,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:13:29.878: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 23 19:13:30.064: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-868 /api/v1/namespaces/watch-868/configmaps/e2e-watch-test-resource-version f3196459-4c53-40b7-b18b-ef20b3a94c13 17249 0 2020-03-23 19:13:30 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 23 19:13:30.065: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-868 /api/v1/namespaces/watch-868/configmaps/e2e-watch-test-resource-version f3196459-4c53-40b7-b18b-ef20b3a94c13 17250 0 2020-03-23 19:13:30 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:13:30.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-868" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":79,"skipped":1312,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:13:30.094: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1626
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 19:13:30.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-826'
Mar 23 19:13:30.367: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 23 19:13:30.367: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1631
Mar 23 19:13:33.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete deployment e2e-test-httpd-deployment --namespace=kubectl-826'
Mar 23 19:13:33.213: INFO: stderr: ""
Mar 23 19:13:33.214: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:13:33.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-826" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":280,"completed":80,"skipped":1320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:13:33.248: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:13:59.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8466" for this suite.

• [SLOW TEST:25.885 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":81,"skipped":1357,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:13:59.134: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar 23 19:13:59.224: INFO: Created pod &Pod{ObjectMeta:{dns-2351  dns-2351 /api/v1/namespaces/dns-2351/pods/dns-2351 5d6938d8-8468-4c60-9e0e-3a1b2d69c31a 17477 0 2020-03-23 19:13:59 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x6fbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x6fbr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x6fbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Mar 23 19:14:03.240: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2351 PodName:dns-2351 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:14:03.240: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Verifying customized DNS server is configured on pod...
Mar 23 19:14:03.475: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2351 PodName:dns-2351 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:14:03.475: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:14:03.741: INFO: Deleting pod dns-2351...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:14:03.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2351" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":82,"skipped":1404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:14:03.814: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Mar 23 19:14:03.889: INFO: PodSpec: initContainers in spec.initContainers
Mar 23 19:14:53.858: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8a2702a0-3a8d-4f2d-9b3a-9cd736cfce41", GenerateName:"", Namespace:"init-container-6989", SelfLink:"/api/v1/namespaces/init-container-6989/pods/pod-init-8a2702a0-3a8d-4f2d-9b3a-9cd736cfce41", UID:"8ebf0020-b370-4d01-84cf-a498650be022", ResourceVersion:"17766", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63720587643, loc:(*time.Location)(0x791d1c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"889072567"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-hdmmv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000dc8c40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hdmmv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hdmmv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hdmmv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005d4ac08), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kube17-worker-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00298c720), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005d4ac90)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005d4acb0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005d4acb8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005d4acbc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720587643, loc:(*time.Location)(0x791d1c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720587643, loc:(*time.Location)(0x791d1c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720587643, loc:(*time.Location)(0x791d1c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720587643, loc:(*time.Location)(0x791d1c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.30.20.153", PodIP:"10.44.0.1", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.44.0.1"}}, StartTime:(*v1.Time)(0xc004c7fae0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002ee45b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002ee4620)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://7b4076fd035796d340c6e63f7c8703697115d0f8092af3fa7852e5b40123b5a5", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004c7fb20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004c7fb00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc005d4ad3f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:14:53.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6989" for this suite.

• [SLOW TEST:50.066 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":83,"skipped":1427,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:14:53.882: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:15:13.990: INFO: Container started at 2020-03-23 19:14:55 +0000 UTC, pod became ready at 2020-03-23 19:15:12 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:15:13.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3559" for this suite.

• [SLOW TEST:20.125 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":84,"skipped":1429,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:15:14.008: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Mar 23 19:15:14.097: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5722" to be "success or failure"
Mar 23 19:15:14.106: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115699ms
Mar 23 19:15:16.111: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013842882s
Mar 23 19:15:18.117: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019541907s
STEP: Saw pod success
Mar 23 19:15:18.117: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar 23 19:15:18.123: INFO: Trying to get logs from node kube17-worker-1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar 23 19:15:18.181: INFO: Waiting for pod pod-host-path-test to disappear
Mar 23 19:15:18.190: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:15:18.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5722" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":85,"skipped":1450,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:15:18.210: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 23 19:15:18.293: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:15:23.934: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:15:51.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2249" for this suite.

• [SLOW TEST:33.572 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":86,"skipped":1463,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:15:51.790: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-b83682db-a06e-4e75-9273-9214c07dfbe0 in namespace container-probe-5385
Mar 23 19:15:55.964: INFO: Started pod liveness-b83682db-a06e-4e75-9273-9214c07dfbe0 in namespace container-probe-5385
STEP: checking the pod's current state and verifying that restartCount is present
Mar 23 19:15:55.968: INFO: Initial restart count of pod liveness-b83682db-a06e-4e75-9273-9214c07dfbe0 is 0
Mar 23 19:16:18.834: INFO: Restart count of pod container-probe-5385/liveness-b83682db-a06e-4e75-9273-9214c07dfbe0 is now 1 (22.865432337s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:16:18.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5385" for this suite.

• [SLOW TEST:27.083 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":87,"skipped":1481,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:16:18.875: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-42d94e20-5f9f-4ee0-85fb-63f39d31c376
STEP: Creating a pod to test consume secrets
Mar 23 19:16:19.063: INFO: Waiting up to 5m0s for pod "pod-secrets-c2ca7083-7347-4a1d-a296-398d1dd42022" in namespace "secrets-1098" to be "success or failure"
Mar 23 19:16:19.071: INFO: Pod "pod-secrets-c2ca7083-7347-4a1d-a296-398d1dd42022": Phase="Pending", Reason="", readiness=false. Elapsed: 7.625754ms
Mar 23 19:16:21.076: INFO: Pod "pod-secrets-c2ca7083-7347-4a1d-a296-398d1dd42022": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013038633s
Mar 23 19:16:23.082: INFO: Pod "pod-secrets-c2ca7083-7347-4a1d-a296-398d1dd42022": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018631001s
STEP: Saw pod success
Mar 23 19:16:23.082: INFO: Pod "pod-secrets-c2ca7083-7347-4a1d-a296-398d1dd42022" satisfied condition "success or failure"
Mar 23 19:16:23.087: INFO: Trying to get logs from node kube17-worker-1 pod pod-secrets-c2ca7083-7347-4a1d-a296-398d1dd42022 container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 19:16:23.128: INFO: Waiting for pod pod-secrets-c2ca7083-7347-4a1d-a296-398d1dd42022 to disappear
Mar 23 19:16:23.132: INFO: Pod pod-secrets-c2ca7083-7347-4a1d-a296-398d1dd42022 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:16:23.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1098" for this suite.
STEP: Destroying namespace "secret-namespace-6362" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":88,"skipped":1505,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:16:23.189: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 19:16:23.317: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3219183-7150-4298-9db6-b535cf2d8a81" in namespace "downward-api-4038" to be "success or failure"
Mar 23 19:16:23.334: INFO: Pod "downwardapi-volume-f3219183-7150-4298-9db6-b535cf2d8a81": Phase="Pending", Reason="", readiness=false. Elapsed: 16.881567ms
Mar 23 19:16:25.367: INFO: Pod "downwardapi-volume-f3219183-7150-4298-9db6-b535cf2d8a81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049559453s
Mar 23 19:16:27.372: INFO: Pod "downwardapi-volume-f3219183-7150-4298-9db6-b535cf2d8a81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054554233s
STEP: Saw pod success
Mar 23 19:16:27.372: INFO: Pod "downwardapi-volume-f3219183-7150-4298-9db6-b535cf2d8a81" satisfied condition "success or failure"
Mar 23 19:16:27.375: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-f3219183-7150-4298-9db6-b535cf2d8a81 container client-container: <nil>
STEP: delete the pod
Mar 23 19:16:27.419: INFO: Waiting for pod downwardapi-volume-f3219183-7150-4298-9db6-b535cf2d8a81 to disappear
Mar 23 19:16:27.423: INFO: Pod downwardapi-volume-f3219183-7150-4298-9db6-b535cf2d8a81 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:16:27.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4038" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":89,"skipped":1513,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:16:27.439: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 19:16:32.334: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 19:16:34.359: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720587792, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720587792, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720587792, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720587792, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 19:16:37.401: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:16:37.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7897" for this suite.
STEP: Destroying namespace "webhook-7897-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.235 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":90,"skipped":1515,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:16:37.674: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-4121/configmap-test-8b3d8ba5-6dfd-4c6c-909c-13924a12ea36
STEP: Creating a pod to test consume configMaps
Mar 23 19:16:37.770: INFO: Waiting up to 5m0s for pod "pod-configmaps-259ea7c6-f243-4908-9aad-68b8b3ddea2c" in namespace "configmap-4121" to be "success or failure"
Mar 23 19:16:37.776: INFO: Pod "pod-configmaps-259ea7c6-f243-4908-9aad-68b8b3ddea2c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.772769ms
Mar 23 19:16:39.781: INFO: Pod "pod-configmaps-259ea7c6-f243-4908-9aad-68b8b3ddea2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01109125s
Mar 23 19:16:41.786: INFO: Pod "pod-configmaps-259ea7c6-f243-4908-9aad-68b8b3ddea2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015909978s
STEP: Saw pod success
Mar 23 19:16:41.786: INFO: Pod "pod-configmaps-259ea7c6-f243-4908-9aad-68b8b3ddea2c" satisfied condition "success or failure"
Mar 23 19:16:41.790: INFO: Trying to get logs from node kube17-worker-1 pod pod-configmaps-259ea7c6-f243-4908-9aad-68b8b3ddea2c container env-test: <nil>
STEP: delete the pod
Mar 23 19:16:41.840: INFO: Waiting for pod pod-configmaps-259ea7c6-f243-4908-9aad-68b8b3ddea2c to disappear
Mar 23 19:16:41.847: INFO: Pod pod-configmaps-259ea7c6-f243-4908-9aad-68b8b3ddea2c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:16:41.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4121" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":91,"skipped":1526,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:16:41.864: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar 23 19:16:41.940: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 19:16:41.960: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 19:16:41.965: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-1 before test
Mar 23 19:16:41.977: INFO: kube-proxy-55v4z from kube-system started at 2020-03-23 18:30:18 +0000 UTC (1 container statuses recorded)
Mar 23 19:16:41.977: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:16:41.977: INFO: openebs-ndm-4pklk from openebs started at 2020-03-23 19:09:58 +0000 UTC (1 container statuses recorded)
Mar 23 19:16:41.977: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 19:16:41.977: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-r5zrl from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:16:41.977: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:16:41.977: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:16:41.977: INFO: weave-net-qbqsz from kube-system started at 2020-03-23 19:09:58 +0000 UTC (2 container statuses recorded)
Mar 23 19:16:41.977: INFO: 	Container weave ready: true, restart count 0
Mar 23 19:16:41.977: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 19:16:41.977: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-2 before test
Mar 23 19:16:42.009: INFO: sonobuoy-e2e-job-efff66848912497f from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:16:42.010: INFO: 	Container e2e ready: true, restart count 0
Mar 23 19:16:42.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:16:42.010: INFO: openebs-admission-server-f67f77588-nqvzf from openebs started at 2020-03-23 19:09:26 +0000 UTC (1 container statuses recorded)
Mar 23 19:16:42.010: INFO: 	Container admission-webhook ready: true, restart count 0
Mar 23 19:16:42.011: INFO: openebs-snapshot-operator-6c4c64d4bc-stpl5 from openebs started at 2020-03-23 19:09:26 +0000 UTC (2 container statuses recorded)
Mar 23 19:16:42.011: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 23 19:16:42.011: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Mar 23 19:16:42.011: INFO: kube-proxy-n567x from kube-system started at 2020-03-23 18:31:10 +0000 UTC (1 container statuses recorded)
Mar 23 19:16:42.011: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:16:42.011: INFO: weave-net-92dzp from kube-system started at 2020-03-23 19:06:40 +0000 UTC (2 container statuses recorded)
Mar 23 19:16:42.012: INFO: 	Container weave ready: true, restart count 0
Mar 23 19:16:42.012: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 19:16:42.012: INFO: sonobuoy from sonobuoy started at 2020-03-23 18:49:13 +0000 UTC (1 container statuses recorded)
Mar 23 19:16:42.012: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 23 19:16:42.012: INFO: openebs-ndm-mwljx from openebs started at 2020-03-23 19:06:50 +0000 UTC (1 container statuses recorded)
Mar 23 19:16:42.012: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 19:16:42.012: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-tx9p5 from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:16:42.012: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:16:42.012: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:16:42.013: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-3 before test
Mar 23 19:16:42.049: INFO: maya-apiserver-569c7c785b-ltfsc from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:16:42.049: INFO: 	Container maya-apiserver ready: true, restart count 3
Mar 23 19:16:42.049: INFO: kube-proxy-2qrv2 from kube-system started at 2020-03-23 18:30:45 +0000 UTC (1 container statuses recorded)
Mar 23 19:16:42.049: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:16:42.049: INFO: weave-net-tqwcx from kube-system started at 2020-03-23 18:30:45 +0000 UTC (2 container statuses recorded)
Mar 23 19:16:42.049: INFO: 	Container weave ready: true, restart count 0
Mar 23 19:16:42.049: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 19:16:42.049: INFO: openebs-localpv-provisioner-5c87bbd974-pwwkj from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:16:42.049: INFO: 	Container openebs-provisioner-hostpath ready: true, restart count 1
Mar 23 19:16:42.049: INFO: openebs-ndm-d9t4c from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:16:42.049: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 19:16:42.049: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-prwqb from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:16:42.049: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:16:42.049: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:16:42.049: INFO: openebs-provisioner-7b8c68bf44-6w5b6 from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:16:42.049: INFO: 	Container openebs-provisioner ready: true, restart count 1
Mar 23 19:16:42.049: INFO: openebs-ndm-operator-5fccfb7976-vzw5m from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:16:42.049: INFO: 	Container node-disk-operator ready: true, restart count 1
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-790776b6-3955-42cc-86d5-c0e90e31ad3b 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-790776b6-3955-42cc-86d5-c0e90e31ad3b off the node kube17-worker-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-790776b6-3955-42cc-86d5-c0e90e31ad3b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:16:56.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7122" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:14.422 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":92,"skipped":1530,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:16:56.287: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:17:09.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7368" for this suite.

• [SLOW TEST:13.242 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":93,"skipped":1532,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:17:09.533: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-a5d024ac-aa9b-4b66-8c8b-45e7c84476c8
STEP: Creating a pod to test consume configMaps
Mar 23 19:17:09.621: INFO: Waiting up to 5m0s for pod "pod-configmaps-2475751e-5cf3-4026-99aa-541dece35e87" in namespace "configmap-9933" to be "success or failure"
Mar 23 19:17:09.631: INFO: Pod "pod-configmaps-2475751e-5cf3-4026-99aa-541dece35e87": Phase="Pending", Reason="", readiness=false. Elapsed: 9.764281ms
Mar 23 19:17:11.640: INFO: Pod "pod-configmaps-2475751e-5cf3-4026-99aa-541dece35e87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019108284s
STEP: Saw pod success
Mar 23 19:17:11.641: INFO: Pod "pod-configmaps-2475751e-5cf3-4026-99aa-541dece35e87" satisfied condition "success or failure"
Mar 23 19:17:11.649: INFO: Trying to get logs from node kube17-worker-1 pod pod-configmaps-2475751e-5cf3-4026-99aa-541dece35e87 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 19:17:11.705: INFO: Waiting for pod pod-configmaps-2475751e-5cf3-4026-99aa-541dece35e87 to disappear
Mar 23 19:17:11.711: INFO: Pod pod-configmaps-2475751e-5cf3-4026-99aa-541dece35e87 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:17:11.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9933" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":94,"skipped":1537,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:17:11.729: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 23 19:17:19.916: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 19:17:19.923: INFO: Pod pod-with-prestop-http-hook still exists
Mar 23 19:17:21.923: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 19:17:21.929: INFO: Pod pod-with-prestop-http-hook still exists
Mar 23 19:17:23.923: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 19:17:23.932: INFO: Pod pod-with-prestop-http-hook still exists
Mar 23 19:17:25.923: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 19:17:25.929: INFO: Pod pod-with-prestop-http-hook still exists
Mar 23 19:17:27.924: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 19:17:27.932: INFO: Pod pod-with-prestop-http-hook still exists
Mar 23 19:17:29.923: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 19:17:30.147: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:17:30.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7894" for this suite.

• [SLOW TEST:18.453 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":95,"skipped":1565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:17:30.183: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1525
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 19:17:30.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-4834'
Mar 23 19:17:30.497: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 23 19:17:30.497: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Mar 23 19:17:30.508: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-pckjh]
Mar 23 19:17:30.508: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-pckjh" in namespace "kubectl-4834" to be "running and ready"
Mar 23 19:17:30.521: INFO: Pod "e2e-test-httpd-rc-pckjh": Phase="Pending", Reason="", readiness=false. Elapsed: 12.388846ms
Mar 23 19:17:32.526: INFO: Pod "e2e-test-httpd-rc-pckjh": Phase="Running", Reason="", readiness=true. Elapsed: 2.01744351s
Mar 23 19:17:32.526: INFO: Pod "e2e-test-httpd-rc-pckjh" satisfied condition "running and ready"
Mar 23 19:17:32.526: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-pckjh]
Mar 23 19:17:32.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 logs rc/e2e-test-httpd-rc --namespace=kubectl-4834'
Mar 23 19:17:32.811: INFO: stderr: ""
Mar 23 19:17:32.811: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.44.0.2. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.44.0.2. Set the 'ServerName' directive globally to suppress this message\n[Mon Mar 23 19:17:31.811160 2020] [mpm_event:notice] [pid 1:tid 140394159299432] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Mar 23 19:17:31.811256 2020] [core:notice] [pid 1:tid 140394159299432] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1530
Mar 23 19:17:32.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete rc e2e-test-httpd-rc --namespace=kubectl-4834'
Mar 23 19:17:33.061: INFO: stderr: ""
Mar 23 19:17:33.061: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:17:33.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4834" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":280,"completed":96,"skipped":1598,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:17:33.088: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 19:17:33.200: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f2fc25a6-ffec-494f-9686-9704d8910c6e" in namespace "projected-2494" to be "success or failure"
Mar 23 19:17:33.209: INFO: Pod "downwardapi-volume-f2fc25a6-ffec-494f-9686-9704d8910c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006078ms
Mar 23 19:17:35.218: INFO: Pod "downwardapi-volume-f2fc25a6-ffec-494f-9686-9704d8910c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017483311s
Mar 23 19:17:37.223: INFO: Pod "downwardapi-volume-f2fc25a6-ffec-494f-9686-9704d8910c6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022891336s
STEP: Saw pod success
Mar 23 19:17:37.223: INFO: Pod "downwardapi-volume-f2fc25a6-ffec-494f-9686-9704d8910c6e" satisfied condition "success or failure"
Mar 23 19:17:37.227: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-f2fc25a6-ffec-494f-9686-9704d8910c6e container client-container: <nil>
STEP: delete the pod
Mar 23 19:17:37.265: INFO: Waiting for pod downwardapi-volume-f2fc25a6-ffec-494f-9686-9704d8910c6e to disappear
Mar 23 19:17:37.273: INFO: Pod downwardapi-volume-f2fc25a6-ffec-494f-9686-9704d8910c6e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:17:37.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2494" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":97,"skipped":1602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:17:37.294: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-86a54b93-1ddc-4c2a-8db7-febe5aeea00f
STEP: Creating a pod to test consume configMaps
Mar 23 19:17:37.404: INFO: Waiting up to 5m0s for pod "pod-configmaps-4d8f1d90-3347-4ff2-8cbd-e72804818947" in namespace "configmap-5639" to be "success or failure"
Mar 23 19:17:37.422: INFO: Pod "pod-configmaps-4d8f1d90-3347-4ff2-8cbd-e72804818947": Phase="Pending", Reason="", readiness=false. Elapsed: 17.858673ms
Mar 23 19:17:39.428: INFO: Pod "pod-configmaps-4d8f1d90-3347-4ff2-8cbd-e72804818947": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024050918s
Mar 23 19:17:41.434: INFO: Pod "pod-configmaps-4d8f1d90-3347-4ff2-8cbd-e72804818947": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030523367s
STEP: Saw pod success
Mar 23 19:17:41.435: INFO: Pod "pod-configmaps-4d8f1d90-3347-4ff2-8cbd-e72804818947" satisfied condition "success or failure"
Mar 23 19:17:41.439: INFO: Trying to get logs from node kube17-worker-1 pod pod-configmaps-4d8f1d90-3347-4ff2-8cbd-e72804818947 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 19:17:41.482: INFO: Waiting for pod pod-configmaps-4d8f1d90-3347-4ff2-8cbd-e72804818947 to disappear
Mar 23 19:17:41.489: INFO: Pod pod-configmaps-4d8f1d90-3347-4ff2-8cbd-e72804818947 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:17:41.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5639" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":98,"skipped":1639,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:17:41.509: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 23 19:17:44.636: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:17:44.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6429" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":99,"skipped":1640,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:17:44.681: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-a3f500d0-05a7-4b05-abe4-47a3688ffb04
Mar 23 19:17:44.771: INFO: Pod name my-hostname-basic-a3f500d0-05a7-4b05-abe4-47a3688ffb04: Found 0 pods out of 1
Mar 23 19:17:49.779: INFO: Pod name my-hostname-basic-a3f500d0-05a7-4b05-abe4-47a3688ffb04: Found 1 pods out of 1
Mar 23 19:17:49.779: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a3f500d0-05a7-4b05-abe4-47a3688ffb04" are running
Mar 23 19:17:49.788: INFO: Pod "my-hostname-basic-a3f500d0-05a7-4b05-abe4-47a3688ffb04-mhh9x" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 19:17:44 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 19:17:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 19:17:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 19:17:44 +0000 UTC Reason: Message:}])
Mar 23 19:17:49.789: INFO: Trying to dial the pod
Mar 23 19:17:54.807: INFO: Controller my-hostname-basic-a3f500d0-05a7-4b05-abe4-47a3688ffb04: Got expected result from replica 1 [my-hostname-basic-a3f500d0-05a7-4b05-abe4-47a3688ffb04-mhh9x]: "my-hostname-basic-a3f500d0-05a7-4b05-abe4-47a3688ffb04-mhh9x", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:17:54.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2728" for this suite.

• [SLOW TEST:10.146 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":100,"skipped":1652,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:17:54.828: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar 23 19:17:54.918: INFO: Waiting up to 5m0s for pod "downward-api-d1a8ef1f-a2f3-4bca-8151-98d3391143f9" in namespace "downward-api-5748" to be "success or failure"
Mar 23 19:17:54.938: INFO: Pod "downward-api-d1a8ef1f-a2f3-4bca-8151-98d3391143f9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.902804ms
Mar 23 19:17:56.944: INFO: Pod "downward-api-d1a8ef1f-a2f3-4bca-8151-98d3391143f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023849401s
STEP: Saw pod success
Mar 23 19:17:56.944: INFO: Pod "downward-api-d1a8ef1f-a2f3-4bca-8151-98d3391143f9" satisfied condition "success or failure"
Mar 23 19:17:56.949: INFO: Trying to get logs from node kube17-worker-1 pod downward-api-d1a8ef1f-a2f3-4bca-8151-98d3391143f9 container dapi-container: <nil>
STEP: delete the pod
Mar 23 19:17:56.993: INFO: Waiting for pod downward-api-d1a8ef1f-a2f3-4bca-8151-98d3391143f9 to disappear
Mar 23 19:17:56.999: INFO: Pod downward-api-d1a8ef1f-a2f3-4bca-8151-98d3391143f9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:17:56.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5748" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":101,"skipped":1656,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:17:57.019: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 19:17:57.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dc4b335a-06d4-4887-ac94-ac37c7b8a672" in namespace "downward-api-2791" to be "success or failure"
Mar 23 19:17:57.140: INFO: Pod "downwardapi-volume-dc4b335a-06d4-4887-ac94-ac37c7b8a672": Phase="Pending", Reason="", readiness=false. Elapsed: 13.88264ms
Mar 23 19:17:59.145: INFO: Pod "downwardapi-volume-dc4b335a-06d4-4887-ac94-ac37c7b8a672": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018965352s
Mar 23 19:18:01.152: INFO: Pod "downwardapi-volume-dc4b335a-06d4-4887-ac94-ac37c7b8a672": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025907474s
STEP: Saw pod success
Mar 23 19:18:01.152: INFO: Pod "downwardapi-volume-dc4b335a-06d4-4887-ac94-ac37c7b8a672" satisfied condition "success or failure"
Mar 23 19:18:01.156: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-dc4b335a-06d4-4887-ac94-ac37c7b8a672 container client-container: <nil>
STEP: delete the pod
Mar 23 19:18:01.202: INFO: Waiting for pod downwardapi-volume-dc4b335a-06d4-4887-ac94-ac37c7b8a672 to disappear
Mar 23 19:18:01.206: INFO: Pod downwardapi-volume-dc4b335a-06d4-4887-ac94-ac37c7b8a672 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:18:01.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2791" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":102,"skipped":1674,"failed":0}

------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:18:01.227: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:18:01.344: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ca74960b-1fcc-452b-8b96-26c3b7b473e4" in namespace "security-context-test-8557" to be "success or failure"
Mar 23 19:18:01.350: INFO: Pod "busybox-user-65534-ca74960b-1fcc-452b-8b96-26c3b7b473e4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.409042ms
Mar 23 19:18:03.364: INFO: Pod "busybox-user-65534-ca74960b-1fcc-452b-8b96-26c3b7b473e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020222238s
Mar 23 19:18:03.365: INFO: Pod "busybox-user-65534-ca74960b-1fcc-452b-8b96-26c3b7b473e4" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:18:03.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8557" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":103,"skipped":1674,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:18:03.381: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-5cfad57a-5bbe-4cc2-a2fb-be02a67f7376
STEP: Creating a pod to test consume secrets
Mar 23 19:18:03.504: INFO: Waiting up to 5m0s for pod "pod-secrets-ee2b9de0-71d7-431e-a445-ea62bf27e2ab" in namespace "secrets-7952" to be "success or failure"
Mar 23 19:18:03.522: INFO: Pod "pod-secrets-ee2b9de0-71d7-431e-a445-ea62bf27e2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 16.967989ms
Mar 23 19:18:05.527: INFO: Pod "pod-secrets-ee2b9de0-71d7-431e-a445-ea62bf27e2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02244381s
Mar 23 19:18:07.533: INFO: Pod "pod-secrets-ee2b9de0-71d7-431e-a445-ea62bf27e2ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028096888s
STEP: Saw pod success
Mar 23 19:18:07.533: INFO: Pod "pod-secrets-ee2b9de0-71d7-431e-a445-ea62bf27e2ab" satisfied condition "success or failure"
Mar 23 19:18:07.537: INFO: Trying to get logs from node kube17-worker-1 pod pod-secrets-ee2b9de0-71d7-431e-a445-ea62bf27e2ab container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 19:18:07.582: INFO: Waiting for pod pod-secrets-ee2b9de0-71d7-431e-a445-ea62bf27e2ab to disappear
Mar 23 19:18:07.587: INFO: Pod pod-secrets-ee2b9de0-71d7-431e-a445-ea62bf27e2ab no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:18:07.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7952" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":104,"skipped":1693,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:18:07.627: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-663c029e-0b9c-46ed-81a2-0780b3d1d33a
STEP: Creating a pod to test consume secrets
Mar 23 19:18:07.729: INFO: Waiting up to 5m0s for pod "pod-secrets-00f25241-f05a-4d25-a5dd-c7262538ebef" in namespace "secrets-6832" to be "success or failure"
Mar 23 19:18:07.744: INFO: Pod "pod-secrets-00f25241-f05a-4d25-a5dd-c7262538ebef": Phase="Pending", Reason="", readiness=false. Elapsed: 15.504549ms
Mar 23 19:18:09.750: INFO: Pod "pod-secrets-00f25241-f05a-4d25-a5dd-c7262538ebef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021547951s
STEP: Saw pod success
Mar 23 19:18:09.751: INFO: Pod "pod-secrets-00f25241-f05a-4d25-a5dd-c7262538ebef" satisfied condition "success or failure"
Mar 23 19:18:09.755: INFO: Trying to get logs from node kube17-worker-1 pod pod-secrets-00f25241-f05a-4d25-a5dd-c7262538ebef container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 19:18:09.803: INFO: Waiting for pod pod-secrets-00f25241-f05a-4d25-a5dd-c7262538ebef to disappear
Mar 23 19:18:09.808: INFO: Pod pod-secrets-00f25241-f05a-4d25-a5dd-c7262538ebef no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:18:09.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6832" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":105,"skipped":1711,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:18:09.826: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-a785651b-d23a-49da-bfdf-46c08a6fad0c
STEP: Creating a pod to test consume configMaps
Mar 23 19:18:09.945: INFO: Waiting up to 5m0s for pod "pod-configmaps-8beff640-cc0b-4e95-beea-9ba2ea55a506" in namespace "configmap-4067" to be "success or failure"
Mar 23 19:18:09.952: INFO: Pod "pod-configmaps-8beff640-cc0b-4e95-beea-9ba2ea55a506": Phase="Pending", Reason="", readiness=false. Elapsed: 6.504785ms
Mar 23 19:18:11.958: INFO: Pod "pod-configmaps-8beff640-cc0b-4e95-beea-9ba2ea55a506": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012137565s
STEP: Saw pod success
Mar 23 19:18:11.958: INFO: Pod "pod-configmaps-8beff640-cc0b-4e95-beea-9ba2ea55a506" satisfied condition "success or failure"
Mar 23 19:18:11.963: INFO: Trying to get logs from node kube17-worker-1 pod pod-configmaps-8beff640-cc0b-4e95-beea-9ba2ea55a506 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 19:18:12.010: INFO: Waiting for pod pod-configmaps-8beff640-cc0b-4e95-beea-9ba2ea55a506 to disappear
Mar 23 19:18:12.015: INFO: Pod pod-configmaps-8beff640-cc0b-4e95-beea-9ba2ea55a506 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:18:12.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4067" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":106,"skipped":1724,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:18:12.034: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar 23 19:18:12.109: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 19:18:12.132: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 19:18:12.137: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-1 before test
Mar 23 19:18:12.151: INFO: kube-proxy-55v4z from kube-system started at 2020-03-23 18:30:18 +0000 UTC (1 container statuses recorded)
Mar 23 19:18:12.151: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:18:12.151: INFO: openebs-ndm-4pklk from openebs started at 2020-03-23 19:09:58 +0000 UTC (1 container statuses recorded)
Mar 23 19:18:12.152: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 19:18:12.152: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-r5zrl from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:18:12.152: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:18:12.152: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:18:12.153: INFO: weave-net-qbqsz from kube-system started at 2020-03-23 19:09:58 +0000 UTC (2 container statuses recorded)
Mar 23 19:18:12.153: INFO: 	Container weave ready: true, restart count 0
Mar 23 19:18:12.153: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 19:18:12.153: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-2 before test
Mar 23 19:18:12.187: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-tx9p5 from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:18:12.187: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:18:12.187: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:18:12.187: INFO: openebs-ndm-mwljx from openebs started at 2020-03-23 19:06:50 +0000 UTC (1 container statuses recorded)
Mar 23 19:18:12.187: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 19:18:12.187: INFO: weave-net-92dzp from kube-system started at 2020-03-23 19:06:40 +0000 UTC (2 container statuses recorded)
Mar 23 19:18:12.187: INFO: 	Container weave ready: true, restart count 0
Mar 23 19:18:12.187: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 19:18:12.187: INFO: sonobuoy from sonobuoy started at 2020-03-23 18:49:13 +0000 UTC (1 container statuses recorded)
Mar 23 19:18:12.187: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 23 19:18:12.187: INFO: sonobuoy-e2e-job-efff66848912497f from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:18:12.187: INFO: 	Container e2e ready: true, restart count 0
Mar 23 19:18:12.187: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:18:12.187: INFO: openebs-admission-server-f67f77588-nqvzf from openebs started at 2020-03-23 19:09:26 +0000 UTC (1 container statuses recorded)
Mar 23 19:18:12.187: INFO: 	Container admission-webhook ready: true, restart count 0
Mar 23 19:18:12.187: INFO: openebs-snapshot-operator-6c4c64d4bc-stpl5 from openebs started at 2020-03-23 19:09:26 +0000 UTC (2 container statuses recorded)
Mar 23 19:18:12.187: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 23 19:18:12.187: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Mar 23 19:18:12.187: INFO: kube-proxy-n567x from kube-system started at 2020-03-23 18:31:10 +0000 UTC (1 container statuses recorded)
Mar 23 19:18:12.187: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:18:12.187: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-3 before test
Mar 23 19:18:12.222: INFO: openebs-provisioner-7b8c68bf44-6w5b6 from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:18:12.223: INFO: 	Container openebs-provisioner ready: true, restart count 1
Mar 23 19:18:12.223: INFO: openebs-ndm-operator-5fccfb7976-vzw5m from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:18:12.223: INFO: 	Container node-disk-operator ready: true, restart count 1
Mar 23 19:18:12.223: INFO: openebs-localpv-provisioner-5c87bbd974-pwwkj from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:18:12.224: INFO: 	Container openebs-provisioner-hostpath ready: true, restart count 1
Mar 23 19:18:12.224: INFO: openebs-ndm-d9t4c from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:18:12.224: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 19:18:12.224: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-prwqb from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:18:12.224: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:18:12.225: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:18:12.225: INFO: kube-proxy-2qrv2 from kube-system started at 2020-03-23 18:30:45 +0000 UTC (1 container statuses recorded)
Mar 23 19:18:12.225: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:18:12.225: INFO: weave-net-tqwcx from kube-system started at 2020-03-23 18:30:45 +0000 UTC (2 container statuses recorded)
Mar 23 19:18:12.225: INFO: 	Container weave ready: true, restart count 0
Mar 23 19:18:12.226: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 19:18:12.226: INFO: maya-apiserver-569c7c785b-ltfsc from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:18:12.226: INFO: 	Container maya-apiserver ready: true, restart count 3
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-acb66de0-4493-44a7-9fa5-e14f8fca0b28 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-acb66de0-4493-44a7-9fa5-e14f8fca0b28 off the node kube17-worker-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-acb66de0-4493-44a7-9fa5-e14f8fca0b28
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:23:20.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2550" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:308.392 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":107,"skipped":1777,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:23:20.428: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 23 19:23:21.790: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar 23 19:23:23.807: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588201, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588201, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588201, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588201, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 19:23:26.844: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:23:26.878: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:23:28.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6215" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:8.273 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":108,"skipped":1791,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:23:28.704: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:23:28.811: INFO: (0) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 28.424863ms)
Mar 23 19:23:28.817: INFO: (1) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.090746ms)
Mar 23 19:23:28.822: INFO: (2) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.73979ms)
Mar 23 19:23:28.827: INFO: (3) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.932807ms)
Mar 23 19:23:28.831: INFO: (4) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.858779ms)
Mar 23 19:23:28.836: INFO: (5) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.724497ms)
Mar 23 19:23:28.841: INFO: (6) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.300563ms)
Mar 23 19:23:28.850: INFO: (7) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.327567ms)
Mar 23 19:23:28.855: INFO: (8) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.497332ms)
Mar 23 19:23:28.860: INFO: (9) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.463101ms)
Mar 23 19:23:28.866: INFO: (10) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.68571ms)
Mar 23 19:23:28.871: INFO: (11) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.96161ms)
Mar 23 19:23:28.876: INFO: (12) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.955812ms)
Mar 23 19:23:28.885: INFO: (13) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.81911ms)
Mar 23 19:23:28.890: INFO: (14) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.851501ms)
Mar 23 19:23:28.895: INFO: (15) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.563045ms)
Mar 23 19:23:28.908: INFO: (16) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.011246ms)
Mar 23 19:23:28.918: INFO: (17) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.782857ms)
Mar 23 19:23:28.933: INFO: (18) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.795451ms)
Mar 23 19:23:28.941: INFO: (19) /api/v1/nodes/kube17-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.085946ms)
[AfterEach] version v1
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:23:28.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1270" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":109,"skipped":1807,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:23:28.971: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:23:29.153: INFO: Creating ReplicaSet my-hostname-basic-3c3bcad1-8fe8-4e1f-ba35-b1d6045c798d
Mar 23 19:23:29.188: INFO: Pod name my-hostname-basic-3c3bcad1-8fe8-4e1f-ba35-b1d6045c798d: Found 0 pods out of 1
Mar 23 19:23:34.199: INFO: Pod name my-hostname-basic-3c3bcad1-8fe8-4e1f-ba35-b1d6045c798d: Found 1 pods out of 1
Mar 23 19:23:34.200: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3c3bcad1-8fe8-4e1f-ba35-b1d6045c798d" is running
Mar 23 19:23:34.210: INFO: Pod "my-hostname-basic-3c3bcad1-8fe8-4e1f-ba35-b1d6045c798d-pp5mg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 19:23:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 19:23:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 19:23:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 19:23:29 +0000 UTC Reason: Message:}])
Mar 23 19:23:34.210: INFO: Trying to dial the pod
Mar 23 19:23:39.233: INFO: Controller my-hostname-basic-3c3bcad1-8fe8-4e1f-ba35-b1d6045c798d: Got expected result from replica 1 [my-hostname-basic-3c3bcad1-8fe8-4e1f-ba35-b1d6045c798d-pp5mg]: "my-hostname-basic-3c3bcad1-8fe8-4e1f-ba35-b1d6045c798d-pp5mg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:23:39.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-193" for this suite.

• [SLOW TEST:10.278 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":110,"skipped":1842,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:23:39.251: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Mar 23 19:23:43.166: INFO: Pod pod-hostip-cac56076-9430-410e-b8b5-43c2d118032d has hostIP: 10.30.20.153
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:23:43.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1577" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":111,"skipped":1850,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:23:43.243: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 19:23:43.317: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ff270eb-9907-4812-87d9-7157ffe0bcf9" in namespace "downward-api-5189" to be "success or failure"
Mar 23 19:23:43.325: INFO: Pod "downwardapi-volume-3ff270eb-9907-4812-87d9-7157ffe0bcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.288123ms
Mar 23 19:23:45.341: INFO: Pod "downwardapi-volume-3ff270eb-9907-4812-87d9-7157ffe0bcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023596996s
Mar 23 19:23:47.347: INFO: Pod "downwardapi-volume-3ff270eb-9907-4812-87d9-7157ffe0bcf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029634448s
STEP: Saw pod success
Mar 23 19:23:47.347: INFO: Pod "downwardapi-volume-3ff270eb-9907-4812-87d9-7157ffe0bcf9" satisfied condition "success or failure"
Mar 23 19:23:47.351: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-3ff270eb-9907-4812-87d9-7157ffe0bcf9 container client-container: <nil>
STEP: delete the pod
Mar 23 19:23:47.390: INFO: Waiting for pod downwardapi-volume-3ff270eb-9907-4812-87d9-7157ffe0bcf9 to disappear
Mar 23 19:23:47.397: INFO: Pod downwardapi-volume-3ff270eb-9907-4812-87d9-7157ffe0bcf9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:23:47.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5189" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":112,"skipped":1860,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:23:47.418: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 19:23:47.510: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8531acd0-b7a8-4a33-b7b4-1df37eff5e0c" in namespace "projected-4550" to be "success or failure"
Mar 23 19:23:47.525: INFO: Pod "downwardapi-volume-8531acd0-b7a8-4a33-b7b4-1df37eff5e0c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.373335ms
Mar 23 19:23:49.531: INFO: Pod "downwardapi-volume-8531acd0-b7a8-4a33-b7b4-1df37eff5e0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020442943s
STEP: Saw pod success
Mar 23 19:23:49.531: INFO: Pod "downwardapi-volume-8531acd0-b7a8-4a33-b7b4-1df37eff5e0c" satisfied condition "success or failure"
Mar 23 19:23:49.535: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-8531acd0-b7a8-4a33-b7b4-1df37eff5e0c container client-container: <nil>
STEP: delete the pod
Mar 23 19:23:49.593: INFO: Waiting for pod downwardapi-volume-8531acd0-b7a8-4a33-b7b4-1df37eff5e0c to disappear
Mar 23 19:23:49.611: INFO: Pod downwardapi-volume-8531acd0-b7a8-4a33-b7b4-1df37eff5e0c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:23:49.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4550" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":113,"skipped":1867,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:23:49.646: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6106.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6106.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 19:23:53.921: INFO: DNS probes using dns-6106/dns-test-c9649260-0a3d-4c10-9289-480c9762d0c2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:23:53.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6106" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":114,"skipped":1894,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:23:53.981: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:23:54.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5492" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":115,"skipped":1903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:23:54.176: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 19:23:55.926: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 19:23:57.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588235, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588235, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588236, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588235, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 19:24:00.971: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:24:00.976: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4794-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:24:02.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7696" for this suite.
STEP: Destroying namespace "webhook-7696-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.441 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":116,"skipped":1964,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:24:02.617: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-f462ff3c-6273-403d-851c-3c51e96257b7 in namespace container-probe-6090
Mar 23 19:24:06.713: INFO: Started pod liveness-f462ff3c-6273-403d-851c-3c51e96257b7 in namespace container-probe-6090
STEP: checking the pod's current state and verifying that restartCount is present
Mar 23 19:24:06.718: INFO: Initial restart count of pod liveness-f462ff3c-6273-403d-851c-3c51e96257b7 is 0
Mar 23 19:24:24.778: INFO: Restart count of pod container-probe-6090/liveness-f462ff3c-6273-403d-851c-3c51e96257b7 is now 1 (18.059795472s elapsed)
Mar 23 19:24:42.832: INFO: Restart count of pod container-probe-6090/liveness-f462ff3c-6273-403d-851c-3c51e96257b7 is now 2 (36.113758785s elapsed)
Mar 23 19:25:02.900: INFO: Restart count of pod container-probe-6090/liveness-f462ff3c-6273-403d-851c-3c51e96257b7 is now 3 (56.181481075s elapsed)
Mar 23 19:25:23.767: INFO: Restart count of pod container-probe-6090/liveness-f462ff3c-6273-403d-851c-3c51e96257b7 is now 4 (1m17.049088825s elapsed)
Mar 23 19:26:26.616: INFO: Restart count of pod container-probe-6090/liveness-f462ff3c-6273-403d-851c-3c51e96257b7 is now 5 (2m19.897517559s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:26:29.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6090" for this suite.

• [SLOW TEST:147.284 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":117,"skipped":1966,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:26:29.915: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:26:30.038: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Creating first CR 
Mar 23 19:26:30.707: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-23T19:26:30Z generation:1 name:name1 resourceVersion:21589 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:94ee6458-b35d-44c6-94fd-21a4953745fc] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 23 19:26:40.717: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-23T19:26:40Z generation:1 name:name2 resourceVersion:21630 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f6f2afc5-f0fe-4871-8db7-6a81d4f46260] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 23 19:26:50.727: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-23T19:26:30Z generation:2 name:name1 resourceVersion:21664 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:94ee6458-b35d-44c6-94fd-21a4953745fc] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 23 19:27:00.740: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-23T19:26:40Z generation:2 name:name2 resourceVersion:21698 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f6f2afc5-f0fe-4871-8db7-6a81d4f46260] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 23 19:27:11.108: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-23T19:26:30Z generation:2 name:name1 resourceVersion:21732 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:94ee6458-b35d-44c6-94fd-21a4953745fc] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 23 19:27:22.279: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-23T19:26:40Z generation:2 name:name2 resourceVersion:21766 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f6f2afc5-f0fe-4871-8db7-6a81d4f46260] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:27:36.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-4443" for this suite.

• [SLOW TEST:66.131 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":118,"skipped":1990,"failed":0}
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:27:36.048: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:27:42.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9652" for this suite.

• [SLOW TEST:6.778 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":119,"skipped":1990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:27:42.828: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-61b7e6ad-1feb-438a-8719-038227e3ddbc
STEP: Creating secret with name s-test-opt-upd-e103bbaa-4476-4e49-b768-745c8c0611a8
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-61b7e6ad-1feb-438a-8719-038227e3ddbc
STEP: Updating secret s-test-opt-upd-e103bbaa-4476-4e49-b768-745c8c0611a8
STEP: Creating secret with name s-test-opt-create-e492a4c9-b952-4f09-980c-55d0333f6f1b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:29:11.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3105" for this suite.

• [SLOW TEST:88.233 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":120,"skipped":2017,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:29:11.069: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 23 19:29:13.676: INFO: Successfully updated pod "pod-update-7582c4e5-d32e-4e44-8aba-3a8455b6d3a8"
STEP: verifying the updated pod is in kubernetes
Mar 23 19:29:13.791: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:29:13.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2966" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":121,"skipped":2066,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:29:13.815: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1489
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 19:29:14.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5620'
Mar 23 19:29:14.553: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 23 19:29:14.553: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1495
Mar 23 19:29:14.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete deployment e2e-test-httpd-deployment --namespace=kubectl-5620'
Mar 23 19:29:14.801: INFO: stderr: ""
Mar 23 19:29:14.802: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:29:14.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5620" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":122,"skipped":2106,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:29:14.965: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar 23 19:29:15.106: INFO: Waiting up to 5m0s for pod "downward-api-23570e23-6247-4567-b77a-b0739dc9e901" in namespace "downward-api-5746" to be "success or failure"
Mar 23 19:29:15.132: INFO: Pod "downward-api-23570e23-6247-4567-b77a-b0739dc9e901": Phase="Pending", Reason="", readiness=false. Elapsed: 26.096189ms
Mar 23 19:29:17.141: INFO: Pod "downward-api-23570e23-6247-4567-b77a-b0739dc9e901": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034251586s
Mar 23 19:29:19.153: INFO: Pod "downward-api-23570e23-6247-4567-b77a-b0739dc9e901": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046378747s
Mar 23 19:29:21.158: INFO: Pod "downward-api-23570e23-6247-4567-b77a-b0739dc9e901": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051524445s
STEP: Saw pod success
Mar 23 19:29:21.158: INFO: Pod "downward-api-23570e23-6247-4567-b77a-b0739dc9e901" satisfied condition "success or failure"
Mar 23 19:29:21.163: INFO: Trying to get logs from node kube17-worker-1 pod downward-api-23570e23-6247-4567-b77a-b0739dc9e901 container dapi-container: <nil>
STEP: delete the pod
Mar 23 19:29:21.204: INFO: Waiting for pod downward-api-23570e23-6247-4567-b77a-b0739dc9e901 to disappear
Mar 23 19:29:21.212: INFO: Pod downward-api-23570e23-6247-4567-b77a-b0739dc9e901 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:29:21.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5746" for this suite.

• [SLOW TEST:6.264 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":123,"skipped":2139,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:29:21.233: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:29:34.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6941" for this suite.

• [SLOW TEST:14.189 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":124,"skipped":2139,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:29:35.424: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-8a61abad-3a67-4ccb-96fb-9b9f1a9bd20c in namespace container-probe-749
Mar 23 19:29:40.378: INFO: Started pod test-webserver-8a61abad-3a67-4ccb-96fb-9b9f1a9bd20c in namespace container-probe-749
STEP: checking the pod's current state and verifying that restartCount is present
Mar 23 19:29:40.461: INFO: Initial restart count of pod test-webserver-8a61abad-3a67-4ccb-96fb-9b9f1a9bd20c is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:33:40.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-749" for this suite.

• [SLOW TEST:245.531 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":125,"skipped":2161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:33:40.956: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-21df964a-b361-47e7-9f54-fb61c6491787
STEP: Creating a pod to test consume secrets
Mar 23 19:33:41.027: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-57619766-4833-411a-956f-d0d34e8de273" in namespace "projected-9936" to be "success or failure"
Mar 23 19:33:41.030: INFO: Pod "pod-projected-secrets-57619766-4833-411a-956f-d0d34e8de273": Phase="Pending", Reason="", readiness=false. Elapsed: 2.66411ms
Mar 23 19:33:43.035: INFO: Pod "pod-projected-secrets-57619766-4833-411a-956f-d0d34e8de273": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007840266s
Mar 23 19:33:45.040: INFO: Pod "pod-projected-secrets-57619766-4833-411a-956f-d0d34e8de273": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012316138s
STEP: Saw pod success
Mar 23 19:33:45.040: INFO: Pod "pod-projected-secrets-57619766-4833-411a-956f-d0d34e8de273" satisfied condition "success or failure"
Mar 23 19:33:45.048: INFO: Trying to get logs from node kube17-worker-1 pod pod-projected-secrets-57619766-4833-411a-956f-d0d34e8de273 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 23 19:33:45.108: INFO: Waiting for pod pod-projected-secrets-57619766-4833-411a-956f-d0d34e8de273 to disappear
Mar 23 19:33:45.111: INFO: Pod pod-projected-secrets-57619766-4833-411a-956f-d0d34e8de273 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:33:45.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9936" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":126,"skipped":2191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:33:45.125: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-da7ff033-dcaf-42d4-8a0d-9a4b7f9cbc21
STEP: Creating secret with name secret-projected-all-test-volume-396c03e4-b0a9-4d79-953b-289ba73f5baa
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 23 19:33:45.209: INFO: Waiting up to 5m0s for pod "projected-volume-2e3d658f-282e-4dbc-ba69-3c54b3b4a90a" in namespace "projected-361" to be "success or failure"
Mar 23 19:33:45.223: INFO: Pod "projected-volume-2e3d658f-282e-4dbc-ba69-3c54b3b4a90a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.603108ms
Mar 23 19:33:47.233: INFO: Pod "projected-volume-2e3d658f-282e-4dbc-ba69-3c54b3b4a90a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023381736s
Mar 23 19:33:50.064: INFO: Pod "projected-volume-2e3d658f-282e-4dbc-ba69-3c54b3b4a90a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.853993901s
STEP: Saw pod success
Mar 23 19:33:50.064: INFO: Pod "projected-volume-2e3d658f-282e-4dbc-ba69-3c54b3b4a90a" satisfied condition "success or failure"
Mar 23 19:33:50.087: INFO: Trying to get logs from node kube17-worker-1 pod projected-volume-2e3d658f-282e-4dbc-ba69-3c54b3b4a90a container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 23 19:33:50.211: INFO: Waiting for pod projected-volume-2e3d658f-282e-4dbc-ba69-3c54b3b4a90a to disappear
Mar 23 19:33:50.215: INFO: Pod projected-volume-2e3d658f-282e-4dbc-ba69-3c54b3b4a90a no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:33:50.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-361" for this suite.

• [SLOW TEST:5.113 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":127,"skipped":2230,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:33:50.246: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 19:33:52.637: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588832, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588832, loc:(*time.Location)(0x791d1c0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5f65f8c764\""}}, CollisionCount:(*int32)(nil)}
Mar 23 19:33:54.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588832, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588832, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588832, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720588832, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 19:33:58.186: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:34:10.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5614" for this suite.
STEP: Destroying namespace "webhook-5614-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:20.302 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":128,"skipped":2265,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:34:10.548: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9899
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-9899
I0323 19:34:10.933340      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-9899, replica count: 2
Mar 23 19:34:13.984: INFO: Creating new exec pod
I0323 19:34:13.984615      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 19:34:19.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=services-9899 execpodjjg9j -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 23 19:34:19.469: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 23 19:34:19.469: INFO: stdout: ""
Mar 23 19:34:19.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=services-9899 execpodjjg9j -- /bin/sh -x -c nc -zv -t -w 2 10.103.208.55 80'
Mar 23 19:34:19.928: INFO: stderr: "+ nc -zv -t -w 2 10.103.208.55 80\nConnection to 10.103.208.55 80 port [tcp/http] succeeded!\n"
Mar 23 19:34:19.929: INFO: stdout: ""
Mar 23 19:34:19.929: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:34:20.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9899" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:9.669 seconds]
[sig-network] Services
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":129,"skipped":2269,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:34:20.217: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1790
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 19:34:20.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-5416'
Mar 23 19:34:20.587: INFO: stderr: ""
Mar 23 19:34:20.587: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 23 19:34:25.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pod e2e-test-httpd-pod --namespace=kubectl-5416 -o json'
Mar 23 19:34:25.859: INFO: stderr: ""
Mar 23 19:34:25.860: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-03-23T19:34:20Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5416\",\n        \"resourceVersion\": \"23853\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5416/pods/e2e-test-httpd-pod\",\n        \"uid\": \"c7031d06-9897-4b23-8716-8de3288e0330\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-6bjng\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kube17-worker-1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-6bjng\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-6bjng\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-23T19:34:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-23T19:34:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-23T19:34:22Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-23T19:34:20Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://caebf1aed678177189cd5d15641f885e3699c41fe82da54189f2de266da96000\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-23T19:34:22Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.30.20.153\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.44.0.3\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.44.0.3\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-23T19:34:20Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 23 19:34:25.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 replace -f - --namespace=kubectl-5416'
Mar 23 19:34:26.495: INFO: stderr: ""
Mar 23 19:34:26.495: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1795
Mar 23 19:34:26.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete pods e2e-test-httpd-pod --namespace=kubectl-5416'
Mar 23 19:34:38.338: INFO: stderr: ""
Mar 23 19:34:38.338: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:34:38.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5416" for this suite.

• [SLOW TEST:18.179 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1786
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":130,"skipped":2275,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:34:38.397: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-33e6d95c-b681-4438-9936-fec18bdd445b
STEP: Creating a pod to test consume configMaps
Mar 23 19:34:38.520: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1626d31d-9e84-440a-ac56-1cbbfe9d8802" in namespace "projected-2141" to be "success or failure"
Mar 23 19:34:38.526: INFO: Pod "pod-projected-configmaps-1626d31d-9e84-440a-ac56-1cbbfe9d8802": Phase="Pending", Reason="", readiness=false. Elapsed: 6.478741ms
Mar 23 19:34:40.541: INFO: Pod "pod-projected-configmaps-1626d31d-9e84-440a-ac56-1cbbfe9d8802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021454486s
STEP: Saw pod success
Mar 23 19:34:40.542: INFO: Pod "pod-projected-configmaps-1626d31d-9e84-440a-ac56-1cbbfe9d8802" satisfied condition "success or failure"
Mar 23 19:34:40.546: INFO: Trying to get logs from node kube17-worker-1 pod pod-projected-configmaps-1626d31d-9e84-440a-ac56-1cbbfe9d8802 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 19:34:40.654: INFO: Waiting for pod pod-projected-configmaps-1626d31d-9e84-440a-ac56-1cbbfe9d8802 to disappear
Mar 23 19:34:40.692: INFO: Pod pod-projected-configmaps-1626d31d-9e84-440a-ac56-1cbbfe9d8802 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:34:40.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2141" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":131,"skipped":2293,"failed":0}

------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:34:40.733: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 23 19:34:40.842: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8570 /api/v1/namespaces/watch-8570/configmaps/e2e-watch-test-label-changed 74976c5b-b94a-4f61-85b9-b3ecf0d00e0b 23976 0 2020-03-23 19:34:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 23 19:34:40.843: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8570 /api/v1/namespaces/watch-8570/configmaps/e2e-watch-test-label-changed 74976c5b-b94a-4f61-85b9-b3ecf0d00e0b 23977 0 2020-03-23 19:34:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 23 19:34:40.843: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8570 /api/v1/namespaces/watch-8570/configmaps/e2e-watch-test-label-changed 74976c5b-b94a-4f61-85b9-b3ecf0d00e0b 23978 0 2020-03-23 19:34:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 23 19:34:50.877: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8570 /api/v1/namespaces/watch-8570/configmaps/e2e-watch-test-label-changed 74976c5b-b94a-4f61-85b9-b3ecf0d00e0b 24041 0 2020-03-23 19:34:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 23 19:34:50.877: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8570 /api/v1/namespaces/watch-8570/configmaps/e2e-watch-test-label-changed 74976c5b-b94a-4f61-85b9-b3ecf0d00e0b 24042 0 2020-03-23 19:34:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar 23 19:34:50.877: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8570 /api/v1/namespaces/watch-8570/configmaps/e2e-watch-test-label-changed 74976c5b-b94a-4f61-85b9-b3ecf0d00e0b 24043 0 2020-03-23 19:34:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:34:50.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8570" for this suite.

• [SLOW TEST:10.158 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":132,"skipped":2293,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:34:50.894: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 23 19:34:50.976: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:34:50.984: INFO: Number of nodes with available pods: 0
Mar 23 19:34:50.984: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:34:51.992: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:34:51.997: INFO: Number of nodes with available pods: 0
Mar 23 19:34:51.997: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:34:53.331: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:34:53.336: INFO: Number of nodes with available pods: 0
Mar 23 19:34:53.336: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:34:54.231: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:34:54.285: INFO: Number of nodes with available pods: 0
Mar 23 19:34:54.285: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:34:54.990: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:34:54.994: INFO: Number of nodes with available pods: 0
Mar 23 19:34:54.994: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:34:55.991: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:34:56.004: INFO: Number of nodes with available pods: 3
Mar 23 19:34:56.004: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 23 19:34:56.034: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:34:56.039: INFO: Number of nodes with available pods: 2
Mar 23 19:34:56.039: INFO: Node kube17-worker-3 is running more than one daemon pod
Mar 23 19:34:57.048: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:34:57.057: INFO: Number of nodes with available pods: 2
Mar 23 19:34:57.057: INFO: Node kube17-worker-3 is running more than one daemon pod
Mar 23 19:34:58.046: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:34:58.051: INFO: Number of nodes with available pods: 2
Mar 23 19:34:58.051: INFO: Node kube17-worker-3 is running more than one daemon pod
Mar 23 19:34:59.044: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:34:59.049: INFO: Number of nodes with available pods: 2
Mar 23 19:34:59.049: INFO: Node kube17-worker-3 is running more than one daemon pod
Mar 23 19:35:00.045: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:35:00.050: INFO: Number of nodes with available pods: 2
Mar 23 19:35:00.050: INFO: Node kube17-worker-3 is running more than one daemon pod
Mar 23 19:35:01.045: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:35:01.049: INFO: Number of nodes with available pods: 2
Mar 23 19:35:01.049: INFO: Node kube17-worker-3 is running more than one daemon pod
Mar 23 19:35:02.046: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:35:02.052: INFO: Number of nodes with available pods: 2
Mar 23 19:35:02.052: INFO: Node kube17-worker-3 is running more than one daemon pod
Mar 23 19:35:03.047: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:35:03.056: INFO: Number of nodes with available pods: 2
Mar 23 19:35:03.056: INFO: Node kube17-worker-3 is running more than one daemon pod
Mar 23 19:35:04.046: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:35:04.051: INFO: Number of nodes with available pods: 2
Mar 23 19:35:04.051: INFO: Node kube17-worker-3 is running more than one daemon pod
Mar 23 19:35:05.054: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:35:05.060: INFO: Number of nodes with available pods: 2
Mar 23 19:35:05.060: INFO: Node kube17-worker-3 is running more than one daemon pod
Mar 23 19:35:06.047: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:35:06.055: INFO: Number of nodes with available pods: 2
Mar 23 19:35:06.056: INFO: Node kube17-worker-3 is running more than one daemon pod
Mar 23 19:35:07.047: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:35:07.057: INFO: Number of nodes with available pods: 2
Mar 23 19:35:07.057: INFO: Node kube17-worker-3 is running more than one daemon pod
Mar 23 19:35:08.045: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:35:08.050: INFO: Number of nodes with available pods: 3
Mar 23 19:35:08.051: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5612, will wait for the garbage collector to delete the pods
Mar 23 19:35:08.123: INFO: Deleting DaemonSet.extensions daemon-set took: 13.581072ms
Mar 23 19:35:09.623: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.500471648s
Mar 23 19:35:20.528: INFO: Number of nodes with available pods: 0
Mar 23 19:35:20.528: INFO: Number of running nodes: 0, number of available pods: 0
Mar 23 19:35:20.531: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5612/daemonsets","resourceVersion":"24229"},"items":null}

Mar 23 19:35:20.534: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5612/pods","resourceVersion":"24229"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:35:20.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5612" for this suite.

• [SLOW TEST:29.679 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":133,"skipped":2295,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:35:20.575: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:35:20.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-221" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":134,"skipped":2334,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:35:20.701: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 23 19:35:28.057: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:35:29.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7317" for this suite.

• [SLOW TEST:8.449 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":135,"skipped":2336,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:35:29.159: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:35:38.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1223" for this suite.

• [SLOW TEST:9.005 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":136,"skipped":2349,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:35:38.164: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-c4250e8f-7031-4f6a-aa04-bf5e7ed19fd6
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-c4250e8f-7031-4f6a-aa04-bf5e7ed19fd6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:35:45.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6930" for this suite.

• [SLOW TEST:6.911 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":137,"skipped":2356,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:35:45.079: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:35:45.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5749" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":138,"skipped":2367,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:35:45.174: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 23 19:35:48.288: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:35:49.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6130" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":139,"skipped":2373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:35:49.147: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:35:49.268: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"653a4d7e-3f36-4a9f-bedf-4a08410af503", Controller:(*bool)(0xc00369602e), BlockOwnerDeletion:(*bool)(0xc00369602f)}}
Mar 23 19:35:49.276: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f5e07981-964f-4642-b2ea-e1555c4ed445", Controller:(*bool)(0xc0036d5f06), BlockOwnerDeletion:(*bool)(0xc0036d5f07)}}
Mar 23 19:35:49.289: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"36621bbd-0067-48fd-aac1-62b4afd52472", Controller:(*bool)(0xc003696216), BlockOwnerDeletion:(*bool)(0xc003696217)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:35:54.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4984" for this suite.

• [SLOW TEST:5.188 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":140,"skipped":2402,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:35:54.336: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-2wp6
STEP: Creating a pod to test atomic-volume-subpath
Mar 23 19:35:54.453: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-2wp6" in namespace "subpath-1567" to be "success or failure"
Mar 23 19:35:54.466: INFO: Pod "pod-subpath-test-downwardapi-2wp6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.655614ms
Mar 23 19:35:56.509: INFO: Pod "pod-subpath-test-downwardapi-2wp6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055850016s
Mar 23 19:35:59.195: INFO: Pod "pod-subpath-test-downwardapi-2wp6": Phase="Running", Reason="", readiness=true. Elapsed: 4.742398058s
Mar 23 19:36:01.200: INFO: Pod "pod-subpath-test-downwardapi-2wp6": Phase="Running", Reason="", readiness=true. Elapsed: 6.747286366s
Mar 23 19:36:03.205: INFO: Pod "pod-subpath-test-downwardapi-2wp6": Phase="Running", Reason="", readiness=true. Elapsed: 8.751917596s
Mar 23 19:36:05.211: INFO: Pod "pod-subpath-test-downwardapi-2wp6": Phase="Running", Reason="", readiness=true. Elapsed: 10.758005036s
Mar 23 19:36:07.215: INFO: Pod "pod-subpath-test-downwardapi-2wp6": Phase="Running", Reason="", readiness=true. Elapsed: 12.762299429s
Mar 23 19:36:09.222: INFO: Pod "pod-subpath-test-downwardapi-2wp6": Phase="Running", Reason="", readiness=true. Elapsed: 14.768890645s
Mar 23 19:36:11.227: INFO: Pod "pod-subpath-test-downwardapi-2wp6": Phase="Running", Reason="", readiness=true. Elapsed: 16.77374666s
Mar 23 19:36:13.548: INFO: Pod "pod-subpath-test-downwardapi-2wp6": Phase="Running", Reason="", readiness=true. Elapsed: 19.095394024s
Mar 23 19:36:15.552: INFO: Pod "pod-subpath-test-downwardapi-2wp6": Phase="Running", Reason="", readiness=true. Elapsed: 21.099450299s
Mar 23 19:36:17.557: INFO: Pod "pod-subpath-test-downwardapi-2wp6": Phase="Running", Reason="", readiness=true. Elapsed: 23.104337648s
Mar 23 19:36:19.562: INFO: Pod "pod-subpath-test-downwardapi-2wp6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 25.10914344s
STEP: Saw pod success
Mar 23 19:36:19.562: INFO: Pod "pod-subpath-test-downwardapi-2wp6" satisfied condition "success or failure"
Mar 23 19:36:19.565: INFO: Trying to get logs from node kube17-worker-1 pod pod-subpath-test-downwardapi-2wp6 container test-container-subpath-downwardapi-2wp6: <nil>
STEP: delete the pod
Mar 23 19:36:19.653: INFO: Waiting for pod pod-subpath-test-downwardapi-2wp6 to disappear
Mar 23 19:36:19.657: INFO: Pod pod-subpath-test-downwardapi-2wp6 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-2wp6
Mar 23 19:36:19.657: INFO: Deleting pod "pod-subpath-test-downwardapi-2wp6" in namespace "subpath-1567"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:36:19.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1567" for this suite.

• [SLOW TEST:25.356 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":141,"skipped":2404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:36:19.704: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 23 19:36:27.930: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 23 19:36:27.936: INFO: Pod pod-with-poststart-http-hook still exists
Mar 23 19:36:29.936: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 23 19:36:29.953: INFO: Pod pod-with-poststart-http-hook still exists
Mar 23 19:36:31.936: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 23 19:36:31.950: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:36:31.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5556" for this suite.

• [SLOW TEST:12.263 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":142,"skipped":2435,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:36:31.970: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Mar 23 19:36:32.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-4386'
Mar 23 19:36:32.596: INFO: stderr: ""
Mar 23 19:36:32.596: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 23 19:36:33.603: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:36:33.603: INFO: Found 0 / 1
Mar 23 19:36:34.603: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:36:34.603: INFO: Found 0 / 1
Mar 23 19:36:35.603: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:36:35.603: INFO: Found 1 / 1
Mar 23 19:36:35.603: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 23 19:36:35.608: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:36:35.609: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 23 19:36:35.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 patch pod agnhost-master-j8dsc --namespace=kubectl-4386 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 23 19:36:35.805: INFO: stderr: ""
Mar 23 19:36:35.805: INFO: stdout: "pod/agnhost-master-j8dsc patched\n"
STEP: checking annotations
Mar 23 19:36:35.811: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:36:35.811: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:36:35.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4386" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":143,"skipped":2501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:36:35.832: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-7615
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7615
STEP: Creating statefulset with conflicting port in namespace statefulset-7615
STEP: Waiting until pod test-pod will start running in namespace statefulset-7615
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7615
Mar 23 19:36:40.750: INFO: Observed stateful pod in namespace: statefulset-7615, name: ss-0, uid: 6d7b7d96-3078-4004-8b14-ecf89eae7c76, status phase: Pending. Waiting for statefulset controller to delete.
Mar 23 19:36:45.632: INFO: Observed stateful pod in namespace: statefulset-7615, name: ss-0, uid: 6d7b7d96-3078-4004-8b14-ecf89eae7c76, status phase: Failed. Waiting for statefulset controller to delete.
Mar 23 19:36:46.870: INFO: Observed stateful pod in namespace: statefulset-7615, name: ss-0, uid: 6d7b7d96-3078-4004-8b14-ecf89eae7c76, status phase: Failed. Waiting for statefulset controller to delete.
Mar 23 19:36:46.906: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7615
STEP: Removing pod with conflicting port in namespace statefulset-7615
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7615 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar 23 19:36:51.934: INFO: Deleting all statefulset in ns statefulset-7615
Mar 23 19:36:51.947: INFO: Scaling statefulset ss to 0
Mar 23 19:37:02.039: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 19:37:02.042: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:37:04.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7615" for this suite.

• [SLOW TEST:28.654 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":144,"skipped":2526,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:37:04.488: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Mar 23 19:37:04.650: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-244672999 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:37:04.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2599" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":145,"skipped":2531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:37:04.835: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4824
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4824
I0323 19:37:05.236711      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-4824, replica count: 2
I0323 19:37:08.288004      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 19:37:11.288615      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 19:37:11.288: INFO: Creating new exec pod
Mar 23 19:37:16.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=services-4824 execpodnlsb5 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 23 19:37:23.034: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 23 19:37:23.034: INFO: stdout: ""
Mar 23 19:37:23.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=services-4824 execpodnlsb5 -- /bin/sh -x -c nc -zv -t -w 2 10.96.137.114 80'
Mar 23 19:37:23.505: INFO: stderr: "+ nc -zv -t -w 2 10.96.137.114 80\nConnection to 10.96.137.114 80 port [tcp/http] succeeded!\n"
Mar 23 19:37:23.505: INFO: stdout: ""
Mar 23 19:37:23.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=services-4824 execpodnlsb5 -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.93 31864'
Mar 23 19:37:23.985: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.93 31864\nConnection to 10.30.20.93 31864 port [tcp/31864] succeeded!\n"
Mar 23 19:37:23.985: INFO: stdout: ""
Mar 23 19:37:23.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=services-4824 execpodnlsb5 -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.153 31864'
Mar 23 19:37:24.472: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.153 31864\nConnection to 10.30.20.153 31864 port [tcp/31864] succeeded!\n"
Mar 23 19:37:24.472: INFO: stdout: ""
Mar 23 19:37:24.472: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:37:24.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4824" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:19.737 seconds]
[sig-network] Services
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":146,"skipped":2579,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:37:24.575: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 23 19:37:24.694: INFO: Waiting up to 5m0s for pod "pod-41035050-59f4-40d2-9418-f0e439df1999" in namespace "emptydir-8132" to be "success or failure"
Mar 23 19:37:24.704: INFO: Pod "pod-41035050-59f4-40d2-9418-f0e439df1999": Phase="Pending", Reason="", readiness=false. Elapsed: 10.543339ms
Mar 23 19:37:26.712: INFO: Pod "pod-41035050-59f4-40d2-9418-f0e439df1999": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017988773s
Mar 23 19:37:28.721: INFO: Pod "pod-41035050-59f4-40d2-9418-f0e439df1999": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026925937s
STEP: Saw pod success
Mar 23 19:37:28.721: INFO: Pod "pod-41035050-59f4-40d2-9418-f0e439df1999" satisfied condition "success or failure"
Mar 23 19:37:28.726: INFO: Trying to get logs from node kube17-worker-1 pod pod-41035050-59f4-40d2-9418-f0e439df1999 container test-container: <nil>
STEP: delete the pod
Mar 23 19:37:28.779: INFO: Waiting for pod pod-41035050-59f4-40d2-9418-f0e439df1999 to disappear
Mar 23 19:37:28.797: INFO: Pod pod-41035050-59f4-40d2-9418-f0e439df1999 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:37:28.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8132" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":147,"skipped":2591,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:37:28.820: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-261
STEP: creating replication controller nodeport-test in namespace services-261
I0323 19:37:28.977096      23 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-261, replica count: 2
I0323 19:37:32.027682      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 19:37:32.027: INFO: Creating new exec pod
Mar 23 19:37:37.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=services-261 execpodgzgmz -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar 23 19:37:41.408: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 23 19:37:41.409: INFO: stdout: ""
Mar 23 19:37:41.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=services-261 execpodgzgmz -- /bin/sh -x -c nc -zv -t -w 2 10.96.16.160 80'
Mar 23 19:37:41.920: INFO: stderr: "+ nc -zv -t -w 2 10.96.16.160 80\nConnection to 10.96.16.160 80 port [tcp/http] succeeded!\n"
Mar 23 19:37:41.920: INFO: stdout: ""
Mar 23 19:37:41.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=services-261 execpodgzgmz -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.139 30707'
Mar 23 19:37:42.409: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.139 30707\nConnection to 10.30.20.139 30707 port [tcp/30707] succeeded!\n"
Mar 23 19:37:42.409: INFO: stdout: ""
Mar 23 19:37:42.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=services-261 execpodgzgmz -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.153 30707'
Mar 23 19:37:42.873: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.153 30707\nConnection to 10.30.20.153 30707 port [tcp/30707] succeeded!\n"
Mar 23 19:37:42.873: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:37:42.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-261" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:14.074 seconds]
[sig-network] Services
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":148,"skipped":2593,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:37:42.895: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 23 19:37:43.495: INFO: Waiting up to 5m0s for pod "pod-76580a51-82bc-483b-b64f-061347c7e910" in namespace "emptydir-1485" to be "success or failure"
Mar 23 19:37:43.500: INFO: Pod "pod-76580a51-82bc-483b-b64f-061347c7e910": Phase="Pending", Reason="", readiness=false. Elapsed: 5.481516ms
Mar 23 19:37:46.287: INFO: Pod "pod-76580a51-82bc-483b-b64f-061347c7e910": Phase="Pending", Reason="", readiness=false. Elapsed: 2.792438925s
Mar 23 19:37:48.299: INFO: Pod "pod-76580a51-82bc-483b-b64f-061347c7e910": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.803965772s
STEP: Saw pod success
Mar 23 19:37:48.299: INFO: Pod "pod-76580a51-82bc-483b-b64f-061347c7e910" satisfied condition "success or failure"
Mar 23 19:37:48.306: INFO: Trying to get logs from node kube17-worker-1 pod pod-76580a51-82bc-483b-b64f-061347c7e910 container test-container: <nil>
STEP: delete the pod
Mar 23 19:37:48.452: INFO: Waiting for pod pod-76580a51-82bc-483b-b64f-061347c7e910 to disappear
Mar 23 19:37:48.488: INFO: Pod pod-76580a51-82bc-483b-b64f-061347c7e910 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:37:48.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1485" for this suite.

• [SLOW TEST:5.631 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":149,"skipped":2606,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:37:48.527: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 19:37:49.455: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 19:37:51.472: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589069, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589069, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589069, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589069, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:37:55.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589069, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589069, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589069, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589069, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:38:01.176: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589069, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589069, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589069, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589069, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 19:38:02.520: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:38:02.530: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5815-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:38:03.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9533" for this suite.
STEP: Destroying namespace "webhook-9533-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.988 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":150,"skipped":2608,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:38:03.528: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-2e8c6595-0133-4a8c-b029-03b7668ca726
STEP: Creating a pod to test consume configMaps
Mar 23 19:38:03.640: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09676c58-539e-4ebe-bdbe-92365385c10a" in namespace "projected-9203" to be "success or failure"
Mar 23 19:38:03.649: INFO: Pod "pod-projected-configmaps-09676c58-539e-4ebe-bdbe-92365385c10a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.255314ms
Mar 23 19:38:05.663: INFO: Pod "pod-projected-configmaps-09676c58-539e-4ebe-bdbe-92365385c10a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023059487s
STEP: Saw pod success
Mar 23 19:38:05.663: INFO: Pod "pod-projected-configmaps-09676c58-539e-4ebe-bdbe-92365385c10a" satisfied condition "success or failure"
Mar 23 19:38:05.671: INFO: Trying to get logs from node kube17-worker-1 pod pod-projected-configmaps-09676c58-539e-4ebe-bdbe-92365385c10a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 19:38:05.714: INFO: Waiting for pod pod-projected-configmaps-09676c58-539e-4ebe-bdbe-92365385c10a to disappear
Mar 23 19:38:05.719: INFO: Pod pod-projected-configmaps-09676c58-539e-4ebe-bdbe-92365385c10a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:38:05.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9203" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":151,"skipped":2627,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:38:05.740: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-de15f933-0a5f-448f-bf26-382d68f3fe24
STEP: Creating a pod to test consume secrets
Mar 23 19:38:05.826: INFO: Waiting up to 5m0s for pod "pod-secrets-63a4a7a6-4489-41c4-9853-bd1e99d415ff" in namespace "secrets-3092" to be "success or failure"
Mar 23 19:38:05.845: INFO: Pod "pod-secrets-63a4a7a6-4489-41c4-9853-bd1e99d415ff": Phase="Pending", Reason="", readiness=false. Elapsed: 19.142941ms
Mar 23 19:38:07.854: INFO: Pod "pod-secrets-63a4a7a6-4489-41c4-9853-bd1e99d415ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028065605s
Mar 23 19:38:09.861: INFO: Pod "pod-secrets-63a4a7a6-4489-41c4-9853-bd1e99d415ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035334439s
STEP: Saw pod success
Mar 23 19:38:09.861: INFO: Pod "pod-secrets-63a4a7a6-4489-41c4-9853-bd1e99d415ff" satisfied condition "success or failure"
Mar 23 19:38:09.890: INFO: Trying to get logs from node kube17-worker-1 pod pod-secrets-63a4a7a6-4489-41c4-9853-bd1e99d415ff container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 19:38:09.967: INFO: Waiting for pod pod-secrets-63a4a7a6-4489-41c4-9853-bd1e99d415ff to disappear
Mar 23 19:38:09.979: INFO: Pod pod-secrets-63a4a7a6-4489-41c4-9853-bd1e99d415ff no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:38:09.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3092" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":152,"skipped":2633,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:38:10.020: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-6084
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6084 to expose endpoints map[]
Mar 23 19:38:10.152: INFO: successfully validated that service multi-endpoint-test in namespace services-6084 exposes endpoints map[] (13.159826ms elapsed)
STEP: Creating pod pod1 in namespace services-6084
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6084 to expose endpoints map[pod1:[100]]
Mar 23 19:38:15.041: INFO: successfully validated that service multi-endpoint-test in namespace services-6084 exposes endpoints map[pod1:[100]] (4.87357543s elapsed)
STEP: Creating pod pod2 in namespace services-6084
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6084 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 23 19:38:19.350: INFO: successfully validated that service multi-endpoint-test in namespace services-6084 exposes endpoints map[pod1:[100] pod2:[101]] (4.289419402s elapsed)
STEP: Deleting pod pod1 in namespace services-6084
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6084 to expose endpoints map[pod2:[101]]
Mar 23 19:38:19.401: INFO: successfully validated that service multi-endpoint-test in namespace services-6084 exposes endpoints map[pod2:[101]] (27.181004ms elapsed)
STEP: Deleting pod pod2 in namespace services-6084
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6084 to expose endpoints map[]
Mar 23 19:38:19.456: INFO: successfully validated that service multi-endpoint-test in namespace services-6084 exposes endpoints map[] (15.098782ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:38:19.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6084" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:9.569 seconds]
[sig-network] Services
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":153,"skipped":2670,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:38:19.595: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:38:19.711: INFO: Create a RollingUpdate DaemonSet
Mar 23 19:38:19.717: INFO: Check that daemon pods launch on every node of the cluster
Mar 23 19:38:19.726: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:38:19.730: INFO: Number of nodes with available pods: 0
Mar 23 19:38:19.730: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:38:20.738: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:38:20.743: INFO: Number of nodes with available pods: 0
Mar 23 19:38:20.743: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:38:21.737: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:38:21.743: INFO: Number of nodes with available pods: 0
Mar 23 19:38:21.743: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:38:22.739: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:38:22.745: INFO: Number of nodes with available pods: 3
Mar 23 19:38:22.745: INFO: Number of running nodes: 3, number of available pods: 3
Mar 23 19:38:22.745: INFO: Update the DaemonSet to trigger a rollout
Mar 23 19:38:22.761: INFO: Updating DaemonSet daemon-set
Mar 23 19:38:30.790: INFO: Roll back the DaemonSet before rollout is complete
Mar 23 19:38:30.806: INFO: Updating DaemonSet daemon-set
Mar 23 19:38:30.806: INFO: Make sure DaemonSet rollback is complete
Mar 23 19:38:30.816: INFO: Wrong image for pod: daemon-set-462ls. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 23 19:38:30.816: INFO: Pod daemon-set-462ls is not available
Mar 23 19:38:30.827: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:38:32.768: INFO: Wrong image for pod: daemon-set-462ls. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 23 19:38:32.768: INFO: Pod daemon-set-462ls is not available
Mar 23 19:38:32.897: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:38:33.834: INFO: Pod daemon-set-l5k5j is not available
Mar 23 19:38:33.840: INFO: DaemonSet pods can't tolerate node kube17-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8153, will wait for the garbage collector to delete the pods
Mar 23 19:38:33.918: INFO: Deleting DaemonSet.extensions daemon-set took: 10.664308ms
Mar 23 19:38:34.419: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.48455ms
Mar 23 19:38:38.624: INFO: Number of nodes with available pods: 0
Mar 23 19:38:38.624: INFO: Number of running nodes: 0, number of available pods: 0
Mar 23 19:38:38.628: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8153/daemonsets","resourceVersion":"25833"},"items":null}

Mar 23 19:38:38.632: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8153/pods","resourceVersion":"25833"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:38:38.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8153" for this suite.

• [SLOW TEST:19.082 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":154,"skipped":2674,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:38:38.678: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Mar 23 19:38:38.788: INFO: namespace kubectl-2729
Mar 23 19:38:38.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-2729'
Mar 23 19:38:39.834: INFO: stderr: ""
Mar 23 19:38:39.834: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 23 19:38:40.841: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:38:40.841: INFO: Found 0 / 1
Mar 23 19:38:41.867: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:38:41.867: INFO: Found 1 / 1
Mar 23 19:38:41.867: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 23 19:38:41.875: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:38:41.875: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 23 19:38:41.875: INFO: wait on agnhost-master startup in kubectl-2729 
Mar 23 19:38:41.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 logs agnhost-master-2gxl5 agnhost-master --namespace=kubectl-2729'
Mar 23 19:38:42.107: INFO: stderr: ""
Mar 23 19:38:42.107: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar 23 19:38:42.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-2729'
Mar 23 19:38:43.416: INFO: stderr: ""
Mar 23 19:38:43.416: INFO: stdout: "service/rm2 exposed\n"
Mar 23 19:38:43.425: INFO: Service rm2 in namespace kubectl-2729 found.
STEP: exposing service
Mar 23 19:38:45.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-2729'
Mar 23 19:38:45.658: INFO: stderr: ""
Mar 23 19:38:45.658: INFO: stdout: "service/rm3 exposed\n"
Mar 23 19:38:45.669: INFO: Service rm3 in namespace kubectl-2729 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:38:47.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2729" for this suite.

• [SLOW TEST:9.023 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1188
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":155,"skipped":2691,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:38:47.701: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:39:07.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4877" for this suite.

• [SLOW TEST:19.572 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":156,"skipped":2707,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:39:07.274: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Mar 23 19:39:07.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 cluster-info'
Mar 23 19:39:07.581: INFO: stderr: ""
Mar 23 19:39:07.581: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:39:07.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8858" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":157,"skipped":2725,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:39:07.608: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 19:39:07.677: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f55cd61e-5c16-47a6-8dbe-034c09226621" in namespace "projected-8067" to be "success or failure"
Mar 23 19:39:07.685: INFO: Pod "downwardapi-volume-f55cd61e-5c16-47a6-8dbe-034c09226621": Phase="Pending", Reason="", readiness=false. Elapsed: 7.84318ms
Mar 23 19:39:09.689: INFO: Pod "downwardapi-volume-f55cd61e-5c16-47a6-8dbe-034c09226621": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012021501s
STEP: Saw pod success
Mar 23 19:39:09.689: INFO: Pod "downwardapi-volume-f55cd61e-5c16-47a6-8dbe-034c09226621" satisfied condition "success or failure"
Mar 23 19:39:09.692: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-f55cd61e-5c16-47a6-8dbe-034c09226621 container client-container: <nil>
STEP: delete the pod
Mar 23 19:39:09.721: INFO: Waiting for pod downwardapi-volume-f55cd61e-5c16-47a6-8dbe-034c09226621 to disappear
Mar 23 19:39:09.725: INFO: Pod downwardapi-volume-f55cd61e-5c16-47a6-8dbe-034c09226621 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:39:09.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8067" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":158,"skipped":2750,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:39:09.738: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4250.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4250.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4250.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4250.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4250.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4250.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 19:39:21.877: INFO: DNS probes using dns-4250/dns-test-4605dfe8-7773-4e77-90b3-9ac70c916d82 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:39:21.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4250" for this suite.

• [SLOW TEST:12.251 seconds]
[sig-network] DNS
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":159,"skipped":2753,"failed":0}
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:39:21.993: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 19:39:22.081: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f44ab28-d1d4-4ae3-a679-51cb78d2a76f" in namespace "downward-api-6223" to be "success or failure"
Mar 23 19:39:22.091: INFO: Pod "downwardapi-volume-6f44ab28-d1d4-4ae3-a679-51cb78d2a76f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.650843ms
Mar 23 19:39:24.096: INFO: Pod "downwardapi-volume-6f44ab28-d1d4-4ae3-a679-51cb78d2a76f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015150974s
STEP: Saw pod success
Mar 23 19:39:24.097: INFO: Pod "downwardapi-volume-6f44ab28-d1d4-4ae3-a679-51cb78d2a76f" satisfied condition "success or failure"
Mar 23 19:39:24.102: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-6f44ab28-d1d4-4ae3-a679-51cb78d2a76f container client-container: <nil>
STEP: delete the pod
Mar 23 19:39:24.142: INFO: Waiting for pod downwardapi-volume-6f44ab28-d1d4-4ae3-a679-51cb78d2a76f to disappear
Mar 23 19:39:24.148: INFO: Pod downwardapi-volume-6f44ab28-d1d4-4ae3-a679-51cb78d2a76f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:39:24.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6223" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":160,"skipped":2753,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:39:24.173: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Mar 23 19:39:26.800: INFO: Successfully updated pod "annotationupdate8fa75cf2-c436-4117-a192-bc656eb00617"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:39:31.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-435" for this suite.

• [SLOW TEST:8.586 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":161,"skipped":2753,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:39:32.762: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 19:39:34.328: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 19:39:36.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589174, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589174, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589174, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589174, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 19:39:42.438: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:39:42.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-769" for this suite.
STEP: Destroying namespace "webhook-769-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.950 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":162,"skipped":2754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:39:42.715: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-3449
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3449
STEP: Deleting pre-stop pod
Mar 23 19:40:08.605: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:40:08.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3449" for this suite.

• [SLOW TEST:25.915 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":163,"skipped":2786,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:40:08.633: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 23 19:40:08.722: INFO: Waiting up to 5m0s for pod "pod-9a04e6c0-85aa-42ae-add0-50ea09dd511c" in namespace "emptydir-6427" to be "success or failure"
Mar 23 19:40:08.729: INFO: Pod "pod-9a04e6c0-85aa-42ae-add0-50ea09dd511c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.25266ms
Mar 23 19:40:10.736: INFO: Pod "pod-9a04e6c0-85aa-42ae-add0-50ea09dd511c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01330405s
Mar 23 19:40:16.815: INFO: Pod "pod-9a04e6c0-85aa-42ae-add0-50ea09dd511c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.092968211s
STEP: Saw pod success
Mar 23 19:40:16.816: INFO: Pod "pod-9a04e6c0-85aa-42ae-add0-50ea09dd511c" satisfied condition "success or failure"
Mar 23 19:40:16.854: INFO: Trying to get logs from node kube17-worker-1 pod pod-9a04e6c0-85aa-42ae-add0-50ea09dd511c container test-container: <nil>
STEP: delete the pod
Mar 23 19:40:19.153: INFO: Waiting for pod pod-9a04e6c0-85aa-42ae-add0-50ea09dd511c to disappear
Mar 23 19:40:19.157: INFO: Pod pod-9a04e6c0-85aa-42ae-add0-50ea09dd511c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:40:19.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6427" for this suite.

• [SLOW TEST:10.539 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":164,"skipped":2801,"failed":0}
S
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:40:19.174: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar 23 19:40:19.239: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Mar 23 19:40:19.931: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 23 19:40:21.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:40:23.993: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:40:29.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:40:29.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:40:31.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:40:35.202: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:40:35.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:40:37.993: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:40:39.993: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:40:41.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:40:43.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589219, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:40:49.118: INFO: Waited 3.100894345s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:40:53.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1789" for this suite.

• [SLOW TEST:33.888 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":165,"skipped":2802,"failed":0}
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:40:53.066: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:40:55.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3697" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":166,"skipped":2806,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:40:55.195: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:40:59.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1323" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":167,"skipped":2856,"failed":0}
SSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:40:59.354: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:40:59.512: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-874207ac-9bca-4d7f-9167-f5e8ea76b2ed" in namespace "security-context-test-3390" to be "success or failure"
Mar 23 19:40:59.531: INFO: Pod "busybox-readonly-false-874207ac-9bca-4d7f-9167-f5e8ea76b2ed": Phase="Pending", Reason="", readiness=false. Elapsed: 18.513585ms
Mar 23 19:41:01.545: INFO: Pod "busybox-readonly-false-874207ac-9bca-4d7f-9167-f5e8ea76b2ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032276552s
Mar 23 19:41:03.552: INFO: Pod "busybox-readonly-false-874207ac-9bca-4d7f-9167-f5e8ea76b2ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039114641s
Mar 23 19:41:03.552: INFO: Pod "busybox-readonly-false-874207ac-9bca-4d7f-9167-f5e8ea76b2ed" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:41:03.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3390" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":168,"skipped":2861,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:41:03.571: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:41:09.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7034" for this suite.

• [SLOW TEST:5.652 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox command in a pod
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":169,"skipped":2866,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:41:09.226: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 19:41:10.812: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 19:41:12.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589270, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589270, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589270, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589270, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 19:41:15.848: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:41:15.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9824" for this suite.
STEP: Destroying namespace "webhook-9824-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.865 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":170,"skipped":2868,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:41:16.099: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-9096
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Mar 23 19:41:16.182: INFO: Found 0 stateful pods, waiting for 3
Mar 23 19:41:26.187: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 19:41:26.188: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 19:41:26.188: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Mar 23 19:41:36.202: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 19:41:36.202: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 19:41:36.202: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 23 19:41:36.254: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 23 19:41:46.306: INFO: Updating stateful set ss2
Mar 23 19:41:46.346: INFO: Waiting for Pod statefulset-9096/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar 23 19:41:56.551: INFO: Found 2 stateful pods, waiting for 3
Mar 23 19:42:06.558: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 19:42:06.558: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 19:42:06.558: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 23 19:42:06.617: INFO: Updating stateful set ss2
Mar 23 19:42:06.640: INFO: Waiting for Pod statefulset-9096/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 23 19:42:16.679: INFO: Updating stateful set ss2
Mar 23 19:42:16.705: INFO: Waiting for StatefulSet statefulset-9096/ss2 to complete update
Mar 23 19:42:16.705: INFO: Waiting for Pod statefulset-9096/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar 23 19:42:26.719: INFO: Deleting all statefulset in ns statefulset-9096
Mar 23 19:42:26.723: INFO: Scaling statefulset ss2 to 0
Mar 23 19:42:56.777: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 19:42:56.783: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:42:56.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9096" for this suite.

• [SLOW TEST:100.816 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":171,"skipped":2868,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:42:56.918: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Mar 23 19:43:01.659: INFO: Successfully updated pod "labelsupdate4e171feb-2415-492b-b798-c830cda9dda1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:43:03.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4872" for this suite.

• [SLOW TEST:6.793 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":172,"skipped":2887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:43:03.712: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:43:03.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8297" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":173,"skipped":2917,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:43:03.947: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-50.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-50.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-50.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-50.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-50.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-50.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-50.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-50.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-50.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-50.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-50.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 212.154.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.154.212_udp@PTR;check="$$(dig +tcp +noall +answer +search 212.154.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.154.212_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-50.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-50.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-50.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-50.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-50.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-50.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-50.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-50.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-50.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-50.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-50.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 212.154.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.154.212_udp@PTR;check="$$(dig +tcp +noall +answer +search 212.154.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.154.212_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 19:43:08.160: INFO: Unable to read wheezy_udp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:08.167: INFO: Unable to read wheezy_tcp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:08.173: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:08.180: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:08.245: INFO: Unable to read jessie_udp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:08.251: INFO: Unable to read jessie_tcp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:08.258: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:08.264: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:08.322: INFO: Lookups using dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145 failed for: [wheezy_udp@dns-test-service.dns-50.svc.cluster.local wheezy_tcp@dns-test-service.dns-50.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local jessie_udp@dns-test-service.dns-50.svc.cluster.local jessie_tcp@dns-test-service.dns-50.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local]

Mar 23 19:43:13.335: INFO: Unable to read wheezy_udp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:13.341: INFO: Unable to read wheezy_tcp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:13.370: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:13.381: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:13.537: INFO: Unable to read jessie_udp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:13.541: INFO: Unable to read jessie_tcp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:13.551: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:13.564: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:13.615: INFO: Lookups using dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145 failed for: [wheezy_udp@dns-test-service.dns-50.svc.cluster.local wheezy_tcp@dns-test-service.dns-50.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local jessie_udp@dns-test-service.dns-50.svc.cluster.local jessie_tcp@dns-test-service.dns-50.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local]

Mar 23 19:43:18.339: INFO: Unable to read wheezy_udp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:18.348: INFO: Unable to read wheezy_tcp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:18.356: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:18.366: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:18.412: INFO: Unable to read jessie_udp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:18.423: INFO: Unable to read jessie_tcp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:18.429: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:18.435: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:18.476: INFO: Lookups using dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145 failed for: [wheezy_udp@dns-test-service.dns-50.svc.cluster.local wheezy_tcp@dns-test-service.dns-50.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local jessie_udp@dns-test-service.dns-50.svc.cluster.local jessie_tcp@dns-test-service.dns-50.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local]

Mar 23 19:43:23.331: INFO: Unable to read wheezy_udp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:23.338: INFO: Unable to read wheezy_tcp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:23.344: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:23.350: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:23.408: INFO: Unable to read jessie_udp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:23.416: INFO: Unable to read jessie_tcp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:23.423: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:23.431: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:23.476: INFO: Lookups using dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145 failed for: [wheezy_udp@dns-test-service.dns-50.svc.cluster.local wheezy_tcp@dns-test-service.dns-50.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local jessie_udp@dns-test-service.dns-50.svc.cluster.local jessie_tcp@dns-test-service.dns-50.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local]

Mar 23 19:43:28.331: INFO: Unable to read wheezy_udp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:28.341: INFO: Unable to read wheezy_tcp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:28.349: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:28.354: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:28.396: INFO: Unable to read jessie_udp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:28.405: INFO: Unable to read jessie_tcp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:28.411: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:28.418: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:28.462: INFO: Lookups using dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145 failed for: [wheezy_udp@dns-test-service.dns-50.svc.cluster.local wheezy_tcp@dns-test-service.dns-50.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local jessie_udp@dns-test-service.dns-50.svc.cluster.local jessie_tcp@dns-test-service.dns-50.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local]

Mar 23 19:43:36.564: INFO: Unable to read wheezy_udp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:36.619: INFO: Unable to read wheezy_tcp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:36.632: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:36.663: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:36.769: INFO: Unable to read jessie_udp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:36.775: INFO: Unable to read jessie_tcp@dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:36.782: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:36.788: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local from pod dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145: the server could not find the requested resource (get pods dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145)
Mar 23 19:43:36.829: INFO: Lookups using dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145 failed for: [wheezy_udp@dns-test-service.dns-50.svc.cluster.local wheezy_tcp@dns-test-service.dns-50.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local jessie_udp@dns-test-service.dns-50.svc.cluster.local jessie_tcp@dns-test-service.dns-50.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-50.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-50.svc.cluster.local]

Mar 23 19:43:38.447: INFO: DNS probes using dns-50/dns-test-582f87b7-ac33-4a17-9f05-5204b64bf145 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:43:38.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-50" for this suite.

• [SLOW TEST:34.736 seconds]
[sig-network] DNS
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":174,"skipped":2950,"failed":0}
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:43:38.683: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-482.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-482.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-482.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-482.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-482.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-482.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 19:43:42.854: INFO: DNS probes using dns-482/dns-test-b64d76c3-f4fd-452a-9498-e53ce7c4fe01 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:43:42.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-482" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":175,"skipped":2950,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:43:42.918: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:43:43.233: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-abb9dd8f-6251-4268-9497-c0d7e16df88b" in namespace "security-context-test-2753" to be "success or failure"
Mar 23 19:43:43.251: INFO: Pod "busybox-privileged-false-abb9dd8f-6251-4268-9497-c0d7e16df88b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.776564ms
Mar 23 19:43:45.258: INFO: Pod "busybox-privileged-false-abb9dd8f-6251-4268-9497-c0d7e16df88b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024993341s
Mar 23 19:43:47.367: INFO: Pod "busybox-privileged-false-abb9dd8f-6251-4268-9497-c0d7e16df88b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.134492465s
Mar 23 19:43:47.367: INFO: Pod "busybox-privileged-false-abb9dd8f-6251-4268-9497-c0d7e16df88b" satisfied condition "success or failure"
Mar 23 19:43:47.389: INFO: Got logs for pod "busybox-privileged-false-abb9dd8f-6251-4268-9497-c0d7e16df88b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:43:47.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2753" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":176,"skipped":2962,"failed":0}
SSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:43:47.409: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 23 19:43:52.045: INFO: Successfully updated pod "adopt-release-bnlxn"
STEP: Checking that the Job readopts the Pod
Mar 23 19:43:52.045: INFO: Waiting up to 15m0s for pod "adopt-release-bnlxn" in namespace "job-7789" to be "adopted"
Mar 23 19:43:52.099: INFO: Pod "adopt-release-bnlxn": Phase="Running", Reason="", readiness=true. Elapsed: 54.070465ms
Mar 23 19:43:54.105: INFO: Pod "adopt-release-bnlxn": Phase="Running", Reason="", readiness=true. Elapsed: 2.060511667s
Mar 23 19:43:54.106: INFO: Pod "adopt-release-bnlxn" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 23 19:43:54.628: INFO: Successfully updated pod "adopt-release-bnlxn"
STEP: Checking that the Job releases the Pod
Mar 23 19:43:54.628: INFO: Waiting up to 15m0s for pod "adopt-release-bnlxn" in namespace "job-7789" to be "released"
Mar 23 19:43:54.634: INFO: Pod "adopt-release-bnlxn": Phase="Running", Reason="", readiness=true. Elapsed: 5.700848ms
Mar 23 19:43:54.634: INFO: Pod "adopt-release-bnlxn" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:43:54.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7789" for this suite.

• [SLOW TEST:7.263 seconds]
[sig-apps] Job
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":177,"skipped":2968,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:43:54.675: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-17cca44a-1832-4bc2-a1a9-3c5158c3ac15
STEP: Creating a pod to test consume secrets
Mar 23 19:43:54.795: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5e70b605-354d-48a8-8ec0-e80d6c6664da" in namespace "projected-1905" to be "success or failure"
Mar 23 19:43:54.818: INFO: Pod "pod-projected-secrets-5e70b605-354d-48a8-8ec0-e80d6c6664da": Phase="Pending", Reason="", readiness=false. Elapsed: 23.136179ms
Mar 23 19:43:56.825: INFO: Pod "pod-projected-secrets-5e70b605-354d-48a8-8ec0-e80d6c6664da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030496198s
Mar 23 19:43:58.831: INFO: Pod "pod-projected-secrets-5e70b605-354d-48a8-8ec0-e80d6c6664da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036256809s
STEP: Saw pod success
Mar 23 19:43:58.831: INFO: Pod "pod-projected-secrets-5e70b605-354d-48a8-8ec0-e80d6c6664da" satisfied condition "success or failure"
Mar 23 19:43:58.837: INFO: Trying to get logs from node kube17-worker-1 pod pod-projected-secrets-5e70b605-354d-48a8-8ec0-e80d6c6664da container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 19:43:58.933: INFO: Waiting for pod pod-projected-secrets-5e70b605-354d-48a8-8ec0-e80d6c6664da to disappear
Mar 23 19:43:58.946: INFO: Pod pod-projected-secrets-5e70b605-354d-48a8-8ec0-e80d6c6664da no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:43:58.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1905" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":178,"skipped":2986,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:43:58.969: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 19:44:00.928: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 19:44:03.002: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589440, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589440, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589441, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589440, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 19:44:06.316: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 23 19:44:10.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 attach --namespace=webhook-9669 to-be-attached-pod -i -c=container1'
Mar 23 19:44:10.918: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:44:10.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9669" for this suite.
STEP: Destroying namespace "webhook-9669-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.052 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":179,"skipped":2998,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:44:11.021: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar 23 19:44:11.085: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 19:44:11.100: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 19:44:11.104: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-1 before test
Mar 23 19:44:11.115: INFO: openebs-ndm-4pklk from openebs started at 2020-03-23 19:09:58 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.116: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 19:44:11.116: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-r5zrl from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:44:11.116: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:44:11.116: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:44:11.116: INFO: weave-net-qbqsz from kube-system started at 2020-03-23 19:09:58 +0000 UTC (2 container statuses recorded)
Mar 23 19:44:11.116: INFO: 	Container weave ready: true, restart count 0
Mar 23 19:44:11.116: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 19:44:11.116: INFO: kube-proxy-55v4z from kube-system started at 2020-03-23 18:30:18 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.116: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:44:11.116: INFO: adopt-release-bnlxn from job-7789 started at 2020-03-23 19:43:47 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.116: INFO: 	Container c ready: true, restart count 0
Mar 23 19:44:11.116: INFO: adopt-release-xjwhl from job-7789 started at 2020-03-23 19:43:47 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.116: INFO: 	Container c ready: true, restart count 0
Mar 23 19:44:11.116: INFO: to-be-attached-pod from webhook-9669 started at 2020-03-23 19:44:06 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.116: INFO: 	Container container1 ready: true, restart count 0
Mar 23 19:44:11.116: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-2 before test
Mar 23 19:44:11.146: INFO: sonobuoy from sonobuoy started at 2020-03-23 18:49:13 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.147: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 23 19:44:11.147: INFO: sonobuoy-e2e-job-efff66848912497f from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:44:11.147: INFO: 	Container e2e ready: true, restart count 0
Mar 23 19:44:11.147: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:44:11.147: INFO: openebs-admission-server-f67f77588-nqvzf from openebs started at 2020-03-23 19:09:26 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.147: INFO: 	Container admission-webhook ready: true, restart count 0
Mar 23 19:44:11.147: INFO: openebs-snapshot-operator-6c4c64d4bc-stpl5 from openebs started at 2020-03-23 19:09:26 +0000 UTC (2 container statuses recorded)
Mar 23 19:44:11.147: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 23 19:44:11.147: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Mar 23 19:44:11.147: INFO: kube-proxy-n567x from kube-system started at 2020-03-23 18:31:10 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.147: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:44:11.147: INFO: weave-net-92dzp from kube-system started at 2020-03-23 19:06:40 +0000 UTC (2 container statuses recorded)
Mar 23 19:44:11.147: INFO: 	Container weave ready: true, restart count 0
Mar 23 19:44:11.147: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 19:44:11.147: INFO: openebs-ndm-mwljx from openebs started at 2020-03-23 19:06:50 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.147: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 19:44:11.147: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-tx9p5 from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:44:11.147: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:44:11.147: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:44:11.147: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-3 before test
Mar 23 19:44:11.174: INFO: openebs-provisioner-7b8c68bf44-6w5b6 from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.174: INFO: 	Container openebs-provisioner ready: true, restart count 1
Mar 23 19:44:11.174: INFO: openebs-ndm-operator-5fccfb7976-vzw5m from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.175: INFO: 	Container node-disk-operator ready: true, restart count 1
Mar 23 19:44:11.175: INFO: openebs-localpv-provisioner-5c87bbd974-pwwkj from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.175: INFO: 	Container openebs-provisioner-hostpath ready: true, restart count 1
Mar 23 19:44:11.175: INFO: openebs-ndm-d9t4c from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.175: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 19:44:11.175: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-prwqb from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:44:11.175: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:44:11.175: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:44:11.175: INFO: kube-proxy-2qrv2 from kube-system started at 2020-03-23 18:30:45 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.175: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:44:11.175: INFO: weave-net-tqwcx from kube-system started at 2020-03-23 18:30:45 +0000 UTC (2 container statuses recorded)
Mar 23 19:44:11.176: INFO: 	Container weave ready: true, restart count 0
Mar 23 19:44:11.176: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 19:44:11.176: INFO: maya-apiserver-569c7c785b-ltfsc from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:44:11.176: INFO: 	Container maya-apiserver ready: true, restart count 3
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15ff066b2b6a1b34], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15ff066b2c3c9610], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:44:12.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8045" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":180,"skipped":3010,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:44:14.146: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:44:14.213: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 23 19:44:14.231: INFO: Number of nodes with available pods: 0
Mar 23 19:44:14.231: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 23 19:44:14.250: INFO: Number of nodes with available pods: 0
Mar 23 19:44:14.251: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:44:15.256: INFO: Number of nodes with available pods: 0
Mar 23 19:44:15.256: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:44:16.262: INFO: Number of nodes with available pods: 0
Mar 23 19:44:16.262: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:44:17.255: INFO: Number of nodes with available pods: 1
Mar 23 19:44:17.256: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 23 19:44:17.279: INFO: Number of nodes with available pods: 1
Mar 23 19:44:17.279: INFO: Number of running nodes: 0, number of available pods: 1
Mar 23 19:44:18.290: INFO: Number of nodes with available pods: 0
Mar 23 19:44:18.290: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 23 19:44:18.309: INFO: Number of nodes with available pods: 0
Mar 23 19:44:18.309: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:44:19.313: INFO: Number of nodes with available pods: 0
Mar 23 19:44:19.313: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:44:20.314: INFO: Number of nodes with available pods: 0
Mar 23 19:44:20.314: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:44:21.314: INFO: Number of nodes with available pods: 0
Mar 23 19:44:21.315: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:44:22.316: INFO: Number of nodes with available pods: 0
Mar 23 19:44:22.316: INFO: Node kube17-worker-1 is running more than one daemon pod
Mar 23 19:44:23.314: INFO: Number of nodes with available pods: 1
Mar 23 19:44:23.314: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4702, will wait for the garbage collector to delete the pods
Mar 23 19:44:23.385: INFO: Deleting DaemonSet.extensions daemon-set took: 11.351573ms
Mar 23 19:44:23.786: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.376945ms
Mar 23 19:44:26.699: INFO: Number of nodes with available pods: 0
Mar 23 19:44:26.699: INFO: Number of running nodes: 0, number of available pods: 0
Mar 23 19:44:26.714: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4702/daemonsets","resourceVersion":"28322"},"items":null}

Mar 23 19:44:26.728: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4702/pods","resourceVersion":"28323"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:44:26.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4702" for this suite.

• [SLOW TEST:12.643 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":181,"skipped":3023,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:44:26.789: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 19:44:28.357: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 19:44:30.372: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589468, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589468, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589468, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720589468, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 19:44:33.645: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
Mar 23 19:44:43.724: INFO: Waiting for webhook configuration to be ready...
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:44:50.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6065" for this suite.
STEP: Destroying namespace "webhook-6065-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:24.428 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":182,"skipped":3025,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:44:51.229: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:44:51.602: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-dff74a67-743e-4e98-89ae-f56b55863c53" in namespace "security-context-test-6566" to be "success or failure"
Mar 23 19:44:51.766: INFO: Pod "alpine-nnp-false-dff74a67-743e-4e98-89ae-f56b55863c53": Phase="Pending", Reason="", readiness=false. Elapsed: 163.82505ms
Mar 23 19:44:54.400: INFO: Pod "alpine-nnp-false-dff74a67-743e-4e98-89ae-f56b55863c53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.797612212s
Mar 23 19:44:56.520: INFO: Pod "alpine-nnp-false-dff74a67-743e-4e98-89ae-f56b55863c53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.91716767s
Mar 23 19:45:00.997: INFO: Pod "alpine-nnp-false-dff74a67-743e-4e98-89ae-f56b55863c53": Phase="Pending", Reason="", readiness=false. Elapsed: 9.394246922s
Mar 23 19:45:07.023: INFO: Pod "alpine-nnp-false-dff74a67-743e-4e98-89ae-f56b55863c53": Phase="Pending", Reason="", readiness=false. Elapsed: 15.420538907s
Mar 23 19:45:09.029: INFO: Pod "alpine-nnp-false-dff74a67-743e-4e98-89ae-f56b55863c53": Phase="Pending", Reason="", readiness=false. Elapsed: 17.426780369s
Mar 23 19:45:11.035: INFO: Pod "alpine-nnp-false-dff74a67-743e-4e98-89ae-f56b55863c53": Phase="Pending", Reason="", readiness=false. Elapsed: 19.432216577s
Mar 23 19:45:15.480: INFO: Pod "alpine-nnp-false-dff74a67-743e-4e98-89ae-f56b55863c53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.877890499s
Mar 23 19:45:15.480: INFO: Pod "alpine-nnp-false-dff74a67-743e-4e98-89ae-f56b55863c53" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:45:15.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6566" for this suite.

• [SLOW TEST:24.521 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:289
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":183,"skipped":3039,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:45:15.751: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-a475b210-9f82-4ca6-a08c-c0fcb950937b
STEP: Creating a pod to test consume secrets
Mar 23 19:45:15.855: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-36904a72-9e9a-45b4-8e16-8afaf0523d34" in namespace "projected-1090" to be "success or failure"
Mar 23 19:45:15.871: INFO: Pod "pod-projected-secrets-36904a72-9e9a-45b4-8e16-8afaf0523d34": Phase="Pending", Reason="", readiness=false. Elapsed: 15.643673ms
Mar 23 19:45:19.207: INFO: Pod "pod-projected-secrets-36904a72-9e9a-45b4-8e16-8afaf0523d34": Phase="Pending", Reason="", readiness=false. Elapsed: 3.352371252s
Mar 23 19:45:21.224: INFO: Pod "pod-projected-secrets-36904a72-9e9a-45b4-8e16-8afaf0523d34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.369166358s
STEP: Saw pod success
Mar 23 19:45:21.224: INFO: Pod "pod-projected-secrets-36904a72-9e9a-45b4-8e16-8afaf0523d34" satisfied condition "success or failure"
Mar 23 19:45:21.234: INFO: Trying to get logs from node kube17-worker-1 pod pod-projected-secrets-36904a72-9e9a-45b4-8e16-8afaf0523d34 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 23 19:45:21.280: INFO: Waiting for pod pod-projected-secrets-36904a72-9e9a-45b4-8e16-8afaf0523d34 to disappear
Mar 23 19:45:21.287: INFO: Pod pod-projected-secrets-36904a72-9e9a-45b4-8e16-8afaf0523d34 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:45:21.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1090" for this suite.

• [SLOW TEST:5.670 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":184,"skipped":3043,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:45:21.424: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 23 19:45:21.512: INFO: Waiting up to 5m0s for pod "pod-51a83f16-efd4-4574-a511-2f0ca201070c" in namespace "emptydir-3669" to be "success or failure"
Mar 23 19:45:21.516: INFO: Pod "pod-51a83f16-efd4-4574-a511-2f0ca201070c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.75637ms
Mar 23 19:45:23.522: INFO: Pod "pod-51a83f16-efd4-4574-a511-2f0ca201070c": Phase="Running", Reason="", readiness=true. Elapsed: 2.009953338s
Mar 23 19:45:25.529: INFO: Pod "pod-51a83f16-efd4-4574-a511-2f0ca201070c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016630609s
STEP: Saw pod success
Mar 23 19:45:25.529: INFO: Pod "pod-51a83f16-efd4-4574-a511-2f0ca201070c" satisfied condition "success or failure"
Mar 23 19:45:25.535: INFO: Trying to get logs from node kube17-worker-1 pod pod-51a83f16-efd4-4574-a511-2f0ca201070c container test-container: <nil>
STEP: delete the pod
Mar 23 19:45:25.611: INFO: Waiting for pod pod-51a83f16-efd4-4574-a511-2f0ca201070c to disappear
Mar 23 19:45:25.616: INFO: Pod pod-51a83f16-efd4-4574-a511-2f0ca201070c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:45:25.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3669" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":185,"skipped":3055,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:45:25.633: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Mar 23 19:45:33.835: INFO: 10 pods remaining
Mar 23 19:45:33.835: INFO: 8 pods has nil DeletionTimestamp
Mar 23 19:45:33.835: INFO: 
Mar 23 19:45:35.690: INFO: 6 pods remaining
Mar 23 19:45:35.690: INFO: 0 pods has nil DeletionTimestamp
Mar 23 19:45:35.690: INFO: 
Mar 23 19:45:35.830: INFO: 0 pods remaining
Mar 23 19:45:35.830: INFO: 0 pods has nil DeletionTimestamp
Mar 23 19:45:35.830: INFO: 
STEP: Gathering metrics
Mar 23 19:45:36.851: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:45:36.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6538" for this suite.

• [SLOW TEST:11.242 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":186,"skipped":3060,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:45:36.875: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:45:41.043: INFO: Waiting up to 5m0s for pod "client-envvars-9454d65e-368b-410b-a000-c19c5298451b" in namespace "pods-6742" to be "success or failure"
Mar 23 19:45:41.050: INFO: Pod "client-envvars-9454d65e-368b-410b-a000-c19c5298451b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.544861ms
Mar 23 19:45:43.055: INFO: Pod "client-envvars-9454d65e-368b-410b-a000-c19c5298451b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012529515s
Mar 23 19:45:45.063: INFO: Pod "client-envvars-9454d65e-368b-410b-a000-c19c5298451b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020069327s
STEP: Saw pod success
Mar 23 19:45:45.063: INFO: Pod "client-envvars-9454d65e-368b-410b-a000-c19c5298451b" satisfied condition "success or failure"
Mar 23 19:45:45.068: INFO: Trying to get logs from node kube17-worker-1 pod client-envvars-9454d65e-368b-410b-a000-c19c5298451b container env3cont: <nil>
STEP: delete the pod
Mar 23 19:45:45.106: INFO: Waiting for pod client-envvars-9454d65e-368b-410b-a000-c19c5298451b to disappear
Mar 23 19:45:45.130: INFO: Pod client-envvars-9454d65e-368b-410b-a000-c19c5298451b no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:45:45.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6742" for this suite.

• [SLOW TEST:8.275 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":187,"skipped":3062,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:45:45.151: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a3dd5816-aa2b-487a-8049-845f1739997d
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a3dd5816-aa2b-487a-8049-845f1739997d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:47:09.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8007" for this suite.

• [SLOW TEST:83.946 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":188,"skipped":3063,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:47:09.097: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6299, will wait for the garbage collector to delete the pods
Mar 23 19:47:13.263: INFO: Deleting Job.batch foo took: 9.835186ms
Mar 23 19:47:13.663: INFO: Terminating Job.batch foo pods took: 400.340024ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:47:59.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6299" for this suite.

• [SLOW TEST:50.704 seconds]
[sig-apps] Job
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":189,"skipped":3072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:47:59.805: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-a36b1928-f077-46b5-888c-c0c7f90aa416
STEP: Creating a pod to test consume configMaps
Mar 23 19:47:59.927: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce889e38-8439-4a39-9896-c90b5dc788e9" in namespace "configmap-1215" to be "success or failure"
Mar 23 19:47:59.945: INFO: Pod "pod-configmaps-ce889e38-8439-4a39-9896-c90b5dc788e9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.444517ms
Mar 23 19:48:01.965: INFO: Pod "pod-configmaps-ce889e38-8439-4a39-9896-c90b5dc788e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.036791071s
Mar 23 19:48:04.269: INFO: Pod "pod-configmaps-ce889e38-8439-4a39-9896-c90b5dc788e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.340770936s
STEP: Saw pod success
Mar 23 19:48:04.269: INFO: Pod "pod-configmaps-ce889e38-8439-4a39-9896-c90b5dc788e9" satisfied condition "success or failure"
Mar 23 19:48:04.291: INFO: Trying to get logs from node kube17-worker-1 pod pod-configmaps-ce889e38-8439-4a39-9896-c90b5dc788e9 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 19:48:05.969: INFO: Waiting for pod pod-configmaps-ce889e38-8439-4a39-9896-c90b5dc788e9 to disappear
Mar 23 19:48:05.983: INFO: Pod pod-configmaps-ce889e38-8439-4a39-9896-c90b5dc788e9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:48:05.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1215" for this suite.

• [SLOW TEST:6.223 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":190,"skipped":3094,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:48:06.034: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-5f81e2da-ceb7-43ba-833b-1523797f4ff3 in namespace container-probe-612
Mar 23 19:48:10.230: INFO: Started pod busybox-5f81e2da-ceb7-43ba-833b-1523797f4ff3 in namespace container-probe-612
STEP: checking the pod's current state and verifying that restartCount is present
Mar 23 19:48:10.238: INFO: Initial restart count of pod busybox-5f81e2da-ceb7-43ba-833b-1523797f4ff3 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:52:11.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-612" for this suite.

• [SLOW TEST:245.422 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":191,"skipped":3107,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:52:11.457: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:52:22.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8219" for this suite.

• [SLOW TEST:11.210 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":192,"skipped":3115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:52:22.673: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 23 19:52:27.341: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4262 pod-service-account-5a6ac16b-7497-48d4-a8c4-e9e3f7f284ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 23 19:52:27.770: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4262 pod-service-account-5a6ac16b-7497-48d4-a8c4-e9e3f7f284ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 23 19:52:28.278: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4262 pod-service-account-5a6ac16b-7497-48d4-a8c4-e9e3f7f284ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:52:28.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4262" for this suite.

• [SLOW TEST:6.053 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":193,"skipped":3138,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:52:28.727: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 23 19:52:28.830: INFO: Waiting up to 5m0s for pod "pod-b81f29a7-7037-4903-bc74-cc77cd726277" in namespace "emptydir-8380" to be "success or failure"
Mar 23 19:52:28.839: INFO: Pod "pod-b81f29a7-7037-4903-bc74-cc77cd726277": Phase="Pending", Reason="", readiness=false. Elapsed: 8.859556ms
Mar 23 19:52:32.959: INFO: Pod "pod-b81f29a7-7037-4903-bc74-cc77cd726277": Phase="Pending", Reason="", readiness=false. Elapsed: 4.128461462s
Mar 23 19:52:34.966: INFO: Pod "pod-b81f29a7-7037-4903-bc74-cc77cd726277": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.135746047s
STEP: Saw pod success
Mar 23 19:52:34.966: INFO: Pod "pod-b81f29a7-7037-4903-bc74-cc77cd726277" satisfied condition "success or failure"
Mar 23 19:52:34.971: INFO: Trying to get logs from node kube17-worker-1 pod pod-b81f29a7-7037-4903-bc74-cc77cd726277 container test-container: <nil>
STEP: delete the pod
Mar 23 19:52:35.045: INFO: Waiting for pod pod-b81f29a7-7037-4903-bc74-cc77cd726277 to disappear
Mar 23 19:52:35.048: INFO: Pod pod-b81f29a7-7037-4903-bc74-cc77cd726277 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:52:35.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8380" for this suite.

• [SLOW TEST:6.344 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":194,"skipped":3164,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:52:35.082: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:52:51.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7106" for this suite.

• [SLOW TEST:16.492 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":195,"skipped":3178,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:52:51.580: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Mar 23 19:52:52.202: INFO: created pod pod-service-account-defaultsa
Mar 23 19:52:52.202: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 23 19:52:52.236: INFO: created pod pod-service-account-mountsa
Mar 23 19:52:52.236: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 23 19:52:52.276: INFO: created pod pod-service-account-nomountsa
Mar 23 19:52:52.277: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 23 19:52:52.288: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 23 19:52:52.289: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 23 19:52:52.308: INFO: created pod pod-service-account-mountsa-mountspec
Mar 23 19:52:52.308: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 23 19:52:52.346: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 23 19:52:52.346: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 23 19:52:52.364: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 23 19:52:52.364: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 23 19:52:52.389: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 23 19:52:52.389: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 23 19:52:52.411: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 23 19:52:52.411: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:52:52.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4882" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":196,"skipped":3188,"failed":0}

------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:52:52.449: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Mar 23 19:52:52.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-6030'
Mar 23 19:52:53.176: INFO: stderr: ""
Mar 23 19:52:53.176: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 23 19:52:53.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6030'
Mar 23 19:52:53.407: INFO: stderr: ""
Mar 23 19:52:53.407: INFO: stdout: "update-demo-nautilus-5rkwb update-demo-nautilus-62v2t "
Mar 23 19:52:53.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-5rkwb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6030'
Mar 23 19:52:53.596: INFO: stderr: ""
Mar 23 19:52:53.596: INFO: stdout: ""
Mar 23 19:52:53.596: INFO: update-demo-nautilus-5rkwb is created but not running
Mar 23 19:52:58.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6030'
Mar 23 19:52:58.940: INFO: stderr: ""
Mar 23 19:52:58.940: INFO: stdout: "update-demo-nautilus-5rkwb update-demo-nautilus-62v2t "
Mar 23 19:52:58.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-5rkwb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6030'
Mar 23 19:52:59.161: INFO: stderr: ""
Mar 23 19:52:59.161: INFO: stdout: "true"
Mar 23 19:52:59.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-5rkwb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6030'
Mar 23 19:52:59.376: INFO: stderr: ""
Mar 23 19:52:59.376: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 19:52:59.377: INFO: validating pod update-demo-nautilus-5rkwb
Mar 23 19:52:59.386: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 19:52:59.386: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 19:52:59.386: INFO: update-demo-nautilus-5rkwb is verified up and running
Mar 23 19:52:59.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-62v2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6030'
Mar 23 19:52:59.570: INFO: stderr: ""
Mar 23 19:52:59.570: INFO: stdout: "true"
Mar 23 19:52:59.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-62v2t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6030'
Mar 23 19:52:59.749: INFO: stderr: ""
Mar 23 19:52:59.749: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 19:52:59.749: INFO: validating pod update-demo-nautilus-62v2t
Mar 23 19:52:59.759: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 19:52:59.759: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 19:52:59.759: INFO: update-demo-nautilus-62v2t is verified up and running
STEP: using delete to clean up resources
Mar 23 19:52:59.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete --grace-period=0 --force -f - --namespace=kubectl-6030'
Mar 23 19:53:00.071: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 19:53:00.071: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 23 19:53:00.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6030'
Mar 23 19:53:00.322: INFO: stderr: "No resources found in kubectl-6030 namespace.\n"
Mar 23 19:53:00.322: INFO: stdout: ""
Mar 23 19:53:00.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -l name=update-demo --namespace=kubectl-6030 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 23 19:53:00.568: INFO: stderr: ""
Mar 23 19:53:00.568: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:53:00.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6030" for this suite.

• [SLOW TEST:8.141 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":197,"skipped":3188,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:53:00.591: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 23 19:53:00.701: INFO: Waiting up to 5m0s for pod "pod-e9f67742-c69a-4211-9225-0caa4738e6b9" in namespace "emptydir-6662" to be "success or failure"
Mar 23 19:53:00.764: INFO: Pod "pod-e9f67742-c69a-4211-9225-0caa4738e6b9": Phase="Pending", Reason="", readiness=false. Elapsed: 62.965097ms
Mar 23 19:53:02.769: INFO: Pod "pod-e9f67742-c69a-4211-9225-0caa4738e6b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068510883s
Mar 23 19:53:04.775: INFO: Pod "pod-e9f67742-c69a-4211-9225-0caa4738e6b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074041646s
STEP: Saw pod success
Mar 23 19:53:04.775: INFO: Pod "pod-e9f67742-c69a-4211-9225-0caa4738e6b9" satisfied condition "success or failure"
Mar 23 19:53:04.778: INFO: Trying to get logs from node kube17-worker-1 pod pod-e9f67742-c69a-4211-9225-0caa4738e6b9 container test-container: <nil>
STEP: delete the pod
Mar 23 19:53:04.823: INFO: Waiting for pod pod-e9f67742-c69a-4211-9225-0caa4738e6b9 to disappear
Mar 23 19:53:04.827: INFO: Pod pod-e9f67742-c69a-4211-9225-0caa4738e6b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:53:04.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6662" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":198,"skipped":3195,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:53:04.844: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1754
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 19:53:04.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5733'
Mar 23 19:53:05.169: INFO: stderr: ""
Mar 23 19:53:05.169: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1759
Mar 23 19:53:05.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete pods e2e-test-httpd-pod --namespace=kubectl-5733'
Mar 23 19:53:09.613: INFO: stderr: ""
Mar 23 19:53:09.614: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:53:09.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5733" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":199,"skipped":3197,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:53:09.669: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:53:20.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4941" for this suite.

• [SLOW TEST:11.302 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":200,"skipped":3199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:53:20.973: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-17c015b8-205f-4e82-9323-7177a7ff4f60
STEP: Creating a pod to test consume secrets
Mar 23 19:53:21.067: INFO: Waiting up to 5m0s for pod "pod-secrets-2ae3af6c-8427-4f34-bb70-81a3236eb904" in namespace "secrets-3556" to be "success or failure"
Mar 23 19:53:21.075: INFO: Pod "pod-secrets-2ae3af6c-8427-4f34-bb70-81a3236eb904": Phase="Pending", Reason="", readiness=false. Elapsed: 7.826982ms
Mar 23 19:53:23.082: INFO: Pod "pod-secrets-2ae3af6c-8427-4f34-bb70-81a3236eb904": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014500765s
Mar 23 19:53:25.092: INFO: Pod "pod-secrets-2ae3af6c-8427-4f34-bb70-81a3236eb904": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024734628s
STEP: Saw pod success
Mar 23 19:53:25.092: INFO: Pod "pod-secrets-2ae3af6c-8427-4f34-bb70-81a3236eb904" satisfied condition "success or failure"
Mar 23 19:53:25.097: INFO: Trying to get logs from node kube17-worker-1 pod pod-secrets-2ae3af6c-8427-4f34-bb70-81a3236eb904 container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 19:53:25.154: INFO: Waiting for pod pod-secrets-2ae3af6c-8427-4f34-bb70-81a3236eb904 to disappear
Mar 23 19:53:25.165: INFO: Pod pod-secrets-2ae3af6c-8427-4f34-bb70-81a3236eb904 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:53:25.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3556" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":201,"skipped":3237,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:53:25.187: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-3298ee4d-17cf-4710-af6f-06d4f2138b4f
STEP: Creating a pod to test consume secrets
Mar 23 19:53:25.318: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-84b17d7b-4630-433d-9f58-00ea3a1cb467" in namespace "projected-2938" to be "success or failure"
Mar 23 19:53:25.352: INFO: Pod "pod-projected-secrets-84b17d7b-4630-433d-9f58-00ea3a1cb467": Phase="Pending", Reason="", readiness=false. Elapsed: 33.319495ms
Mar 23 19:53:27.358: INFO: Pod "pod-projected-secrets-84b17d7b-4630-433d-9f58-00ea3a1cb467": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039051553s
Mar 23 19:53:29.364: INFO: Pod "pod-projected-secrets-84b17d7b-4630-433d-9f58-00ea3a1cb467": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044984557s
STEP: Saw pod success
Mar 23 19:53:29.364: INFO: Pod "pod-projected-secrets-84b17d7b-4630-433d-9f58-00ea3a1cb467" satisfied condition "success or failure"
Mar 23 19:53:29.368: INFO: Trying to get logs from node kube17-worker-1 pod pod-projected-secrets-84b17d7b-4630-433d-9f58-00ea3a1cb467 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 23 19:53:29.987: INFO: Waiting for pod pod-projected-secrets-84b17d7b-4630-433d-9f58-00ea3a1cb467 to disappear
Mar 23 19:53:29.996: INFO: Pod pod-projected-secrets-84b17d7b-4630-433d-9f58-00ea3a1cb467 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:53:29.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2938" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":202,"skipped":3248,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:53:30.055: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 19:53:31.093: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 19:53:33.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590011, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590011, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590011, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590011, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 19:53:36.198: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:53:36.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8442" for this suite.
STEP: Destroying namespace "webhook-8442-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.465 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":203,"skipped":3255,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:53:36.522: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-169
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-169
Mar 23 19:53:36.718: INFO: Found 0 stateful pods, waiting for 1
Mar 23 19:53:46.726: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar 23 19:53:46.778: INFO: Deleting all statefulset in ns statefulset-169
Mar 23 19:53:46.785: INFO: Scaling statefulset ss to 0
Mar 23 19:54:17.333: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 19:54:17.371: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:54:17.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-169" for this suite.

• [SLOW TEST:40.966 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":204,"skipped":3272,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:54:17.491: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Mar 23 19:54:17.574: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:54:21.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6745" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":205,"skipped":3285,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:54:21.438: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 23 19:54:21.577: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1508 /api/v1/namespaces/watch-1508/configmaps/e2e-watch-test-watch-closed e6d4fa62-c0e6-453d-9b4c-19e2ca3a5c54 31671 0 2020-03-23 19:54:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 23 19:54:21.578: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1508 /api/v1/namespaces/watch-1508/configmaps/e2e-watch-test-watch-closed e6d4fa62-c0e6-453d-9b4c-19e2ca3a5c54 31672 0 2020-03-23 19:54:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 23 19:54:21.601: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1508 /api/v1/namespaces/watch-1508/configmaps/e2e-watch-test-watch-closed e6d4fa62-c0e6-453d-9b4c-19e2ca3a5c54 31673 0 2020-03-23 19:54:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 23 19:54:21.601: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1508 /api/v1/namespaces/watch-1508/configmaps/e2e-watch-test-watch-closed e6d4fa62-c0e6-453d-9b4c-19e2ca3a5c54 31674 0 2020-03-23 19:54:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:54:21.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1508" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":206,"skipped":3304,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:54:21.623: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Mar 23 19:54:21.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-5754'
Mar 23 19:54:22.354: INFO: stderr: ""
Mar 23 19:54:22.354: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 23 19:54:22.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5754'
Mar 23 19:54:22.619: INFO: stderr: ""
Mar 23 19:54:22.619: INFO: stdout: "update-demo-nautilus-nh6xh update-demo-nautilus-sp6wj "
Mar 23 19:54:22.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-nh6xh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5754'
Mar 23 19:54:22.823: INFO: stderr: ""
Mar 23 19:54:22.823: INFO: stdout: ""
Mar 23 19:54:22.824: INFO: update-demo-nautilus-nh6xh is created but not running
Mar 23 19:54:27.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5754'
Mar 23 19:54:28.053: INFO: stderr: ""
Mar 23 19:54:28.053: INFO: stdout: "update-demo-nautilus-nh6xh update-demo-nautilus-sp6wj "
Mar 23 19:54:28.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-nh6xh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5754'
Mar 23 19:54:28.250: INFO: stderr: ""
Mar 23 19:54:28.250: INFO: stdout: "true"
Mar 23 19:54:28.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-nh6xh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5754'
Mar 23 19:54:28.451: INFO: stderr: ""
Mar 23 19:54:28.451: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 19:54:28.451: INFO: validating pod update-demo-nautilus-nh6xh
Mar 23 19:54:28.461: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 19:54:28.461: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 19:54:28.461: INFO: update-demo-nautilus-nh6xh is verified up and running
Mar 23 19:54:28.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-sp6wj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5754'
Mar 23 19:54:28.652: INFO: stderr: ""
Mar 23 19:54:28.652: INFO: stdout: "true"
Mar 23 19:54:28.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-nautilus-sp6wj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5754'
Mar 23 19:54:28.859: INFO: stderr: ""
Mar 23 19:54:28.859: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 19:54:28.859: INFO: validating pod update-demo-nautilus-sp6wj
Mar 23 19:54:28.869: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 19:54:28.869: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 19:54:28.869: INFO: update-demo-nautilus-sp6wj is verified up and running
STEP: rolling-update to new replication controller
Mar 23 19:54:28.874: INFO: scanned /root for discovery docs: <nil>
Mar 23 19:54:28.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-5754'
Mar 23 19:54:55.093: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 23 19:54:55.093: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 23 19:54:55.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5754'
Mar 23 19:54:55.306: INFO: stderr: ""
Mar 23 19:54:55.306: INFO: stdout: "update-demo-kitten-468fq update-demo-kitten-rckhw "
Mar 23 19:54:55.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-kitten-468fq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5754'
Mar 23 19:54:55.495: INFO: stderr: ""
Mar 23 19:54:55.495: INFO: stdout: "true"
Mar 23 19:54:55.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-kitten-468fq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5754'
Mar 23 19:54:55.678: INFO: stderr: ""
Mar 23 19:54:55.678: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 23 19:54:55.678: INFO: validating pod update-demo-kitten-468fq
Mar 23 19:54:55.686: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 23 19:54:55.686: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 23 19:54:55.686: INFO: update-demo-kitten-468fq is verified up and running
Mar 23 19:54:55.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-kitten-rckhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5754'
Mar 23 19:54:55.864: INFO: stderr: ""
Mar 23 19:54:55.864: INFO: stdout: "true"
Mar 23 19:54:55.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods update-demo-kitten-rckhw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5754'
Mar 23 19:54:56.066: INFO: stderr: ""
Mar 23 19:54:56.066: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 23 19:54:56.066: INFO: validating pod update-demo-kitten-rckhw
Mar 23 19:54:56.076: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 23 19:54:56.076: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 23 19:54:56.076: INFO: update-demo-kitten-rckhw is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:54:56.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5754" for this suite.

• [SLOW TEST:34.472 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":207,"skipped":3306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:54:56.095: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-nplf
STEP: Creating a pod to test atomic-volume-subpath
Mar 23 19:54:56.196: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-nplf" in namespace "subpath-3827" to be "success or failure"
Mar 23 19:54:56.217: INFO: Pod "pod-subpath-test-projected-nplf": Phase="Pending", Reason="", readiness=false. Elapsed: 20.04388ms
Mar 23 19:54:58.222: INFO: Pod "pod-subpath-test-projected-nplf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025379349s
Mar 23 19:55:00.228: INFO: Pod "pod-subpath-test-projected-nplf": Phase="Running", Reason="", readiness=true. Elapsed: 4.031230562s
Mar 23 19:55:03.823: INFO: Pod "pod-subpath-test-projected-nplf": Phase="Running", Reason="", readiness=true. Elapsed: 7.626009673s
Mar 23 19:55:06.476: INFO: Pod "pod-subpath-test-projected-nplf": Phase="Running", Reason="", readiness=true. Elapsed: 10.279092018s
Mar 23 19:55:08.482: INFO: Pod "pod-subpath-test-projected-nplf": Phase="Running", Reason="", readiness=true. Elapsed: 12.28547422s
Mar 23 19:55:10.488: INFO: Pod "pod-subpath-test-projected-nplf": Phase="Running", Reason="", readiness=true. Elapsed: 14.291848309s
Mar 23 19:55:12.494: INFO: Pod "pod-subpath-test-projected-nplf": Phase="Running", Reason="", readiness=true. Elapsed: 16.297874778s
Mar 23 19:55:14.501: INFO: Pod "pod-subpath-test-projected-nplf": Phase="Running", Reason="", readiness=true. Elapsed: 18.304217388s
Mar 23 19:55:16.509: INFO: Pod "pod-subpath-test-projected-nplf": Phase="Running", Reason="", readiness=true. Elapsed: 20.312118807s
Mar 23 19:55:18.515: INFO: Pod "pod-subpath-test-projected-nplf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.318003435s
STEP: Saw pod success
Mar 23 19:55:18.515: INFO: Pod "pod-subpath-test-projected-nplf" satisfied condition "success or failure"
Mar 23 19:55:18.521: INFO: Trying to get logs from node kube17-worker-1 pod pod-subpath-test-projected-nplf container test-container-subpath-projected-nplf: <nil>
STEP: delete the pod
Mar 23 19:55:21.668: INFO: Waiting for pod pod-subpath-test-projected-nplf to disappear
Mar 23 19:55:21.678: INFO: Pod pod-subpath-test-projected-nplf no longer exists
STEP: Deleting pod pod-subpath-test-projected-nplf
Mar 23 19:55:21.679: INFO: Deleting pod "pod-subpath-test-projected-nplf" in namespace "subpath-3827"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:55:21.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3827" for this suite.

• [SLOW TEST:25.642 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":208,"skipped":3332,"failed":0}
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:55:21.738: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 23 19:55:32.363: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3006 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:55:32.363: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:55:32.594: INFO: Exec stderr: ""
Mar 23 19:55:32.594: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3006 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:55:32.594: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:55:32.832: INFO: Exec stderr: ""
Mar 23 19:55:32.832: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3006 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:55:32.832: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:55:33.052: INFO: Exec stderr: ""
Mar 23 19:55:33.052: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3006 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:55:33.053: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:55:33.280: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 23 19:55:33.280: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3006 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:55:33.280: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:55:33.535: INFO: Exec stderr: ""
Mar 23 19:55:33.535: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3006 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:55:33.536: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:55:33.760: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 23 19:55:33.760: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3006 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:55:33.760: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:55:33.989: INFO: Exec stderr: ""
Mar 23 19:55:33.989: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3006 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:55:33.989: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:55:34.214: INFO: Exec stderr: ""
Mar 23 19:55:34.214: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3006 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:55:34.214: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:55:34.475: INFO: Exec stderr: ""
Mar 23 19:55:34.476: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3006 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:55:34.476: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:55:34.720: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:55:34.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3006" for this suite.

• [SLOW TEST:13.028 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":209,"skipped":3332,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:55:34.767: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 19:55:34.868: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5acc80d-3264-4bd3-bb6a-1c6bb42f7062" in namespace "downward-api-7288" to be "success or failure"
Mar 23 19:55:34.875: INFO: Pod "downwardapi-volume-d5acc80d-3264-4bd3-bb6a-1c6bb42f7062": Phase="Pending", Reason="", readiness=false. Elapsed: 7.25369ms
Mar 23 19:55:37.106: INFO: Pod "downwardapi-volume-d5acc80d-3264-4bd3-bb6a-1c6bb42f7062": Phase="Pending", Reason="", readiness=false. Elapsed: 2.238243844s
Mar 23 19:55:39.112: INFO: Pod "downwardapi-volume-d5acc80d-3264-4bd3-bb6a-1c6bb42f7062": Phase="Pending", Reason="", readiness=false. Elapsed: 4.244830474s
Mar 23 19:55:41.119: INFO: Pod "downwardapi-volume-d5acc80d-3264-4bd3-bb6a-1c6bb42f7062": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.251409694s
STEP: Saw pod success
Mar 23 19:55:41.119: INFO: Pod "downwardapi-volume-d5acc80d-3264-4bd3-bb6a-1c6bb42f7062" satisfied condition "success or failure"
Mar 23 19:55:41.124: INFO: Trying to get logs from node kube17-worker-3 pod downwardapi-volume-d5acc80d-3264-4bd3-bb6a-1c6bb42f7062 container client-container: <nil>
STEP: delete the pod
Mar 23 19:55:41.191: INFO: Waiting for pod downwardapi-volume-d5acc80d-3264-4bd3-bb6a-1c6bb42f7062 to disappear
Mar 23 19:55:41.206: INFO: Pod downwardapi-volume-d5acc80d-3264-4bd3-bb6a-1c6bb42f7062 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:55:41.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7288" for this suite.

• [SLOW TEST:6.463 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":210,"skipped":3334,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:55:41.230: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 23 19:55:45.338: INFO: &Pod{ObjectMeta:{send-events-7faca458-1208-4a78-bc7d-7d8b27b6d064  events-5253 /api/v1/namespaces/events-5253/pods/send-events-7faca458-1208-4a78-bc7d-7d8b27b6d064 0ae86237-14c7-4ae4-98af-4c7bdc669ada 32246 0 2020-03-23 19:55:41 +0000 UTC <nil> <nil> map[name:foo time:300892616] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r9bk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r9bk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r9bk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:55:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:55:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:55:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:55:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.139,PodIP:10.36.0.5,StartTime:2020-03-23 19:55:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 19:55:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://e77ffa1b94079bb5e7c59b7e317beb9dadd69cd90145013152c1138511d13364,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.0.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar 23 19:55:47.344: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 23 19:55:49.351: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:55:49.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5253" for this suite.

• [SLOW TEST:8.179 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":211,"skipped":3351,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:55:49.414: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 23 19:55:49.489: INFO: Waiting up to 5m0s for pod "pod-581dfa19-6f9d-4574-a838-fc49b5f4211a" in namespace "emptydir-9051" to be "success or failure"
Mar 23 19:55:49.502: INFO: Pod "pod-581dfa19-6f9d-4574-a838-fc49b5f4211a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.658328ms
Mar 23 19:55:51.509: INFO: Pod "pod-581dfa19-6f9d-4574-a838-fc49b5f4211a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020712054s
Mar 23 19:55:53.516: INFO: Pod "pod-581dfa19-6f9d-4574-a838-fc49b5f4211a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027097613s
Mar 23 19:55:55.522: INFO: Pod "pod-581dfa19-6f9d-4574-a838-fc49b5f4211a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033492113s
STEP: Saw pod success
Mar 23 19:55:55.522: INFO: Pod "pod-581dfa19-6f9d-4574-a838-fc49b5f4211a" satisfied condition "success or failure"
Mar 23 19:55:55.527: INFO: Trying to get logs from node kube17-worker-3 pod pod-581dfa19-6f9d-4574-a838-fc49b5f4211a container test-container: <nil>
STEP: delete the pod
Mar 23 19:55:55.562: INFO: Waiting for pod pod-581dfa19-6f9d-4574-a838-fc49b5f4211a to disappear
Mar 23 19:55:55.566: INFO: Pod pod-581dfa19-6f9d-4574-a838-fc49b5f4211a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:55:55.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9051" for this suite.

• [SLOW TEST:6.173 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":212,"skipped":3354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:55:55.587: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar 23 19:55:55.661: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 19:55:55.682: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 19:55:55.687: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-1 before test
Mar 23 19:55:55.702: INFO: kube-proxy-55v4z from kube-system started at 2020-03-23 18:30:18 +0000 UTC (1 container statuses recorded)
Mar 23 19:55:55.702: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:55:55.702: INFO: test-pod from e2e-kubelet-etc-hosts-3006 started at 2020-03-23 19:55:24 +0000 UTC (3 container statuses recorded)
Mar 23 19:55:55.702: INFO: 	Container busybox-1 ready: true, restart count 0
Mar 23 19:55:55.702: INFO: 	Container busybox-2 ready: true, restart count 0
Mar 23 19:55:55.702: INFO: 	Container busybox-3 ready: true, restart count 0
Mar 23 19:55:55.702: INFO: test-host-network-pod from e2e-kubelet-etc-hosts-3006 started at 2020-03-23 19:55:28 +0000 UTC (2 container statuses recorded)
Mar 23 19:55:55.702: INFO: 	Container busybox-1 ready: true, restart count 0
Mar 23 19:55:55.702: INFO: 	Container busybox-2 ready: true, restart count 0
Mar 23 19:55:55.702: INFO: openebs-ndm-4pklk from openebs started at 2020-03-23 19:09:58 +0000 UTC (1 container statuses recorded)
Mar 23 19:55:55.702: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 19:55:55.702: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-r5zrl from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:55:55.702: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 23 19:55:55.702: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:55:55.702: INFO: weave-net-qbqsz from kube-system started at 2020-03-23 19:09:58 +0000 UTC (2 container statuses recorded)
Mar 23 19:55:55.702: INFO: 	Container weave ready: true, restart count 0
Mar 23 19:55:55.702: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 19:55:55.702: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-2 before test
Mar 23 19:55:55.746: INFO: kube-proxy-n567x from kube-system started at 2020-03-23 18:31:10 +0000 UTC (1 container statuses recorded)
Mar 23 19:55:55.746: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:55:55.746: INFO: weave-net-92dzp from kube-system started at 2020-03-23 19:06:40 +0000 UTC (2 container statuses recorded)
Mar 23 19:55:55.746: INFO: 	Container weave ready: true, restart count 0
Mar 23 19:55:55.746: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 19:55:55.746: INFO: sonobuoy from sonobuoy started at 2020-03-23 18:49:13 +0000 UTC (1 container statuses recorded)
Mar 23 19:55:55.746: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 23 19:55:55.746: INFO: sonobuoy-e2e-job-efff66848912497f from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:55:55.746: INFO: 	Container e2e ready: true, restart count 0
Mar 23 19:55:55.746: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:55:55.746: INFO: openebs-admission-server-f67f77588-nqvzf from openebs started at 2020-03-23 19:09:26 +0000 UTC (1 container statuses recorded)
Mar 23 19:55:55.747: INFO: 	Container admission-webhook ready: true, restart count 0
Mar 23 19:55:55.747: INFO: openebs-snapshot-operator-6c4c64d4bc-stpl5 from openebs started at 2020-03-23 19:09:26 +0000 UTC (2 container statuses recorded)
Mar 23 19:55:55.747: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 23 19:55:55.747: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Mar 23 19:55:55.747: INFO: openebs-ndm-mwljx from openebs started at 2020-03-23 19:06:50 +0000 UTC (1 container statuses recorded)
Mar 23 19:55:55.747: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 19:55:55.747: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-tx9p5 from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:55:55.747: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 23 19:55:55.747: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:55:55.747: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-3 before test
Mar 23 19:55:55.760: INFO: openebs-provisioner-7b8c68bf44-6w5b6 from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:55:55.760: INFO: 	Container openebs-provisioner ready: true, restart count 1
Mar 23 19:55:55.760: INFO: openebs-ndm-operator-5fccfb7976-vzw5m from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:55:55.760: INFO: 	Container node-disk-operator ready: true, restart count 1
Mar 23 19:55:55.760: INFO: openebs-localpv-provisioner-5c87bbd974-pwwkj from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:55:55.760: INFO: 	Container openebs-provisioner-hostpath ready: true, restart count 1
Mar 23 19:55:55.760: INFO: openebs-ndm-d9t4c from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:55:55.760: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 19:55:55.760: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-prwqb from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 19:55:55.760: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 23 19:55:55.760: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:55:55.760: INFO: send-events-7faca458-1208-4a78-bc7d-7d8b27b6d064 from events-5253 started at 2020-03-23 19:55:41 +0000 UTC (1 container statuses recorded)
Mar 23 19:55:55.760: INFO: 	Container p ready: true, restart count 0
Mar 23 19:55:55.760: INFO: kube-proxy-2qrv2 from kube-system started at 2020-03-23 18:30:45 +0000 UTC (1 container statuses recorded)
Mar 23 19:55:55.761: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:55:55.761: INFO: weave-net-tqwcx from kube-system started at 2020-03-23 18:30:45 +0000 UTC (2 container statuses recorded)
Mar 23 19:55:55.761: INFO: 	Container weave ready: true, restart count 0
Mar 23 19:55:55.761: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 19:55:55.761: INFO: maya-apiserver-569c7c785b-ltfsc from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 19:55:55.761: INFO: 	Container maya-apiserver ready: true, restart count 3
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-62da3485-1c46-4b5b-a5d6-1b57e0dfc2c6 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-62da3485-1c46-4b5b-a5d6-1b57e0dfc2c6 off the node kube17-worker-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-62da3485-1c46-4b5b-a5d6-1b57e0dfc2c6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:56:02.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-40" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:7.223 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":213,"skipped":3377,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:56:02.813: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Mar 23 19:56:02.960: INFO: Waiting up to 5m0s for pod "client-containers-439082d0-7dab-499d-9a42-ba9102a0704f" in namespace "containers-3356" to be "success or failure"
Mar 23 19:56:02.967: INFO: Pod "client-containers-439082d0-7dab-499d-9a42-ba9102a0704f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.254427ms
Mar 23 19:56:04.972: INFO: Pod "client-containers-439082d0-7dab-499d-9a42-ba9102a0704f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010922129s
Mar 23 19:56:06.987: INFO: Pod "client-containers-439082d0-7dab-499d-9a42-ba9102a0704f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026484726s
STEP: Saw pod success
Mar 23 19:56:06.987: INFO: Pod "client-containers-439082d0-7dab-499d-9a42-ba9102a0704f" satisfied condition "success or failure"
Mar 23 19:56:06.996: INFO: Trying to get logs from node kube17-worker-1 pod client-containers-439082d0-7dab-499d-9a42-ba9102a0704f container test-container: <nil>
STEP: delete the pod
Mar 23 19:56:07.049: INFO: Waiting for pod client-containers-439082d0-7dab-499d-9a42-ba9102a0704f to disappear
Mar 23 19:56:07.058: INFO: Pod client-containers-439082d0-7dab-499d-9a42-ba9102a0704f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:56:07.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3356" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":214,"skipped":3389,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:56:07.079: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-c42bb in namespace proxy-9246
I0323 19:56:07.210538      23 runners.go:189] Created replication controller with name: proxy-service-c42bb, namespace: proxy-9246, replica count: 1
I0323 19:56:08.265187      23 runners.go:189] proxy-service-c42bb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 19:56:09.265479      23 runners.go:189] proxy-service-c42bb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 19:56:10.266351      23 runners.go:189] proxy-service-c42bb Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 19:56:10.289: INFO: setup took 3.12550958s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 23 19:56:10.347: INFO: (0) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 52.855574ms)
Mar 23 19:56:10.347: INFO: (0) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 53.009752ms)
Mar 23 19:56:10.347: INFO: (0) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 53.489138ms)
Mar 23 19:56:10.361: INFO: (0) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 68.279624ms)
Mar 23 19:56:10.367: INFO: (0) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 73.686586ms)
Mar 23 19:56:10.369: INFO: (0) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 75.444749ms)
Mar 23 19:56:10.369: INFO: (0) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 77.902822ms)
Mar 23 19:56:10.370: INFO: (0) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 78.145978ms)
Mar 23 19:56:10.370: INFO: (0) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 80.0973ms)
Mar 23 19:56:10.370: INFO: (0) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 75.994778ms)
Mar 23 19:56:10.378: INFO: (0) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 87.620223ms)
Mar 23 19:56:10.378: INFO: (0) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 86.493322ms)
Mar 23 19:56:10.380: INFO: (0) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 86.980419ms)
Mar 23 19:56:10.382: INFO: (0) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 89.114735ms)
Mar 23 19:56:10.383: INFO: (0) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 90.913585ms)
Mar 23 19:56:10.384: INFO: (0) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 90.744282ms)
Mar 23 19:56:10.410: INFO: (1) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 21.774166ms)
Mar 23 19:56:10.412: INFO: (1) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 25.260853ms)
Mar 23 19:56:10.412: INFO: (1) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 25.529959ms)
Mar 23 19:56:10.414: INFO: (1) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 27.066472ms)
Mar 23 19:56:10.415: INFO: (1) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 28.017114ms)
Mar 23 19:56:10.417: INFO: (1) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 28.732978ms)
Mar 23 19:56:10.417: INFO: (1) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 29.640631ms)
Mar 23 19:56:10.424: INFO: (1) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 38.225027ms)
Mar 23 19:56:10.425: INFO: (1) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 36.67139ms)
Mar 23 19:56:10.426: INFO: (1) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 37.362932ms)
Mar 23 19:56:10.426: INFO: (1) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 38.354195ms)
Mar 23 19:56:10.427: INFO: (1) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 41.912828ms)
Mar 23 19:56:10.428: INFO: (1) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 43.209778ms)
Mar 23 19:56:10.433: INFO: (1) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 48.003031ms)
Mar 23 19:56:10.434: INFO: (1) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 48.559293ms)
Mar 23 19:56:10.434: INFO: (1) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 46.774767ms)
Mar 23 19:56:10.455: INFO: (2) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 19.685165ms)
Mar 23 19:56:10.463: INFO: (2) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 28.094957ms)
Mar 23 19:56:10.464: INFO: (2) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 28.39673ms)
Mar 23 19:56:10.470: INFO: (2) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 34.942873ms)
Mar 23 19:56:10.471: INFO: (2) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 35.398957ms)
Mar 23 19:56:10.471: INFO: (2) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 35.596697ms)
Mar 23 19:56:10.471: INFO: (2) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 36.043883ms)
Mar 23 19:56:10.471: INFO: (2) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 36.002805ms)
Mar 23 19:56:10.471: INFO: (2) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 35.890887ms)
Mar 23 19:56:10.472: INFO: (2) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 36.967402ms)
Mar 23 19:56:10.472: INFO: (2) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 37.287143ms)
Mar 23 19:56:10.472: INFO: (2) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 36.859496ms)
Mar 23 19:56:10.479: INFO: (2) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 44.169179ms)
Mar 23 19:56:10.481: INFO: (2) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 45.865908ms)
Mar 23 19:56:10.481: INFO: (2) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 46.549303ms)
Mar 23 19:56:10.482: INFO: (2) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 46.654441ms)
Mar 23 19:56:10.490: INFO: (3) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 8.698615ms)
Mar 23 19:56:10.501: INFO: (3) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 17.699679ms)
Mar 23 19:56:10.501: INFO: (3) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 18.84118ms)
Mar 23 19:56:10.504: INFO: (3) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 20.889291ms)
Mar 23 19:56:10.507: INFO: (3) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 23.570689ms)
Mar 23 19:56:10.508: INFO: (3) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 24.889615ms)
Mar 23 19:56:10.508: INFO: (3) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 25.432911ms)
Mar 23 19:56:10.510: INFO: (3) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 26.416034ms)
Mar 23 19:56:10.520: INFO: (3) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 36.34338ms)
Mar 23 19:56:10.521: INFO: (3) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 37.920856ms)
Mar 23 19:56:10.521: INFO: (3) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 38.893764ms)
Mar 23 19:56:10.521: INFO: (3) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 38.436716ms)
Mar 23 19:56:10.521: INFO: (3) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 38.551403ms)
Mar 23 19:56:10.525: INFO: (3) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 41.806195ms)
Mar 23 19:56:10.525: INFO: (3) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 42.283018ms)
Mar 23 19:56:10.527: INFO: (3) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 44.233353ms)
Mar 23 19:56:10.543: INFO: (4) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 11.131143ms)
Mar 23 19:56:10.543: INFO: (4) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 11.757494ms)
Mar 23 19:56:10.543: INFO: (4) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 12.23393ms)
Mar 23 19:56:10.543: INFO: (4) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 12.632865ms)
Mar 23 19:56:10.544: INFO: (4) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 13.771852ms)
Mar 23 19:56:10.545: INFO: (4) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 15.521096ms)
Mar 23 19:56:10.546: INFO: (4) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 15.44009ms)
Mar 23 19:56:10.546: INFO: (4) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 15.997624ms)
Mar 23 19:56:10.546: INFO: (4) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 16.850311ms)
Mar 23 19:56:10.547: INFO: (4) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 19.419817ms)
Mar 23 19:56:10.551: INFO: (4) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 19.990975ms)
Mar 23 19:56:10.553: INFO: (4) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 24.376891ms)
Mar 23 19:56:10.553: INFO: (4) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 24.957101ms)
Mar 23 19:56:10.553: INFO: (4) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 23.830607ms)
Mar 23 19:56:10.554: INFO: (4) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 26.007793ms)
Mar 23 19:56:10.555: INFO: (4) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 25.666123ms)
Mar 23 19:56:10.582: INFO: (5) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 25.226949ms)
Mar 23 19:56:10.582: INFO: (5) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 26.970226ms)
Mar 23 19:56:10.582: INFO: (5) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 26.172776ms)
Mar 23 19:56:10.582: INFO: (5) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 27.497561ms)
Mar 23 19:56:10.582: INFO: (5) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 27.564136ms)
Mar 23 19:56:10.583: INFO: (5) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 25.993305ms)
Mar 23 19:56:10.583: INFO: (5) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 27.514595ms)
Mar 23 19:56:10.583: INFO: (5) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 27.329678ms)
Mar 23 19:56:10.583: INFO: (5) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 26.537968ms)
Mar 23 19:56:10.583: INFO: (5) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 26.70144ms)
Mar 23 19:56:10.587: INFO: (5) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 29.952037ms)
Mar 23 19:56:10.590: INFO: (5) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 32.758261ms)
Mar 23 19:56:10.591: INFO: (5) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 35.221382ms)
Mar 23 19:56:10.592: INFO: (5) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 35.704403ms)
Mar 23 19:56:10.592: INFO: (5) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 36.515167ms)
Mar 23 19:56:10.596: INFO: (5) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 38.859907ms)
Mar 23 19:56:10.612: INFO: (6) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 15.859203ms)
Mar 23 19:56:10.613: INFO: (6) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 15.521529ms)
Mar 23 19:56:10.614: INFO: (6) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 16.47375ms)
Mar 23 19:56:10.614: INFO: (6) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 18.073457ms)
Mar 23 19:56:10.616: INFO: (6) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 18.726013ms)
Mar 23 19:56:10.616: INFO: (6) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 19.009791ms)
Mar 23 19:56:10.616: INFO: (6) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 20.624702ms)
Mar 23 19:56:10.617: INFO: (6) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 20.052054ms)
Mar 23 19:56:10.617: INFO: (6) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 20.121708ms)
Mar 23 19:56:10.618: INFO: (6) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 21.660271ms)
Mar 23 19:56:10.618: INFO: (6) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 21.256282ms)
Mar 23 19:56:10.618: INFO: (6) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 20.987646ms)
Mar 23 19:56:10.619: INFO: (6) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 21.222021ms)
Mar 23 19:56:10.619: INFO: (6) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 21.638766ms)
Mar 23 19:56:10.620: INFO: (6) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 22.785692ms)
Mar 23 19:56:10.620: INFO: (6) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 22.162572ms)
Mar 23 19:56:10.639: INFO: (7) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 18.388508ms)
Mar 23 19:56:10.640: INFO: (7) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 19.968784ms)
Mar 23 19:56:10.641: INFO: (7) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 20.623048ms)
Mar 23 19:56:10.641: INFO: (7) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 20.882856ms)
Mar 23 19:56:10.642: INFO: (7) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 22.284866ms)
Mar 23 19:56:10.642: INFO: (7) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 21.391202ms)
Mar 23 19:56:10.642: INFO: (7) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 21.265234ms)
Mar 23 19:56:10.643: INFO: (7) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 22.968534ms)
Mar 23 19:56:10.644: INFO: (7) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 24.008087ms)
Mar 23 19:56:10.645: INFO: (7) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 24.59337ms)
Mar 23 19:56:10.674: INFO: (7) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 52.720121ms)
Mar 23 19:56:10.677: INFO: (7) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 55.886738ms)
Mar 23 19:56:10.687: INFO: (7) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 66.677845ms)
Mar 23 19:56:10.692: INFO: (7) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 70.68223ms)
Mar 23 19:56:10.700: INFO: (7) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 79.760796ms)
Mar 23 19:56:10.703: INFO: (7) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 82.29612ms)
Mar 23 19:56:10.863: INFO: (8) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 159.086575ms)
Mar 23 19:56:10.864: INFO: (8) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 160.860122ms)
Mar 23 19:56:10.870: INFO: (8) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 166.29794ms)
Mar 23 19:56:10.871: INFO: (8) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 167.040083ms)
Mar 23 19:56:10.871: INFO: (8) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 167.607035ms)
Mar 23 19:56:10.871: INFO: (8) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 167.608232ms)
Mar 23 19:56:10.875: INFO: (8) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 172.059753ms)
Mar 23 19:56:10.876: INFO: (8) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 172.477573ms)
Mar 23 19:56:10.877: INFO: (8) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 172.488295ms)
Mar 23 19:56:10.877: INFO: (8) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 173.303139ms)
Mar 23 19:56:10.899: INFO: (8) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 195.651701ms)
Mar 23 19:56:10.899: INFO: (8) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 196.044032ms)
Mar 23 19:56:10.927: INFO: (8) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 223.107934ms)
Mar 23 19:56:10.927: INFO: (8) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 223.607331ms)
Mar 23 19:56:10.928: INFO: (8) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 224.064688ms)
Mar 23 19:56:10.928: INFO: (8) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 224.362574ms)
Mar 23 19:56:10.950: INFO: (9) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 16.97322ms)
Mar 23 19:56:10.950: INFO: (9) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 20.515599ms)
Mar 23 19:56:10.962: INFO: (9) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 29.597059ms)
Mar 23 19:56:10.962: INFO: (9) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 32.815744ms)
Mar 23 19:56:10.965: INFO: (9) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 32.895106ms)
Mar 23 19:56:10.965: INFO: (9) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 32.649291ms)
Mar 23 19:56:10.966: INFO: (9) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 34.686707ms)
Mar 23 19:56:10.973: INFO: (9) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 42.917842ms)
Mar 23 19:56:10.974: INFO: (9) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 43.264252ms)
Mar 23 19:56:10.974: INFO: (9) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 42.429102ms)
Mar 23 19:56:10.977: INFO: (9) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 46.122433ms)
Mar 23 19:56:10.977: INFO: (9) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 44.605969ms)
Mar 23 19:56:10.978: INFO: (9) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 44.890624ms)
Mar 23 19:56:10.978: INFO: (9) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 48.373924ms)
Mar 23 19:56:10.978: INFO: (9) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 46.543522ms)
Mar 23 19:56:10.978: INFO: (9) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 48.626723ms)
Mar 23 19:56:10.991: INFO: (10) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 11.490048ms)
Mar 23 19:56:10.993: INFO: (10) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 13.510261ms)
Mar 23 19:56:10.993: INFO: (10) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 14.568516ms)
Mar 23 19:56:10.994: INFO: (10) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 14.698026ms)
Mar 23 19:56:10.995: INFO: (10) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 16.291533ms)
Mar 23 19:56:10.996: INFO: (10) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 16.822325ms)
Mar 23 19:56:10.998: INFO: (10) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 18.955843ms)
Mar 23 19:56:10.999: INFO: (10) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 19.174718ms)
Mar 23 19:56:11.000: INFO: (10) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 20.730698ms)
Mar 23 19:56:11.001: INFO: (10) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 20.864311ms)
Mar 23 19:56:11.002: INFO: (10) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 23.120362ms)
Mar 23 19:56:11.003: INFO: (10) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 24.052535ms)
Mar 23 19:56:11.003: INFO: (10) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 23.487575ms)
Mar 23 19:56:11.004: INFO: (10) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 24.536165ms)
Mar 23 19:56:11.004: INFO: (10) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 24.255166ms)
Mar 23 19:56:11.004: INFO: (10) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 24.487641ms)
Mar 23 19:56:11.015: INFO: (11) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 9.868903ms)
Mar 23 19:56:11.020: INFO: (11) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 15.287831ms)
Mar 23 19:56:11.021: INFO: (11) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 16.152033ms)
Mar 23 19:56:11.022: INFO: (11) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 15.937765ms)
Mar 23 19:56:11.022: INFO: (11) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 16.839874ms)
Mar 23 19:56:11.025: INFO: (11) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 19.582946ms)
Mar 23 19:56:11.025: INFO: (11) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 19.743733ms)
Mar 23 19:56:11.026: INFO: (11) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 21.536098ms)
Mar 23 19:56:11.027: INFO: (11) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 21.553142ms)
Mar 23 19:56:11.027: INFO: (11) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 21.521168ms)
Mar 23 19:56:11.027: INFO: (11) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 20.970513ms)
Mar 23 19:56:11.028: INFO: (11) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 22.566144ms)
Mar 23 19:56:11.028: INFO: (11) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 22.845362ms)
Mar 23 19:56:11.028: INFO: (11) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 21.969643ms)
Mar 23 19:56:11.028: INFO: (11) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 22.479098ms)
Mar 23 19:56:11.028: INFO: (11) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 22.417974ms)
Mar 23 19:56:11.044: INFO: (12) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 14.662411ms)
Mar 23 19:56:11.046: INFO: (12) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 16.085615ms)
Mar 23 19:56:11.047: INFO: (12) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 17.102414ms)
Mar 23 19:56:11.049: INFO: (12) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 18.695974ms)
Mar 23 19:56:11.050: INFO: (12) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 19.817175ms)
Mar 23 19:56:11.050: INFO: (12) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 21.346818ms)
Mar 23 19:56:11.052: INFO: (12) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 21.506584ms)
Mar 23 19:56:11.052: INFO: (12) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 22.102406ms)
Mar 23 19:56:11.052: INFO: (12) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 21.822554ms)
Mar 23 19:56:11.052: INFO: (12) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 21.650773ms)
Mar 23 19:56:11.054: INFO: (12) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 24.426389ms)
Mar 23 19:56:11.055: INFO: (12) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 25.5556ms)
Mar 23 19:56:11.055: INFO: (12) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 25.710047ms)
Mar 23 19:56:11.059: INFO: (12) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 29.197001ms)
Mar 23 19:56:11.059: INFO: (12) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 28.943502ms)
Mar 23 19:56:11.059: INFO: (12) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 29.968484ms)
Mar 23 19:56:11.078: INFO: (13) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 18.541575ms)
Mar 23 19:56:11.079: INFO: (13) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 17.573231ms)
Mar 23 19:56:11.079: INFO: (13) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 19.047959ms)
Mar 23 19:56:11.079: INFO: (13) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 18.771723ms)
Mar 23 19:56:11.080: INFO: (13) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 20.070444ms)
Mar 23 19:56:11.080: INFO: (13) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 19.285003ms)
Mar 23 19:56:11.080: INFO: (13) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 18.629014ms)
Mar 23 19:56:11.080: INFO: (13) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 19.37574ms)
Mar 23 19:56:11.080: INFO: (13) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 19.519188ms)
Mar 23 19:56:11.081: INFO: (13) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 19.835482ms)
Mar 23 19:56:11.081: INFO: (13) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 19.548596ms)
Mar 23 19:56:11.084: INFO: (13) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 24.909603ms)
Mar 23 19:56:11.085: INFO: (13) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 24.393784ms)
Mar 23 19:56:11.090: INFO: (13) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 30.064322ms)
Mar 23 19:56:11.091: INFO: (13) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 31.336322ms)
Mar 23 19:56:11.091: INFO: (13) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 30.760445ms)
Mar 23 19:56:11.110: INFO: (14) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 17.982056ms)
Mar 23 19:56:11.111: INFO: (14) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 18.647408ms)
Mar 23 19:56:11.112: INFO: (14) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 19.20753ms)
Mar 23 19:56:11.112: INFO: (14) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 19.111775ms)
Mar 23 19:56:11.112: INFO: (14) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 19.0571ms)
Mar 23 19:56:11.113: INFO: (14) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 21.199346ms)
Mar 23 19:56:11.113: INFO: (14) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 21.490879ms)
Mar 23 19:56:11.114: INFO: (14) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 21.966927ms)
Mar 23 19:56:11.115: INFO: (14) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 23.56172ms)
Mar 23 19:56:11.115: INFO: (14) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 22.481333ms)
Mar 23 19:56:11.116: INFO: (14) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 23.141521ms)
Mar 23 19:56:11.118: INFO: (14) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 25.83956ms)
Mar 23 19:56:11.118: INFO: (14) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 25.254923ms)
Mar 23 19:56:11.118: INFO: (14) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 25.235717ms)
Mar 23 19:56:11.118: INFO: (14) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 26.521831ms)
Mar 23 19:56:11.119: INFO: (14) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 26.943907ms)
Mar 23 19:56:11.127: INFO: (15) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 7.532374ms)
Mar 23 19:56:11.131: INFO: (15) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 11.045611ms)
Mar 23 19:56:11.132: INFO: (15) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 13.024019ms)
Mar 23 19:56:11.134: INFO: (15) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 13.530165ms)
Mar 23 19:56:11.135: INFO: (15) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 15.333689ms)
Mar 23 19:56:11.137: INFO: (15) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 16.524376ms)
Mar 23 19:56:11.137: INFO: (15) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 16.670725ms)
Mar 23 19:56:11.137: INFO: (15) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 16.74525ms)
Mar 23 19:56:11.139: INFO: (15) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 19.30303ms)
Mar 23 19:56:11.139: INFO: (15) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 19.966001ms)
Mar 23 19:56:11.140: INFO: (15) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 19.974615ms)
Mar 23 19:56:11.141: INFO: (15) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 21.063982ms)
Mar 23 19:56:11.142: INFO: (15) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 21.707768ms)
Mar 23 19:56:11.143: INFO: (15) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 22.050629ms)
Mar 23 19:56:11.143: INFO: (15) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 23.339629ms)
Mar 23 19:56:11.143: INFO: (15) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 23.559414ms)
Mar 23 19:56:11.161: INFO: (16) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 17.416674ms)
Mar 23 19:56:11.162: INFO: (16) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 16.231455ms)
Mar 23 19:56:11.163: INFO: (16) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 19.069866ms)
Mar 23 19:56:11.163: INFO: (16) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 18.037381ms)
Mar 23 19:56:11.165: INFO: (16) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 20.305736ms)
Mar 23 19:56:11.165: INFO: (16) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 18.288139ms)
Mar 23 19:56:11.169: INFO: (16) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 24.331195ms)
Mar 23 19:56:11.170: INFO: (16) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 23.645273ms)
Mar 23 19:56:11.171: INFO: (16) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 26.239992ms)
Mar 23 19:56:11.172: INFO: (16) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 26.506754ms)
Mar 23 19:56:11.172: INFO: (16) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 27.733453ms)
Mar 23 19:56:11.172: INFO: (16) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 26.247038ms)
Mar 23 19:56:11.172: INFO: (16) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 27.651575ms)
Mar 23 19:56:11.172: INFO: (16) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 26.585799ms)
Mar 23 19:56:11.173: INFO: (16) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 26.768302ms)
Mar 23 19:56:11.174: INFO: (16) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 27.716369ms)
Mar 23 19:56:11.189: INFO: (17) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 12.462745ms)
Mar 23 19:56:11.189: INFO: (17) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 14.525429ms)
Mar 23 19:56:11.191: INFO: (17) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 15.540881ms)
Mar 23 19:56:11.192: INFO: (17) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 14.898901ms)
Mar 23 19:56:11.192: INFO: (17) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 14.974704ms)
Mar 23 19:56:11.192: INFO: (17) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 15.084467ms)
Mar 23 19:56:11.194: INFO: (17) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 19.705143ms)
Mar 23 19:56:11.194: INFO: (17) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 17.966131ms)
Mar 23 19:56:11.195: INFO: (17) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 19.564489ms)
Mar 23 19:56:11.195: INFO: (17) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 19.890269ms)
Mar 23 19:56:11.199: INFO: (17) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 21.764286ms)
Mar 23 19:56:11.200: INFO: (17) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 23.033707ms)
Mar 23 19:56:11.200: INFO: (17) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 24.002391ms)
Mar 23 19:56:11.201: INFO: (17) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 25.153345ms)
Mar 23 19:56:11.202: INFO: (17) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 25.953719ms)
Mar 23 19:56:11.204: INFO: (17) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 27.198326ms)
Mar 23 19:56:11.234: INFO: (18) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 27.056399ms)
Mar 23 19:56:11.235: INFO: (18) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 28.820789ms)
Mar 23 19:56:11.235: INFO: (18) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 29.36566ms)
Mar 23 19:56:11.235: INFO: (18) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 27.817666ms)
Mar 23 19:56:11.235: INFO: (18) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 29.926702ms)
Mar 23 19:56:11.235: INFO: (18) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 28.162292ms)
Mar 23 19:56:11.235: INFO: (18) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 30.795441ms)
Mar 23 19:56:11.235: INFO: (18) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 31.257724ms)
Mar 23 19:56:11.239: INFO: (18) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 32.242139ms)
Mar 23 19:56:11.236: INFO: (18) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 28.353542ms)
Mar 23 19:56:11.237: INFO: (18) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 30.142448ms)
Mar 23 19:56:11.238: INFO: (18) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 31.338179ms)
Mar 23 19:56:11.238: INFO: (18) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 31.849867ms)
Mar 23 19:56:11.238: INFO: (18) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 33.156059ms)
Mar 23 19:56:11.239: INFO: (18) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 31.977436ms)
Mar 23 19:56:11.239: INFO: (18) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 32.110613ms)
Mar 23 19:56:11.258: INFO: (19) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname1/proxy/: foo (200; 14.411456ms)
Mar 23 19:56:11.261: INFO: (19) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">test<... (200; 16.267746ms)
Mar 23 19:56:11.261: INFO: (19) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm/proxy/rewriteme">test</a> (200; 16.637727ms)
Mar 23 19:56:11.262: INFO: (19) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname1/proxy/: tls baz (200; 19.75045ms)
Mar 23 19:56:11.263: INFO: (19) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:460/proxy/: tls baz (200; 18.279818ms)
Mar 23 19:56:11.264: INFO: (19) /api/v1/namespaces/proxy-9246/services/proxy-service-c42bb:portname2/proxy/: bar (200; 19.756206ms)
Mar 23 19:56:11.265: INFO: (19) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname1/proxy/: foo (200; 20.271109ms)
Mar 23 19:56:11.266: INFO: (19) /api/v1/namespaces/proxy-9246/services/https:proxy-service-c42bb:tlsportname2/proxy/: tls qux (200; 21.812747ms)
Mar 23 19:56:11.266: INFO: (19) /api/v1/namespaces/proxy-9246/services/http:proxy-service-c42bb:portname2/proxy/: bar (200; 22.321058ms)
Mar 23 19:56:11.268: INFO: (19) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:1080/proxy/rewriteme">... (200; 23.344446ms)
Mar 23 19:56:11.268: INFO: (19) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 23.729115ms)
Mar 23 19:56:11.270: INFO: (19) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:462/proxy/: tls qux (200; 27.00091ms)
Mar 23 19:56:11.270: INFO: (19) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 25.1033ms)
Mar 23 19:56:11.270: INFO: (19) /api/v1/namespaces/proxy-9246/pods/proxy-service-c42bb-qsgvm:162/proxy/: bar (200; 25.458841ms)
Mar 23 19:56:11.270: INFO: (19) /api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/: <a href="/api/v1/namespaces/proxy-9246/pods/https:proxy-service-c42bb-qsgvm:443/proxy/tlsrewritem... (200; 26.835344ms)
Mar 23 19:56:11.269: INFO: (19) /api/v1/namespaces/proxy-9246/pods/http:proxy-service-c42bb-qsgvm:160/proxy/: foo (200; 23.713228ms)
STEP: deleting ReplicationController proxy-service-c42bb in namespace proxy-9246, will wait for the garbage collector to delete the pods
Mar 23 19:56:11.378: INFO: Deleting ReplicationController proxy-service-c42bb took: 52.424804ms
Mar 23 19:56:11.779: INFO: Terminating ReplicationController proxy-service-c42bb pods took: 400.292531ms
[AfterEach] version v1
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:56:18.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9246" for this suite.

• [SLOW TEST:11.317 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":215,"skipped":3410,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:56:18.402: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:56:18.502: INFO: Creating deployment "test-recreate-deployment"
Mar 23 19:56:18.516: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 23 19:56:18.536: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 23 19:56:20.547: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 23 19:56:20.552: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 23 19:56:20.574: INFO: Updating deployment test-recreate-deployment
Mar 23 19:56:20.574: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar 23 19:56:20.764: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-12 /apis/apps/v1/namespaces/deployment-12/deployments/test-recreate-deployment e9eb3af7-1ecc-4dde-93b1-fc691313f5ef 32594 2 2020-03-23 19:56:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001fb9c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-23 19:56:20 +0000 UTC,LastTransitionTime:2020-03-23 19:56:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-03-23 19:56:20 +0000 UTC,LastTransitionTime:2020-03-23 19:56:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 23 19:56:20.775: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-12 /apis/apps/v1/namespaces/deployment-12/replicasets/test-recreate-deployment-5f94c574ff 38dc3a84-f807-43b1-a853-c5cd825ab134 32592 1 2020-03-23 19:56:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment e9eb3af7-1ecc-4dde-93b1-fc691313f5ef 0xc00207e3d7 0xc00207e3d8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00207e458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 19:56:20.775: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 23 19:56:20.776: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-12 /apis/apps/v1/namespaces/deployment-12/replicasets/test-recreate-deployment-799c574856 e3dd3ba8-ed6c-4ec3-ade9-9e3b0c0fd564 32583 2 2020-03-23 19:56:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment e9eb3af7-1ecc-4dde-93b1-fc691313f5ef 0xc00207e567 0xc00207e568}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00207e678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 19:56:20.784: INFO: Pod "test-recreate-deployment-5f94c574ff-7654b" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-7654b test-recreate-deployment-5f94c574ff- deployment-12 /api/v1/namespaces/deployment-12/pods/test-recreate-deployment-5f94c574ff-7654b 23bd66c2-f6be-4454-a9d2-3e870dd68faf 32595 0 2020-03-23 19:56:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 38dc3a84-f807-43b1-a853-c5cd825ab134 0xc00207f197 0xc00207f198}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dbjzw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dbjzw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dbjzw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:56:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:56:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.153,PodIP:,StartTime:2020-03-23 19:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:56:20.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-12" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":216,"skipped":3437,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:56:20.807: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:57:20.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5858" for this suite.

• [SLOW TEST:60.157 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":217,"skipped":3457,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:57:20.968: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1357
STEP: creating an pod
Mar 23 19:57:21.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-8990 -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 23 19:57:21.267: INFO: stderr: ""
Mar 23 19:57:21.267: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Mar 23 19:57:21.267: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 23 19:57:21.267: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8990" to be "running and ready, or succeeded"
Mar 23 19:57:21.282: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.081564ms
Mar 23 19:57:23.287: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.019847246s
Mar 23 19:57:23.287: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 23 19:57:23.287: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 23 19:57:23.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 logs logs-generator logs-generator --namespace=kubectl-8990'
Mar 23 19:57:23.525: INFO: stderr: ""
Mar 23 19:57:23.525: INFO: stdout: "I0323 19:57:22.515541       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/rhb 581\nI0323 19:57:22.715824       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/hdh 591\nI0323 19:57:22.915783       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/srp6 251\nI0323 19:57:23.115821       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/rl8 514\nI0323 19:57:23.315770       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/r942 300\nI0323 19:57:23.515782       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/bnc9 485\n"
Mar 23 19:57:25.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 logs logs-generator logs-generator --namespace=kubectl-8990'
Mar 23 19:57:25.746: INFO: stderr: ""
Mar 23 19:57:25.746: INFO: stdout: "I0323 19:57:22.515541       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/rhb 581\nI0323 19:57:22.715824       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/hdh 591\nI0323 19:57:22.915783       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/srp6 251\nI0323 19:57:23.115821       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/rl8 514\nI0323 19:57:23.315770       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/r942 300\nI0323 19:57:23.515782       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/bnc9 485\nI0323 19:57:23.715835       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/dd8 535\nI0323 19:57:23.915836       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/8hn5 518\nI0323 19:57:24.115840       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/ffb 386\nI0323 19:57:24.315797       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/rtf 234\nI0323 19:57:24.515825       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/xkv 449\nI0323 19:57:24.715891       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/c9nh 454\nI0323 19:57:24.915832       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/gbp 514\nI0323 19:57:25.115899       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/27x 547\nI0323 19:57:25.315843       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/8dl6 208\nI0323 19:57:25.515968       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/tsl6 325\nI0323 19:57:25.715889       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/mtp 599\n"
STEP: limiting log lines
Mar 23 19:57:25.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 logs logs-generator logs-generator --namespace=kubectl-8990 --tail=1'
Mar 23 19:57:25.986: INFO: stderr: ""
Mar 23 19:57:25.986: INFO: stdout: "I0323 19:57:25.915765       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/zwq8 210\n"
Mar 23 19:57:25.986: INFO: got output "I0323 19:57:25.915765       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/zwq8 210\n"
STEP: limiting log bytes
Mar 23 19:57:25.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 logs logs-generator logs-generator --namespace=kubectl-8990 --limit-bytes=1'
Mar 23 19:57:26.220: INFO: stderr: ""
Mar 23 19:57:26.220: INFO: stdout: "I"
Mar 23 19:57:26.220: INFO: got output "I"
STEP: exposing timestamps
Mar 23 19:57:26.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 logs logs-generator logs-generator --namespace=kubectl-8990 --tail=1 --timestamps'
Mar 23 19:57:26.480: INFO: stderr: ""
Mar 23 19:57:26.480: INFO: stdout: "2020-03-23T19:57:26.316150552Z I0323 19:57:26.315870       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/pp5b 399\n"
Mar 23 19:57:26.480: INFO: got output "2020-03-23T19:57:26.316150552Z I0323 19:57:26.315870       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/pp5b 399\n"
STEP: restricting to a time range
Mar 23 19:57:28.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 logs logs-generator logs-generator --namespace=kubectl-8990 --since=1s'
Mar 23 19:57:29.214: INFO: stderr: ""
Mar 23 19:57:29.214: INFO: stdout: "I0323 19:57:28.315911       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/sjd 419\nI0323 19:57:28.515867       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/8q6 554\nI0323 19:57:28.715877       1 logs_generator.go:76] 31 POST /api/v1/namespaces/kube-system/pods/vrl 390\nI0323 19:57:28.915882       1 logs_generator.go:76] 32 POST /api/v1/namespaces/default/pods/rkkw 277\nI0323 19:57:29.115765       1 logs_generator.go:76] 33 GET /api/v1/namespaces/default/pods/d9p 518\n"
Mar 23 19:57:29.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 logs logs-generator logs-generator --namespace=kubectl-8990 --since=24h'
Mar 23 19:57:29.449: INFO: stderr: ""
Mar 23 19:57:29.449: INFO: stdout: "I0323 19:57:22.515541       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/rhb 581\nI0323 19:57:22.715824       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/hdh 591\nI0323 19:57:22.915783       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/srp6 251\nI0323 19:57:23.115821       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/rl8 514\nI0323 19:57:23.315770       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/r942 300\nI0323 19:57:23.515782       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/bnc9 485\nI0323 19:57:23.715835       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/dd8 535\nI0323 19:57:23.915836       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/8hn5 518\nI0323 19:57:24.115840       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/ffb 386\nI0323 19:57:24.315797       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/rtf 234\nI0323 19:57:24.515825       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/xkv 449\nI0323 19:57:24.715891       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/c9nh 454\nI0323 19:57:24.915832       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/gbp 514\nI0323 19:57:25.115899       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/27x 547\nI0323 19:57:25.315843       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/8dl6 208\nI0323 19:57:25.515968       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/tsl6 325\nI0323 19:57:25.715889       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/mtp 599\nI0323 19:57:25.915765       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/zwq8 210\nI0323 19:57:26.115812       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/n4vv 467\nI0323 19:57:26.315870       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/pp5b 399\nI0323 19:57:26.515790       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/98sw 208\nI0323 19:57:26.715813       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/xss 599\nI0323 19:57:26.915778       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/dtpd 521\nI0323 19:57:27.115862       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/f2j 547\nI0323 19:57:27.315923       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/qss 506\nI0323 19:57:27.515815       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/shtq 240\nI0323 19:57:27.715920       1 logs_generator.go:76] 26 POST /api/v1/namespaces/ns/pods/2mm 349\nI0323 19:57:27.915866       1 logs_generator.go:76] 27 GET /api/v1/namespaces/ns/pods/g9r2 578\nI0323 19:57:28.115900       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/pct 538\nI0323 19:57:28.315911       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/sjd 419\nI0323 19:57:28.515867       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/8q6 554\nI0323 19:57:28.715877       1 logs_generator.go:76] 31 POST /api/v1/namespaces/kube-system/pods/vrl 390\nI0323 19:57:28.915882       1 logs_generator.go:76] 32 POST /api/v1/namespaces/default/pods/rkkw 277\nI0323 19:57:29.115765       1 logs_generator.go:76] 33 GET /api/v1/namespaces/default/pods/d9p 518\nI0323 19:57:29.315892       1 logs_generator.go:76] 34 PUT /api/v1/namespaces/kube-system/pods/rwrg 338\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1363
Mar 23 19:57:29.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete pod logs-generator --namespace=kubectl-8990'
Mar 23 19:57:38.356: INFO: stderr: ""
Mar 23 19:57:38.356: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:57:38.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8990" for this suite.

• [SLOW TEST:17.412 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1353
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":218,"skipped":3459,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:57:38.380: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-6549
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 23 19:57:38.456: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 23 19:58:04.720: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.44.0.1:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6549 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:58:04.720: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:58:04.976: INFO: Found all expected endpoints: [netserver-0]
Mar 23 19:58:04.982: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.0.5:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6549 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:58:04.982: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:58:05.251: INFO: Found all expected endpoints: [netserver-1]
Mar 23 19:58:05.258: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.36.0.5:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6549 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 19:58:05.258: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 19:58:05.679: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:58:05.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6549" for this suite.

• [SLOW TEST:27.327 seconds]
[sig-network] Networking
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":219,"skipped":3475,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:58:05.709: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Mar 23 19:58:09.902: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-244672999 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar 23 19:58:20.089: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:58:20.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-403" for this suite.

• [SLOW TEST:14.407 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":220,"skipped":3483,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:58:20.118: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 23 19:58:20.250: INFO: Waiting up to 5m0s for pod "pod-eec36a08-b448-4490-907a-fc59d495a860" in namespace "emptydir-2267" to be "success or failure"
Mar 23 19:58:20.256: INFO: Pod "pod-eec36a08-b448-4490-907a-fc59d495a860": Phase="Pending", Reason="", readiness=false. Elapsed: 5.818788ms
Mar 23 19:58:22.262: INFO: Pod "pod-eec36a08-b448-4490-907a-fc59d495a860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01215816s
Mar 23 19:58:24.269: INFO: Pod "pod-eec36a08-b448-4490-907a-fc59d495a860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018973952s
STEP: Saw pod success
Mar 23 19:58:24.269: INFO: Pod "pod-eec36a08-b448-4490-907a-fc59d495a860" satisfied condition "success or failure"
Mar 23 19:58:24.274: INFO: Trying to get logs from node kube17-worker-1 pod pod-eec36a08-b448-4490-907a-fc59d495a860 container test-container: <nil>
STEP: delete the pod
Mar 23 19:58:24.323: INFO: Waiting for pod pod-eec36a08-b448-4490-907a-fc59d495a860 to disappear
Mar 23 19:58:24.331: INFO: Pod pod-eec36a08-b448-4490-907a-fc59d495a860 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:58:24.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2267" for this suite.

• [SLOW TEST:5.527 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":221,"skipped":3490,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:58:25.648: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:58:25.759: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 23 19:58:27.056: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:58:27.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6008" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":222,"skipped":3503,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:58:27.255: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:58:27.333: INFO: Creating deployment "webserver-deployment"
Mar 23 19:58:27.341: INFO: Waiting for observed generation 1
Mar 23 19:58:29.357: INFO: Waiting for all required pods to come up
Mar 23 19:58:29.370: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 23 19:58:31.412: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 23 19:58:31.430: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 23 19:58:31.449: INFO: Updating deployment webserver-deployment
Mar 23 19:58:31.449: INFO: Waiting for observed generation 2
Mar 23 19:58:33.470: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 23 19:58:33.476: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 23 19:58:33.480: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 23 19:58:33.496: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 23 19:58:33.496: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 23 19:58:33.500: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 23 19:58:33.509: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 23 19:58:33.509: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 23 19:58:33.520: INFO: Updating deployment webserver-deployment
Mar 23 19:58:33.520: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 23 19:58:33.535: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 23 19:58:33.560: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar 23 19:58:33.665: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4881 /apis/apps/v1/namespaces/deployment-4881/deployments/webserver-deployment 4e5ecefa-a895-40b4-be49-9fa9e7751bdb 33603 3 2020-03-23 19:58:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003861ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-03-23 19:58:31 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-23 19:58:33 +0000 UTC,LastTransitionTime:2020-03-23 19:58:33 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 23 19:58:33.721: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-4881 /apis/apps/v1/namespaces/deployment-4881/replicasets/webserver-deployment-c7997dcc8 a1be4b70-11c6-4afc-943a-74f3c4e69a62 33594 3 2020-03-23 19:58:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4e5ecefa-a895-40b4-be49-9fa9e7751bdb 0xc003835867 0xc003835868}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0038358d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 19:58:33.721: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 23 19:58:33.721: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-4881 /apis/apps/v1/namespaces/deployment-4881/replicasets/webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 33591 3 2020-03-23 19:58:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4e5ecefa-a895-40b4-be49-9fa9e7751bdb 0xc0038357a7 0xc0038357a8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003835808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 23 19:58:33.751: INFO: Pod "webserver-deployment-595b5b9587-5zxm5" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5zxm5 webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-5zxm5 7f14a3e5-54bb-487f-b0cb-70cb144a7679 33607 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc003861ea7 0xc003861ea8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.751: INFO: Pod "webserver-deployment-595b5b9587-6cmk8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6cmk8 webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-6cmk8 e5a0f7a0-7c68-410c-9686-ebede02bb26f 33458 0 2020-03-23 19:58:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc003861fc0 0xc003861fc1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.139,PodIP:10.36.0.6,StartTime:2020-03-23 19:58:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 19:58:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://81edfad382d6f60dfee9e81b95d3d2d68fff39298e5ffdd85f189c81bb789d33,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.0.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.752: INFO: Pod "webserver-deployment-595b5b9587-7kfbl" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7kfbl webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-7kfbl 91bbe4af-927d-4d43-875e-7bcbe60b46ae 33624 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a2140 0xc0038a2141}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.753: INFO: Pod "webserver-deployment-595b5b9587-bgz5t" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bgz5t webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-bgz5t db610eb5-ab7d-4df4-9cd1-109daf74f42f 33445 0 2020-03-23 19:58:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a2260 0xc0038a2261}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.93,PodIP:10.42.0.5,StartTime:2020-03-23 19:58:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 19:58:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8aa090002dc0141a6f39c41202bafd8da918e2bf46f05da0847fa29a34ae33a8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.754: INFO: Pod "webserver-deployment-595b5b9587-c27kg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-c27kg webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-c27kg c65be4ce-5ab5-48ce-8d38-b953c7859e2d 33606 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a2430 0xc0038a2431}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.754: INFO: Pod "webserver-deployment-595b5b9587-c55pt" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-c55pt webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-c55pt 9583a8d7-2246-48d8-9cb7-40f85433920f 33461 0 2020-03-23 19:58:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a2540 0xc0038a2541}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.139,PodIP:10.36.0.7,StartTime:2020-03-23 19:58:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 19:58:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://508590631ccaeed06054d6408f8b4da92b5de4a3b90ca05b405941df69761a6c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.0.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.755: INFO: Pod "webserver-deployment-595b5b9587-d57dp" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-d57dp webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-d57dp 96423b5d-b903-4aad-8c10-9cc55c079ae4 33618 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a26c0 0xc0038a26c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.755: INFO: Pod "webserver-deployment-595b5b9587-g6rvc" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-g6rvc webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-g6rvc a8c762ed-c8ec-40e9-89b8-911dd43b32b1 33625 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a27a7 0xc0038a27a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.756: INFO: Pod "webserver-deployment-595b5b9587-jqxkx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jqxkx webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-jqxkx d0a8fd90-3672-4abf-96bc-8a50cff47e83 33614 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a2897 0xc0038a2898}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.756: INFO: Pod "webserver-deployment-595b5b9587-mn5mx" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mn5mx webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-mn5mx 686ebde5-bd2f-4fe8-bb3d-8209e00c470d 33469 0 2020-03-23 19:58:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a29b0 0xc0038a29b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.153,PodIP:10.44.0.2,StartTime:2020-03-23 19:58:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 19:58:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://01cfde43592909288770e54ffe52f6285e9ada0a5a07c9e442d96fd498be3559,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.44.0.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.757: INFO: Pod "webserver-deployment-595b5b9587-n46th" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-n46th webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-n46th 05e56fb3-8f90-4f77-97d6-c6630f7823e3 33620 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a2b20 0xc0038a2b21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.758: INFO: Pod "webserver-deployment-595b5b9587-nzvgz" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nzvgz webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-nzvgz 94f81908-35fa-423c-9720-c26504485174 33455 0 2020-03-23 19:58:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a2c30 0xc0038a2c31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.93,PodIP:10.42.0.7,StartTime:2020-03-23 19:58:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 19:58:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://47d5c3f3d21ddda5b89b5619b24023fa4d267b5340eea5bfc3a1c9c3a17cc4d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.758: INFO: Pod "webserver-deployment-595b5b9587-rl6g6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rl6g6 webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-rl6g6 8652edb6-53f0-4c8e-bbc8-e1683e35e4d0 33622 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a2dc0 0xc0038a2dc1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.153,PodIP:,StartTime:2020-03-23 19:58:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.759: INFO: Pod "webserver-deployment-595b5b9587-rmdt9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rmdt9 webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-rmdt9 46a64a34-e0b5-41a6-823f-bd27eb11cd05 33617 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a2f17 0xc0038a2f18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.759: INFO: Pod "webserver-deployment-595b5b9587-tggnm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tggnm webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-tggnm d357c905-f186-4bcf-ad6d-c2cb2251f5c2 33613 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a3017 0xc0038a3018}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.760: INFO: Pod "webserver-deployment-595b5b9587-tz4gj" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tz4gj webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-tz4gj 0e48292d-fdfe-40fc-8774-8f05409500e3 33476 0 2020-03-23 19:58:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a3107 0xc0038a3108}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.153,PodIP:10.44.0.3,StartTime:2020-03-23 19:58:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 19:58:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c2bc330af66a51385eae8a913cb59f483000f5b4a4e61287af5cc87955d5957f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.44.0.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.760: INFO: Pod "webserver-deployment-595b5b9587-v4cxp" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-v4cxp webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-v4cxp 5b001852-1813-4d89-9371-b06daeea38ca 33623 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a3290 0xc0038a3291}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.760: INFO: Pod "webserver-deployment-595b5b9587-xbhxv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xbhxv webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-xbhxv 95b51e94-c35b-4cb4-955d-b5bd9dfe833e 33450 0 2020-03-23 19:58:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a33a0 0xc0038a33a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.93,PodIP:10.42.0.6,StartTime:2020-03-23 19:58:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 19:58:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ad93c7624f3675a751560281544599b1b556eeb5aa0c442f39d7811b97101ba5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.761: INFO: Pod "webserver-deployment-595b5b9587-xxxct" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xxxct webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-xxxct b98988c8-d349-4517-b27d-839e5dae72e0 33464 0 2020-03-23 19:58:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a3520 0xc0038a3521}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.139,PodIP:10.36.0.8,StartTime:2020-03-23 19:58:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 19:58:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ae1dcbf6ecb2a80a1f26c706b85a8b6e1d1c38ba56bb16e8d5e2ccf8c21a3e5e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.0.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.761: INFO: Pod "webserver-deployment-595b5b9587-zhxsj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zhxsj webserver-deployment-595b5b9587- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-595b5b9587-zhxsj e30af98c-2dd9-4be6-b730-5498ca236380 33616 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f6981-f5fb-4a9f-9d0c-611cd58610af 0xc0038a36a0 0xc0038a36a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.762: INFO: Pod "webserver-deployment-c7997dcc8-c46b7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-c46b7 webserver-deployment-c7997dcc8- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-c7997dcc8-c46b7 0649bce0-ab2c-46e7-980f-5022f45b4c40 33516 0 2020-03-23 19:58:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a1be4b70-11c6-4afc-943a-74f3c4e69a62 0xc0038a37b7 0xc0038a37b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.139,PodIP:,StartTime:2020-03-23 19:58:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.763: INFO: Pod "webserver-deployment-c7997dcc8-gtrb2" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gtrb2 webserver-deployment-c7997dcc8- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-c7997dcc8-gtrb2 b2d3939c-01cd-4915-b906-d1bd5766ba22 33525 0 2020-03-23 19:58:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a1be4b70-11c6-4afc-943a-74f3c4e69a62 0xc0038a3947 0xc0038a3948}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.93,PodIP:,StartTime:2020-03-23 19:58:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.763: INFO: Pod "webserver-deployment-c7997dcc8-h24g6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-h24g6 webserver-deployment-c7997dcc8- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-c7997dcc8-h24g6 201fefa4-4b11-4f44-994e-068b781f0ca6 33518 0 2020-03-23 19:58:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a1be4b70-11c6-4afc-943a-74f3c4e69a62 0xc0038a3ad0 0xc0038a3ad1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.153,PodIP:,StartTime:2020-03-23 19:58:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.764: INFO: Pod "webserver-deployment-c7997dcc8-hh7zd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hh7zd webserver-deployment-c7997dcc8- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-c7997dcc8-hh7zd a4f870e2-e661-4e64-be19-018406cf843d 33609 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a1be4b70-11c6-4afc-943a-74f3c4e69a62 0xc0038a3c47 0xc0038a3c48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.764: INFO: Pod "webserver-deployment-c7997dcc8-nnt5n" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nnt5n webserver-deployment-c7997dcc8- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-c7997dcc8-nnt5n 9101901a-3019-4433-ba9b-5a76417877b4 33619 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a1be4b70-11c6-4afc-943a-74f3c4e69a62 0xc0038a3db0 0xc0038a3db1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.765: INFO: Pod "webserver-deployment-c7997dcc8-nssxv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nssxv webserver-deployment-c7997dcc8- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-c7997dcc8-nssxv 265e97dd-838f-47d8-9c5d-7e3652639012 33621 0 2020-03-23 19:58:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a1be4b70-11c6-4afc-943a-74f3c4e69a62 0xc0038a3ee7 0xc0038a3ee8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.765: INFO: Pod "webserver-deployment-c7997dcc8-rgwcv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-rgwcv webserver-deployment-c7997dcc8- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-c7997dcc8-rgwcv b134f7be-be07-462c-bf51-ad89d7829739 33546 0 2020-03-23 19:58:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a1be4b70-11c6-4afc-943a-74f3c4e69a62 0xc0038a3ff7 0xc0038a3ff8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.139,PodIP:,StartTime:2020-03-23 19:58:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:58:33.765: INFO: Pod "webserver-deployment-c7997dcc8-s6kc7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-s6kc7 webserver-deployment-c7997dcc8- deployment-4881 /api/v1/namespaces/deployment-4881/pods/webserver-deployment-c7997dcc8-s6kc7 e5d6c8e5-1b67-43db-8069-4cd4ad2f8daf 33542 0 2020-03-23 19:58:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a1be4b70-11c6-4afc-943a-74f3c4e69a62 0xc004592207 0xc004592208}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflht,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 19:58:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.153,PodIP:,StartTime:2020-03-23 19:58:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:58:33.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4881" for this suite.

• [SLOW TEST:6.714 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":223,"skipped":3525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:58:33.970: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:58:34.090: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:58:35.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2789" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":224,"skipped":3551,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:58:35.191: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 23 19:58:47.569: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:58:47.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5960" for this suite.

• [SLOW TEST:12.396 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":225,"skipped":3593,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:58:47.588: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Mar 23 19:58:47.652: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Mar 23 19:58:47.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-1573'
Mar 23 19:58:48.083: INFO: stderr: ""
Mar 23 19:58:48.083: INFO: stdout: "service/agnhost-slave created\n"
Mar 23 19:58:48.083: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Mar 23 19:58:48.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-1573'
Mar 23 19:58:48.560: INFO: stderr: ""
Mar 23 19:58:48.560: INFO: stdout: "service/agnhost-master created\n"
Mar 23 19:58:48.560: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 23 19:58:48.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-1573'
Mar 23 19:58:48.987: INFO: stderr: ""
Mar 23 19:58:48.987: INFO: stdout: "service/frontend created\n"
Mar 23 19:58:48.988: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 23 19:58:48.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-1573'
Mar 23 19:58:49.402: INFO: stderr: ""
Mar 23 19:58:49.402: INFO: stdout: "deployment.apps/frontend created\n"
Mar 23 19:58:49.403: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 23 19:58:49.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-1573'
Mar 23 19:58:49.775: INFO: stderr: ""
Mar 23 19:58:49.775: INFO: stdout: "deployment.apps/agnhost-master created\n"
Mar 23 19:58:49.775: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 23 19:58:49.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-1573'
Mar 23 19:58:50.257: INFO: stderr: ""
Mar 23 19:58:50.257: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Mar 23 19:58:50.257: INFO: Waiting for all frontend pods to be Running.
Mar 23 19:58:55.308: INFO: Waiting for frontend to serve content.
Mar 23 19:58:55.349: INFO: Trying to add a new entry to the guestbook.
Mar 23 19:58:55.368: INFO: Verifying that added entry can be retrieved.
Mar 23 19:58:55.387: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Mar 23 19:59:00.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete --grace-period=0 --force -f - --namespace=kubectl-1573'
Mar 23 19:59:00.651: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 19:59:00.651: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar 23 19:59:00.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete --grace-period=0 --force -f - --namespace=kubectl-1573'
Mar 23 19:59:00.866: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 19:59:00.866: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 23 19:59:00.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete --grace-period=0 --force -f - --namespace=kubectl-1573'
Mar 23 19:59:01.127: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 19:59:01.127: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 23 19:59:01.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete --grace-period=0 --force -f - --namespace=kubectl-1573'
Mar 23 19:59:01.354: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 19:59:01.354: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 23 19:59:01.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete --grace-period=0 --force -f - --namespace=kubectl-1573'
Mar 23 19:59:01.585: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 19:59:01.585: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 23 19:59:01.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete --grace-period=0 --force -f - --namespace=kubectl-1573'
Mar 23 19:59:01.726: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 19:59:01.726: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:59:01.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1573" for this suite.

• [SLOW TEST:14.161 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:380
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":226,"skipped":3608,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:59:01.749: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:59:08.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9111" for this suite.
STEP: Destroying namespace "nsdeletetest-8601" for this suite.
Mar 23 19:59:08.057: INFO: Namespace nsdeletetest-8601 was already deleted
STEP: Destroying namespace "nsdeletetest-6730" for this suite.

• [SLOW TEST:6.327 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":227,"skipped":3613,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:59:08.080: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 19:59:09.232: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590349, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590349, loc:(*time.Location)(0x791d1c0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5f65f8c764\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590349, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590349, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:59:11.699: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590349, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590349, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590349, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590349, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:59:13.241: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590349, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590349, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590349, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590349, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 19:59:16.272: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 19:59:16.277: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:59:17.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7400" for this suite.
STEP: Destroying namespace "webhook-7400-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.579 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":228,"skipped":3638,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:59:17.659: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-eb3ca2de-703e-4dda-b022-cd587e63ccce
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:59:21.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-879" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":229,"skipped":3661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:59:21.841: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Mar 23 19:59:23.108: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:59:23.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5837" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":230,"skipped":3707,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:59:23.131: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar 23 19:59:23.239: INFO: Waiting up to 5m0s for pod "downward-api-f6eb7e77-26de-4cde-bfbf-d6242c67ce97" in namespace "downward-api-2552" to be "success or failure"
Mar 23 19:59:23.248: INFO: Pod "downward-api-f6eb7e77-26de-4cde-bfbf-d6242c67ce97": Phase="Pending", Reason="", readiness=false. Elapsed: 8.740811ms
Mar 23 19:59:25.255: INFO: Pod "downward-api-f6eb7e77-26de-4cde-bfbf-d6242c67ce97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015622921s
STEP: Saw pod success
Mar 23 19:59:25.255: INFO: Pod "downward-api-f6eb7e77-26de-4cde-bfbf-d6242c67ce97" satisfied condition "success or failure"
Mar 23 19:59:25.261: INFO: Trying to get logs from node kube17-worker-1 pod downward-api-f6eb7e77-26de-4cde-bfbf-d6242c67ce97 container dapi-container: <nil>
STEP: delete the pod
Mar 23 19:59:25.304: INFO: Waiting for pod downward-api-f6eb7e77-26de-4cde-bfbf-d6242c67ce97 to disappear
Mar 23 19:59:25.309: INFO: Pod downward-api-f6eb7e77-26de-4cde-bfbf-d6242c67ce97 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:59:25.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2552" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":231,"skipped":3709,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:59:25.328: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9826
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9826
STEP: creating replication controller externalsvc in namespace services-9826
I0323 19:59:25.504642      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-9826, replica count: 2
I0323 19:59:28.555907      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 23 19:59:28.612: INFO: Creating new exec pod
Mar 23 19:59:32.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=services-9826 execpod84nn4 -- /bin/sh -x -c nslookup clusterip-service'
Mar 23 19:59:33.131: INFO: stderr: "+ nslookup clusterip-service\n"
Mar 23 19:59:33.131: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-9826.svc.cluster.local\tcanonical name = externalsvc.services-9826.svc.cluster.local.\nName:\texternalsvc.services-9826.svc.cluster.local\nAddress: 10.97.81.24\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9826, will wait for the garbage collector to delete the pods
Mar 23 19:59:33.202: INFO: Deleting ReplicationController externalsvc took: 14.443825ms
Mar 23 19:59:33.304: INFO: Terminating ReplicationController externalsvc pods took: 102.835338ms
Mar 23 19:59:48.356: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 19:59:48.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9826" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:23.077 seconds]
[sig-network] Services
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":232,"skipped":3735,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 19:59:48.405: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 23 19:59:48.819: INFO: Pod name wrapped-volume-race-79564957-7bc2-4494-b21f-c64bfccdbd49: Found 0 pods out of 5
Mar 23 19:59:53.830: INFO: Pod name wrapped-volume-race-79564957-7bc2-4494-b21f-c64bfccdbd49: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-79564957-7bc2-4494-b21f-c64bfccdbd49 in namespace emptydir-wrapper-5448, will wait for the garbage collector to delete the pods
Mar 23 20:00:05.956: INFO: Deleting ReplicationController wrapped-volume-race-79564957-7bc2-4494-b21f-c64bfccdbd49 took: 14.399601ms
Mar 23 20:00:06.457: INFO: Terminating ReplicationController wrapped-volume-race-79564957-7bc2-4494-b21f-c64bfccdbd49 pods took: 500.48921ms
STEP: Creating RC which spawns configmap-volume pods
Mar 23 20:00:20.990: INFO: Pod name wrapped-volume-race-29bf74bc-bbd1-4ccd-824f-6e30fe8e385e: Found 0 pods out of 5
Mar 23 20:00:26.005: INFO: Pod name wrapped-volume-race-29bf74bc-bbd1-4ccd-824f-6e30fe8e385e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-29bf74bc-bbd1-4ccd-824f-6e30fe8e385e in namespace emptydir-wrapper-5448, will wait for the garbage collector to delete the pods
Mar 23 20:00:38.116: INFO: Deleting ReplicationController wrapped-volume-race-29bf74bc-bbd1-4ccd-824f-6e30fe8e385e took: 17.431653ms
Mar 23 20:00:38.616: INFO: Terminating ReplicationController wrapped-volume-race-29bf74bc-bbd1-4ccd-824f-6e30fe8e385e pods took: 500.536522ms
STEP: Creating RC which spawns configmap-volume pods
Mar 23 20:00:45.354: INFO: Pod name wrapped-volume-race-dddf347a-7792-4df0-9851-1f4fd9a0bba7: Found 1 pods out of 5
Mar 23 20:00:50.365: INFO: Pod name wrapped-volume-race-dddf347a-7792-4df0-9851-1f4fd9a0bba7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-dddf347a-7792-4df0-9851-1f4fd9a0bba7 in namespace emptydir-wrapper-5448, will wait for the garbage collector to delete the pods
Mar 23 20:01:04.490: INFO: Deleting ReplicationController wrapped-volume-race-dddf347a-7792-4df0-9851-1f4fd9a0bba7 took: 13.119441ms
Mar 23 20:01:05.090: INFO: Terminating ReplicationController wrapped-volume-race-dddf347a-7792-4df0-9851-1f4fd9a0bba7 pods took: 600.346537ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:01:19.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5448" for this suite.

• [SLOW TEST:90.644 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":233,"skipped":3738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:01:19.051: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1275
STEP: creating the pod
Mar 23 20:01:19.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 create -f - --namespace=kubectl-3516'
Mar 23 20:01:19.707: INFO: stderr: ""
Mar 23 20:01:19.707: INFO: stdout: "pod/pause created\n"
Mar 23 20:01:19.707: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 23 20:01:19.707: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3516" to be "running and ready"
Mar 23 20:01:19.728: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 20.718527ms
Mar 23 20:01:21.735: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027237652s
Mar 23 20:01:23.741: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.033277319s
Mar 23 20:01:23.741: INFO: Pod "pause" satisfied condition "running and ready"
Mar 23 20:01:23.741: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 23 20:01:23.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 label pods pause testing-label=testing-label-value --namespace=kubectl-3516'
Mar 23 20:01:23.875: INFO: stderr: ""
Mar 23 20:01:23.875: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 23 20:01:23.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pod pause -L testing-label --namespace=kubectl-3516'
Mar 23 20:01:24.056: INFO: stderr: ""
Mar 23 20:01:24.057: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 23 20:01:24.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 label pods pause testing-label- --namespace=kubectl-3516'
Mar 23 20:01:24.299: INFO: stderr: ""
Mar 23 20:01:24.299: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 23 20:01:24.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pod pause -L testing-label --namespace=kubectl-3516'
Mar 23 20:01:24.515: INFO: stderr: ""
Mar 23 20:01:24.515: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1282
STEP: using delete to clean up resources
Mar 23 20:01:24.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 delete --grace-period=0 --force -f - --namespace=kubectl-3516'
Mar 23 20:01:24.735: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 20:01:24.735: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 23 20:01:24.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get rc,svc -l name=pause --no-headers --namespace=kubectl-3516'
Mar 23 20:01:24.948: INFO: stderr: "No resources found in kubectl-3516 namespace.\n"
Mar 23 20:01:24.948: INFO: stdout: ""
Mar 23 20:01:24.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 get pods -l name=pause --namespace=kubectl-3516 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 23 20:01:25.150: INFO: stderr: ""
Mar 23 20:01:25.150: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:01:25.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3516" for this suite.

• [SLOW TEST:6.111 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1272
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":234,"skipped":3763,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:01:25.163: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar 23 20:01:31.291: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6420 PodName:pod-sharedvolume-a16358d5-80bb-4d19-a0d9-f62c4cd7c66c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 20:01:31.291: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 20:01:31.542: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:01:31.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6420" for this suite.

• [SLOW TEST:6.396 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":235,"skipped":3781,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:01:31.560: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-caf089fa-09a3-4544-8b5f-b84a99a3b886
STEP: Creating a pod to test consume configMaps
Mar 23 20:01:31.646: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a3a73327-1e16-49cd-9c79-2f3e73ec1a32" in namespace "projected-2229" to be "success or failure"
Mar 23 20:01:31.660: INFO: Pod "pod-projected-configmaps-a3a73327-1e16-49cd-9c79-2f3e73ec1a32": Phase="Pending", Reason="", readiness=false. Elapsed: 13.483545ms
Mar 23 20:01:33.666: INFO: Pod "pod-projected-configmaps-a3a73327-1e16-49cd-9c79-2f3e73ec1a32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019612415s
Mar 23 20:01:35.672: INFO: Pod "pod-projected-configmaps-a3a73327-1e16-49cd-9c79-2f3e73ec1a32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025868108s
STEP: Saw pod success
Mar 23 20:01:35.672: INFO: Pod "pod-projected-configmaps-a3a73327-1e16-49cd-9c79-2f3e73ec1a32" satisfied condition "success or failure"
Mar 23 20:01:35.678: INFO: Trying to get logs from node kube17-worker-1 pod pod-projected-configmaps-a3a73327-1e16-49cd-9c79-2f3e73ec1a32 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 20:01:35.749: INFO: Waiting for pod pod-projected-configmaps-a3a73327-1e16-49cd-9c79-2f3e73ec1a32 to disappear
Mar 23 20:01:35.757: INFO: Pod pod-projected-configmaps-a3a73327-1e16-49cd-9c79-2f3e73ec1a32 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:01:35.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2229" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":236,"skipped":3788,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:01:35.781: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-4e68f45f-e935-4445-99d1-b8103f4224cf
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:01:35.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3275" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":237,"skipped":3790,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:01:35.872: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-8qmf
STEP: Creating a pod to test atomic-volume-subpath
Mar 23 20:01:35.998: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8qmf" in namespace "subpath-8710" to be "success or failure"
Mar 23 20:01:36.016: INFO: Pod "pod-subpath-test-secret-8qmf": Phase="Pending", Reason="", readiness=false. Elapsed: 18.253593ms
Mar 23 20:01:38.022: INFO: Pod "pod-subpath-test-secret-8qmf": Phase="Running", Reason="", readiness=true. Elapsed: 2.024158796s
Mar 23 20:01:40.029: INFO: Pod "pod-subpath-test-secret-8qmf": Phase="Running", Reason="", readiness=true. Elapsed: 4.031090331s
Mar 23 20:01:44.330: INFO: Pod "pod-subpath-test-secret-8qmf": Phase="Running", Reason="", readiness=true. Elapsed: 8.331771544s
Mar 23 20:01:46.336: INFO: Pod "pod-subpath-test-secret-8qmf": Phase="Running", Reason="", readiness=true. Elapsed: 10.337962436s
Mar 23 20:01:48.345: INFO: Pod "pod-subpath-test-secret-8qmf": Phase="Running", Reason="", readiness=true. Elapsed: 12.346682454s
Mar 23 20:01:50.356: INFO: Pod "pod-subpath-test-secret-8qmf": Phase="Running", Reason="", readiness=true. Elapsed: 14.357659722s
Mar 23 20:01:52.362: INFO: Pod "pod-subpath-test-secret-8qmf": Phase="Running", Reason="", readiness=true. Elapsed: 16.363804217s
Mar 23 20:01:54.371: INFO: Pod "pod-subpath-test-secret-8qmf": Phase="Running", Reason="", readiness=true. Elapsed: 18.37251117s
Mar 23 20:01:56.386: INFO: Pod "pod-subpath-test-secret-8qmf": Phase="Running", Reason="", readiness=true. Elapsed: 20.387440467s
Mar 23 20:01:58.392: INFO: Pod "pod-subpath-test-secret-8qmf": Phase="Running", Reason="", readiness=true. Elapsed: 22.393803855s
Mar 23 20:02:00.398: INFO: Pod "pod-subpath-test-secret-8qmf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.400220167s
STEP: Saw pod success
Mar 23 20:02:00.399: INFO: Pod "pod-subpath-test-secret-8qmf" satisfied condition "success or failure"
Mar 23 20:02:00.403: INFO: Trying to get logs from node kube17-worker-1 pod pod-subpath-test-secret-8qmf container test-container-subpath-secret-8qmf: <nil>
STEP: delete the pod
Mar 23 20:02:00.451: INFO: Waiting for pod pod-subpath-test-secret-8qmf to disappear
Mar 23 20:02:00.461: INFO: Pod pod-subpath-test-secret-8qmf no longer exists
STEP: Deleting pod pod-subpath-test-secret-8qmf
Mar 23 20:02:00.461: INFO: Deleting pod "pod-subpath-test-secret-8qmf" in namespace "subpath-8710"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:02:00.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8710" for this suite.

• [SLOW TEST:24.619 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":238,"skipped":3790,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:02:00.494: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-444641a7-c9f1-4002-b503-19a38eedd9a0
STEP: Creating a pod to test consume secrets
Mar 23 20:02:00.615: INFO: Waiting up to 5m0s for pod "pod-secrets-a83a8df5-d73d-48e9-a9e6-b432ffd9592f" in namespace "secrets-850" to be "success or failure"
Mar 23 20:02:00.646: INFO: Pod "pod-secrets-a83a8df5-d73d-48e9-a9e6-b432ffd9592f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.99117ms
Mar 23 20:02:02.653: INFO: Pod "pod-secrets-a83a8df5-d73d-48e9-a9e6-b432ffd9592f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037816747s
Mar 23 20:02:04.660: INFO: Pod "pod-secrets-a83a8df5-d73d-48e9-a9e6-b432ffd9592f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045261707s
STEP: Saw pod success
Mar 23 20:02:04.660: INFO: Pod "pod-secrets-a83a8df5-d73d-48e9-a9e6-b432ffd9592f" satisfied condition "success or failure"
Mar 23 20:02:04.666: INFO: Trying to get logs from node kube17-worker-1 pod pod-secrets-a83a8df5-d73d-48e9-a9e6-b432ffd9592f container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 20:02:04.718: INFO: Waiting for pod pod-secrets-a83a8df5-d73d-48e9-a9e6-b432ffd9592f to disappear
Mar 23 20:02:04.724: INFO: Pod pod-secrets-a83a8df5-d73d-48e9-a9e6-b432ffd9592f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:02:04.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-850" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":239,"skipped":3793,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:02:04.748: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 20:02:04.848: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32fbb8f4-8ab7-4944-a509-e67a6fac012d" in namespace "projected-2586" to be "success or failure"
Mar 23 20:02:04.857: INFO: Pod "downwardapi-volume-32fbb8f4-8ab7-4944-a509-e67a6fac012d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.892867ms
Mar 23 20:02:06.862: INFO: Pod "downwardapi-volume-32fbb8f4-8ab7-4944-a509-e67a6fac012d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013922124s
STEP: Saw pod success
Mar 23 20:02:06.862: INFO: Pod "downwardapi-volume-32fbb8f4-8ab7-4944-a509-e67a6fac012d" satisfied condition "success or failure"
Mar 23 20:02:06.866: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-32fbb8f4-8ab7-4944-a509-e67a6fac012d container client-container: <nil>
STEP: delete the pod
Mar 23 20:02:06.916: INFO: Waiting for pod downwardapi-volume-32fbb8f4-8ab7-4944-a509-e67a6fac012d to disappear
Mar 23 20:02:06.922: INFO: Pod downwardapi-volume-32fbb8f4-8ab7-4944-a509-e67a6fac012d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:02:06.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2586" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":240,"skipped":3798,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:02:06.938: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 23 20:02:07.025: INFO: Waiting up to 5m0s for pod "pod-45848ee4-659b-4623-b272-69b1b15be919" in namespace "emptydir-8722" to be "success or failure"
Mar 23 20:02:07.031: INFO: Pod "pod-45848ee4-659b-4623-b272-69b1b15be919": Phase="Pending", Reason="", readiness=false. Elapsed: 5.827006ms
Mar 23 20:02:09.037: INFO: Pod "pod-45848ee4-659b-4623-b272-69b1b15be919": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01235748s
STEP: Saw pod success
Mar 23 20:02:09.037: INFO: Pod "pod-45848ee4-659b-4623-b272-69b1b15be919" satisfied condition "success or failure"
Mar 23 20:02:09.042: INFO: Trying to get logs from node kube17-worker-1 pod pod-45848ee4-659b-4623-b272-69b1b15be919 container test-container: <nil>
STEP: delete the pod
Mar 23 20:02:09.093: INFO: Waiting for pod pod-45848ee4-659b-4623-b272-69b1b15be919 to disappear
Mar 23 20:02:09.098: INFO: Pod pod-45848ee4-659b-4623-b272-69b1b15be919 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:02:09.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8722" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":241,"skipped":3810,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:02:09.116: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 23 20:02:15.321: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 20:02:15.331: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 23 20:02:17.331: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 20:02:17.338: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 23 20:02:19.331: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 20:02:19.337: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 23 20:02:21.331: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 20:02:21.338: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 23 20:02:23.331: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 20:02:23.338: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 23 20:02:25.331: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 20:02:25.337: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 23 20:02:27.331: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 20:02:27.337: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 23 20:02:29.331: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 20:02:29.338: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:02:29.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2689" for this suite.

• [SLOW TEST:20.238 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":242,"skipped":3811,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:02:29.360: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 20:02:30.147: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0d71260-c36e-4dec-97e9-82d7a083c932" in namespace "projected-1006" to be "success or failure"
Mar 23 20:02:30.161: INFO: Pod "downwardapi-volume-d0d71260-c36e-4dec-97e9-82d7a083c932": Phase="Pending", Reason="", readiness=false. Elapsed: 13.829338ms
Mar 23 20:02:32.167: INFO: Pod "downwardapi-volume-d0d71260-c36e-4dec-97e9-82d7a083c932": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019793251s
STEP: Saw pod success
Mar 23 20:02:32.167: INFO: Pod "downwardapi-volume-d0d71260-c36e-4dec-97e9-82d7a083c932" satisfied condition "success or failure"
Mar 23 20:02:32.171: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-d0d71260-c36e-4dec-97e9-82d7a083c932 container client-container: <nil>
STEP: delete the pod
Mar 23 20:02:32.260: INFO: Waiting for pod downwardapi-volume-d0d71260-c36e-4dec-97e9-82d7a083c932 to disappear
Mar 23 20:02:32.265: INFO: Pod downwardapi-volume-d0d71260-c36e-4dec-97e9-82d7a083c932 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:02:32.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1006" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":243,"skipped":3842,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:02:32.279: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 20:02:33.429: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 20:02:35.447: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590553, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590553, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590553, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590553, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 20:02:38.568: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:02:38.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3180" for this suite.
STEP: Destroying namespace "webhook-3180-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.582 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":244,"skipped":3863,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:02:38.862: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-10130ed0-a2da-47cb-b39f-ec7ae62a0a6f
STEP: Creating a pod to test consume configMaps
Mar 23 20:02:38.933: INFO: Waiting up to 5m0s for pod "pod-configmaps-e4afa7ec-b88c-47ba-85c2-c3543ef67789" in namespace "configmap-2090" to be "success or failure"
Mar 23 20:02:38.942: INFO: Pod "pod-configmaps-e4afa7ec-b88c-47ba-85c2-c3543ef67789": Phase="Pending", Reason="", readiness=false. Elapsed: 9.095485ms
Mar 23 20:02:40.951: INFO: Pod "pod-configmaps-e4afa7ec-b88c-47ba-85c2-c3543ef67789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017466548s
Mar 23 20:02:42.957: INFO: Pod "pod-configmaps-e4afa7ec-b88c-47ba-85c2-c3543ef67789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023521885s
STEP: Saw pod success
Mar 23 20:02:42.957: INFO: Pod "pod-configmaps-e4afa7ec-b88c-47ba-85c2-c3543ef67789" satisfied condition "success or failure"
Mar 23 20:02:42.962: INFO: Trying to get logs from node kube17-worker-1 pod pod-configmaps-e4afa7ec-b88c-47ba-85c2-c3543ef67789 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 20:02:43.009: INFO: Waiting for pod pod-configmaps-e4afa7ec-b88c-47ba-85c2-c3543ef67789 to disappear
Mar 23 20:02:43.018: INFO: Pod pod-configmaps-e4afa7ec-b88c-47ba-85c2-c3543ef67789 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:02:43.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2090" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":245,"skipped":3888,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:02:43.035: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-ece9ae51-b826-4b9c-acea-b5d0749bb151
STEP: Creating configMap with name cm-test-opt-upd-e559dcbd-7249-430e-b0e5-d07dd6177abc
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ece9ae51-b826-4b9c-acea-b5d0749bb151
STEP: Updating configmap cm-test-opt-upd-e559dcbd-7249-430e-b0e5-d07dd6177abc
STEP: Creating configMap with name cm-test-opt-create-54e31792-90ad-4715-af08-a71b88c8cde5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:02:51.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3947" for this suite.

• [SLOW TEST:8.306 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":246,"skipped":3900,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:02:51.344: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar 23 20:02:51.424: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 20:02:51.446: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 20:02:51.451: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-1 before test
Mar 23 20:02:51.464: INFO: weave-net-qbqsz from kube-system started at 2020-03-23 19:09:58 +0000 UTC (2 container statuses recorded)
Mar 23 20:02:51.464: INFO: 	Container weave ready: true, restart count 0
Mar 23 20:02:51.464: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 20:02:51.464: INFO: pod-configmaps-eb40ad02-2cf6-4ae6-87cf-2fdb9e682e4c from configmap-3947 started at 2020-03-23 20:02:44 +0000 UTC (3 container statuses recorded)
Mar 23 20:02:51.464: INFO: 	Container createcm-volume-test ready: true, restart count 0
Mar 23 20:02:51.465: INFO: 	Container delcm-volume-test ready: true, restart count 0
Mar 23 20:02:51.465: INFO: 	Container updcm-volume-test ready: true, restart count 0
Mar 23 20:02:51.465: INFO: openebs-ndm-4pklk from openebs started at 2020-03-23 19:09:58 +0000 UTC (1 container statuses recorded)
Mar 23 20:02:51.465: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 20:02:51.465: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-r5zrl from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 20:02:51.465: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 23 20:02:51.465: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 20:02:51.466: INFO: kube-proxy-55v4z from kube-system started at 2020-03-23 18:30:18 +0000 UTC (1 container statuses recorded)
Mar 23 20:02:51.466: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 20:02:51.466: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-2 before test
Mar 23 20:02:51.505: INFO: weave-net-92dzp from kube-system started at 2020-03-23 19:06:40 +0000 UTC (2 container statuses recorded)
Mar 23 20:02:51.506: INFO: 	Container weave ready: true, restart count 0
Mar 23 20:02:51.506: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 20:02:51.506: INFO: sonobuoy from sonobuoy started at 2020-03-23 18:49:13 +0000 UTC (1 container statuses recorded)
Mar 23 20:02:51.506: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 23 20:02:51.506: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-tx9p5 from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 20:02:51.506: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 23 20:02:51.506: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 20:02:51.506: INFO: kube-proxy-n567x from kube-system started at 2020-03-23 18:31:10 +0000 UTC (1 container statuses recorded)
Mar 23 20:02:51.506: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 20:02:51.506: INFO: sonobuoy-e2e-job-efff66848912497f from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 20:02:51.506: INFO: 	Container e2e ready: true, restart count 0
Mar 23 20:02:51.506: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 20:02:51.506: INFO: openebs-admission-server-f67f77588-nqvzf from openebs started at 2020-03-23 19:09:26 +0000 UTC (1 container statuses recorded)
Mar 23 20:02:51.506: INFO: 	Container admission-webhook ready: true, restart count 0
Mar 23 20:02:51.507: INFO: openebs-snapshot-operator-6c4c64d4bc-stpl5 from openebs started at 2020-03-23 19:09:26 +0000 UTC (2 container statuses recorded)
Mar 23 20:02:51.507: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 23 20:02:51.507: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Mar 23 20:02:51.507: INFO: openebs-ndm-mwljx from openebs started at 2020-03-23 19:06:50 +0000 UTC (1 container statuses recorded)
Mar 23 20:02:51.507: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 20:02:51.507: INFO: 
Logging pods the kubelet thinks is on node kube17-worker-3 before test
Mar 23 20:02:51.546: INFO: maya-apiserver-569c7c785b-ltfsc from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 20:02:51.546: INFO: 	Container maya-apiserver ready: true, restart count 3
Mar 23 20:02:51.546: INFO: openebs-provisioner-7b8c68bf44-6w5b6 from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 20:02:51.546: INFO: 	Container openebs-provisioner ready: true, restart count 1
Mar 23 20:02:51.546: INFO: openebs-localpv-provisioner-5c87bbd974-pwwkj from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 20:02:51.546: INFO: 	Container openebs-provisioner-hostpath ready: true, restart count 1
Mar 23 20:02:51.546: INFO: kube-proxy-2qrv2 from kube-system started at 2020-03-23 18:30:45 +0000 UTC (1 container statuses recorded)
Mar 23 20:02:51.546: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 20:02:51.546: INFO: weave-net-tqwcx from kube-system started at 2020-03-23 18:30:45 +0000 UTC (2 container statuses recorded)
Mar 23 20:02:51.547: INFO: 	Container weave ready: true, restart count 0
Mar 23 20:02:51.547: INFO: 	Container weave-npc ready: true, restart count 0
Mar 23 20:02:51.547: INFO: openebs-ndm-operator-5fccfb7976-vzw5m from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 20:02:51.547: INFO: 	Container node-disk-operator ready: true, restart count 1
Mar 23 20:02:51.547: INFO: openebs-ndm-d9t4c from openebs started at 2020-03-23 18:31:36 +0000 UTC (1 container statuses recorded)
Mar 23 20:02:51.547: INFO: 	Container node-disk-manager ready: true, restart count 0
Mar 23 20:02:51.547: INFO: sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-prwqb from sonobuoy started at 2020-03-23 18:49:21 +0000 UTC (2 container statuses recorded)
Mar 23 20:02:51.547: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 23 20:02:51.547: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node kube17-worker-1
STEP: verifying the node has the label node kube17-worker-2
STEP: verifying the node has the label node kube17-worker-3
Mar 23 20:02:51.639: INFO: Pod pod-configmaps-eb40ad02-2cf6-4ae6-87cf-2fdb9e682e4c requesting resource cpu=0m on Node kube17-worker-1
Mar 23 20:02:51.639: INFO: Pod kube-proxy-2qrv2 requesting resource cpu=0m on Node kube17-worker-3
Mar 23 20:02:51.640: INFO: Pod kube-proxy-55v4z requesting resource cpu=0m on Node kube17-worker-1
Mar 23 20:02:51.640: INFO: Pod kube-proxy-n567x requesting resource cpu=0m on Node kube17-worker-2
Mar 23 20:02:51.640: INFO: Pod weave-net-92dzp requesting resource cpu=20m on Node kube17-worker-2
Mar 23 20:02:51.640: INFO: Pod weave-net-qbqsz requesting resource cpu=20m on Node kube17-worker-1
Mar 23 20:02:51.640: INFO: Pod weave-net-tqwcx requesting resource cpu=20m on Node kube17-worker-3
Mar 23 20:02:51.640: INFO: Pod maya-apiserver-569c7c785b-ltfsc requesting resource cpu=0m on Node kube17-worker-3
Mar 23 20:02:51.640: INFO: Pod openebs-admission-server-f67f77588-nqvzf requesting resource cpu=0m on Node kube17-worker-2
Mar 23 20:02:51.640: INFO: Pod openebs-localpv-provisioner-5c87bbd974-pwwkj requesting resource cpu=0m on Node kube17-worker-3
Mar 23 20:02:51.640: INFO: Pod openebs-ndm-4pklk requesting resource cpu=0m on Node kube17-worker-1
Mar 23 20:02:51.640: INFO: Pod openebs-ndm-d9t4c requesting resource cpu=0m on Node kube17-worker-3
Mar 23 20:02:51.640: INFO: Pod openebs-ndm-mwljx requesting resource cpu=0m on Node kube17-worker-2
Mar 23 20:02:51.640: INFO: Pod openebs-ndm-operator-5fccfb7976-vzw5m requesting resource cpu=0m on Node kube17-worker-3
Mar 23 20:02:51.640: INFO: Pod openebs-provisioner-7b8c68bf44-6w5b6 requesting resource cpu=0m on Node kube17-worker-3
Mar 23 20:02:51.640: INFO: Pod openebs-snapshot-operator-6c4c64d4bc-stpl5 requesting resource cpu=0m on Node kube17-worker-2
Mar 23 20:02:51.640: INFO: Pod sonobuoy requesting resource cpu=0m on Node kube17-worker-2
Mar 23 20:02:51.640: INFO: Pod sonobuoy-e2e-job-efff66848912497f requesting resource cpu=0m on Node kube17-worker-2
Mar 23 20:02:51.640: INFO: Pod sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-prwqb requesting resource cpu=0m on Node kube17-worker-3
Mar 23 20:02:51.640: INFO: Pod sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-r5zrl requesting resource cpu=0m on Node kube17-worker-1
Mar 23 20:02:51.640: INFO: Pod sonobuoy-systemd-logs-daemon-set-460ed1e0d50c4780-tx9p5 requesting resource cpu=0m on Node kube17-worker-2
STEP: Starting Pods to consume most of the cluster CPU.
Mar 23 20:02:51.640: INFO: Creating a pod which consumes cpu=1386m on Node kube17-worker-1
Mar 23 20:02:51.655: INFO: Creating a pod which consumes cpu=1386m on Node kube17-worker-2
Mar 23 20:02:51.684: INFO: Creating a pod which consumes cpu=1386m on Node kube17-worker-3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-040780ae-f673-43d8-a0cd-e965c2582293.15ff07700ea0a8ba], Reason = [Scheduled], Message = [Successfully assigned sched-pred-613/filler-pod-040780ae-f673-43d8-a0cd-e965c2582293 to kube17-worker-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-040780ae-f673-43d8-a0cd-e965c2582293.15ff07704fff9c96], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-040780ae-f673-43d8-a0cd-e965c2582293.15ff0770544ce995], Reason = [Created], Message = [Created container filler-pod-040780ae-f673-43d8-a0cd-e965c2582293]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-040780ae-f673-43d8-a0cd-e965c2582293.15ff0770655f57fb], Reason = [Started], Message = [Started container filler-pod-040780ae-f673-43d8-a0cd-e965c2582293]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-52a85a4c-e03e-4666-9fa4-70bb135a9049.15ff07700d54dde0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-613/filler-pod-52a85a4c-e03e-4666-9fa4-70bb135a9049 to kube17-worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-52a85a4c-e03e-4666-9fa4-70bb135a9049.15ff07707d67d089], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-52a85a4c-e03e-4666-9fa4-70bb135a9049.15ff077080fd804b], Reason = [Created], Message = [Created container filler-pod-52a85a4c-e03e-4666-9fa4-70bb135a9049]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-52a85a4c-e03e-4666-9fa4-70bb135a9049.15ff0770926779c7], Reason = [Started], Message = [Started container filler-pod-52a85a4c-e03e-4666-9fa4-70bb135a9049]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dcb3ac83-084d-4a09-a684-48ee7be5da72.15ff0770100e952a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-613/filler-pod-dcb3ac83-084d-4a09-a684-48ee7be5da72 to kube17-worker-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dcb3ac83-084d-4a09-a684-48ee7be5da72.15ff07704e0eaac2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dcb3ac83-084d-4a09-a684-48ee7be5da72.15ff077051b37d4d], Reason = [Created], Message = [Created container filler-pod-dcb3ac83-084d-4a09-a684-48ee7be5da72]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dcb3ac83-084d-4a09-a684-48ee7be5da72.15ff077062f12e22], Reason = [Started], Message = [Started container filler-pod-dcb3ac83-084d-4a09-a684-48ee7be5da72]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15ff07710496695c], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node kube17-worker-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kube17-worker-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kube17-worker-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:02:57.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-613" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:5.685 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":247,"skipped":3918,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:02:57.029: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Mar 23 20:02:57.112: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:03:26.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2344" for this suite.

• [SLOW TEST:29.862 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":248,"skipped":3920,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:03:26.893: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 20:03:26.979: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55f7e992-4e49-4ff1-9f97-681c1bdd9654" in namespace "projected-7701" to be "success or failure"
Mar 23 20:03:26.989: INFO: Pod "downwardapi-volume-55f7e992-4e49-4ff1-9f97-681c1bdd9654": Phase="Pending", Reason="", readiness=false. Elapsed: 9.643339ms
Mar 23 20:03:28.995: INFO: Pod "downwardapi-volume-55f7e992-4e49-4ff1-9f97-681c1bdd9654": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015561821s
STEP: Saw pod success
Mar 23 20:03:28.995: INFO: Pod "downwardapi-volume-55f7e992-4e49-4ff1-9f97-681c1bdd9654" satisfied condition "success or failure"
Mar 23 20:03:29.000: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-55f7e992-4e49-4ff1-9f97-681c1bdd9654 container client-container: <nil>
STEP: delete the pod
Mar 23 20:03:29.052: INFO: Waiting for pod downwardapi-volume-55f7e992-4e49-4ff1-9f97-681c1bdd9654 to disappear
Mar 23 20:03:29.057: INFO: Pod downwardapi-volume-55f7e992-4e49-4ff1-9f97-681c1bdd9654 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:03:29.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7701" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":249,"skipped":3932,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:03:29.077: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 23 20:03:30.931: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar 23 20:03:32.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590610, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590610, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590611, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590610, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 20:03:36.005: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 20:03:36.011: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:03:37.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7104" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:8.347 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":250,"skipped":3943,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:03:37.443: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-a7278faf-8592-4f83-9b78-e6e543c0aa8c
STEP: Creating a pod to test consume configMaps
Mar 23 20:03:37.536: INFO: Waiting up to 5m0s for pod "pod-configmaps-93df0e90-6f7d-4066-beb4-9da302b7cb0c" in namespace "configmap-1999" to be "success or failure"
Mar 23 20:03:37.550: INFO: Pod "pod-configmaps-93df0e90-6f7d-4066-beb4-9da302b7cb0c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.978411ms
Mar 23 20:03:39.555: INFO: Pod "pod-configmaps-93df0e90-6f7d-4066-beb4-9da302b7cb0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019174637s
STEP: Saw pod success
Mar 23 20:03:39.556: INFO: Pod "pod-configmaps-93df0e90-6f7d-4066-beb4-9da302b7cb0c" satisfied condition "success or failure"
Mar 23 20:03:39.560: INFO: Trying to get logs from node kube17-worker-1 pod pod-configmaps-93df0e90-6f7d-4066-beb4-9da302b7cb0c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 20:03:39.598: INFO: Waiting for pod pod-configmaps-93df0e90-6f7d-4066-beb4-9da302b7cb0c to disappear
Mar 23 20:03:39.604: INFO: Pod pod-configmaps-93df0e90-6f7d-4066-beb4-9da302b7cb0c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:03:39.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1999" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":251,"skipped":3995,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:03:39.624: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-c6f363ea-cdd9-4312-b431-050c61224fdd
STEP: Creating secret with name s-test-opt-upd-4554bbe8-7496-4834-a104-8b096f07f80d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c6f363ea-cdd9-4312-b431-050c61224fdd
STEP: Updating secret s-test-opt-upd-4554bbe8-7496-4834-a104-8b096f07f80d
STEP: Creating secret with name s-test-opt-create-b3ae9f48-2b9c-49f5-a2e6-80dd9c5fdaa6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:03:46.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1807" for this suite.

• [SLOW TEST:6.489 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":252,"skipped":4043,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:03:46.115: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 20:03:47.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590626, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590626, loc:(*time.Location)(0x791d1c0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5f65f8c764\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590627, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590627, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Mar 23 20:03:49.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590627, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590627, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590628, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590626, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 20:03:52.095: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:04:02.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2582" for this suite.
STEP: Destroying namespace "webhook-2582-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.825 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":253,"skipped":4051,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:04:02.940: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Mar 23 20:04:03.014: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-244672999 proxy --unix-socket=/tmp/kubectl-proxy-unix289768750/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:04:03.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8015" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":254,"skipped":4053,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:04:03.178: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 23 20:04:03.284: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 23 20:04:24.300: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 20:04:29.747: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:04:51.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5738" for this suite.

• [SLOW TEST:48.336 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":255,"skipped":4059,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:04:51.519: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 20:04:53.156: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 20:04:55.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590693, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590693, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590693, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590693, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 20:04:58.207: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:04:58.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5439" for this suite.
STEP: Destroying namespace "webhook-5439-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.240 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":256,"skipped":4068,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:04:58.773: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 20:04:58.856: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 23 20:05:03.874: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 23 20:05:03.874: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 23 20:05:05.887: INFO: Creating deployment "test-rollover-deployment"
Mar 23 20:05:05.904: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 23 20:05:07.945: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 23 20:05:07.955: INFO: Ensure that both replica sets have 1 created replica
Mar 23 20:05:07.964: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 23 20:05:07.975: INFO: Updating deployment test-rollover-deployment
Mar 23 20:05:07.975: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 23 20:05:09.986: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 23 20:05:09.995: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 23 20:05:10.010: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 20:05:10.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590708, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 20:05:12.023: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 20:05:12.023: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590710, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 20:05:14.022: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 20:05:14.023: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590710, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 20:05:16.023: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 20:05:16.024: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590710, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 20:05:18.030: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 20:05:18.030: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590710, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 20:05:20.172: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 20:05:20.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590710, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720590705, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 20:05:22.024: INFO: 
Mar 23 20:05:22.024: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar 23 20:05:22.037: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3502 /apis/apps/v1/namespaces/deployment-3502/deployments/test-rollover-deployment d590bac1-78d4-4fd0-a4b3-c007dffa1365 37975 2 2020-03-23 20:05:05 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004007388 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-23 20:05:05 +0000 UTC,LastTransitionTime:2020-03-23 20:05:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-03-23 20:05:20 +0000 UTC,LastTransitionTime:2020-03-23 20:05:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 23 20:05:22.044: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-3502 /apis/apps/v1/namespaces/deployment-3502/replicasets/test-rollover-deployment-574d6dfbff 754d6b15-1b83-4c8a-af3c-b113dbcfedd0 37960 2 2020-03-23 20:05:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment d590bac1-78d4-4fd0-a4b3-c007dffa1365 0xc004007827 0xc004007828}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004007898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 23 20:05:22.044: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 23 20:05:22.044: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3502 /apis/apps/v1/namespaces/deployment-3502/replicasets/test-rollover-controller a8abbbe0-768d-4b36-81e8-f95648a00967 37974 2 2020-03-23 20:04:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment d590bac1-78d4-4fd0-a4b3-c007dffa1365 0xc004007757 0xc004007758}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0040077b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 20:05:22.045: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-3502 /apis/apps/v1/namespaces/deployment-3502/replicasets/test-rollover-deployment-f6c94f66c 5cf36c80-7f6c-4067-8de8-2f9266334491 37901 2 2020-03-23 20:05:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment d590bac1-78d4-4fd0-a4b3-c007dffa1365 0xc004007900 0xc004007901}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004007978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 20:05:22.051: INFO: Pod "test-rollover-deployment-574d6dfbff-m4t2r" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-m4t2r test-rollover-deployment-574d6dfbff- deployment-3502 /api/v1/namespaces/deployment-3502/pods/test-rollover-deployment-574d6dfbff-m4t2r f602a783-3529-4f2f-89f7-b34a6b4d005a 37919 0 2020-03-23 20:05:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff 754d6b15-1b83-4c8a-af3c-b113dbcfedd0 0xc0043e0f07 0xc0043e0f08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mcpd2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mcpd2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mcpd2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 20:05:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 20:05:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 20:05:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 20:05:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.153,PodIP:10.44.0.2,StartTime:2020-03-23 20:05:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 20:05:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://f88d7f64a9a58feb8348152b5d254b63a07e5cd943dd1bd2d70caf4ea24b227c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.44.0.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:05:22.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3502" for this suite.

• [SLOW TEST:23.299 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":257,"skipped":4095,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:05:22.073: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-9d1dd2a8-714b-4da2-8108-041a7dfa1ca2
STEP: Creating a pod to test consume secrets
Mar 23 20:05:22.180: INFO: Waiting up to 5m0s for pod "pod-secrets-a32d6507-de03-41ad-ad46-ded10544b983" in namespace "secrets-8244" to be "success or failure"
Mar 23 20:05:22.186: INFO: Pod "pod-secrets-a32d6507-de03-41ad-ad46-ded10544b983": Phase="Pending", Reason="", readiness=false. Elapsed: 5.628436ms
Mar 23 20:05:24.191: INFO: Pod "pod-secrets-a32d6507-de03-41ad-ad46-ded10544b983": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010954042s
Mar 23 20:05:26.197: INFO: Pod "pod-secrets-a32d6507-de03-41ad-ad46-ded10544b983": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016787698s
STEP: Saw pod success
Mar 23 20:05:26.197: INFO: Pod "pod-secrets-a32d6507-de03-41ad-ad46-ded10544b983" satisfied condition "success or failure"
Mar 23 20:05:26.202: INFO: Trying to get logs from node kube17-worker-1 pod pod-secrets-a32d6507-de03-41ad-ad46-ded10544b983 container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 20:05:26.266: INFO: Waiting for pod pod-secrets-a32d6507-de03-41ad-ad46-ded10544b983 to disappear
Mar 23 20:05:26.270: INFO: Pod pod-secrets-a32d6507-de03-41ad-ad46-ded10544b983 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:05:26.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8244" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":258,"skipped":4102,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:05:26.296: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 23 20:05:28.416: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:05:28.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5228" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":259,"skipped":4114,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:05:28.465: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:05:28.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-4323" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":260,"skipped":4121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:05:28.697: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-1271
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1271
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1271
Mar 23 20:05:28.813: INFO: Found 0 stateful pods, waiting for 1
Mar 23 20:05:38.820: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 23 20:05:38.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 20:05:39.414: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 20:05:39.414: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 20:05:39.414: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 20:05:39.422: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 23 20:05:49.428: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 20:05:49.428: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 20:05:50.901: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999993s
Mar 23 20:05:51.909: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.544890208s
Mar 23 20:05:52.915: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.537274207s
Mar 23 20:05:53.924: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.530904585s
Mar 23 20:05:54.932: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.521972016s
Mar 23 20:05:55.940: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.514530182s
Mar 23 20:05:56.946: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.506258338s
Mar 23 20:05:57.957: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.499742923s
Mar 23 20:05:58.964: INFO: Verifying statefulset ss doesn't scale past 1 for another 488.86247ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1271
Mar 23 20:05:59.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:06:00.469: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 20:06:00.469: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 20:06:00.469: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 20:06:00.476: INFO: Found 1 stateful pods, waiting for 3
Mar 23 20:06:10.483: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 20:06:10.484: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 20:06:10.484: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 23 20:06:10.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 20:06:10.866: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 20:06:10.866: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 20:06:10.866: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 20:06:10.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 20:06:11.351: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 20:06:11.351: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 20:06:11.351: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 20:06:11.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 20:06:11.736: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 20:06:11.736: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 20:06:11.736: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 20:06:11.736: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 20:06:12.204: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 23 20:06:22.513: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 20:06:22.514: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 20:06:22.514: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 20:06:22.679: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999907s
Mar 23 20:06:23.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.881472332s
Mar 23 20:06:24.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.865315993s
Mar 23 20:06:25.745: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.827048596s
Mar 23 20:06:26.753: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.815938811s
Mar 23 20:06:27.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.807593925s
Mar 23 20:06:28.769: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.799669727s
Mar 23 20:06:29.775: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.791596256s
Mar 23 20:06:30.785: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.785075148s
Mar 23 20:06:31.793: INFO: Verifying statefulset ss doesn't scale past 3 for another 775.631747ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1271
Mar 23 20:06:32.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:06:33.259: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 20:06:33.259: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 20:06:33.259: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 20:06:33.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:06:33.758: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 20:06:33.758: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 20:06:33.758: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 20:06:33.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:06:34.124: INFO: rc: 1
Mar 23 20:06:34.124: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server: 

error:
exit status 1
Mar 23 20:06:44.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:06:44.318: INFO: rc: 1
Mar 23 20:06:44.318: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:06:54.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:06:54.548: INFO: rc: 1
Mar 23 20:06:54.548: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:07:04.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:07:04.759: INFO: rc: 1
Mar 23 20:07:04.759: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:07:14.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:07:14.958: INFO: rc: 1
Mar 23 20:07:14.958: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:07:24.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:07:25.480: INFO: rc: 1
Mar 23 20:07:25.480: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:07:35.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:07:35.681: INFO: rc: 1
Mar 23 20:07:35.681: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:07:45.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:07:45.869: INFO: rc: 1
Mar 23 20:07:45.869: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:07:55.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:07:56.059: INFO: rc: 1
Mar 23 20:07:56.059: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:08:06.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:08:08.416: INFO: rc: 1
Mar 23 20:08:08.416: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:08:18.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:08:18.597: INFO: rc: 1
Mar 23 20:08:18.597: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:08:28.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:08:28.792: INFO: rc: 1
Mar 23 20:08:28.792: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:08:38.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:08:40.061: INFO: rc: 1
Mar 23 20:08:40.061: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:08:50.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:08:50.257: INFO: rc: 1
Mar 23 20:08:50.257: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:09:00.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:09:00.448: INFO: rc: 1
Mar 23 20:09:00.449: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:09:10.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:09:10.657: INFO: rc: 1
Mar 23 20:09:10.657: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:09:20.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:09:20.839: INFO: rc: 1
Mar 23 20:09:20.839: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:09:30.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:09:31.033: INFO: rc: 1
Mar 23 20:09:31.033: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:09:41.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:09:41.226: INFO: rc: 1
Mar 23 20:09:41.226: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:09:51.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:09:51.430: INFO: rc: 1
Mar 23 20:09:51.430: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:10:01.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:10:01.624: INFO: rc: 1
Mar 23 20:10:01.624: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:10:11.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:10:11.818: INFO: rc: 1
Mar 23 20:10:11.818: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:10:21.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:10:22.020: INFO: rc: 1
Mar 23 20:10:22.020: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:10:32.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:10:32.617: INFO: rc: 1
Mar 23 20:10:32.617: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:10:42.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:10:42.813: INFO: rc: 1
Mar 23 20:10:42.813: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:10:52.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:10:53.013: INFO: rc: 1
Mar 23 20:10:53.013: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:11:03.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:11:03.206: INFO: rc: 1
Mar 23 20:11:03.207: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:11:13.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:11:13.414: INFO: rc: 1
Mar 23 20:11:13.414: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:11:23.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:11:23.609: INFO: rc: 1
Mar 23 20:11:23.609: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 20:11:33.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 exec --namespace=statefulset-1271 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:11:33.806: INFO: rc: 1
Mar 23 20:11:33.806: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Mar 23 20:11:33.806: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar 23 20:11:33.830: INFO: Deleting all statefulset in ns statefulset-1271
Mar 23 20:11:33.834: INFO: Scaling statefulset ss to 0
Mar 23 20:11:33.848: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 20:11:33.853: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:11:33.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1271" for this suite.

• [SLOW TEST:365.233 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":261,"skipped":4144,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:11:33.930: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar 23 20:11:34.010: INFO: Waiting up to 5m0s for pod "downward-api-cecee13f-d0f2-4d8d-94c5-f7e594f46202" in namespace "downward-api-7635" to be "success or failure"
Mar 23 20:11:34.022: INFO: Pod "downward-api-cecee13f-d0f2-4d8d-94c5-f7e594f46202": Phase="Pending", Reason="", readiness=false. Elapsed: 12.110631ms
Mar 23 20:11:36.029: INFO: Pod "downward-api-cecee13f-d0f2-4d8d-94c5-f7e594f46202": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018849002s
STEP: Saw pod success
Mar 23 20:11:36.029: INFO: Pod "downward-api-cecee13f-d0f2-4d8d-94c5-f7e594f46202" satisfied condition "success or failure"
Mar 23 20:11:36.033: INFO: Trying to get logs from node kube17-worker-1 pod downward-api-cecee13f-d0f2-4d8d-94c5-f7e594f46202 container dapi-container: <nil>
STEP: delete the pod
Mar 23 20:11:36.096: INFO: Waiting for pod downward-api-cecee13f-d0f2-4d8d-94c5-f7e594f46202 to disappear
Mar 23 20:11:36.102: INFO: Pod downward-api-cecee13f-d0f2-4d8d-94c5-f7e594f46202 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:11:36.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7635" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":262,"skipped":4165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:11:36.125: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Mar 23 20:11:38.771: INFO: Successfully updated pod "labelsupdate1fd8664c-0ef6-4ca7-85ad-352e45e88eff"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:11:41.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4444" for this suite.

• [SLOW TEST:5.417 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":263,"skipped":4191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:11:41.550: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 20:11:41.772: INFO: (0) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 19.480305ms)
Mar 23 20:11:41.784: INFO: (1) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.175762ms)
Mar 23 20:11:41.789: INFO: (2) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.284799ms)
Mar 23 20:11:41.797: INFO: (3) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.579699ms)
Mar 23 20:11:41.808: INFO: (4) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.169028ms)
Mar 23 20:11:41.817: INFO: (5) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.795728ms)
Mar 23 20:11:41.826: INFO: (6) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.432418ms)
Mar 23 20:11:41.834: INFO: (7) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.031939ms)
Mar 23 20:11:41.843: INFO: (8) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.753874ms)
Mar 23 20:11:41.859: INFO: (9) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 15.962915ms)
Mar 23 20:11:41.865: INFO: (10) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.127255ms)
Mar 23 20:11:41.871: INFO: (11) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.015535ms)
Mar 23 20:11:41.877: INFO: (12) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.916704ms)
Mar 23 20:11:41.883: INFO: (13) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.411866ms)
Mar 23 20:11:41.899: INFO: (14) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 15.98902ms)
Mar 23 20:11:41.907: INFO: (15) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.530275ms)
Mar 23 20:11:41.915: INFO: (16) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.208776ms)
Mar 23 20:11:41.920: INFO: (17) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.007948ms)
Mar 23 20:11:41.931: INFO: (18) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.505532ms)
Mar 23 20:11:41.938: INFO: (19) /api/v1/nodes/kube17-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.660138ms)
[AfterEach] version v1
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:11:41.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2049" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":264,"skipped":4252,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:11:41.959: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Mar 23 20:11:42.049: INFO: Waiting up to 5m0s for pod "client-containers-3f200fe5-1b15-43db-bcc4-73d651d09b35" in namespace "containers-9919" to be "success or failure"
Mar 23 20:11:42.063: INFO: Pod "client-containers-3f200fe5-1b15-43db-bcc4-73d651d09b35": Phase="Pending", Reason="", readiness=false. Elapsed: 14.037282ms
Mar 23 20:11:44.070: INFO: Pod "client-containers-3f200fe5-1b15-43db-bcc4-73d651d09b35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021156418s
Mar 23 20:11:46.081: INFO: Pod "client-containers-3f200fe5-1b15-43db-bcc4-73d651d09b35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031991827s
STEP: Saw pod success
Mar 23 20:11:46.081: INFO: Pod "client-containers-3f200fe5-1b15-43db-bcc4-73d651d09b35" satisfied condition "success or failure"
Mar 23 20:11:46.089: INFO: Trying to get logs from node kube17-worker-1 pod client-containers-3f200fe5-1b15-43db-bcc4-73d651d09b35 container test-container: <nil>
STEP: delete the pod
Mar 23 20:11:46.154: INFO: Waiting for pod client-containers-3f200fe5-1b15-43db-bcc4-73d651d09b35 to disappear
Mar 23 20:11:46.161: INFO: Pod client-containers-3f200fe5-1b15-43db-bcc4-73d651d09b35 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:11:46.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9919" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":265,"skipped":4281,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:11:46.180: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 20:11:47.370: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 20:11:49.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720591107, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720591107, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720591107, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720591107, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 20:11:52.424: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:11:52.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4473" for this suite.
STEP: Destroying namespace "webhook-4473-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.794 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":266,"skipped":4288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:11:52.989: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 20:11:53.069: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37e4756c-d4d3-4fb6-9de9-6e177e226603" in namespace "projected-9890" to be "success or failure"
Mar 23 20:11:53.090: INFO: Pod "downwardapi-volume-37e4756c-d4d3-4fb6-9de9-6e177e226603": Phase="Pending", Reason="", readiness=false. Elapsed: 21.315335ms
Mar 23 20:11:55.101: INFO: Pod "downwardapi-volume-37e4756c-d4d3-4fb6-9de9-6e177e226603": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032701572s
STEP: Saw pod success
Mar 23 20:11:55.102: INFO: Pod "downwardapi-volume-37e4756c-d4d3-4fb6-9de9-6e177e226603" satisfied condition "success or failure"
Mar 23 20:11:55.106: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-37e4756c-d4d3-4fb6-9de9-6e177e226603 container client-container: <nil>
STEP: delete the pod
Mar 23 20:11:55.165: INFO: Waiting for pod downwardapi-volume-37e4756c-d4d3-4fb6-9de9-6e177e226603 to disappear
Mar 23 20:11:55.168: INFO: Pod downwardapi-volume-37e4756c-d4d3-4fb6-9de9-6e177e226603 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:11:55.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9890" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":267,"skipped":4319,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:11:55.190: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 20:11:55.273: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar 23 20:12:01.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-3285 create -f -'
Mar 23 20:12:01.863: INFO: stderr: ""
Mar 23 20:12:01.863: INFO: stdout: "e2e-test-crd-publish-openapi-8160-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 23 20:12:01.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-3285 delete e2e-test-crd-publish-openapi-8160-crds test-foo'
Mar 23 20:12:02.065: INFO: stderr: ""
Mar 23 20:12:02.065: INFO: stdout: "e2e-test-crd-publish-openapi-8160-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 23 20:12:02.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-3285 apply -f -'
Mar 23 20:12:02.516: INFO: stderr: ""
Mar 23 20:12:02.516: INFO: stdout: "e2e-test-crd-publish-openapi-8160-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 23 20:12:02.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-3285 delete e2e-test-crd-publish-openapi-8160-crds test-foo'
Mar 23 20:12:02.696: INFO: stderr: ""
Mar 23 20:12:02.696: INFO: stdout: "e2e-test-crd-publish-openapi-8160-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 23 20:12:02.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-3285 create -f -'
Mar 23 20:12:03.089: INFO: rc: 1
Mar 23 20:12:03.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-3285 apply -f -'
Mar 23 20:12:03.507: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar 23 20:12:03.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-3285 create -f -'
Mar 23 20:12:03.898: INFO: rc: 1
Mar 23 20:12:03.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 --namespace=crd-publish-openapi-3285 apply -f -'
Mar 23 20:12:04.275: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 23 20:12:04.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 explain e2e-test-crd-publish-openapi-8160-crds'
Mar 23 20:12:04.722: INFO: stderr: ""
Mar 23 20:12:04.722: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8160-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 23 20:12:04.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 explain e2e-test-crd-publish-openapi-8160-crds.metadata'
Mar 23 20:12:05.201: INFO: stderr: ""
Mar 23 20:12:05.201: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8160-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 23 20:12:05.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 explain e2e-test-crd-publish-openapi-8160-crds.spec'
Mar 23 20:12:05.630: INFO: stderr: ""
Mar 23 20:12:05.630: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8160-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 23 20:12:05.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 explain e2e-test-crd-publish-openapi-8160-crds.spec.bars'
Mar 23 20:12:06.061: INFO: stderr: ""
Mar 23 20:12:06.061: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8160-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 23 20:12:06.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-244672999 explain e2e-test-crd-publish-openapi-8160-crds.spec.bars2'
Mar 23 20:12:06.508: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:12:11.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3285" for this suite.

• [SLOW TEST:17.805 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":268,"skipped":4322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:12:12.998: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7837.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7837.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7837.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7837.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 20:12:17.221: INFO: DNS probes using dns-test-6dc1f026-087a-4e2f-bdb9-11bb6f0192ff succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7837.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7837.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7837.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7837.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 20:12:21.317: INFO: File wheezy_udp@dns-test-service-3.dns-7837.svc.cluster.local from pod  dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 23 20:12:21.324: INFO: File jessie_udp@dns-test-service-3.dns-7837.svc.cluster.local from pod  dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 23 20:12:21.324: INFO: Lookups using dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 failed for: [wheezy_udp@dns-test-service-3.dns-7837.svc.cluster.local jessie_udp@dns-test-service-3.dns-7837.svc.cluster.local]

Mar 23 20:12:26.332: INFO: File wheezy_udp@dns-test-service-3.dns-7837.svc.cluster.local from pod  dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 23 20:12:26.340: INFO: File jessie_udp@dns-test-service-3.dns-7837.svc.cluster.local from pod  dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 23 20:12:26.340: INFO: Lookups using dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 failed for: [wheezy_udp@dns-test-service-3.dns-7837.svc.cluster.local jessie_udp@dns-test-service-3.dns-7837.svc.cluster.local]

Mar 23 20:12:31.331: INFO: File wheezy_udp@dns-test-service-3.dns-7837.svc.cluster.local from pod  dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 23 20:12:31.340: INFO: File jessie_udp@dns-test-service-3.dns-7837.svc.cluster.local from pod  dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 23 20:12:31.340: INFO: Lookups using dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 failed for: [wheezy_udp@dns-test-service-3.dns-7837.svc.cluster.local jessie_udp@dns-test-service-3.dns-7837.svc.cluster.local]

Mar 23 20:12:36.332: INFO: File wheezy_udp@dns-test-service-3.dns-7837.svc.cluster.local from pod  dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 23 20:12:36.339: INFO: File jessie_udp@dns-test-service-3.dns-7837.svc.cluster.local from pod  dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 23 20:12:36.339: INFO: Lookups using dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 failed for: [wheezy_udp@dns-test-service-3.dns-7837.svc.cluster.local jessie_udp@dns-test-service-3.dns-7837.svc.cluster.local]

Mar 23 20:12:42.292: INFO: File wheezy_udp@dns-test-service-3.dns-7837.svc.cluster.local from pod  dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 contains '' instead of 'bar.example.com.'
Mar 23 20:12:42.346: INFO: File jessie_udp@dns-test-service-3.dns-7837.svc.cluster.local from pod  dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 23 20:12:42.347: INFO: Lookups using dns-7837/dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 failed for: [wheezy_udp@dns-test-service-3.dns-7837.svc.cluster.local jessie_udp@dns-test-service-3.dns-7837.svc.cluster.local]

Mar 23 20:12:46.341: INFO: DNS probes using dns-test-4d20bf79-4328-4d49-ae9c-48f7ea95ab04 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7837.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7837.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7837.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7837.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 20:12:50.626: INFO: DNS probes using dns-test-a83c337d-1ebe-4d73-baf6-5f6614576de4 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:12:50.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7837" for this suite.

• [SLOW TEST:37.723 seconds]
[sig-network] DNS
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":269,"skipped":4345,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:12:50.721: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5756 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5756;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5756 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5756;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5756.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5756.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5756.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5756.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5756.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5756.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5756.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5756.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5756.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5756.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5756.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5756.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5756.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 106.204.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.204.106_udp@PTR;check="$$(dig +tcp +noall +answer +search 106.204.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.204.106_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5756 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5756;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5756 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5756;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5756.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5756.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5756.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5756.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5756.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5756.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5756.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5756.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5756.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5756.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5756.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5756.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5756.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 106.204.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.204.106_udp@PTR;check="$$(dig +tcp +noall +answer +search 106.204.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.204.106_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 20:12:54.945: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:54.950: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:54.963: INFO: Unable to read wheezy_udp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:54.971: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:54.977: INFO: Unable to read wheezy_udp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:54.996: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:55.005: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:55.013: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:55.054: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:55.060: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:55.066: INFO: Unable to read jessie_udp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:55.073: INFO: Unable to read jessie_tcp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:55.079: INFO: Unable to read jessie_udp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:55.086: INFO: Unable to read jessie_tcp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:55.091: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:55.097: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:12:55.133: INFO: Lookups using dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5756 wheezy_tcp@dns-test-service.dns-5756 wheezy_udp@dns-test-service.dns-5756.svc wheezy_tcp@dns-test-service.dns-5756.svc wheezy_udp@_http._tcp.dns-test-service.dns-5756.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5756.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5756 jessie_tcp@dns-test-service.dns-5756 jessie_udp@dns-test-service.dns-5756.svc jessie_tcp@dns-test-service.dns-5756.svc jessie_udp@_http._tcp.dns-test-service.dns-5756.svc jessie_tcp@_http._tcp.dns-test-service.dns-5756.svc]

Mar 23 20:13:00.145: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.154: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.160: INFO: Unable to read wheezy_udp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.166: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.173: INFO: Unable to read wheezy_udp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.178: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.183: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.190: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.234: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.244: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.252: INFO: Unable to read jessie_udp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.258: INFO: Unable to read jessie_tcp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.264: INFO: Unable to read jessie_udp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.271: INFO: Unable to read jessie_tcp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.279: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.292: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:00.336: INFO: Lookups using dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5756 wheezy_tcp@dns-test-service.dns-5756 wheezy_udp@dns-test-service.dns-5756.svc wheezy_tcp@dns-test-service.dns-5756.svc wheezy_udp@_http._tcp.dns-test-service.dns-5756.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5756.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5756 jessie_tcp@dns-test-service.dns-5756 jessie_udp@dns-test-service.dns-5756.svc jessie_tcp@dns-test-service.dns-5756.svc jessie_udp@_http._tcp.dns-test-service.dns-5756.svc jessie_tcp@_http._tcp.dns-test-service.dns-5756.svc]

Mar 23 20:13:05.145: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.154: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.165: INFO: Unable to read wheezy_udp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.192: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.211: INFO: Unable to read wheezy_udp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.239: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.247: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.273: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.323: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.329: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.336: INFO: Unable to read jessie_udp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.342: INFO: Unable to read jessie_tcp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.348: INFO: Unable to read jessie_udp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.353: INFO: Unable to read jessie_tcp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.359: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.365: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:05.402: INFO: Lookups using dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5756 wheezy_tcp@dns-test-service.dns-5756 wheezy_udp@dns-test-service.dns-5756.svc wheezy_tcp@dns-test-service.dns-5756.svc wheezy_udp@_http._tcp.dns-test-service.dns-5756.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5756.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5756 jessie_tcp@dns-test-service.dns-5756 jessie_udp@dns-test-service.dns-5756.svc jessie_tcp@dns-test-service.dns-5756.svc jessie_udp@_http._tcp.dns-test-service.dns-5756.svc jessie_tcp@_http._tcp.dns-test-service.dns-5756.svc]

Mar 23 20:13:10.144: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.155: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.172: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.180: INFO: Unable to read wheezy_udp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.187: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.194: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.201: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.284: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.292: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.330: INFO: Unable to read jessie_udp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.336: INFO: Unable to read jessie_tcp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.341: INFO: Unable to read jessie_udp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.346: INFO: Unable to read jessie_tcp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.365: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.370: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:10.400: INFO: Lookups using dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5756 wheezy_tcp@dns-test-service.dns-5756 wheezy_udp@dns-test-service.dns-5756.svc wheezy_tcp@dns-test-service.dns-5756.svc wheezy_udp@_http._tcp.dns-test-service.dns-5756.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5756.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5756 jessie_tcp@dns-test-service.dns-5756 jessie_udp@dns-test-service.dns-5756.svc jessie_tcp@dns-test-service.dns-5756.svc jessie_udp@_http._tcp.dns-test-service.dns-5756.svc jessie_tcp@_http._tcp.dns-test-service.dns-5756.svc]

Mar 23 20:13:15.142: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.148: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.155: INFO: Unable to read wheezy_udp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.161: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.167: INFO: Unable to read wheezy_udp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.173: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.179: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.191: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.248: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.258: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.263: INFO: Unable to read jessie_udp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.269: INFO: Unable to read jessie_tcp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.277: INFO: Unable to read jessie_udp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.283: INFO: Unable to read jessie_tcp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.290: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.297: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:15.364: INFO: Lookups using dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5756 wheezy_tcp@dns-test-service.dns-5756 wheezy_udp@dns-test-service.dns-5756.svc wheezy_tcp@dns-test-service.dns-5756.svc wheezy_udp@_http._tcp.dns-test-service.dns-5756.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5756.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5756 jessie_tcp@dns-test-service.dns-5756 jessie_udp@dns-test-service.dns-5756.svc jessie_tcp@dns-test-service.dns-5756.svc jessie_udp@_http._tcp.dns-test-service.dns-5756.svc jessie_tcp@_http._tcp.dns-test-service.dns-5756.svc]

Mar 23 20:13:20.148: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.156: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.180: INFO: Unable to read wheezy_udp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.191: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.199: INFO: Unable to read wheezy_udp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.206: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.212: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.217: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.270: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.275: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.281: INFO: Unable to read jessie_udp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.286: INFO: Unable to read jessie_tcp@dns-test-service.dns-5756 from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.295: INFO: Unable to read jessie_udp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.327: INFO: Unable to read jessie_tcp@dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.334: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.340: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5756.svc from pod dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5: the server could not find the requested resource (get pods dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5)
Mar 23 20:13:20.381: INFO: Lookups using dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5756 wheezy_tcp@dns-test-service.dns-5756 wheezy_udp@dns-test-service.dns-5756.svc wheezy_tcp@dns-test-service.dns-5756.svc wheezy_udp@_http._tcp.dns-test-service.dns-5756.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5756.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5756 jessie_tcp@dns-test-service.dns-5756 jessie_udp@dns-test-service.dns-5756.svc jessie_tcp@dns-test-service.dns-5756.svc jessie_udp@_http._tcp.dns-test-service.dns-5756.svc jessie_tcp@_http._tcp.dns-test-service.dns-5756.svc]

Mar 23 20:13:25.483: INFO: DNS probes using dns-5756/dns-test-93d37512-386b-4a0d-8de1-1e5dbbcc1cb5 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:13:25.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5756" for this suite.

• [SLOW TEST:35.013 seconds]
[sig-network] DNS
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":270,"skipped":4350,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:13:25.735: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar 23 20:13:56.467: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:13:56.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8684" for this suite.

• [SLOW TEST:30.776 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":271,"skipped":4355,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:13:56.517: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 23 20:13:56.704: INFO: Waiting up to 5m0s for pod "pod-3416c9ca-c8f6-42b5-b28a-859ec60b2852" in namespace "emptydir-1401" to be "success or failure"
Mar 23 20:13:56.716: INFO: Pod "pod-3416c9ca-c8f6-42b5-b28a-859ec60b2852": Phase="Pending", Reason="", readiness=false. Elapsed: 11.770266ms
Mar 23 20:13:58.724: INFO: Pod "pod-3416c9ca-c8f6-42b5-b28a-859ec60b2852": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019576452s
Mar 23 20:14:00.731: INFO: Pod "pod-3416c9ca-c8f6-42b5-b28a-859ec60b2852": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026508468s
STEP: Saw pod success
Mar 23 20:14:00.731: INFO: Pod "pod-3416c9ca-c8f6-42b5-b28a-859ec60b2852" satisfied condition "success or failure"
Mar 23 20:14:00.737: INFO: Trying to get logs from node kube17-worker-1 pod pod-3416c9ca-c8f6-42b5-b28a-859ec60b2852 container test-container: <nil>
STEP: delete the pod
Mar 23 20:14:00.820: INFO: Waiting for pod pod-3416c9ca-c8f6-42b5-b28a-859ec60b2852 to disappear
Mar 23 20:14:00.826: INFO: Pod pod-3416c9ca-c8f6-42b5-b28a-859ec60b2852 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:14:00.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1401" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":272,"skipped":4404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:14:00.877: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-1193
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 23 20:14:01.035: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 23 20:14:21.296: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.44.0.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1193 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 20:14:21.296: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 20:14:22.536: INFO: Found all expected endpoints: [netserver-0]
Mar 23 20:14:22.542: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.0.5 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1193 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 20:14:22.542: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 20:14:23.789: INFO: Found all expected endpoints: [netserver-1]
Mar 23 20:14:23.799: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.36.0.6 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1193 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 20:14:23.799: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
Mar 23 20:14:25.030: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:14:25.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1193" for this suite.

• [SLOW TEST:24.175 seconds]
[sig-network] Networking
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":273,"skipped":4464,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:14:25.052: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-aa216f93-b136-41e4-8fc6-80a57ccaacce
STEP: Creating a pod to test consume secrets
Mar 23 20:14:25.195: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-74d0e789-c07b-4f31-83d1-dc81a7c085c2" in namespace "projected-1493" to be "success or failure"
Mar 23 20:14:25.215: INFO: Pod "pod-projected-secrets-74d0e789-c07b-4f31-83d1-dc81a7c085c2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.785888ms
Mar 23 20:14:27.223: INFO: Pod "pod-projected-secrets-74d0e789-c07b-4f31-83d1-dc81a7c085c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.027764425s
Mar 23 20:14:29.252: INFO: Pod "pod-projected-secrets-74d0e789-c07b-4f31-83d1-dc81a7c085c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057085967s
STEP: Saw pod success
Mar 23 20:14:29.253: INFO: Pod "pod-projected-secrets-74d0e789-c07b-4f31-83d1-dc81a7c085c2" satisfied condition "success or failure"
Mar 23 20:14:29.685: INFO: Trying to get logs from node kube17-worker-1 pod pod-projected-secrets-74d0e789-c07b-4f31-83d1-dc81a7c085c2 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 23 20:14:29.739: INFO: Waiting for pod pod-projected-secrets-74d0e789-c07b-4f31-83d1-dc81a7c085c2 to disappear
Mar 23 20:14:29.744: INFO: Pod pod-projected-secrets-74d0e789-c07b-4f31-83d1-dc81a7c085c2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:14:29.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1493" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":274,"skipped":4485,"failed":0}

------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:14:29.766: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:14:33.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1679" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":275,"skipped":4485,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:14:33.897: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Mar 23 20:14:33.984: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:14:38.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8160" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":276,"skipped":4487,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:14:38.360: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 20:14:39.269: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 20:14:41.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720591279, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720591279, loc:(*time.Location)(0x791d1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720591279, loc:(*time.Location)(0x791d1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720591279, loc:(*time.Location)(0x791d1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 20:14:44.965: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 23 20:14:45.007: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:14:45.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4972" for this suite.
STEP: Destroying namespace "webhook-4972-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.844 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":277,"skipped":4497,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:14:45.208: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 23 20:14:45.292: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f5ea087-ddbf-4004-bac5-f996c77f0e14" in namespace "downward-api-2866" to be "success or failure"
Mar 23 20:14:45.319: INFO: Pod "downwardapi-volume-2f5ea087-ddbf-4004-bac5-f996c77f0e14": Phase="Pending", Reason="", readiness=false. Elapsed: 27.578532ms
Mar 23 20:14:47.326: INFO: Pod "downwardapi-volume-2f5ea087-ddbf-4004-bac5-f996c77f0e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034076433s
Mar 23 20:14:49.333: INFO: Pod "downwardapi-volume-2f5ea087-ddbf-4004-bac5-f996c77f0e14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041118223s
STEP: Saw pod success
Mar 23 20:14:49.333: INFO: Pod "downwardapi-volume-2f5ea087-ddbf-4004-bac5-f996c77f0e14" satisfied condition "success or failure"
Mar 23 20:14:49.338: INFO: Trying to get logs from node kube17-worker-1 pod downwardapi-volume-2f5ea087-ddbf-4004-bac5-f996c77f0e14 container client-container: <nil>
STEP: delete the pod
Mar 23 20:14:49.409: INFO: Waiting for pod downwardapi-volume-2f5ea087-ddbf-4004-bac5-f996c77f0e14 to disappear
Mar 23 20:14:49.416: INFO: Pod downwardapi-volume-2f5ea087-ddbf-4004-bac5-f996c77f0e14 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:14:49.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2866" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":278,"skipped":4505,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:14:49.440: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 23 20:14:52.631: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:14:52.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2628" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":279,"skipped":4506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 20:14:52.690: INFO: >>> kubeConfig: /tmp/kubeconfig-244672999
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 23 20:14:52.819: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 23 20:14:59.110: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 23 20:14:59.111: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar 23 20:14:59.251: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8652 /apis/apps/v1/namespaces/deployment-8652/deployments/test-cleanup-deployment da7ffec3-158b-476f-9f44-b848a937984d 41222 1 2020-03-23 20:14:59 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ebc518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 23 20:14:59.317: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-8652 /apis/apps/v1/namespaces/deployment-8652/replicasets/test-cleanup-deployment-55ffc6b7b6 c786743b-ee4b-4ecb-afcf-052c40c975de 41233 1 2020-03-23 20:14:59 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment da7ffec3-158b-476f-9f44-b848a937984d 0xc003ee46d7 0xc003ee46d8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ee47c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 20:14:59.317: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 23 20:14:59.318: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8652 /apis/apps/v1/namespaces/deployment-8652/replicasets/test-cleanup-controller 07485330-6f24-4b9f-adbf-25c5f3e2de6b 41226 1 2020-03-23 20:14:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment da7ffec3-158b-476f-9f44-b848a937984d 0xc003ee4547 0xc003ee4548}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003ee4608 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 23 20:14:59.334: INFO: Pod "test-cleanup-controller-sd8mw" is available:
&Pod{ObjectMeta:{test-cleanup-controller-sd8mw test-cleanup-controller- deployment-8652 /api/v1/namespaces/deployment-8652/pods/test-cleanup-controller-sd8mw e03dd77c-a547-4c88-a552-115cce3c0f3a 41194 0 2020-03-23 20:14:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 07485330-6f24-4b9f-adbf-25c5f3e2de6b 0xc003ebc947 0xc003ebc948}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-469xj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-469xj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-469xj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 20:14:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 20:14:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 20:14:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 20:14:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.153,PodIP:10.44.0.1,StartTime:2020-03-23 20:14:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 20:14:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://fdcddd0f888f79a61f20ce9ba0016a5c20228ce2f9b2ebb890a4ae538a60393c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.44.0.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 20:14:59.335: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-xqn89" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-xqn89 test-cleanup-deployment-55ffc6b7b6- deployment-8652 /api/v1/namespaces/deployment-8652/pods/test-cleanup-deployment-55ffc6b7b6-xqn89 5a366b60-fc1a-4083-9ce7-2d8b7aac6bf6 41231 0 2020-03-23 20:14:59 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 c786743b-ee4b-4ecb-afcf-052c40c975de 0xc003ebcac7 0xc003ebcac8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-469xj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-469xj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-469xj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube17-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 20:14:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 20:14:59.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8652" for this suite.

• [SLOW TEST:6.672 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.4-beta.0.54+12bf0cb73007af/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":280,"skipped":4538,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSMar 23 20:14:59.362: INFO: Running AfterSuite actions on all nodes
Mar 23 20:14:59.362: INFO: Running AfterSuite actions on node 1
Mar 23 20:14:59.362: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4562,"failed":0}

Ran 280 of 4842 Specs in 5072.570 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4562 Skipped
PASS

Ginkgo ran 1 suite in 1h24m35.623227313s
Test Suite Passed
