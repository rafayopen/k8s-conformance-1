I0622 05:10:25.818031      22 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-119706190
I0622 05:10:25.818070      22 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0622 05:10:25.818229      22 e2e.go:109] Starting e2e run "7996dc72-da4e-477c-96dc-85212d05a2ff" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1592802624 - Will randomize all specs
Will run 280 of 4843 specs

Jun 22 05:10:25.831: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:10:25.835: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 22 05:10:25.859: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 22 05:10:25.927: INFO: 26 / 26 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 22 05:10:25.927: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Jun 22 05:10:25.927: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 22 05:10:25.944: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jun 22 05:10:25.944: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 22 05:10:25.944: INFO: e2e test version: v1.17.7
Jun 22 05:10:25.946: INFO: kube-apiserver version: v1.17.7
Jun 22 05:10:25.946: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:10:25.972: INFO: Cluster IP family: ipv4
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:10:25.972: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
Jun 22 05:10:26.056: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jun 22 05:10:26.105: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7989
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun 22 05:10:34.312: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7989 PodName:pod-sharedvolume-9d041f0a-9042-43e8-bfdc-2fb33c11b5e1 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:10:34.312: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:10:34.475: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:10:34.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7989" for this suite.

• [SLOW TEST:8.531 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":1,"skipped":0,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:10:34.503: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3886
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:10:50.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3886" for this suite.

• [SLOW TEST:16.432 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":2,"skipped":13,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:10:50.936: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Jun 22 05:10:51.114: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 05:11:51.165: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:11:51.179: INFO: Starting informer...
STEP: Starting pod...
Jun 22 05:11:51.405: INFO: Pod is running on node4. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun 22 05:11:51.433: INFO: Pod wasn't evicted. Proceeding
Jun 22 05:11:51.433: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun 22 05:13:06.457: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:13:06.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-476" for this suite.

• [SLOW TEST:135.542 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":3,"skipped":22,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:13:06.478: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1146
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:14:06.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1146" for this suite.

• [SLOW TEST:60.217 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":4,"skipped":30,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:14:06.695: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4711
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Jun 22 05:14:06.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 cluster-info'
Jun 22 05:14:07.099: INFO: stderr: ""
Jun 22 05:14:07.099: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:14:07.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4711" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":5,"skipped":36,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:14:07.117: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8802
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 05:14:07.850: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 22 05:14:09.875: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 05:14:11.882: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 05:14:13.882: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 05:14:15.884: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 05:14:17.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399647, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 05:14:20.941: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:14:21.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8802" for this suite.
STEP: Destroying namespace "webhook-8802-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.148 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":6,"skipped":39,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:14:21.266: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-76e030e8-5633-4052-9441-cefb812bf1e3
STEP: Creating a pod to test consume configMaps
Jun 22 05:14:21.506: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3e476965-a835-475f-aee4-ee57fad8116e" in namespace "projected-6892" to be "success or failure"
Jun 22 05:14:21.513: INFO: Pod "pod-projected-configmaps-3e476965-a835-475f-aee4-ee57fad8116e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.025332ms
Jun 22 05:14:23.522: INFO: Pod "pod-projected-configmaps-3e476965-a835-475f-aee4-ee57fad8116e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016028192s
Jun 22 05:14:25.530: INFO: Pod "pod-projected-configmaps-3e476965-a835-475f-aee4-ee57fad8116e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024027733s
Jun 22 05:14:27.537: INFO: Pod "pod-projected-configmaps-3e476965-a835-475f-aee4-ee57fad8116e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030767367s
STEP: Saw pod success
Jun 22 05:14:27.537: INFO: Pod "pod-projected-configmaps-3e476965-a835-475f-aee4-ee57fad8116e" satisfied condition "success or failure"
Jun 22 05:14:27.543: INFO: Trying to get logs from node node4 pod pod-projected-configmaps-3e476965-a835-475f-aee4-ee57fad8116e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 05:14:27.604: INFO: Waiting for pod pod-projected-configmaps-3e476965-a835-475f-aee4-ee57fad8116e to disappear
Jun 22 05:14:27.609: INFO: Pod pod-projected-configmaps-3e476965-a835-475f-aee4-ee57fad8116e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:14:27.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6892" for this suite.

• [SLOW TEST:6.361 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":7,"skipped":44,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:14:27.628: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 05:14:27.826: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73b53f06-5fdd-4fd0-af5a-eaae72e52df1" in namespace "downward-api-4698" to be "success or failure"
Jun 22 05:14:27.837: INFO: Pod "downwardapi-volume-73b53f06-5fdd-4fd0-af5a-eaae72e52df1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.775348ms
Jun 22 05:14:29.848: INFO: Pod "downwardapi-volume-73b53f06-5fdd-4fd0-af5a-eaae72e52df1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022450462s
STEP: Saw pod success
Jun 22 05:14:29.848: INFO: Pod "downwardapi-volume-73b53f06-5fdd-4fd0-af5a-eaae72e52df1" satisfied condition "success or failure"
Jun 22 05:14:29.859: INFO: Trying to get logs from node node4 pod downwardapi-volume-73b53f06-5fdd-4fd0-af5a-eaae72e52df1 container client-container: <nil>
STEP: delete the pod
Jun 22 05:14:29.924: INFO: Waiting for pod downwardapi-volume-73b53f06-5fdd-4fd0-af5a-eaae72e52df1 to disappear
Jun 22 05:14:29.930: INFO: Pod downwardapi-volume-73b53f06-5fdd-4fd0-af5a-eaae72e52df1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:14:29.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4698" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":8,"skipped":72,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:14:29.977: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3731
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 22 05:14:30.193: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3731 /api/v1/namespaces/watch-3731/configmaps/e2e-watch-test-configmap-a 8b81b76e-cf52-4d58-87fc-fef5dea61665 8606 0 2020-06-22 05:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 05:14:30.193: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3731 /api/v1/namespaces/watch-3731/configmaps/e2e-watch-test-configmap-a 8b81b76e-cf52-4d58-87fc-fef5dea61665 8606 0 2020-06-22 05:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 22 05:14:40.213: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3731 /api/v1/namespaces/watch-3731/configmaps/e2e-watch-test-configmap-a 8b81b76e-cf52-4d58-87fc-fef5dea61665 8672 0 2020-06-22 05:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 22 05:14:40.213: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3731 /api/v1/namespaces/watch-3731/configmaps/e2e-watch-test-configmap-a 8b81b76e-cf52-4d58-87fc-fef5dea61665 8672 0 2020-06-22 05:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 22 05:14:50.229: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3731 /api/v1/namespaces/watch-3731/configmaps/e2e-watch-test-configmap-a 8b81b76e-cf52-4d58-87fc-fef5dea61665 8714 0 2020-06-22 05:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 05:14:50.230: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3731 /api/v1/namespaces/watch-3731/configmaps/e2e-watch-test-configmap-a 8b81b76e-cf52-4d58-87fc-fef5dea61665 8714 0 2020-06-22 05:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 22 05:15:00.249: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3731 /api/v1/namespaces/watch-3731/configmaps/e2e-watch-test-configmap-a 8b81b76e-cf52-4d58-87fc-fef5dea61665 8758 0 2020-06-22 05:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 05:15:00.249: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3731 /api/v1/namespaces/watch-3731/configmaps/e2e-watch-test-configmap-a 8b81b76e-cf52-4d58-87fc-fef5dea61665 8758 0 2020-06-22 05:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 22 05:15:10.265: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3731 /api/v1/namespaces/watch-3731/configmaps/e2e-watch-test-configmap-b c85fbabf-8271-4216-9cc5-b7d81656c9a5 8799 0 2020-06-22 05:15:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 05:15:10.266: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3731 /api/v1/namespaces/watch-3731/configmaps/e2e-watch-test-configmap-b c85fbabf-8271-4216-9cc5-b7d81656c9a5 8799 0 2020-06-22 05:15:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 22 05:15:20.281: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3731 /api/v1/namespaces/watch-3731/configmaps/e2e-watch-test-configmap-b c85fbabf-8271-4216-9cc5-b7d81656c9a5 8842 0 2020-06-22 05:15:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 05:15:20.281: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3731 /api/v1/namespaces/watch-3731/configmaps/e2e-watch-test-configmap-b c85fbabf-8271-4216-9cc5-b7d81656c9a5 8842 0 2020-06-22 05:15:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:15:30.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3731" for this suite.

• [SLOW TEST:60.333 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":9,"skipped":92,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:15:30.310: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-210
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-8fcf2b7b-d5c6-46f7-a894-54ca734a0bd6
STEP: Creating a pod to test consume configMaps
Jun 22 05:15:30.530: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e4bc6f3c-92f5-437d-87d8-a3cbd1295f1d" in namespace "projected-210" to be "success or failure"
Jun 22 05:15:30.545: INFO: Pod "pod-projected-configmaps-e4bc6f3c-92f5-437d-87d8-a3cbd1295f1d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.200278ms
Jun 22 05:15:32.552: INFO: Pod "pod-projected-configmaps-e4bc6f3c-92f5-437d-87d8-a3cbd1295f1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021078965s
Jun 22 05:15:34.558: INFO: Pod "pod-projected-configmaps-e4bc6f3c-92f5-437d-87d8-a3cbd1295f1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027027524s
STEP: Saw pod success
Jun 22 05:15:34.558: INFO: Pod "pod-projected-configmaps-e4bc6f3c-92f5-437d-87d8-a3cbd1295f1d" satisfied condition "success or failure"
Jun 22 05:15:34.566: INFO: Trying to get logs from node node4 pod pod-projected-configmaps-e4bc6f3c-92f5-437d-87d8-a3cbd1295f1d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 05:15:34.617: INFO: Waiting for pod pod-projected-configmaps-e4bc6f3c-92f5-437d-87d8-a3cbd1295f1d to disappear
Jun 22 05:15:34.625: INFO: Pod pod-projected-configmaps-e4bc6f3c-92f5-437d-87d8-a3cbd1295f1d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:15:34.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-210" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":10,"skipped":97,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:15:34.643: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8892
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 22 05:15:34.846: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:15:53.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8892" for this suite.

• [SLOW TEST:18.893 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":11,"skipped":115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:15:53.537: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 05:15:54.930: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 05:15:56.953: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399754, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399754, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399754, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728399754, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 05:16:00.010: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:16:00.017: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8678-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:16:05.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6504" for this suite.
STEP: Destroying namespace "webhook-6504-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.420 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":12,"skipped":183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:16:05.958: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8572
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:16:06.227: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 05:16:06.247: INFO: Number of nodes with available pods: 0
Jun 22 05:16:06.247: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:07.265: INFO: Number of nodes with available pods: 0
Jun 22 05:16:07.265: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:08.270: INFO: Number of nodes with available pods: 0
Jun 22 05:16:08.270: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:09.264: INFO: Number of nodes with available pods: 0
Jun 22 05:16:09.264: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:10.261: INFO: Number of nodes with available pods: 0
Jun 22 05:16:10.261: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:11.266: INFO: Number of nodes with available pods: 0
Jun 22 05:16:11.266: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:12.283: INFO: Number of nodes with available pods: 0
Jun 22 05:16:12.283: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:13.269: INFO: Number of nodes with available pods: 0
Jun 22 05:16:13.269: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:14.270: INFO: Number of nodes with available pods: 0
Jun 22 05:16:14.270: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:15.268: INFO: Number of nodes with available pods: 0
Jun 22 05:16:15.268: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:16.260: INFO: Number of nodes with available pods: 0
Jun 22 05:16:16.260: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:17.264: INFO: Number of nodes with available pods: 0
Jun 22 05:16:17.264: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:18.277: INFO: Number of nodes with available pods: 0
Jun 22 05:16:18.277: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:22.970: INFO: Number of nodes with available pods: 0
Jun 22 05:16:22.970: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:23.262: INFO: Number of nodes with available pods: 0
Jun 22 05:16:23.262: INFO: Node node1 is running more than one daemon pod
Jun 22 05:16:24.265: INFO: Number of nodes with available pods: 2
Jun 22 05:16:24.265: INFO: Node node2 is running more than one daemon pod
Jun 22 05:16:25.265: INFO: Number of nodes with available pods: 3
Jun 22 05:16:25.265: INFO: Node node4 is running more than one daemon pod
Jun 22 05:16:26.263: INFO: Number of nodes with available pods: 4
Jun 22 05:16:26.263: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 22 05:16:26.316: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:26.316: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:26.316: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:26.316: INFO: Wrong image for pod: daemon-set-v4cp2. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:27.337: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:27.337: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:27.337: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:27.337: INFO: Wrong image for pod: daemon-set-v4cp2. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:28.337: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:28.337: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:28.337: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:28.337: INFO: Wrong image for pod: daemon-set-v4cp2. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:28.337: INFO: Pod daemon-set-v4cp2 is not available
Jun 22 05:16:29.336: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:29.336: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:29.336: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:29.336: INFO: Pod daemon-set-kz9td is not available
Jun 22 05:16:30.331: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:30.331: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:30.331: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:30.331: INFO: Pod daemon-set-kz9td is not available
Jun 22 05:16:31.334: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:31.334: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:31.334: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:31.334: INFO: Pod daemon-set-kz9td is not available
Jun 22 05:16:32.336: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:32.336: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:32.336: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:32.336: INFO: Pod daemon-set-kz9td is not available
Jun 22 05:16:33.333: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:33.333: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:33.333: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:33.333: INFO: Pod daemon-set-kz9td is not available
Jun 22 05:16:34.337: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:34.337: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:34.337: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:34.337: INFO: Pod daemon-set-kz9td is not available
Jun 22 05:16:35.336: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:35.336: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:35.336: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:35.336: INFO: Pod daemon-set-kz9td is not available
Jun 22 05:16:36.337: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:36.337: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:36.337: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:36.337: INFO: Pod daemon-set-kz9td is not available
Jun 22 05:16:37.336: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:37.336: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:37.336: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:37.336: INFO: Pod daemon-set-kz9td is not available
Jun 22 05:16:38.336: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:38.336: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:38.336: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:38.336: INFO: Pod daemon-set-kz9td is not available
Jun 22 05:16:39.334: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:39.334: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:39.334: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:39.334: INFO: Pod daemon-set-kz9td is not available
Jun 22 05:16:40.569: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:40.569: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:40.569: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:40.569: INFO: Pod daemon-set-kz9td is not available
Jun 22 05:16:41.341: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:41.342: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:41.342: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:42.336: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:42.336: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:42.336: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:43.334: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:43.334: INFO: Pod daemon-set-98k68 is not available
Jun 22 05:16:43.334: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:43.334: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:44.332: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:44.332: INFO: Pod daemon-set-98k68 is not available
Jun 22 05:16:44.332: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:44.332: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:45.344: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:45.344: INFO: Pod daemon-set-98k68 is not available
Jun 22 05:16:45.344: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:45.344: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:46.335: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:46.336: INFO: Pod daemon-set-98k68 is not available
Jun 22 05:16:46.336: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:46.336: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:47.334: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:47.334: INFO: Pod daemon-set-98k68 is not available
Jun 22 05:16:47.334: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:47.334: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:48.336: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:48.336: INFO: Pod daemon-set-98k68 is not available
Jun 22 05:16:48.336: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:48.336: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:49.333: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:49.333: INFO: Pod daemon-set-98k68 is not available
Jun 22 05:16:49.333: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:49.333: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:50.335: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:50.335: INFO: Pod daemon-set-98k68 is not available
Jun 22 05:16:50.335: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:50.335: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:51.338: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:51.338: INFO: Pod daemon-set-98k68 is not available
Jun 22 05:16:51.338: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:51.338: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:52.334: INFO: Wrong image for pod: daemon-set-98k68. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:52.334: INFO: Pod daemon-set-98k68 is not available
Jun 22 05:16:52.334: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:52.334: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:53.333: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:53.333: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:53.333: INFO: Pod daemon-set-rhbdn is not available
Jun 22 05:16:54.334: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:54.334: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:54.334: INFO: Pod daemon-set-rhbdn is not available
Jun 22 05:16:55.332: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:55.332: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:55.332: INFO: Pod daemon-set-rhbdn is not available
Jun 22 05:16:56.339: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:56.339: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:56.339: INFO: Pod daemon-set-rhbdn is not available
Jun 22 05:16:57.332: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:57.332: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:57.332: INFO: Pod daemon-set-rhbdn is not available
Jun 22 05:16:58.333: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:58.333: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:58.333: INFO: Pod daemon-set-rhbdn is not available
Jun 22 05:16:59.337: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:59.337: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:16:59.337: INFO: Pod daemon-set-rhbdn is not available
Jun 22 05:17:00.334: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:00.334: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:00.334: INFO: Pod daemon-set-rhbdn is not available
Jun 22 05:17:01.337: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:01.337: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:01.337: INFO: Pod daemon-set-rhbdn is not available
Jun 22 05:17:02.332: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:02.332: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:02.332: INFO: Pod daemon-set-rhbdn is not available
Jun 22 05:17:03.337: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:03.337: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:03.337: INFO: Pod daemon-set-rhbdn is not available
Jun 22 05:17:04.618: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:04.618: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:04.618: INFO: Pod daemon-set-rhbdn is not available
Jun 22 05:17:05.332: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:05.332: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:05.332: INFO: Pod daemon-set-rhbdn is not available
Jun 22 05:17:06.336: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:06.336: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:07.336: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:07.336: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:07.336: INFO: Pod daemon-set-kkw2b is not available
Jun 22 05:17:08.332: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:08.333: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:08.333: INFO: Pod daemon-set-kkw2b is not available
Jun 22 05:17:09.337: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:09.337: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:09.337: INFO: Pod daemon-set-kkw2b is not available
Jun 22 05:17:10.333: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:10.333: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:10.333: INFO: Pod daemon-set-kkw2b is not available
Jun 22 05:17:11.331: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:11.331: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:11.331: INFO: Pod daemon-set-kkw2b is not available
Jun 22 05:17:12.334: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:12.335: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:12.335: INFO: Pod daemon-set-kkw2b is not available
Jun 22 05:17:13.334: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:13.335: INFO: Wrong image for pod: daemon-set-kkw2b. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:13.335: INFO: Pod daemon-set-kkw2b is not available
Jun 22 05:17:14.336: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:14.336: INFO: Pod daemon-set-s7mks is not available
Jun 22 05:17:15.337: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:15.337: INFO: Pod daemon-set-s7mks is not available
Jun 22 05:17:16.334: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:17.336: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:17.336: INFO: Pod daemon-set-gdq9f is not available
Jun 22 05:17:18.336: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:18.336: INFO: Pod daemon-set-gdq9f is not available
Jun 22 05:17:19.332: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:19.332: INFO: Pod daemon-set-gdq9f is not available
Jun 22 05:17:20.335: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:20.335: INFO: Pod daemon-set-gdq9f is not available
Jun 22 05:17:21.335: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:21.335: INFO: Pod daemon-set-gdq9f is not available
Jun 22 05:17:22.332: INFO: Wrong image for pod: daemon-set-gdq9f. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 05:17:22.332: INFO: Pod daemon-set-gdq9f is not available
Jun 22 05:17:23.333: INFO: Pod daemon-set-8gl8j is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 22 05:17:23.353: INFO: Number of nodes with available pods: 3
Jun 22 05:17:23.353: INFO: Node node3 is running more than one daemon pod
Jun 22 05:17:24.369: INFO: Number of nodes with available pods: 3
Jun 22 05:17:24.369: INFO: Node node3 is running more than one daemon pod
Jun 22 05:17:25.368: INFO: Number of nodes with available pods: 3
Jun 22 05:17:25.368: INFO: Node node3 is running more than one daemon pod
Jun 22 05:17:26.371: INFO: Number of nodes with available pods: 3
Jun 22 05:17:26.371: INFO: Node node3 is running more than one daemon pod
Jun 22 05:17:27.366: INFO: Number of nodes with available pods: 3
Jun 22 05:17:27.366: INFO: Node node3 is running more than one daemon pod
Jun 22 05:17:28.374: INFO: Number of nodes with available pods: 3
Jun 22 05:17:28.374: INFO: Node node3 is running more than one daemon pod
Jun 22 05:17:29.372: INFO: Number of nodes with available pods: 3
Jun 22 05:17:29.372: INFO: Node node3 is running more than one daemon pod
Jun 22 05:17:30.369: INFO: Number of nodes with available pods: 3
Jun 22 05:17:30.369: INFO: Node node3 is running more than one daemon pod
Jun 22 05:17:31.372: INFO: Number of nodes with available pods: 4
Jun 22 05:17:31.372: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8572, will wait for the garbage collector to delete the pods
Jun 22 05:17:31.474: INFO: Deleting DaemonSet.extensions daemon-set took: 15.631247ms
Jun 22 05:17:31.876: INFO: Terminating DaemonSet.extensions daemon-set pods took: 402.481985ms
Jun 22 05:17:43.584: INFO: Number of nodes with available pods: 0
Jun 22 05:17:43.584: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 05:17:43.590: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8572/daemonsets","resourceVersion":"9795"},"items":null}

Jun 22 05:17:43.597: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8572/pods","resourceVersion":"9795"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:17:43.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8572" for this suite.

• [SLOW TEST:97.686 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":13,"skipped":217,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:17:43.644: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7086
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:17:51.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7086" for this suite.

• [SLOW TEST:8.221 seconds]
[sig-apps] Job
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":14,"skipped":236,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:17:51.865: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3744
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jun 22 05:17:56.632: INFO: Successfully updated pod "labelsupdateed2c9e43-91f5-4aab-af27-560a48d81da5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:17:58.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3744" for this suite.

• [SLOW TEST:6.814 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":15,"skipped":240,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:17:58.681: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3460
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3460 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3460;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3460 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3460;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3460.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3460.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3460.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3460.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3460.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3460.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3460.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3460.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3460.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3460.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3460.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3460.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3460.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 13.82.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.82.13_udp@PTR;check="$$(dig +tcp +noall +answer +search 13.82.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.82.13_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3460 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3460;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3460 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3460;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3460.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3460.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3460.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3460.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3460.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3460.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3460.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3460.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3460.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3460.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3460.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3460.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3460.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 13.82.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.82.13_udp@PTR;check="$$(dig +tcp +noall +answer +search 13.82.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.82.13_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 05:18:22.945: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:22.953: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:22.958: INFO: Unable to read wheezy_udp@dns-test-service.dns-3460 from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:22.965: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3460 from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:22.980: INFO: Unable to read wheezy_udp@dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:22.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:22.994: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:23.001: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:23.048: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:23.057: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:23.065: INFO: Unable to read jessie_udp@dns-test-service.dns-3460 from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:23.070: INFO: Unable to read jessie_tcp@dns-test-service.dns-3460 from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:23.077: INFO: Unable to read jessie_udp@dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:23.084: INFO: Unable to read jessie_tcp@dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:23.089: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:23.096: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:23.134: INFO: Lookups using dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3460 wheezy_tcp@dns-test-service.dns-3460 wheezy_udp@dns-test-service.dns-3460.svc wheezy_tcp@dns-test-service.dns-3460.svc wheezy_udp@_http._tcp.dns-test-service.dns-3460.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3460.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3460 jessie_tcp@dns-test-service.dns-3460 jessie_udp@dns-test-service.dns-3460.svc jessie_tcp@dns-test-service.dns-3460.svc jessie_udp@_http._tcp.dns-test-service.dns-3460.svc jessie_tcp@_http._tcp.dns-test-service.dns-3460.svc]

Jun 22 05:18:28.142: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.149: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.157: INFO: Unable to read wheezy_udp@dns-test-service.dns-3460 from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.164: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3460 from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.171: INFO: Unable to read wheezy_udp@dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.182: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.189: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.194: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.242: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.248: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.253: INFO: Unable to read jessie_udp@dns-test-service.dns-3460 from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.259: INFO: Unable to read jessie_tcp@dns-test-service.dns-3460 from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.265: INFO: Unable to read jessie_udp@dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.274: INFO: Unable to read jessie_tcp@dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.280: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.287: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:28.334: INFO: Lookups using dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3460 wheezy_tcp@dns-test-service.dns-3460 wheezy_udp@dns-test-service.dns-3460.svc wheezy_tcp@dns-test-service.dns-3460.svc wheezy_udp@_http._tcp.dns-test-service.dns-3460.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3460.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3460 jessie_tcp@dns-test-service.dns-3460 jessie_udp@dns-test-service.dns-3460.svc jessie_tcp@dns-test-service.dns-3460.svc jessie_udp@_http._tcp.dns-test-service.dns-3460.svc jessie_tcp@_http._tcp.dns-test-service.dns-3460.svc]

Jun 22 05:18:33.143: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.150: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.159: INFO: Unable to read wheezy_udp@dns-test-service.dns-3460 from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.168: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3460 from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.176: INFO: Unable to read wheezy_udp@dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.189: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.194: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.201: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.259: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.264: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.270: INFO: Unable to read jessie_udp@dns-test-service.dns-3460 from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.280: INFO: Unable to read jessie_tcp@dns-test-service.dns-3460 from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.286: INFO: Unable to read jessie_udp@dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.291: INFO: Unable to read jessie_tcp@dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.297: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.303: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3460.svc from pod dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858: the server could not find the requested resource (get pods dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858)
Jun 22 05:18:33.345: INFO: Lookups using dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3460 wheezy_tcp@dns-test-service.dns-3460 wheezy_udp@dns-test-service.dns-3460.svc wheezy_tcp@dns-test-service.dns-3460.svc wheezy_udp@_http._tcp.dns-test-service.dns-3460.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3460.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3460 jessie_tcp@dns-test-service.dns-3460 jessie_udp@dns-test-service.dns-3460.svc jessie_tcp@dns-test-service.dns-3460.svc jessie_udp@_http._tcp.dns-test-service.dns-3460.svc jessie_tcp@_http._tcp.dns-test-service.dns-3460.svc]

Jun 22 05:18:38.340: INFO: DNS probes using dns-3460/dns-test-be3e38e4-646b-42fc-a7e8-a79a268e8858 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:18:38.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3460" for this suite.

• [SLOW TEST:39.860 seconds]
[sig-network] DNS
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":16,"skipped":280,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:18:38.541: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jun 22 05:18:38.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-3096'
Jun 22 05:18:39.207: INFO: stderr: ""
Jun 22 05:18:39.207: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 05:18:39.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3096'
Jun 22 05:18:39.335: INFO: stderr: ""
Jun 22 05:18:39.335: INFO: stdout: "update-demo-nautilus-mm54n update-demo-nautilus-shxh2 "
Jun 22 05:18:39.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-mm54n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Jun 22 05:18:39.456: INFO: stderr: ""
Jun 22 05:18:39.456: INFO: stdout: ""
Jun 22 05:18:39.456: INFO: update-demo-nautilus-mm54n is created but not running
Jun 22 05:18:44.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3096'
Jun 22 05:18:44.569: INFO: stderr: ""
Jun 22 05:18:44.569: INFO: stdout: "update-demo-nautilus-mm54n update-demo-nautilus-shxh2 "
Jun 22 05:18:44.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-mm54n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Jun 22 05:18:44.677: INFO: stderr: ""
Jun 22 05:18:44.677: INFO: stdout: "true"
Jun 22 05:18:44.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-mm54n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Jun 22 05:18:44.780: INFO: stderr: ""
Jun 22 05:18:44.780: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 05:18:44.780: INFO: validating pod update-demo-nautilus-mm54n
Jun 22 05:18:44.788: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 05:18:44.788: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 05:18:44.788: INFO: update-demo-nautilus-mm54n is verified up and running
Jun 22 05:18:44.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-shxh2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Jun 22 05:18:44.911: INFO: stderr: ""
Jun 22 05:18:44.911: INFO: stdout: "true"
Jun 22 05:18:44.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-shxh2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Jun 22 05:18:45.029: INFO: stderr: ""
Jun 22 05:18:45.029: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 05:18:45.029: INFO: validating pod update-demo-nautilus-shxh2
Jun 22 05:18:45.038: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 05:18:45.039: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 05:18:45.039: INFO: update-demo-nautilus-shxh2 is verified up and running
STEP: scaling down the replication controller
Jun 22 05:18:45.044: INFO: scanned /root for discovery docs: <nil>
Jun 22 05:18:45.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-3096'
Jun 22 05:18:46.185: INFO: stderr: ""
Jun 22 05:18:46.185: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 05:18:46.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3096'
Jun 22 05:18:46.316: INFO: stderr: ""
Jun 22 05:18:46.316: INFO: stdout: "update-demo-nautilus-mm54n update-demo-nautilus-shxh2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 22 05:18:51.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3096'
Jun 22 05:18:51.415: INFO: stderr: ""
Jun 22 05:18:51.415: INFO: stdout: "update-demo-nautilus-mm54n update-demo-nautilus-shxh2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 22 05:18:56.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3096'
Jun 22 05:18:56.562: INFO: stderr: ""
Jun 22 05:18:56.562: INFO: stdout: "update-demo-nautilus-shxh2 "
Jun 22 05:18:56.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-shxh2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Jun 22 05:18:56.673: INFO: stderr: ""
Jun 22 05:18:56.673: INFO: stdout: "true"
Jun 22 05:18:56.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-shxh2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Jun 22 05:18:56.769: INFO: stderr: ""
Jun 22 05:18:56.769: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 05:18:56.769: INFO: validating pod update-demo-nautilus-shxh2
Jun 22 05:18:56.783: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 05:18:56.784: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 05:18:56.784: INFO: update-demo-nautilus-shxh2 is verified up and running
STEP: scaling up the replication controller
Jun 22 05:18:56.785: INFO: scanned /root for discovery docs: <nil>
Jun 22 05:18:56.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-3096'
Jun 22 05:18:57.935: INFO: stderr: ""
Jun 22 05:18:57.935: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 05:18:57.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3096'
Jun 22 05:18:58.062: INFO: stderr: ""
Jun 22 05:18:58.062: INFO: stdout: "update-demo-nautilus-fnnvv update-demo-nautilus-shxh2 "
Jun 22 05:18:58.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-fnnvv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Jun 22 05:18:58.164: INFO: stderr: ""
Jun 22 05:18:58.164: INFO: stdout: ""
Jun 22 05:18:58.164: INFO: update-demo-nautilus-fnnvv is created but not running
Jun 22 05:19:03.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3096'
Jun 22 05:19:03.281: INFO: stderr: ""
Jun 22 05:19:03.281: INFO: stdout: "update-demo-nautilus-fnnvv update-demo-nautilus-shxh2 "
Jun 22 05:19:03.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-fnnvv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Jun 22 05:19:03.381: INFO: stderr: ""
Jun 22 05:19:03.381: INFO: stdout: "true"
Jun 22 05:19:03.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-fnnvv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Jun 22 05:19:03.485: INFO: stderr: ""
Jun 22 05:19:03.485: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 05:19:03.485: INFO: validating pod update-demo-nautilus-fnnvv
Jun 22 05:19:03.492: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 05:19:03.492: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 05:19:03.492: INFO: update-demo-nautilus-fnnvv is verified up and running
Jun 22 05:19:03.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-shxh2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Jun 22 05:19:03.616: INFO: stderr: ""
Jun 22 05:19:03.616: INFO: stdout: "true"
Jun 22 05:19:03.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-shxh2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Jun 22 05:19:03.721: INFO: stderr: ""
Jun 22 05:19:03.721: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 05:19:03.721: INFO: validating pod update-demo-nautilus-shxh2
Jun 22 05:19:03.730: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 05:19:03.730: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 05:19:03.730: INFO: update-demo-nautilus-shxh2 is verified up and running
STEP: using delete to clean up resources
Jun 22 05:19:03.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete --grace-period=0 --force -f - --namespace=kubectl-3096'
Jun 22 05:19:03.843: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 05:19:03.843: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 22 05:19:03.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3096'
Jun 22 05:19:03.956: INFO: stderr: "No resources found in kubectl-3096 namespace.\n"
Jun 22 05:19:03.956: INFO: stdout: ""
Jun 22 05:19:03.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -l name=update-demo --namespace=kubectl-3096 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 05:19:04.062: INFO: stderr: ""
Jun 22 05:19:04.062: INFO: stdout: "update-demo-nautilus-fnnvv\nupdate-demo-nautilus-shxh2\n"
Jun 22 05:19:04.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3096'
Jun 22 05:19:04.728: INFO: stderr: "No resources found in kubectl-3096 namespace.\n"
Jun 22 05:19:04.728: INFO: stdout: ""
Jun 22 05:19:04.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -l name=update-demo --namespace=kubectl-3096 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 05:19:04.875: INFO: stderr: ""
Jun 22 05:19:04.875: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:19:04.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3096" for this suite.

• [SLOW TEST:26.353 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":17,"skipped":287,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:19:04.895: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5764
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5764
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5764
Jun 22 05:19:05.107: INFO: Found 0 stateful pods, waiting for 1
Jun 22 05:19:15.119: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 22 05:19:15.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-5764 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 05:19:15.383: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 05:19:15.383: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 05:19:15.383: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 05:19:15.390: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 22 05:19:25.398: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 05:19:25.398: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 05:19:25.427: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999939s
Jun 22 05:19:26.436: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993791231s
Jun 22 05:19:27.445: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984536838s
Jun 22 05:19:28.454: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.976631234s
Jun 22 05:19:29.461: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.967471093s
Jun 22 05:19:30.469: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.960002668s
Jun 22 05:19:31.480: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.950942581s
Jun 22 05:19:32.488: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.940018944s
Jun 22 05:19:33.499: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.933556899s
Jun 22 05:19:34.509: INFO: Verifying statefulset ss doesn't scale past 1 for another 922.378757ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5764
Jun 22 05:19:35.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-5764 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 05:19:35.783: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 05:19:35.783: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 05:19:35.783: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 05:19:35.789: INFO: Found 1 stateful pods, waiting for 3
Jun 22 05:19:45.795: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 05:19:45.795: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 05:19:45.795: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 22 05:19:45.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-5764 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 05:19:46.133: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 05:19:46.133: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 05:19:46.133: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 05:19:46.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-5764 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 05:19:46.442: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 05:19:46.442: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 05:19:46.442: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 05:19:46.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-5764 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 05:19:46.767: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 05:19:46.767: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 05:19:46.767: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 05:19:46.767: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 05:19:46.776: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 22 05:19:56.791: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 05:19:56.791: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 05:19:56.791: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 05:19:56.809: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999958s
Jun 22 05:19:57.815: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993242139s
Jun 22 05:19:58.823: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987653772s
Jun 22 05:19:59.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979450131s
Jun 22 05:20:00.840: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970828502s
Jun 22 05:20:01.848: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.960381006s
Jun 22 05:20:02.856: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.953784608s
Jun 22 05:20:03.865: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.946473436s
Jun 22 05:20:04.875: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.937440539s
Jun 22 05:20:05.882: INFO: Verifying statefulset ss doesn't scale past 3 for another 927.311949ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5764
Jun 22 05:20:06.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-5764 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 05:20:07.180: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 05:20:07.180: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 05:20:07.180: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 05:20:07.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-5764 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 05:20:07.460: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 05:20:07.461: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 05:20:07.461: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 05:20:07.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-5764 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 05:20:07.790: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 05:20:07.790: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 05:20:07.790: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 05:20:07.790: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun 22 05:20:27.820: INFO: Deleting all statefulset in ns statefulset-5764
Jun 22 05:20:27.827: INFO: Scaling statefulset ss to 0
Jun 22 05:20:27.845: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 05:20:27.850: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:20:27.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5764" for this suite.

• [SLOW TEST:83.010 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":18,"skipped":323,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:20:27.905: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5146
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Jun 22 05:20:28.091: INFO: Waiting up to 5m0s for pod "client-containers-05cbee5d-5b3a-4a36-8e0b-523b56dc3149" in namespace "containers-5146" to be "success or failure"
Jun 22 05:20:28.099: INFO: Pod "client-containers-05cbee5d-5b3a-4a36-8e0b-523b56dc3149": Phase="Pending", Reason="", readiness=false. Elapsed: 8.469874ms
Jun 22 05:20:30.105: INFO: Pod "client-containers-05cbee5d-5b3a-4a36-8e0b-523b56dc3149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014042382s
Jun 22 05:20:32.112: INFO: Pod "client-containers-05cbee5d-5b3a-4a36-8e0b-523b56dc3149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021176442s
STEP: Saw pod success
Jun 22 05:20:32.112: INFO: Pod "client-containers-05cbee5d-5b3a-4a36-8e0b-523b56dc3149" satisfied condition "success or failure"
Jun 22 05:20:32.117: INFO: Trying to get logs from node node4 pod client-containers-05cbee5d-5b3a-4a36-8e0b-523b56dc3149 container test-container: <nil>
STEP: delete the pod
Jun 22 05:20:32.167: INFO: Waiting for pod client-containers-05cbee5d-5b3a-4a36-8e0b-523b56dc3149 to disappear
Jun 22 05:20:32.174: INFO: Pod client-containers-05cbee5d-5b3a-4a36-8e0b-523b56dc3149 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:20:32.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5146" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":19,"skipped":329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:20:32.187: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9740
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4669
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1540
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:20:38.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9740" for this suite.
STEP: Destroying namespace "nsdeletetest-4669" for this suite.
Jun 22 05:20:38.864: INFO: Namespace nsdeletetest-4669 was already deleted
STEP: Destroying namespace "nsdeletetest-1540" for this suite.

• [SLOW TEST:6.688 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":20,"skipped":352,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:20:38.875: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 05:20:39.067: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47b9c0bd-1390-411e-9389-157f82333c92" in namespace "downward-api-255" to be "success or failure"
Jun 22 05:20:39.075: INFO: Pod "downwardapi-volume-47b9c0bd-1390-411e-9389-157f82333c92": Phase="Pending", Reason="", readiness=false. Elapsed: 8.345971ms
Jun 22 05:20:41.085: INFO: Pod "downwardapi-volume-47b9c0bd-1390-411e-9389-157f82333c92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018170747s
STEP: Saw pod success
Jun 22 05:20:41.085: INFO: Pod "downwardapi-volume-47b9c0bd-1390-411e-9389-157f82333c92" satisfied condition "success or failure"
Jun 22 05:20:41.091: INFO: Trying to get logs from node node4 pod downwardapi-volume-47b9c0bd-1390-411e-9389-157f82333c92 container client-container: <nil>
STEP: delete the pod
Jun 22 05:20:41.126: INFO: Waiting for pod downwardapi-volume-47b9c0bd-1390-411e-9389-157f82333c92 to disappear
Jun 22 05:20:41.130: INFO: Pod downwardapi-volume-47b9c0bd-1390-411e-9389-157f82333c92 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:20:41.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-255" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":21,"skipped":365,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:20:41.149: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 05:20:41.345: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12cf2848-c767-4297-87fd-2291b8663493" in namespace "projected-6395" to be "success or failure"
Jun 22 05:20:41.352: INFO: Pod "downwardapi-volume-12cf2848-c767-4297-87fd-2291b8663493": Phase="Pending", Reason="", readiness=false. Elapsed: 7.028885ms
Jun 22 05:20:43.358: INFO: Pod "downwardapi-volume-12cf2848-c767-4297-87fd-2291b8663493": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012632944s
STEP: Saw pod success
Jun 22 05:20:43.358: INFO: Pod "downwardapi-volume-12cf2848-c767-4297-87fd-2291b8663493" satisfied condition "success or failure"
Jun 22 05:20:43.363: INFO: Trying to get logs from node node4 pod downwardapi-volume-12cf2848-c767-4297-87fd-2291b8663493 container client-container: <nil>
STEP: delete the pod
Jun 22 05:20:43.395: INFO: Waiting for pod downwardapi-volume-12cf2848-c767-4297-87fd-2291b8663493 to disappear
Jun 22 05:20:43.400: INFO: Pod downwardapi-volume-12cf2848-c767-4297-87fd-2291b8663493 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:20:43.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6395" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":22,"skipped":374,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:20:43.418: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7159
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:20:43.626: INFO: (0) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 24.86209ms)
Jun 22 05:20:43.634: INFO: (1) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 7.516534ms)
Jun 22 05:20:43.639: INFO: (2) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.680536ms)
Jun 22 05:20:43.648: INFO: (3) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 8.29277ms)
Jun 22 05:20:43.657: INFO: (4) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 9.031545ms)
Jun 22 05:20:43.664: INFO: (5) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 7.339871ms)
Jun 22 05:20:43.671: INFO: (6) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.410281ms)
Jun 22 05:20:43.679: INFO: (7) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 7.711978ms)
Jun 22 05:20:43.684: INFO: (8) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.482663ms)
Jun 22 05:20:43.691: INFO: (9) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.956623ms)
Jun 22 05:20:43.700: INFO: (10) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 8.975374ms)
Jun 22 05:20:43.707: INFO: (11) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.754568ms)
Jun 22 05:20:43.714: INFO: (12) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.680788ms)
Jun 22 05:20:43.722: INFO: (13) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 7.727817ms)
Jun 22 05:20:43.728: INFO: (14) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.728619ms)
Jun 22 05:20:43.735: INFO: (15) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.385111ms)
Jun 22 05:20:43.743: INFO: (16) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 7.80672ms)
Jun 22 05:20:43.752: INFO: (17) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 8.754859ms)
Jun 22 05:20:43.760: INFO: (18) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 8.327581ms)
Jun 22 05:20:43.769: INFO: (19) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 8.599096ms)
[AfterEach] version v1
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:20:43.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7159" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":23,"skipped":393,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:20:43.792: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8557
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-8557
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8557 to expose endpoints map[]
Jun 22 05:20:44.084: INFO: successfully validated that service endpoint-test2 in namespace services-8557 exposes endpoints map[] (19.331397ms elapsed)
STEP: Creating pod pod1 in namespace services-8557
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8557 to expose endpoints map[pod1:[80]]
Jun 22 05:20:47.200: INFO: successfully validated that service endpoint-test2 in namespace services-8557 exposes endpoints map[pod1:[80]] (3.077993726s elapsed)
STEP: Creating pod pod2 in namespace services-8557
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8557 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 22 05:20:50.299: INFO: successfully validated that service endpoint-test2 in namespace services-8557 exposes endpoints map[pod1:[80] pod2:[80]] (3.09088649s elapsed)
STEP: Deleting pod pod1 in namespace services-8557
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8557 to expose endpoints map[pod2:[80]]
Jun 22 05:20:51.352: INFO: successfully validated that service endpoint-test2 in namespace services-8557 exposes endpoints map[pod2:[80]] (1.035766866s elapsed)
STEP: Deleting pod pod2 in namespace services-8557
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8557 to expose endpoints map[]
Jun 22 05:20:51.373: INFO: successfully validated that service endpoint-test2 in namespace services-8557 exposes endpoints map[] (9.179079ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:20:51.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8557" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.675 seconds]
[sig-network] Services
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":24,"skipped":404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:20:51.468: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1357
STEP: creating an pod
Jun 22 05:20:51.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-5972 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 22 05:20:51.819: INFO: stderr: ""
Jun 22 05:20:51.819: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Jun 22 05:20:51.819: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 22 05:20:51.819: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5972" to be "running and ready, or succeeded"
Jun 22 05:20:51.826: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.140246ms
Jun 22 05:20:53.832: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.012925819s
Jun 22 05:20:53.832: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 22 05:20:53.832: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun 22 05:20:53.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 logs logs-generator logs-generator --namespace=kubectl-5972'
Jun 22 05:20:53.946: INFO: stderr: ""
Jun 22 05:20:53.946: INFO: stdout: "I0622 05:20:52.984881       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/2jvg 384\nI0622 05:20:53.188905       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/nrp 271\nI0622 05:20:53.385135       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/h77g 496\nI0622 05:20:53.585324       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/kc6 496\nI0622 05:20:53.786962       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/9gw 540\n"
STEP: limiting log lines
Jun 22 05:20:53.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 logs logs-generator logs-generator --namespace=kubectl-5972 --tail=1'
Jun 22 05:20:54.115: INFO: stderr: ""
Jun 22 05:20:54.115: INFO: stdout: "I0622 05:20:53.985424       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/fzjh 480\n"
Jun 22 05:20:54.115: INFO: got output "I0622 05:20:53.985424       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/fzjh 480\n"
STEP: limiting log bytes
Jun 22 05:20:54.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 logs logs-generator logs-generator --namespace=kubectl-5972 --limit-bytes=1'
Jun 22 05:20:54.286: INFO: stderr: ""
Jun 22 05:20:54.286: INFO: stdout: "I"
Jun 22 05:20:54.286: INFO: got output "I"
STEP: exposing timestamps
Jun 22 05:20:54.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 logs logs-generator logs-generator --namespace=kubectl-5972 --tail=1 --timestamps'
Jun 22 05:20:54.437: INFO: stderr: ""
Jun 22 05:20:54.437: INFO: stdout: "2020-06-22T05:20:54.385464487Z I0622 05:20:54.385222       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/qkv 438\n"
Jun 22 05:20:54.437: INFO: got output "2020-06-22T05:20:54.385464487Z I0622 05:20:54.385222       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/qkv 438\n"
STEP: restricting to a time range
Jun 22 05:20:56.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 logs logs-generator logs-generator --namespace=kubectl-5972 --since=1s'
Jun 22 05:20:57.081: INFO: stderr: ""
Jun 22 05:20:57.081: INFO: stdout: "I0622 05:20:56.187396       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/cxq6 440\nI0622 05:20:56.385055       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/79nt 347\nI0622 05:20:56.585414       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/jr9n 239\nI0622 05:20:56.785098       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/gwx 464\nI0622 05:20:56.985115       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/gjp9 577\n"
Jun 22 05:20:57.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 logs logs-generator logs-generator --namespace=kubectl-5972 --since=24h'
Jun 22 05:20:57.216: INFO: stderr: ""
Jun 22 05:20:57.216: INFO: stdout: "I0622 05:20:52.984881       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/2jvg 384\nI0622 05:20:53.188905       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/nrp 271\nI0622 05:20:53.385135       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/h77g 496\nI0622 05:20:53.585324       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/kc6 496\nI0622 05:20:53.786962       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/9gw 540\nI0622 05:20:53.985424       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/fzjh 480\nI0622 05:20:54.185111       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/xd6h 272\nI0622 05:20:54.385222       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/qkv 438\nI0622 05:20:54.585100       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/ww2 305\nI0622 05:20:54.787338       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/6fb 520\nI0622 05:20:54.988705       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/pzn 590\nI0622 05:20:55.187437       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/pwpk 277\nI0622 05:20:55.386950       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/bx2 238\nI0622 05:20:55.585067       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/bs4 329\nI0622 05:20:55.785137       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/twx 433\nI0622 05:20:55.985169       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/fkg 241\nI0622 05:20:56.187396       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/cxq6 440\nI0622 05:20:56.385055       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/79nt 347\nI0622 05:20:56.585414       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/jr9n 239\nI0622 05:20:56.785098       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/gwx 464\nI0622 05:20:56.985115       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/gjp9 577\nI0622 05:20:57.185225       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/nbf 527\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1363
Jun 22 05:20:57.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete pod logs-generator --namespace=kubectl-5972'
Jun 22 05:20:59.847: INFO: stderr: ""
Jun 22 05:20:59.847: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:20:59.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5972" for this suite.

• [SLOW TEST:8.402 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1353
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":25,"skipped":430,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:20:59.872: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3880
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:21:03.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3880" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":26,"skipped":437,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:21:03.126: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2438
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:21:03.299: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:21:09.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2438" for this suite.

• [SLOW TEST:6.262 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":27,"skipped":462,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:21:09.388: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 05:21:09.588: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87c34e0c-af1e-4d68-9c11-b393edb695ef" in namespace "projected-5583" to be "success or failure"
Jun 22 05:21:09.595: INFO: Pod "downwardapi-volume-87c34e0c-af1e-4d68-9c11-b393edb695ef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.569934ms
Jun 22 05:21:11.602: INFO: Pod "downwardapi-volume-87c34e0c-af1e-4d68-9c11-b393edb695ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013018971s
STEP: Saw pod success
Jun 22 05:21:11.602: INFO: Pod "downwardapi-volume-87c34e0c-af1e-4d68-9c11-b393edb695ef" satisfied condition "success or failure"
Jun 22 05:21:11.609: INFO: Trying to get logs from node node4 pod downwardapi-volume-87c34e0c-af1e-4d68-9c11-b393edb695ef container client-container: <nil>
STEP: delete the pod
Jun 22 05:21:11.664: INFO: Waiting for pod downwardapi-volume-87c34e0c-af1e-4d68-9c11-b393edb695ef to disappear
Jun 22 05:21:11.673: INFO: Pod downwardapi-volume-87c34e0c-af1e-4d68-9c11-b393edb695ef no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:21:11.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5583" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":28,"skipped":491,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:21:11.695: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8802
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun 22 05:21:11.883: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 05:21:11.907: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 05:21:11.913: INFO: 
Logging pods the kubelet thinks is on node node1 before test
Jun 22 05:21:11.931: INFO: kube-proxy-qbghn from kube-system started at 2020-06-22 04:48:30 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 05:21:11.931: INFO: kube-controller-manager-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 05:21:11.931: INFO: kube-scheduler-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 05:21:11.931: INFO: metrics-server-76b7895b66-f4lch from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 05:21:11.931: INFO: cocktail-alarm-collector-6cfd6bbb9c-kl9zt from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container collector ready: true, restart count 0
Jun 22 05:21:11.931: INFO: calico-node-qkngx from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 05:21:11.931: INFO: haproxy-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container haproxy ready: true, restart count 1
Jun 22 05:21:11.931: INFO: calico-kube-controllers-77c4b7448-pvlhp from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 05:21:11.931: INFO: kube-apiserver-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 05:21:11.931: INFO: cocktail-batch-server-75c78755c5-c94nd from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container batch ready: true, restart count 0
Jun 22 05:21:11.931: INFO: cocktail-monitoring-76bc9fcf6f-r9tw9 from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container monitoring ready: true, restart count 0
Jun 22 05:21:11.931: INFO: cocktail-api-cmdb-0 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 05:21:11.931: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-kwffd from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:21:11.931: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 05:21:11.931: INFO: coredns-ddbf59985-7mcct from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container coredns ready: true, restart count 0
Jun 22 05:21:11.931: INFO: cocktail-cluster-api-69794f5b78-9htrk from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:11.931: INFO: 	Container cluster-api ready: true, restart count 1
Jun 22 05:21:11.931: INFO: 
Logging pods the kubelet thinks is on node node2 before test
Jun 22 05:21:12.003: INFO: kube-apiserver-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 05:21:12.003: INFO: kube-controller-manager-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 05:21:12.003: INFO: calico-node-xt4w8 from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 05:21:12.003: INFO: kube-proxy-chh9k from kube-system started at 2020-06-22 04:48:19 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 05:21:12.003: INFO: cocktail-metric-collector-5987649879-49kjv from cocktail-system started at 2020-06-22 04:50:15 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container statcollector ready: true, restart count 0
Jun 22 05:21:12.003: INFO: cocktail-dashboard-6bd44cf6c8-mmhlx from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container cocktail-dashboard ready: true, restart count 0
Jun 22 05:21:12.003: INFO: cocktail-build-queue-678588c57d-h5dd4 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container nats-streaming ready: true, restart count 0
Jun 22 05:21:12.003: INFO: cocktail-api-cmdb-2 from cocktail-system started at 2020-06-22 04:52:57 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 05:21:12.003: INFO: haproxy-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 05:21:12.003: INFO: kube-scheduler-node2 from kube-system started at 2020-06-22 04:48:10 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 05:21:12.003: INFO: cocktail-dashboard-queue-845dd499b5-zzxrl from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container dashboard-queue ready: true, restart count 0
Jun 22 05:21:12.003: INFO: addon-manager-5bb9bbcdcc-j2qhp from cocktail-addon started at 2020-06-22 04:49:54 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container addon-manager ready: true, restart count 0
Jun 22 05:21:12.003: INFO: local-storage-nfs-client-provisioner-745d66655-5qmjz from cocktail-addon started at 2020-06-22 04:50:09 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container nfs-client-provisioner ready: true, restart count 0
Jun 22 05:21:12.003: INFO: cocktail-cluster-health-checker-7456f8c49-x62nj from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container checker ready: true, restart count 0
Jun 22 05:21:12.003: INFO: cocktail-dashboard-session-7fb7444f46-xcm2x from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container dashboard-session ready: true, restart count 0
Jun 22 05:21:12.003: INFO: cocktail-api-server-84467f9bf8-8h74n from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container api-server ready: true, restart count 0
Jun 22 05:21:12.003: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-jrq4m from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:21:12.003: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:21:12.003: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 05:21:12.003: INFO: 
Logging pods the kubelet thinks is on node node3 before test
Jun 22 05:21:12.030: INFO: kube-apiserver-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 05:21:12.030: INFO: metrics-server-76b7895b66-nz47p from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 05:21:12.030: INFO: cocktail-build-api-7968df5d66-kqcnw from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container build-api ready: true, restart count 0
Jun 22 05:21:12.030: INFO: kube-controller-manager-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 05:21:12.030: INFO: calico-node-lhxv5 from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 05:21:12.030: INFO: coredns-ddbf59985-ld84l from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container coredns ready: true, restart count 0
Jun 22 05:21:12.030: INFO: cocktail-monitoring-tsdb-799959dd7c-bg2nj from cocktail-system started at 2020-06-22 04:50:15 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container db ready: true, restart count 0
Jun 22 05:21:12.030: INFO: cocktail-dashboard-proxy-6557bc4fbf-fh5f9 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container dashboard-proxy-01 ready: true, restart count 0
Jun 22 05:21:12.030: INFO: kube-scheduler-node3 from kube-system started at 2020-06-22 04:48:05 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 05:21:12.030: INFO: cocktail-api-cmdb-1 from cocktail-system started at 2020-06-22 04:51:32 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 05:21:12.030: INFO: haproxy-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 05:21:12.030: INFO: kube-proxy-sb9hq from kube-system started at 2020-06-22 04:48:28 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 05:21:12.030: INFO: cocktail-package-75488df5f8-7p55f from cocktail-system started at 2020-06-22 04:50:13 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container package ready: true, restart count 1
Jun 22 05:21:12.030: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-88h64 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:21:12.030: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:21:12.030: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 05:21:12.030: INFO: 
Logging pods the kubelet thinks is on node node4 before test
Jun 22 05:21:12.043: INFO: kube-proxy-42p24 from kube-system started at 2020-06-22 04:49:39 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.043: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 05:21:12.043: INFO: pod-adoption from replication-controller-3880 started at 2020-06-22 05:21:00 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.043: INFO: 	Container pod-adoption ready: false, restart count 0
Jun 22 05:21:12.043: INFO: sonobuoy from sonobuoy started at 2020-06-22 05:09:07 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.043: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 05:21:12.043: INFO: sonobuoy-e2e-job-6934f20e32d94a27 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:21:12.043: INFO: 	Container e2e ready: true, restart count 0
Jun 22 05:21:12.043: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:21:12.043: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-h48z4 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:21:12.043: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:21:12.043: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 05:21:12.043: INFO: haproxy-node4 from kube-system started at 2020-06-22 04:50:03 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.043: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 05:21:12.043: INFO: calico-node-q9h8x from kube-system started at 2020-06-22 04:49:39 +0000 UTC (1 container statuses recorded)
Jun 22 05:21:12.043: INFO: 	Container calico-node ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-7384a05c-4ffe-49f6-a4df-0e87b4412ed0 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-7384a05c-4ffe-49f6-a4df-0e87b4412ed0 off the node node4
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7384a05c-4ffe-49f6-a4df-0e87b4412ed0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:26:18.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8802" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:306.532 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":29,"skipped":505,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:26:18.228: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:26:18.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4217" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":30,"skipped":510,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:26:18.429: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1512
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-7a0e0c09-0f91-4ac1-883c-2731c8d9b73d
STEP: Creating a pod to test consume configMaps
Jun 22 05:26:18.619: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ba37ce4d-64a8-4ed8-a5b7-7c3bab43f518" in namespace "projected-1512" to be "success or failure"
Jun 22 05:26:18.624: INFO: Pod "pod-projected-configmaps-ba37ce4d-64a8-4ed8-a5b7-7c3bab43f518": Phase="Pending", Reason="", readiness=false. Elapsed: 5.416841ms
Jun 22 05:26:20.632: INFO: Pod "pod-projected-configmaps-ba37ce4d-64a8-4ed8-a5b7-7c3bab43f518": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013002071s
STEP: Saw pod success
Jun 22 05:26:20.632: INFO: Pod "pod-projected-configmaps-ba37ce4d-64a8-4ed8-a5b7-7c3bab43f518" satisfied condition "success or failure"
Jun 22 05:26:20.636: INFO: Trying to get logs from node node4 pod pod-projected-configmaps-ba37ce4d-64a8-4ed8-a5b7-7c3bab43f518 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 05:26:20.689: INFO: Waiting for pod pod-projected-configmaps-ba37ce4d-64a8-4ed8-a5b7-7c3bab43f518 to disappear
Jun 22 05:26:20.695: INFO: Pod pod-projected-configmaps-ba37ce4d-64a8-4ed8-a5b7-7c3bab43f518 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:26:20.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1512" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":31,"skipped":536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:26:20.714: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7075
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 05:26:20.944: INFO: Number of nodes with available pods: 0
Jun 22 05:26:20.944: INFO: Node node1 is running more than one daemon pod
Jun 22 05:26:21.966: INFO: Number of nodes with available pods: 0
Jun 22 05:26:21.966: INFO: Node node1 is running more than one daemon pod
Jun 22 05:26:22.959: INFO: Number of nodes with available pods: 2
Jun 22 05:26:22.959: INFO: Node node2 is running more than one daemon pod
Jun 22 05:26:23.961: INFO: Number of nodes with available pods: 4
Jun 22 05:26:23.961: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 22 05:26:24.012: INFO: Number of nodes with available pods: 3
Jun 22 05:26:24.012: INFO: Node node3 is running more than one daemon pod
Jun 22 05:26:25.025: INFO: Number of nodes with available pods: 3
Jun 22 05:26:25.025: INFO: Node node3 is running more than one daemon pod
Jun 22 05:26:26.028: INFO: Number of nodes with available pods: 3
Jun 22 05:26:26.028: INFO: Node node3 is running more than one daemon pod
Jun 22 05:26:27.029: INFO: Number of nodes with available pods: 3
Jun 22 05:26:27.029: INFO: Node node3 is running more than one daemon pod
Jun 22 05:26:28.029: INFO: Number of nodes with available pods: 3
Jun 22 05:26:28.029: INFO: Node node3 is running more than one daemon pod
Jun 22 05:26:29.027: INFO: Number of nodes with available pods: 3
Jun 22 05:26:29.027: INFO: Node node3 is running more than one daemon pod
Jun 22 05:26:30.026: INFO: Number of nodes with available pods: 4
Jun 22 05:26:30.026: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7075, will wait for the garbage collector to delete the pods
Jun 22 05:26:30.105: INFO: Deleting DaemonSet.extensions daemon-set took: 19.41579ms
Jun 22 05:26:30.505: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.327445ms
Jun 22 05:26:43.516: INFO: Number of nodes with available pods: 0
Jun 22 05:26:43.516: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 05:26:43.521: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7075/daemonsets","resourceVersion":"13275"},"items":null}

Jun 22 05:26:43.526: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7075/pods","resourceVersion":"13275"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:26:43.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7075" for this suite.

• [SLOW TEST:22.855 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":32,"skipped":580,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:26:43.569: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 22 05:26:43.745: INFO: Waiting up to 5m0s for pod "pod-022c2528-e2d4-4209-9c46-3a164f45eb66" in namespace "emptydir-8504" to be "success or failure"
Jun 22 05:26:43.750: INFO: Pod "pod-022c2528-e2d4-4209-9c46-3a164f45eb66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.312829ms
Jun 22 05:26:45.761: INFO: Pod "pod-022c2528-e2d4-4209-9c46-3a164f45eb66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015502382s
STEP: Saw pod success
Jun 22 05:26:45.761: INFO: Pod "pod-022c2528-e2d4-4209-9c46-3a164f45eb66" satisfied condition "success or failure"
Jun 22 05:26:45.766: INFO: Trying to get logs from node node4 pod pod-022c2528-e2d4-4209-9c46-3a164f45eb66 container test-container: <nil>
STEP: delete the pod
Jun 22 05:26:45.798: INFO: Waiting for pod pod-022c2528-e2d4-4209-9c46-3a164f45eb66 to disappear
Jun 22 05:26:45.803: INFO: Pod pod-022c2528-e2d4-4209-9c46-3a164f45eb66 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:26:45.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8504" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":33,"skipped":581,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:26:45.821: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2626
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun 22 05:26:46.018: INFO: Waiting up to 5m0s for pod "downward-api-aee5bc16-13bf-470a-a50c-855628372d5d" in namespace "downward-api-2626" to be "success or failure"
Jun 22 05:26:46.026: INFO: Pod "downward-api-aee5bc16-13bf-470a-a50c-855628372d5d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.72479ms
Jun 22 05:26:48.040: INFO: Pod "downward-api-aee5bc16-13bf-470a-a50c-855628372d5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021003356s
STEP: Saw pod success
Jun 22 05:26:48.040: INFO: Pod "downward-api-aee5bc16-13bf-470a-a50c-855628372d5d" satisfied condition "success or failure"
Jun 22 05:26:48.045: INFO: Trying to get logs from node node4 pod downward-api-aee5bc16-13bf-470a-a50c-855628372d5d container dapi-container: <nil>
STEP: delete the pod
Jun 22 05:26:48.105: INFO: Waiting for pod downward-api-aee5bc16-13bf-470a-a50c-855628372d5d to disappear
Jun 22 05:26:48.110: INFO: Pod downward-api-aee5bc16-13bf-470a-a50c-855628372d5d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:26:48.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2626" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":34,"skipped":588,"failed":0}

------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:26:48.130: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-9628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Jun 22 05:26:48.309: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 05:27:48.357: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:27:48.362: INFO: Starting informer...
STEP: Starting pods...
Jun 22 05:27:48.591: INFO: Pod1 is running on node4. Tainting Node
Jun 22 05:27:50.827: INFO: Pod2 is running on node4. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun 22 05:27:57.971: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 22 05:28:18.331: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:28:18.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9628" for this suite.

• [SLOW TEST:90.242 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":35,"skipped":588,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:28:18.371: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Jun 22 05:28:18.545: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-119706190 proxy --unix-socket=/tmp/kubectl-proxy-unix244262301/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:28:18.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4441" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":36,"skipped":592,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:28:18.639: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:28:20.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1778" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":37,"skipped":598,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:28:20.928: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9601
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-9601
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 05:28:21.119: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 05:28:39.325: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.37.149.15 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9601 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:28:39.325: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:28:40.498: INFO: Found all expected endpoints: [netserver-0]
Jun 22 05:28:40.506: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.37.11.14 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9601 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:28:40.506: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:28:41.669: INFO: Found all expected endpoints: [netserver-1]
Jun 22 05:28:41.676: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.135.13 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9601 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:28:41.676: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:28:42.840: INFO: Found all expected endpoints: [netserver-2]
Jun 22 05:28:42.847: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.41.130.170 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9601 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:28:42.847: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:28:44.004: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:28:44.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9601" for this suite.

• [SLOW TEST:23.099 seconds]
[sig-network] Networking
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":38,"skipped":612,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:28:44.027: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8455
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:28:44.202: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 22 05:28:46.265: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:28:47.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8455" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":39,"skipped":619,"failed":0}

------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:28:47.298: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7221
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 22 05:28:52.020: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7c086358-23a2-4d14-8965-011903dd6778"
Jun 22 05:28:52.020: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7c086358-23a2-4d14-8965-011903dd6778" in namespace "pods-7221" to be "terminated due to deadline exceeded"
Jun 22 05:28:52.027: INFO: Pod "pod-update-activedeadlineseconds-7c086358-23a2-4d14-8965-011903dd6778": Phase="Running", Reason="", readiness=true. Elapsed: 6.855361ms
Jun 22 05:28:54.033: INFO: Pod "pod-update-activedeadlineseconds-7c086358-23a2-4d14-8965-011903dd6778": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.012946199s
Jun 22 05:28:54.033: INFO: Pod "pod-update-activedeadlineseconds-7c086358-23a2-4d14-8965-011903dd6778" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:28:54.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7221" for this suite.

• [SLOW TEST:6.757 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":40,"skipped":619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:28:54.056: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5238
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:28:54.226: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:29:54.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5238" for this suite.

• [SLOW TEST:60.189 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":41,"skipped":657,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:29:54.245: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5840
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Jun 22 05:29:54.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=kubectl-5840 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun 22 05:29:57.096: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun 22 05:29:57.096: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:29:59.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5840" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":280,"completed":42,"skipped":672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:29:59.127: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8043
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8043
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8043
STEP: Creating statefulset with conflicting port in namespace statefulset-8043
STEP: Waiting until pod test-pod will start running in namespace statefulset-8043
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8043
Jun 22 05:30:03.365: INFO: Observed stateful pod in namespace: statefulset-8043, name: ss-0, uid: a282a83d-0abd-4761-bcaa-ce66f4b0c5e0, status phase: Pending. Waiting for statefulset controller to delete.
Jun 22 05:30:03.944: INFO: Observed stateful pod in namespace: statefulset-8043, name: ss-0, uid: a282a83d-0abd-4761-bcaa-ce66f4b0c5e0, status phase: Failed. Waiting for statefulset controller to delete.
Jun 22 05:30:03.956: INFO: Observed stateful pod in namespace: statefulset-8043, name: ss-0, uid: a282a83d-0abd-4761-bcaa-ce66f4b0c5e0, status phase: Failed. Waiting for statefulset controller to delete.
Jun 22 05:30:03.966: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8043
STEP: Removing pod with conflicting port in namespace statefulset-8043
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8043 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun 22 05:30:08.016: INFO: Deleting all statefulset in ns statefulset-8043
Jun 22 05:30:08.020: INFO: Scaling statefulset ss to 0
Jun 22 05:30:18.048: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 05:30:18.052: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:30:18.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8043" for this suite.

• [SLOW TEST:18.967 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":43,"skipped":743,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:30:18.094: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9870
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:30:18.271: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:30:24.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9870" for this suite.

• [SLOW TEST:6.433 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":44,"skipped":750,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:30:24.527: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2075
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jun 22 05:30:24.701: INFO: PodSpec: initContainers in spec.initContainers
Jun 22 05:31:08.335: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-bc8e6b4b-07f5-48c3-8015-7fd9122d4bff", GenerateName:"", Namespace:"init-container-2075", SelfLink:"/api/v1/namespaces/init-container-2075/pods/pod-init-bc8e6b4b-07f5-48c3-8015-7fd9122d4bff", UID:"94feae23-432b-44ef-a5f0-0c462eeb2b04", ResourceVersion:"15181", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63728400624, loc:(*time.Location)(0x7925240)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"701069102"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.41.130.175/32", "cni.projectcalico.org/podIPs":"10.41.130.175/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-t2wjv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002c8c240), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-t2wjv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-t2wjv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-t2wjv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002ee5f68), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node4", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001e40120), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002ee5ff0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002430010)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002430018), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00243001c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400624, loc:(*time.Location)(0x7925240)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400624, loc:(*time.Location)(0x7925240)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400624, loc:(*time.Location)(0x7925240)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400624, loc:(*time.Location)(0x7925240)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.144", PodIP:"10.41.130.175", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.41.130.175"}}, StartTime:(*v1.Time)(0xc002f43fe0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000df3a40)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000df3ab0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://0af33fb2dd30bde2ec8c0eea4ea449bc78b027a12ab7e050c324d5faa4676ed6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0031aa040), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0031aa020), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc002430094)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:31:08.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2075" for this suite.

• [SLOW TEST:43.827 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":45,"skipped":775,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:31:08.354: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4210
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-041945a6-d37d-4804-ab0a-42ea7bd3d509
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-041945a6-d37d-4804-ab0a-42ea7bd3d509
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:31:12.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4210" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":46,"skipped":822,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:31:12.641: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 05:31:13.262: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 05:31:15.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400673, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400673, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400673, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400673, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 05:31:18.318: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:31:18.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4771" for this suite.
STEP: Destroying namespace "webhook-4771-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.922 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":47,"skipped":837,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:31:18.563: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5769
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:31:18.769: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-5ba9b9b9-3c6b-41c2-b63d-e40c089a11db" in namespace "security-context-test-5769" to be "success or failure"
Jun 22 05:31:18.783: INFO: Pod "alpine-nnp-false-5ba9b9b9-3c6b-41c2-b63d-e40c089a11db": Phase="Pending", Reason="", readiness=false. Elapsed: 14.484899ms
Jun 22 05:31:20.788: INFO: Pod "alpine-nnp-false-5ba9b9b9-3c6b-41c2-b63d-e40c089a11db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018986245s
Jun 22 05:31:22.794: INFO: Pod "alpine-nnp-false-5ba9b9b9-3c6b-41c2-b63d-e40c089a11db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02542053s
Jun 22 05:31:24.804: INFO: Pod "alpine-nnp-false-5ba9b9b9-3c6b-41c2-b63d-e40c089a11db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034703195s
Jun 22 05:31:24.804: INFO: Pod "alpine-nnp-false-5ba9b9b9-3c6b-41c2-b63d-e40c089a11db" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:31:24.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5769" for this suite.

• [SLOW TEST:6.272 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:289
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":48,"skipped":844,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:31:24.835: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 05:31:25.600: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 05:31:27.617: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400685, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400685, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400685, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400685, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 05:31:30.653: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:31:30.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3283" for this suite.
STEP: Destroying namespace "webhook-3283-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.073 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":49,"skipped":847,"failed":0}
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:31:30.908: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6345
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Jun 22 05:31:31.115: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-119706190 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:31:31.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6345" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":50,"skipped":847,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:31:31.236: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6242
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 05:31:31.994: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 05:31:34.013: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400692, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400692, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400692, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400691, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 05:31:37.054: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun 22 05:31:37.087: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:31:37.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6242" for this suite.
STEP: Destroying namespace "webhook-6242-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.025 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":51,"skipped":850,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:31:37.261: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1853
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-508640a3-a8ac-4147-865c-6d085dc4bfc1
STEP: Creating a pod to test consume configMaps
Jun 22 05:31:37.478: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dfc9f99b-2e27-4ae9-92a9-c9dc871e62b3" in namespace "projected-1853" to be "success or failure"
Jun 22 05:31:37.494: INFO: Pod "pod-projected-configmaps-dfc9f99b-2e27-4ae9-92a9-c9dc871e62b3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.585021ms
Jun 22 05:31:39.503: INFO: Pod "pod-projected-configmaps-dfc9f99b-2e27-4ae9-92a9-c9dc871e62b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025230703s
STEP: Saw pod success
Jun 22 05:31:39.503: INFO: Pod "pod-projected-configmaps-dfc9f99b-2e27-4ae9-92a9-c9dc871e62b3" satisfied condition "success or failure"
Jun 22 05:31:39.510: INFO: Trying to get logs from node node4 pod pod-projected-configmaps-dfc9f99b-2e27-4ae9-92a9-c9dc871e62b3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 05:31:39.548: INFO: Waiting for pod pod-projected-configmaps-dfc9f99b-2e27-4ae9-92a9-c9dc871e62b3 to disappear
Jun 22 05:31:39.555: INFO: Pod pod-projected-configmaps-dfc9f99b-2e27-4ae9-92a9-c9dc871e62b3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:31:39.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1853" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":52,"skipped":855,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:31:39.582: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3619
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-7817
STEP: Creating secret with name secret-test-146a36ef-adce-4a6d-8d26-819d5a8237cc
STEP: Creating a pod to test consume secrets
Jun 22 05:31:39.963: INFO: Waiting up to 5m0s for pod "pod-secrets-1ee97462-3ab2-4f62-a472-39379501b14c" in namespace "secrets-3619" to be "success or failure"
Jun 22 05:31:39.969: INFO: Pod "pod-secrets-1ee97462-3ab2-4f62-a472-39379501b14c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.547846ms
Jun 22 05:31:41.975: INFO: Pod "pod-secrets-1ee97462-3ab2-4f62-a472-39379501b14c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012265376s
STEP: Saw pod success
Jun 22 05:31:41.975: INFO: Pod "pod-secrets-1ee97462-3ab2-4f62-a472-39379501b14c" satisfied condition "success or failure"
Jun 22 05:31:41.980: INFO: Trying to get logs from node node4 pod pod-secrets-1ee97462-3ab2-4f62-a472-39379501b14c container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 05:31:42.017: INFO: Waiting for pod pod-secrets-1ee97462-3ab2-4f62-a472-39379501b14c to disappear
Jun 22 05:31:42.022: INFO: Pod pod-secrets-1ee97462-3ab2-4f62-a472-39379501b14c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:31:42.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3619" for this suite.
STEP: Destroying namespace "secret-namespace-7817" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":53,"skipped":870,"failed":0}

------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:31:42.047: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7625
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:31:42.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7625" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":54,"skipped":870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:31:42.292: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8917
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8917, will wait for the garbage collector to delete the pods
Jun 22 05:31:46.539: INFO: Deleting Job.batch foo took: 14.823456ms
Jun 22 05:31:46.940: INFO: Terminating Job.batch foo pods took: 400.303404ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:32:23.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8917" for this suite.

• [SLOW TEST:41.369 seconds]
[sig-apps] Job
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":55,"skipped":894,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:32:23.661: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2413
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:32:39.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2413" for this suite.

• [SLOW TEST:16.267 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":56,"skipped":943,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:32:39.929: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6870
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-aa400041-a67e-4f3c-956a-cb9394f6021c
STEP: Creating a pod to test consume configMaps
Jun 22 05:32:40.120: INFO: Waiting up to 5m0s for pod "pod-configmaps-aa43f189-4c8e-46c8-a69b-f7eeda71edd6" in namespace "configmap-6870" to be "success or failure"
Jun 22 05:32:40.127: INFO: Pod "pod-configmaps-aa43f189-4c8e-46c8-a69b-f7eeda71edd6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.32755ms
Jun 22 05:32:42.135: INFO: Pod "pod-configmaps-aa43f189-4c8e-46c8-a69b-f7eeda71edd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01491536s
STEP: Saw pod success
Jun 22 05:32:42.135: INFO: Pod "pod-configmaps-aa43f189-4c8e-46c8-a69b-f7eeda71edd6" satisfied condition "success or failure"
Jun 22 05:32:42.141: INFO: Trying to get logs from node node4 pod pod-configmaps-aa43f189-4c8e-46c8-a69b-f7eeda71edd6 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 05:32:42.178: INFO: Waiting for pod pod-configmaps-aa43f189-4c8e-46c8-a69b-f7eeda71edd6 to disappear
Jun 22 05:32:42.184: INFO: Pod pod-configmaps-aa43f189-4c8e-46c8-a69b-f7eeda71edd6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:32:42.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6870" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":57,"skipped":972,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:32:42.199: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3440
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:32:59.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3440" for this suite.

• [SLOW TEST:17.274 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":58,"skipped":992,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:32:59.472: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1651
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:32:59.655: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3fb54c51-093a-45b0-8e29-2e058a600014" in namespace "security-context-test-1651" to be "success or failure"
Jun 22 05:32:59.661: INFO: Pod "busybox-privileged-false-3fb54c51-093a-45b0-8e29-2e058a600014": Phase="Pending", Reason="", readiness=false. Elapsed: 6.161287ms
Jun 22 05:33:01.668: INFO: Pod "busybox-privileged-false-3fb54c51-093a-45b0-8e29-2e058a600014": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013640604s
Jun 22 05:33:03.679: INFO: Pod "busybox-privileged-false-3fb54c51-093a-45b0-8e29-2e058a600014": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024296057s
Jun 22 05:33:03.679: INFO: Pod "busybox-privileged-false-3fb54c51-093a-45b0-8e29-2e058a600014" satisfied condition "success or failure"
Jun 22 05:33:03.693: INFO: Got logs for pod "busybox-privileged-false-3fb54c51-093a-45b0-8e29-2e058a600014": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:33:03.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1651" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":59,"skipped":1003,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:33:03.712: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:33:25.912: INFO: Container started at 2020-06-22 05:33:05 +0000 UTC, pod became ready at 2020-06-22 05:33:25 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:33:25.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7420" for this suite.

• [SLOW TEST:22.220 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":60,"skipped":1084,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:33:25.933: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7727
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 22 05:33:26.115: INFO: Waiting up to 5m0s for pod "pod-75bd8eeb-9e3e-43ae-9947-4c870e124d38" in namespace "emptydir-7727" to be "success or failure"
Jun 22 05:33:26.131: INFO: Pod "pod-75bd8eeb-9e3e-43ae-9947-4c870e124d38": Phase="Pending", Reason="", readiness=false. Elapsed: 9.283861ms
Jun 22 05:33:28.137: INFO: Pod "pod-75bd8eeb-9e3e-43ae-9947-4c870e124d38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015680546s
Jun 22 05:33:30.144: INFO: Pod "pod-75bd8eeb-9e3e-43ae-9947-4c870e124d38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022209224s
Jun 22 05:33:32.152: INFO: Pod "pod-75bd8eeb-9e3e-43ae-9947-4c870e124d38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030288734s
STEP: Saw pod success
Jun 22 05:33:32.152: INFO: Pod "pod-75bd8eeb-9e3e-43ae-9947-4c870e124d38" satisfied condition "success or failure"
Jun 22 05:33:32.158: INFO: Trying to get logs from node node4 pod pod-75bd8eeb-9e3e-43ae-9947-4c870e124d38 container test-container: <nil>
STEP: delete the pod
Jun 22 05:33:32.190: INFO: Waiting for pod pod-75bd8eeb-9e3e-43ae-9947-4c870e124d38 to disappear
Jun 22 05:33:32.197: INFO: Pod pod-75bd8eeb-9e3e-43ae-9947-4c870e124d38 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:33:32.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7727" for this suite.

• [SLOW TEST:6.281 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":61,"skipped":1105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:33:32.214: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-457
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 05:33:32.908: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 05:33:34.926: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400812, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400812, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400812, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400812, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 05:33:37.963: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:33:50.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-457" for this suite.
STEP: Destroying namespace "webhook-457-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.099 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":62,"skipped":1140,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:33:50.314: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1997
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 22 05:33:50.522: INFO: Waiting up to 5m0s for pod "pod-f26a986c-fff7-443c-97cf-1e5d240d8b2e" in namespace "emptydir-1997" to be "success or failure"
Jun 22 05:33:50.529: INFO: Pod "pod-f26a986c-fff7-443c-97cf-1e5d240d8b2e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.30241ms
Jun 22 05:33:52.536: INFO: Pod "pod-f26a986c-fff7-443c-97cf-1e5d240d8b2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013876759s
STEP: Saw pod success
Jun 22 05:33:52.536: INFO: Pod "pod-f26a986c-fff7-443c-97cf-1e5d240d8b2e" satisfied condition "success or failure"
Jun 22 05:33:52.542: INFO: Trying to get logs from node node4 pod pod-f26a986c-fff7-443c-97cf-1e5d240d8b2e container test-container: <nil>
STEP: delete the pod
Jun 22 05:33:52.578: INFO: Waiting for pod pod-f26a986c-fff7-443c-97cf-1e5d240d8b2e to disappear
Jun 22 05:33:52.590: INFO: Pod pod-f26a986c-fff7-443c-97cf-1e5d240d8b2e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:33:52.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1997" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":63,"skipped":1157,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:33:52.614: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 05:33:52.803: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9dbdc12d-2757-4325-8fcb-b2a5f2bd0d89" in namespace "projected-3521" to be "success or failure"
Jun 22 05:33:52.818: INFO: Pod "downwardapi-volume-9dbdc12d-2757-4325-8fcb-b2a5f2bd0d89": Phase="Pending", Reason="", readiness=false. Elapsed: 14.724174ms
Jun 22 05:33:54.824: INFO: Pod "downwardapi-volume-9dbdc12d-2757-4325-8fcb-b2a5f2bd0d89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020934345s
STEP: Saw pod success
Jun 22 05:33:54.824: INFO: Pod "downwardapi-volume-9dbdc12d-2757-4325-8fcb-b2a5f2bd0d89" satisfied condition "success or failure"
Jun 22 05:33:54.828: INFO: Trying to get logs from node node4 pod downwardapi-volume-9dbdc12d-2757-4325-8fcb-b2a5f2bd0d89 container client-container: <nil>
STEP: delete the pod
Jun 22 05:33:54.864: INFO: Waiting for pod downwardapi-volume-9dbdc12d-2757-4325-8fcb-b2a5f2bd0d89 to disappear
Jun 22 05:33:54.869: INFO: Pod downwardapi-volume-9dbdc12d-2757-4325-8fcb-b2a5f2bd0d89 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:33:54.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3521" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":64,"skipped":1162,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:33:54.890: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-5898
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:33:55.061: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Creating first CR 
Jun 22 05:34:00.678: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-22T05:34:00Z generation:1 name:name1 resourceVersion:16723 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:7eea1edd-b17e-41ac-b7c9-eb56049e778d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun 22 05:34:10.686: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-22T05:34:10Z generation:1 name:name2 resourceVersion:16770 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1237678d-62e2-48f2-a02d-e39438d323f1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun 22 05:34:20.700: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-22T05:34:00Z generation:2 name:name1 resourceVersion:16813 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:7eea1edd-b17e-41ac-b7c9-eb56049e778d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun 22 05:34:30.714: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-22T05:34:10Z generation:2 name:name2 resourceVersion:16856 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1237678d-62e2-48f2-a02d-e39438d323f1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun 22 05:34:40.733: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-22T05:34:00Z generation:2 name:name1 resourceVersion:16898 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:7eea1edd-b17e-41ac-b7c9-eb56049e778d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun 22 05:34:50.751: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-22T05:34:10Z generation:2 name:name2 resourceVersion:16942 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1237678d-62e2-48f2-a02d-e39438d323f1] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:35:01.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5898" for this suite.

• [SLOW TEST:66.393 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":65,"skipped":1163,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:35:01.284: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8414
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jun 22 05:35:01.461: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:35:26.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8414" for this suite.

• [SLOW TEST:25.009 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":66,"skipped":1163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:35:26.296: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 22 05:35:27.020: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 22 05:35:29.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400927, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400927, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400927, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728400927, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 05:35:32.099: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:35:32.105: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:35:38.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2662" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:11.857 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":67,"skipped":1196,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:35:38.153: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5670
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun 22 05:35:38.347: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:35:47.102: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:36:07.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5670" for this suite.

• [SLOW TEST:28.905 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":68,"skipped":1215,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:36:07.059: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4649
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jun 22 05:36:17.290: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0622 05:36:17.290007      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:36:17.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4649" for this suite.

• [SLOW TEST:10.252 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":69,"skipped":1227,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:36:17.311: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4894
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun 22 05:36:17.493: INFO: Waiting up to 5m0s for pod "downward-api-7b0635c9-83c3-46b0-94e6-208aefcdf5a1" in namespace "downward-api-4894" to be "success or failure"
Jun 22 05:36:17.501: INFO: Pod "downward-api-7b0635c9-83c3-46b0-94e6-208aefcdf5a1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.037655ms
Jun 22 05:36:19.508: INFO: Pod "downward-api-7b0635c9-83c3-46b0-94e6-208aefcdf5a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015542864s
STEP: Saw pod success
Jun 22 05:36:19.509: INFO: Pod "downward-api-7b0635c9-83c3-46b0-94e6-208aefcdf5a1" satisfied condition "success or failure"
Jun 22 05:36:19.514: INFO: Trying to get logs from node node4 pod downward-api-7b0635c9-83c3-46b0-94e6-208aefcdf5a1 container dapi-container: <nil>
STEP: delete the pod
Jun 22 05:36:19.561: INFO: Waiting for pod downward-api-7b0635c9-83c3-46b0-94e6-208aefcdf5a1 to disappear
Jun 22 05:36:19.567: INFO: Pod downward-api-7b0635c9-83c3-46b0-94e6-208aefcdf5a1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:36:19.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4894" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":70,"skipped":1272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:36:19.584: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9697
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:36:19.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 version'
Jun 22 05:36:19.853: INFO: stderr: ""
Jun 22 05:36:19.853: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.7\", GitCommit:\"b4455102ef392bf7d594ef96b97a4caa79d729d9\", GitTreeState:\"clean\", BuildDate:\"2020-06-17T11:39:47Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.7\", GitCommit:\"b4455102ef392bf7d594ef96b97a4caa79d729d9\", GitTreeState:\"clean\", BuildDate:\"2020-06-17T11:32:20Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:36:19.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9697" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":71,"skipped":1298,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:36:19.878: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-2050
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 22 05:36:26.114: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2050 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:36:26.114: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:36:26.290: INFO: Exec stderr: ""
Jun 22 05:36:26.290: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2050 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:36:26.290: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:36:26.457: INFO: Exec stderr: ""
Jun 22 05:36:26.458: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2050 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:36:26.458: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:36:26.619: INFO: Exec stderr: ""
Jun 22 05:36:26.619: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2050 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:36:26.619: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:36:26.770: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 22 05:36:26.770: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2050 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:36:26.770: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:36:26.926: INFO: Exec stderr: ""
Jun 22 05:36:26.926: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2050 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:36:26.926: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:36:27.084: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 22 05:36:27.084: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2050 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:36:27.084: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:36:27.251: INFO: Exec stderr: ""
Jun 22 05:36:27.251: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2050 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:36:27.251: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:36:27.412: INFO: Exec stderr: ""
Jun 22 05:36:27.412: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2050 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:36:27.412: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:36:27.562: INFO: Exec stderr: ""
Jun 22 05:36:27.562: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2050 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:36:27.562: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:36:27.740: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:36:27.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2050" for this suite.

• [SLOW TEST:7.880 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":72,"skipped":1306,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:36:27.757: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2248
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Jun 22 05:36:27.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-2248'
Jun 22 05:36:28.354: INFO: stderr: ""
Jun 22 05:36:28.354: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 05:36:28.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2248'
Jun 22 05:36:28.478: INFO: stderr: ""
Jun 22 05:36:28.478: INFO: stdout: "update-demo-nautilus-4fk2h update-demo-nautilus-pkhtn "
Jun 22 05:36:28.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-4fk2h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2248'
Jun 22 05:36:28.591: INFO: stderr: ""
Jun 22 05:36:28.591: INFO: stdout: ""
Jun 22 05:36:28.591: INFO: update-demo-nautilus-4fk2h is created but not running
Jun 22 05:36:33.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2248'
Jun 22 05:36:33.710: INFO: stderr: ""
Jun 22 05:36:33.710: INFO: stdout: "update-demo-nautilus-4fk2h update-demo-nautilus-pkhtn "
Jun 22 05:36:33.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-4fk2h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2248'
Jun 22 05:36:33.834: INFO: stderr: ""
Jun 22 05:36:33.834: INFO: stdout: "true"
Jun 22 05:36:33.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-4fk2h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2248'
Jun 22 05:36:33.939: INFO: stderr: ""
Jun 22 05:36:33.939: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 05:36:33.939: INFO: validating pod update-demo-nautilus-4fk2h
Jun 22 05:36:33.947: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 05:36:33.947: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 05:36:33.947: INFO: update-demo-nautilus-4fk2h is verified up and running
Jun 22 05:36:33.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-pkhtn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2248'
Jun 22 05:36:34.064: INFO: stderr: ""
Jun 22 05:36:34.064: INFO: stdout: "true"
Jun 22 05:36:34.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-pkhtn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2248'
Jun 22 05:36:34.166: INFO: stderr: ""
Jun 22 05:36:34.166: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 05:36:34.166: INFO: validating pod update-demo-nautilus-pkhtn
Jun 22 05:36:34.174: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 05:36:34.174: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 05:36:34.174: INFO: update-demo-nautilus-pkhtn is verified up and running
STEP: rolling-update to new replication controller
Jun 22 05:36:34.176: INFO: scanned /root for discovery docs: <nil>
Jun 22 05:36:34.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2248'
Jun 22 05:36:59.832: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 22 05:36:59.832: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 05:36:59.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2248'
Jun 22 05:36:59.962: INFO: stderr: ""
Jun 22 05:36:59.962: INFO: stdout: "update-demo-kitten-x4xwm update-demo-kitten-zc4s9 "
Jun 22 05:36:59.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-kitten-x4xwm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2248'
Jun 22 05:37:00.080: INFO: stderr: ""
Jun 22 05:37:00.080: INFO: stdout: "true"
Jun 22 05:37:00.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-kitten-x4xwm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2248'
Jun 22 05:37:00.190: INFO: stderr: ""
Jun 22 05:37:00.190: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 22 05:37:00.190: INFO: validating pod update-demo-kitten-x4xwm
Jun 22 05:37:00.200: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 22 05:37:00.200: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 22 05:37:00.200: INFO: update-demo-kitten-x4xwm is verified up and running
Jun 22 05:37:00.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-kitten-zc4s9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2248'
Jun 22 05:37:00.318: INFO: stderr: ""
Jun 22 05:37:00.319: INFO: stdout: "true"
Jun 22 05:37:00.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-kitten-zc4s9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2248'
Jun 22 05:37:00.436: INFO: stderr: ""
Jun 22 05:37:00.436: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 22 05:37:00.436: INFO: validating pod update-demo-kitten-zc4s9
Jun 22 05:37:00.447: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 22 05:37:00.447: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 22 05:37:00.447: INFO: update-demo-kitten-zc4s9 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:37:00.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2248" for this suite.

• [SLOW TEST:32.711 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":73,"skipped":1320,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:37:00.468: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-rzvq
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 05:37:00.667: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rzvq" in namespace "subpath-4006" to be "success or failure"
Jun 22 05:37:00.672: INFO: Pod "pod-subpath-test-downwardapi-rzvq": Phase="Pending", Reason="", readiness=false. Elapsed: 5.566834ms
Jun 22 05:37:02.680: INFO: Pod "pod-subpath-test-downwardapi-rzvq": Phase="Running", Reason="", readiness=true. Elapsed: 2.012852918s
Jun 22 05:37:04.687: INFO: Pod "pod-subpath-test-downwardapi-rzvq": Phase="Running", Reason="", readiness=true. Elapsed: 4.019858746s
Jun 22 05:37:06.699: INFO: Pod "pod-subpath-test-downwardapi-rzvq": Phase="Running", Reason="", readiness=true. Elapsed: 6.032705375s
Jun 22 05:37:08.708: INFO: Pod "pod-subpath-test-downwardapi-rzvq": Phase="Running", Reason="", readiness=true. Elapsed: 8.041305615s
Jun 22 05:37:10.716: INFO: Pod "pod-subpath-test-downwardapi-rzvq": Phase="Running", Reason="", readiness=true. Elapsed: 10.049703842s
Jun 22 05:37:12.725: INFO: Pod "pod-subpath-test-downwardapi-rzvq": Phase="Running", Reason="", readiness=true. Elapsed: 12.057850724s
Jun 22 05:37:14.731: INFO: Pod "pod-subpath-test-downwardapi-rzvq": Phase="Running", Reason="", readiness=true. Elapsed: 14.063977463s
Jun 22 05:37:16.742: INFO: Pod "pod-subpath-test-downwardapi-rzvq": Phase="Running", Reason="", readiness=true. Elapsed: 16.074998454s
Jun 22 05:37:18.747: INFO: Pod "pod-subpath-test-downwardapi-rzvq": Phase="Running", Reason="", readiness=true. Elapsed: 18.079849799s
Jun 22 05:37:20.755: INFO: Pod "pod-subpath-test-downwardapi-rzvq": Phase="Running", Reason="", readiness=true. Elapsed: 20.088538141s
Jun 22 05:37:22.763: INFO: Pod "pod-subpath-test-downwardapi-rzvq": Phase="Running", Reason="", readiness=true. Elapsed: 22.096096071s
Jun 22 05:37:24.771: INFO: Pod "pod-subpath-test-downwardapi-rzvq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.104315884s
STEP: Saw pod success
Jun 22 05:37:24.771: INFO: Pod "pod-subpath-test-downwardapi-rzvq" satisfied condition "success or failure"
Jun 22 05:37:24.779: INFO: Trying to get logs from node node4 pod pod-subpath-test-downwardapi-rzvq container test-container-subpath-downwardapi-rzvq: <nil>
STEP: delete the pod
Jun 22 05:37:24.817: INFO: Waiting for pod pod-subpath-test-downwardapi-rzvq to disappear
Jun 22 05:37:24.822: INFO: Pod pod-subpath-test-downwardapi-rzvq no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rzvq
Jun 22 05:37:24.822: INFO: Deleting pod "pod-subpath-test-downwardapi-rzvq" in namespace "subpath-4006"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:37:24.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4006" for this suite.

• [SLOW TEST:24.381 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":74,"skipped":1322,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:37:24.849: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:37:27.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5585" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":75,"skipped":1327,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:37:27.088: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:37:27.261: INFO: Creating ReplicaSet my-hostname-basic-e6aa27d4-dadd-48f1-a93f-6364b72fdce8
Jun 22 05:37:27.273: INFO: Pod name my-hostname-basic-e6aa27d4-dadd-48f1-a93f-6364b72fdce8: Found 0 pods out of 1
Jun 22 05:37:32.280: INFO: Pod name my-hostname-basic-e6aa27d4-dadd-48f1-a93f-6364b72fdce8: Found 1 pods out of 1
Jun 22 05:37:32.281: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e6aa27d4-dadd-48f1-a93f-6364b72fdce8" is running
Jun 22 05:37:32.285: INFO: Pod "my-hostname-basic-e6aa27d4-dadd-48f1-a93f-6364b72fdce8-8h9tn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 05:37:27 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 05:37:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 05:37:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 05:37:27 +0000 UTC Reason: Message:}])
Jun 22 05:37:32.285: INFO: Trying to dial the pod
Jun 22 05:37:37.305: INFO: Controller my-hostname-basic-e6aa27d4-dadd-48f1-a93f-6364b72fdce8: Got expected result from replica 1 [my-hostname-basic-e6aa27d4-dadd-48f1-a93f-6364b72fdce8-8h9tn]: "my-hostname-basic-e6aa27d4-dadd-48f1-a93f-6364b72fdce8-8h9tn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:37:37.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4507" for this suite.

• [SLOW TEST:10.234 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":76,"skipped":1348,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:37:37.322: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4200
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 22 05:37:37.502: INFO: Waiting up to 5m0s for pod "pod-936fd628-ea37-4f7f-a3da-0705b9a05b69" in namespace "emptydir-4200" to be "success or failure"
Jun 22 05:37:37.510: INFO: Pod "pod-936fd628-ea37-4f7f-a3da-0705b9a05b69": Phase="Pending", Reason="", readiness=false. Elapsed: 7.977234ms
Jun 22 05:37:39.517: INFO: Pod "pod-936fd628-ea37-4f7f-a3da-0705b9a05b69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015547354s
STEP: Saw pod success
Jun 22 05:37:39.517: INFO: Pod "pod-936fd628-ea37-4f7f-a3da-0705b9a05b69" satisfied condition "success or failure"
Jun 22 05:37:39.523: INFO: Trying to get logs from node node4 pod pod-936fd628-ea37-4f7f-a3da-0705b9a05b69 container test-container: <nil>
STEP: delete the pod
Jun 22 05:37:39.557: INFO: Waiting for pod pod-936fd628-ea37-4f7f-a3da-0705b9a05b69 to disappear
Jun 22 05:37:39.562: INFO: Pod pod-936fd628-ea37-4f7f-a3da-0705b9a05b69 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:37:39.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4200" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":77,"skipped":1359,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:37:39.581: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6439
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:37:46.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6439" for this suite.

• [SLOW TEST:7.206 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":78,"skipped":1392,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:37:46.787: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1596
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jun 22 05:37:46.954: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:38:15.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1596" for this suite.

• [SLOW TEST:28.284 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":79,"skipped":1397,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:38:15.071: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9484
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1585
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 05:38:15.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9484'
Jun 22 05:38:15.373: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 05:38:15.373: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Jun 22 05:38:15.383: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Jun 22 05:38:15.385: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jun 22 05:38:15.404: INFO: scanned /root for discovery docs: <nil>
Jun 22 05:38:15.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-9484'
Jun 22 05:38:31.715: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 22 05:38:31.715: INFO: stdout: "Created e2e-test-httpd-rc-b7cd8574e92bd01fb06c1b5dfb25d2f2\nScaling up e2e-test-httpd-rc-b7cd8574e92bd01fb06c1b5dfb25d2f2 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-b7cd8574e92bd01fb06c1b5dfb25d2f2 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-b7cd8574e92bd01fb06c1b5dfb25d2f2 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jun 22 05:38:31.715: INFO: stdout: "Created e2e-test-httpd-rc-b7cd8574e92bd01fb06c1b5dfb25d2f2\nScaling up e2e-test-httpd-rc-b7cd8574e92bd01fb06c1b5dfb25d2f2 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-b7cd8574e92bd01fb06c1b5dfb25d2f2 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-b7cd8574e92bd01fb06c1b5dfb25d2f2 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jun 22 05:38:31.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-9484'
Jun 22 05:38:31.835: INFO: stderr: ""
Jun 22 05:38:31.835: INFO: stdout: "e2e-test-httpd-rc-b7cd8574e92bd01fb06c1b5dfb25d2f2-4g9cg e2e-test-httpd-rc-kr8v7 "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Jun 22 05:38:36.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-9484'
Jun 22 05:38:36.963: INFO: stderr: ""
Jun 22 05:38:36.963: INFO: stdout: "e2e-test-httpd-rc-b7cd8574e92bd01fb06c1b5dfb25d2f2-4g9cg "
Jun 22 05:38:36.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods e2e-test-httpd-rc-b7cd8574e92bd01fb06c1b5dfb25d2f2-4g9cg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9484'
Jun 22 05:38:37.068: INFO: stderr: ""
Jun 22 05:38:37.068: INFO: stdout: "true"
Jun 22 05:38:37.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods e2e-test-httpd-rc-b7cd8574e92bd01fb06c1b5dfb25d2f2-4g9cg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9484'
Jun 22 05:38:37.186: INFO: stderr: ""
Jun 22 05:38:37.186: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jun 22 05:38:37.186: INFO: e2e-test-httpd-rc-b7cd8574e92bd01fb06c1b5dfb25d2f2-4g9cg is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
Jun 22 05:38:37.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete rc e2e-test-httpd-rc --namespace=kubectl-9484'
Jun 22 05:38:37.305: INFO: stderr: ""
Jun 22 05:38:37.305: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:38:37.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9484" for this suite.

• [SLOW TEST:22.257 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1580
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":280,"completed":80,"skipped":1415,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:38:37.329: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1463
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jun 22 05:38:42.083: INFO: Successfully updated pod "annotationupdate8d7d3872-199c-43de-a4cd-ad432e3b8c64"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:38:44.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1463" for this suite.

• [SLOW TEST:6.807 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":81,"skipped":1458,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:38:44.137: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 05:38:45.737: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 05:38:47.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401125, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401125, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401125, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401125, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 05:38:50.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:38:51.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3690" for this suite.
STEP: Destroying namespace "webhook-3690-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.113 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":82,"skipped":1481,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:38:51.250: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9007
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-6382129f-b9bd-42d5-acfc-4717e2b423b9
STEP: Creating secret with name s-test-opt-upd-06422ec3-6f1e-4189-9f99-fee1e750330d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-6382129f-b9bd-42d5-acfc-4717e2b423b9
STEP: Updating secret s-test-opt-upd-06422ec3-6f1e-4189-9f99-fee1e750330d
STEP: Creating secret with name s-test-opt-create-22478b7b-134f-4eef-9235-8c64fbb1c8f6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:38:55.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9007" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":83,"skipped":1485,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:38:55.641: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3465
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3465
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3465
I0622 05:38:55.869808      22 runners.go:189] Created replication controller with name: externalname-service, namespace: services-3465, replica count: 2
Jun 22 05:38:58.924: INFO: Creating new exec pod
I0622 05:38:58.924606      22 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 05:39:01.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=services-3465 execpod6z8jq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 22 05:39:02.247: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 22 05:39:02.247: INFO: stdout: ""
Jun 22 05:39:02.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=services-3465 execpod6z8jq -- /bin/sh -x -c nc -zv -t -w 2 10.104.169.192 80'
Jun 22 05:39:02.541: INFO: stderr: "+ nc -zv -t -w 2 10.104.169.192 80\nConnection to 10.104.169.192 80 port [tcp/http] succeeded!\n"
Jun 22 05:39:02.541: INFO: stdout: ""
Jun 22 05:39:02.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=services-3465 execpod6z8jq -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.143 31295'
Jun 22 05:39:02.834: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.143 31295\nConnection to 192.168.1.143 31295 port [tcp/31295] succeeded!\n"
Jun 22 05:39:02.834: INFO: stdout: ""
Jun 22 05:39:02.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=services-3465 execpod6z8jq -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.141 31295'
Jun 22 05:39:03.150: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.141 31295\nConnection to 192.168.1.141 31295 port [tcp/31295] succeeded!\n"
Jun 22 05:39:03.150: INFO: stdout: ""
Jun 22 05:39:03.150: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:39:03.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3465" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.593 seconds]
[sig-network] Services
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":84,"skipped":1503,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:39:03.235: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5040
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-b264
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 05:39:03.445: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-b264" in namespace "subpath-5040" to be "success or failure"
Jun 22 05:39:03.449: INFO: Pod "pod-subpath-test-secret-b264": Phase="Pending", Reason="", readiness=false. Elapsed: 4.547652ms
Jun 22 05:39:05.461: INFO: Pod "pod-subpath-test-secret-b264": Phase="Running", Reason="", readiness=true. Elapsed: 2.016542973s
Jun 22 05:39:07.466: INFO: Pod "pod-subpath-test-secret-b264": Phase="Running", Reason="", readiness=true. Elapsed: 4.021390367s
Jun 22 05:39:09.476: INFO: Pod "pod-subpath-test-secret-b264": Phase="Running", Reason="", readiness=true. Elapsed: 6.031148442s
Jun 22 05:39:11.483: INFO: Pod "pod-subpath-test-secret-b264": Phase="Running", Reason="", readiness=true. Elapsed: 8.038304413s
Jun 22 05:39:13.490: INFO: Pod "pod-subpath-test-secret-b264": Phase="Running", Reason="", readiness=true. Elapsed: 10.045296931s
Jun 22 05:39:15.497: INFO: Pod "pod-subpath-test-secret-b264": Phase="Running", Reason="", readiness=true. Elapsed: 12.052097595s
Jun 22 05:39:17.507: INFO: Pod "pod-subpath-test-secret-b264": Phase="Running", Reason="", readiness=true. Elapsed: 14.062158166s
Jun 22 05:39:19.514: INFO: Pod "pod-subpath-test-secret-b264": Phase="Running", Reason="", readiness=true. Elapsed: 16.068856198s
Jun 22 05:39:21.521: INFO: Pod "pod-subpath-test-secret-b264": Phase="Running", Reason="", readiness=true. Elapsed: 18.076173073s
Jun 22 05:39:23.530: INFO: Pod "pod-subpath-test-secret-b264": Phase="Running", Reason="", readiness=true. Elapsed: 20.085036899s
Jun 22 05:39:25.536: INFO: Pod "pod-subpath-test-secret-b264": Phase="Running", Reason="", readiness=true. Elapsed: 22.091590578s
Jun 22 05:39:27.545: INFO: Pod "pod-subpath-test-secret-b264": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.099855091s
STEP: Saw pod success
Jun 22 05:39:27.545: INFO: Pod "pod-subpath-test-secret-b264" satisfied condition "success or failure"
Jun 22 05:39:27.552: INFO: Trying to get logs from node node4 pod pod-subpath-test-secret-b264 container test-container-subpath-secret-b264: <nil>
STEP: delete the pod
Jun 22 05:39:27.588: INFO: Waiting for pod pod-subpath-test-secret-b264 to disappear
Jun 22 05:39:27.593: INFO: Pod pod-subpath-test-secret-b264 no longer exists
STEP: Deleting pod pod-subpath-test-secret-b264
Jun 22 05:39:27.593: INFO: Deleting pod "pod-subpath-test-secret-b264" in namespace "subpath-5040"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:39:27.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5040" for this suite.

• [SLOW TEST:24.378 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":85,"skipped":1508,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:39:27.613: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5812
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 22 05:39:27.829: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5812 /api/v1/namespaces/watch-5812/configmaps/e2e-watch-test-resource-version c049508f-7cde-494d-90ab-ed69a947e1e5 19170 0 2020-06-22 05:39:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 05:39:27.829: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5812 /api/v1/namespaces/watch-5812/configmaps/e2e-watch-test-resource-version c049508f-7cde-494d-90ab-ed69a947e1e5 19171 0 2020-06-22 05:39:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:39:27.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5812" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":86,"skipped":1530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:39:27.846: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7003
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 05:39:28.037: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83d20828-62c7-4e07-81eb-177bc2e25db1" in namespace "downward-api-7003" to be "success or failure"
Jun 22 05:39:28.051: INFO: Pod "downwardapi-volume-83d20828-62c7-4e07-81eb-177bc2e25db1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.362185ms
Jun 22 05:39:30.059: INFO: Pod "downwardapi-volume-83d20828-62c7-4e07-81eb-177bc2e25db1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022185429s
STEP: Saw pod success
Jun 22 05:39:30.059: INFO: Pod "downwardapi-volume-83d20828-62c7-4e07-81eb-177bc2e25db1" satisfied condition "success or failure"
Jun 22 05:39:30.066: INFO: Trying to get logs from node node4 pod downwardapi-volume-83d20828-62c7-4e07-81eb-177bc2e25db1 container client-container: <nil>
STEP: delete the pod
Jun 22 05:39:30.112: INFO: Waiting for pod downwardapi-volume-83d20828-62c7-4e07-81eb-177bc2e25db1 to disappear
Jun 22 05:39:30.119: INFO: Pod downwardapi-volume-83d20828-62c7-4e07-81eb-177bc2e25db1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:39:30.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7003" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":87,"skipped":1558,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:39:30.134: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3915
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Jun 22 05:39:30.324: INFO: Waiting up to 5m0s for pod "var-expansion-6cec427e-7dbc-4db6-9ee3-2a7d06b390e4" in namespace "var-expansion-3915" to be "success or failure"
Jun 22 05:39:30.328: INFO: Pod "var-expansion-6cec427e-7dbc-4db6-9ee3-2a7d06b390e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.962031ms
Jun 22 05:39:32.334: INFO: Pod "var-expansion-6cec427e-7dbc-4db6-9ee3-2a7d06b390e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009803475s
STEP: Saw pod success
Jun 22 05:39:32.334: INFO: Pod "var-expansion-6cec427e-7dbc-4db6-9ee3-2a7d06b390e4" satisfied condition "success or failure"
Jun 22 05:39:32.339: INFO: Trying to get logs from node node4 pod var-expansion-6cec427e-7dbc-4db6-9ee3-2a7d06b390e4 container dapi-container: <nil>
STEP: delete the pod
Jun 22 05:39:32.374: INFO: Waiting for pod var-expansion-6cec427e-7dbc-4db6-9ee3-2a7d06b390e4 to disappear
Jun 22 05:39:32.379: INFO: Pod var-expansion-6cec427e-7dbc-4db6-9ee3-2a7d06b390e4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:39:32.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3915" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":88,"skipped":1563,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:39:32.394: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2067
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:39:32.612: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3084b958-e404-4280-9425-6f0f7f0722e1", Controller:(*bool)(0xc002e7c666), BlockOwnerDeletion:(*bool)(0xc002e7c667)}}
Jun 22 05:39:32.624: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"cf0361e1-5038-49a4-9324-f48a6dbbf959", Controller:(*bool)(0xc002e7c866), BlockOwnerDeletion:(*bool)(0xc002e7c867)}}
Jun 22 05:39:32.636: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d629e6fe-878e-4abc-9a87-2203eeaeefba", Controller:(*bool)(0xc002e5c436), BlockOwnerDeletion:(*bool)(0xc002e5c437)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:39:37.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2067" for this suite.

• [SLOW TEST:5.292 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":89,"skipped":1583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:39:37.686: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8131
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jun 22 05:39:40.438: INFO: Successfully updated pod "labelsupdate04ca136a-11ec-4b3e-b534-583ccc31e12c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:39:42.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8131" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":90,"skipped":1621,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:39:42.492: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 05:39:42.669: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1947067-8b37-41ee-b46e-44c606b4bf32" in namespace "downward-api-3732" to be "success or failure"
Jun 22 05:39:42.675: INFO: Pod "downwardapi-volume-d1947067-8b37-41ee-b46e-44c606b4bf32": Phase="Pending", Reason="", readiness=false. Elapsed: 5.595875ms
Jun 22 05:39:44.683: INFO: Pod "downwardapi-volume-d1947067-8b37-41ee-b46e-44c606b4bf32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013928461s
Jun 22 05:39:46.692: INFO: Pod "downwardapi-volume-d1947067-8b37-41ee-b46e-44c606b4bf32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023066113s
STEP: Saw pod success
Jun 22 05:39:46.692: INFO: Pod "downwardapi-volume-d1947067-8b37-41ee-b46e-44c606b4bf32" satisfied condition "success or failure"
Jun 22 05:39:46.700: INFO: Trying to get logs from node node4 pod downwardapi-volume-d1947067-8b37-41ee-b46e-44c606b4bf32 container client-container: <nil>
STEP: delete the pod
Jun 22 05:39:46.743: INFO: Waiting for pod downwardapi-volume-d1947067-8b37-41ee-b46e-44c606b4bf32 to disappear
Jun 22 05:39:46.752: INFO: Pod downwardapi-volume-d1947067-8b37-41ee-b46e-44c606b4bf32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:39:46.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3732" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":91,"skipped":1633,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:39:46.772: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3364
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-6d52bcfb-5abf-4ae3-aa5c-9b678db8666a
STEP: Creating a pod to test consume secrets
Jun 22 05:39:46.971: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b053d4e9-05bf-4b1b-9761-ae7313acb6d8" in namespace "projected-3364" to be "success or failure"
Jun 22 05:39:46.978: INFO: Pod "pod-projected-secrets-b053d4e9-05bf-4b1b-9761-ae7313acb6d8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.259228ms
Jun 22 05:39:48.983: INFO: Pod "pod-projected-secrets-b053d4e9-05bf-4b1b-9761-ae7313acb6d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012605933s
Jun 22 05:39:50.994: INFO: Pod "pod-projected-secrets-b053d4e9-05bf-4b1b-9761-ae7313acb6d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02346126s
STEP: Saw pod success
Jun 22 05:39:50.994: INFO: Pod "pod-projected-secrets-b053d4e9-05bf-4b1b-9761-ae7313acb6d8" satisfied condition "success or failure"
Jun 22 05:39:51.004: INFO: Trying to get logs from node node4 pod pod-projected-secrets-b053d4e9-05bf-4b1b-9761-ae7313acb6d8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 05:39:51.036: INFO: Waiting for pod pod-projected-secrets-b053d4e9-05bf-4b1b-9761-ae7313acb6d8 to disappear
Jun 22 05:39:51.041: INFO: Pod pod-projected-secrets-b053d4e9-05bf-4b1b-9761-ae7313acb6d8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:39:51.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3364" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":92,"skipped":1647,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:39:51.057: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3647
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:39:51.225: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun 22 05:39:59.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-3647 create -f -'
Jun 22 05:40:00.485: INFO: stderr: ""
Jun 22 05:40:00.485: INFO: stdout: "e2e-test-crd-publish-openapi-8701-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 22 05:40:00.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-3647 delete e2e-test-crd-publish-openapi-8701-crds test-foo'
Jun 22 05:40:00.613: INFO: stderr: ""
Jun 22 05:40:00.613: INFO: stdout: "e2e-test-crd-publish-openapi-8701-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 22 05:40:00.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-3647 apply -f -'
Jun 22 05:40:00.990: INFO: stderr: ""
Jun 22 05:40:00.990: INFO: stdout: "e2e-test-crd-publish-openapi-8701-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 22 05:40:00.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-3647 delete e2e-test-crd-publish-openapi-8701-crds test-foo'
Jun 22 05:40:01.117: INFO: stderr: ""
Jun 22 05:40:01.117: INFO: stdout: "e2e-test-crd-publish-openapi-8701-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun 22 05:40:01.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-3647 create -f -'
Jun 22 05:40:01.339: INFO: rc: 1
Jun 22 05:40:01.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-3647 apply -f -'
Jun 22 05:40:01.686: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun 22 05:40:01.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-3647 create -f -'
Jun 22 05:40:02.010: INFO: rc: 1
Jun 22 05:40:02.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-3647 apply -f -'
Jun 22 05:40:02.326: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun 22 05:40:02.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 explain e2e-test-crd-publish-openapi-8701-crds'
Jun 22 05:40:02.584: INFO: stderr: ""
Jun 22 05:40:02.584: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8701-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun 22 05:40:02.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 explain e2e-test-crd-publish-openapi-8701-crds.metadata'
Jun 22 05:40:02.943: INFO: stderr: ""
Jun 22 05:40:02.943: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8701-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 22 05:40:02.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 explain e2e-test-crd-publish-openapi-8701-crds.spec'
Jun 22 05:40:03.271: INFO: stderr: ""
Jun 22 05:40:03.271: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8701-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 22 05:40:03.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 explain e2e-test-crd-publish-openapi-8701-crds.spec.bars'
Jun 22 05:40:03.613: INFO: stderr: ""
Jun 22 05:40:03.613: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8701-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun 22 05:40:03.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 explain e2e-test-crd-publish-openapi-8701-crds.spec.bars2'
Jun 22 05:40:03.934: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:40:07.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3647" for this suite.

• [SLOW TEST:16.787 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":93,"skipped":1666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:40:07.848: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6591
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 22 05:40:08.026: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 22 05:40:13.036: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:40:14.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6591" for this suite.

• [SLOW TEST:6.243 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":94,"skipped":1702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:40:14.091: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-4228
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4228 to expose endpoints map[]
Jun 22 05:40:14.288: INFO: Get endpoints failed (5.951072ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jun 22 05:40:15.295: INFO: successfully validated that service multi-endpoint-test in namespace services-4228 exposes endpoints map[] (1.012297635s elapsed)
STEP: Creating pod pod1 in namespace services-4228
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4228 to expose endpoints map[pod1:[100]]
Jun 22 05:40:17.351: INFO: successfully validated that service multi-endpoint-test in namespace services-4228 exposes endpoints map[pod1:[100]] (2.039393482s elapsed)
STEP: Creating pod pod2 in namespace services-4228
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4228 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 22 05:40:19.415: INFO: successfully validated that service multi-endpoint-test in namespace services-4228 exposes endpoints map[pod1:[100] pod2:[101]] (2.056904761s elapsed)
STEP: Deleting pod pod1 in namespace services-4228
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4228 to expose endpoints map[pod2:[101]]
Jun 22 05:40:20.460: INFO: successfully validated that service multi-endpoint-test in namespace services-4228 exposes endpoints map[pod2:[101]] (1.034154852s elapsed)
STEP: Deleting pod pod2 in namespace services-4228
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4228 to expose endpoints map[]
Jun 22 05:40:21.493: INFO: successfully validated that service multi-endpoint-test in namespace services-4228 exposes endpoints map[] (1.022019714s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:40:21.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4228" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.478 seconds]
[sig-network] Services
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":95,"skipped":1725,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:40:21.571: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:40:21.795: INFO: Create a RollingUpdate DaemonSet
Jun 22 05:40:21.803: INFO: Check that daemon pods launch on every node of the cluster
Jun 22 05:40:21.813: INFO: Number of nodes with available pods: 0
Jun 22 05:40:21.813: INFO: Node node1 is running more than one daemon pod
Jun 22 05:40:22.832: INFO: Number of nodes with available pods: 0
Jun 22 05:40:22.832: INFO: Node node1 is running more than one daemon pod
Jun 22 05:40:23.842: INFO: Number of nodes with available pods: 1
Jun 22 05:40:23.842: INFO: Node node2 is running more than one daemon pod
Jun 22 05:40:24.830: INFO: Number of nodes with available pods: 4
Jun 22 05:40:24.830: INFO: Number of running nodes: 4, number of available pods: 4
Jun 22 05:40:24.830: INFO: Update the DaemonSet to trigger a rollout
Jun 22 05:40:24.843: INFO: Updating DaemonSet daemon-set
Jun 22 05:40:28.878: INFO: Roll back the DaemonSet before rollout is complete
Jun 22 05:40:28.892: INFO: Updating DaemonSet daemon-set
Jun 22 05:40:28.892: INFO: Make sure DaemonSet rollback is complete
Jun 22 05:40:28.898: INFO: Wrong image for pod: daemon-set-zhtmq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 22 05:40:28.898: INFO: Pod daemon-set-zhtmq is not available
Jun 22 05:40:29.925: INFO: Wrong image for pod: daemon-set-zhtmq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 22 05:40:29.925: INFO: Pod daemon-set-zhtmq is not available
Jun 22 05:40:30.925: INFO: Wrong image for pod: daemon-set-zhtmq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 22 05:40:30.925: INFO: Pod daemon-set-zhtmq is not available
Jun 22 05:40:31.923: INFO: Wrong image for pod: daemon-set-zhtmq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 22 05:40:31.923: INFO: Pod daemon-set-zhtmq is not available
Jun 22 05:40:32.925: INFO: Pod daemon-set-dcdk5 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7170, will wait for the garbage collector to delete the pods
Jun 22 05:40:33.019: INFO: Deleting DaemonSet.extensions daemon-set took: 14.492627ms
Jun 22 05:40:33.530: INFO: Terminating DaemonSet.extensions daemon-set pods took: 510.995818ms
Jun 22 05:41:45.634: INFO: Number of nodes with available pods: 0
Jun 22 05:41:45.634: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 05:41:45.641: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7170/daemonsets","resourceVersion":"20281"},"items":null}

Jun 22 05:41:45.647: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7170/pods","resourceVersion":"20281"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:41:45.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7170" for this suite.

• [SLOW TEST:84.123 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":96,"skipped":1741,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:41:45.694: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4512
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4512.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4512.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 05:41:47.927: INFO: DNS probes using dns-test-20ce7311-c2f3-4fd9-9e07-5ed12e878cca succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4512.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4512.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 05:41:50.002: INFO: File wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local from pod  dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 05:41:50.009: INFO: File jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local from pod  dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 05:41:50.009: INFO: Lookups using dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac failed for: [wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local]

Jun 22 05:41:55.023: INFO: File wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local from pod  dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 05:41:55.030: INFO: File jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local from pod  dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 05:41:55.030: INFO: Lookups using dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac failed for: [wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local]

Jun 22 05:42:00.017: INFO: File wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local from pod  dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 05:42:00.025: INFO: File jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local from pod  dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 05:42:00.025: INFO: Lookups using dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac failed for: [wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local]

Jun 22 05:42:05.024: INFO: File wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local from pod  dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 05:42:05.032: INFO: File jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local from pod  dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 05:42:05.033: INFO: Lookups using dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac failed for: [wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local]

Jun 22 05:42:10.017: INFO: File wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local from pod  dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 05:42:10.026: INFO: File jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local from pod  dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 05:42:10.026: INFO: Lookups using dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac failed for: [wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local]

Jun 22 05:42:15.018: INFO: File wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local from pod  dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 05:42:15.026: INFO: File jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local from pod  dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 05:42:15.026: INFO: Lookups using dns-4512/dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac failed for: [wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local]

Jun 22 05:42:20.021: INFO: DNS probes using dns-test-ba20cb93-53c1-44f4-890a-1a5efc79abac succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4512.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4512.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4512.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4512.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 05:42:24.171: INFO: DNS probes using dns-test-0094d87c-8330-4199-acb2-444247f35a1c succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:42:24.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4512" for this suite.

• [SLOW TEST:38.576 seconds]
[sig-network] DNS
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":97,"skipped":1800,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:42:24.271: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3452
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-6f49ab6c-cbe0-4957-b0df-d49fc10ed81f
STEP: Creating a pod to test consume configMaps
Jun 22 05:42:24.483: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a38bb7f7-3b4c-4a5f-8101-0f22d9bb1442" in namespace "projected-3452" to be "success or failure"
Jun 22 05:42:24.489: INFO: Pod "pod-projected-configmaps-a38bb7f7-3b4c-4a5f-8101-0f22d9bb1442": Phase="Pending", Reason="", readiness=false. Elapsed: 5.571605ms
Jun 22 05:42:26.494: INFO: Pod "pod-projected-configmaps-a38bb7f7-3b4c-4a5f-8101-0f22d9bb1442": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011139073s
STEP: Saw pod success
Jun 22 05:42:26.494: INFO: Pod "pod-projected-configmaps-a38bb7f7-3b4c-4a5f-8101-0f22d9bb1442" satisfied condition "success or failure"
Jun 22 05:42:26.499: INFO: Trying to get logs from node node4 pod pod-projected-configmaps-a38bb7f7-3b4c-4a5f-8101-0f22d9bb1442 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 05:42:26.575: INFO: Waiting for pod pod-projected-configmaps-a38bb7f7-3b4c-4a5f-8101-0f22d9bb1442 to disappear
Jun 22 05:42:26.581: INFO: Pod pod-projected-configmaps-a38bb7f7-3b4c-4a5f-8101-0f22d9bb1442 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:42:26.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3452" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":98,"skipped":1806,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:42:26.608: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 05:42:26.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84f4e782-299c-4a0e-be3f-2b994dcc787e" in namespace "downward-api-7035" to be "success or failure"
Jun 22 05:42:26.809: INFO: Pod "downwardapi-volume-84f4e782-299c-4a0e-be3f-2b994dcc787e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.101948ms
Jun 22 05:42:28.822: INFO: Pod "downwardapi-volume-84f4e782-299c-4a0e-be3f-2b994dcc787e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023461266s
STEP: Saw pod success
Jun 22 05:42:28.822: INFO: Pod "downwardapi-volume-84f4e782-299c-4a0e-be3f-2b994dcc787e" satisfied condition "success or failure"
Jun 22 05:42:28.826: INFO: Trying to get logs from node node4 pod downwardapi-volume-84f4e782-299c-4a0e-be3f-2b994dcc787e container client-container: <nil>
STEP: delete the pod
Jun 22 05:42:28.871: INFO: Waiting for pod downwardapi-volume-84f4e782-299c-4a0e-be3f-2b994dcc787e to disappear
Jun 22 05:42:28.875: INFO: Pod downwardapi-volume-84f4e782-299c-4a0e-be3f-2b994dcc787e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:42:28.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7035" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":99,"skipped":1866,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:42:28.897: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1684
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun 22 05:42:31.624: INFO: Successfully updated pod "adopt-release-dmqfk"
STEP: Checking that the Job readopts the Pod
Jun 22 05:42:31.625: INFO: Waiting up to 15m0s for pod "adopt-release-dmqfk" in namespace "job-1684" to be "adopted"
Jun 22 05:42:31.634: INFO: Pod "adopt-release-dmqfk": Phase="Running", Reason="", readiness=true. Elapsed: 9.410814ms
Jun 22 05:42:33.641: INFO: Pod "adopt-release-dmqfk": Phase="Running", Reason="", readiness=true. Elapsed: 2.016535164s
Jun 22 05:42:33.641: INFO: Pod "adopt-release-dmqfk" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun 22 05:42:34.172: INFO: Successfully updated pod "adopt-release-dmqfk"
STEP: Checking that the Job releases the Pod
Jun 22 05:42:34.172: INFO: Waiting up to 15m0s for pod "adopt-release-dmqfk" in namespace "job-1684" to be "released"
Jun 22 05:42:34.182: INFO: Pod "adopt-release-dmqfk": Phase="Running", Reason="", readiness=true. Elapsed: 9.690669ms
Jun 22 05:42:34.182: INFO: Pod "adopt-release-dmqfk" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:42:34.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1684" for this suite.

• [SLOW TEST:5.326 seconds]
[sig-apps] Job
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":100,"skipped":1871,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:42:34.223: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9130
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jun 22 05:42:34.445: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:42:42.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9130" for this suite.

• [SLOW TEST:8.535 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":101,"skipped":1895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:42:42.759: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7199
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:42:42.948: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 22 05:42:51.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-7199 create -f -'
Jun 22 05:42:52.198: INFO: stderr: ""
Jun 22 05:42:52.198: INFO: stdout: "e2e-test-crd-publish-openapi-2347-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 22 05:42:52.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-7199 delete e2e-test-crd-publish-openapi-2347-crds test-cr'
Jun 22 05:42:52.382: INFO: stderr: ""
Jun 22 05:42:52.382: INFO: stdout: "e2e-test-crd-publish-openapi-2347-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 22 05:42:52.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-7199 apply -f -'
Jun 22 05:42:52.736: INFO: stderr: ""
Jun 22 05:42:52.736: INFO: stdout: "e2e-test-crd-publish-openapi-2347-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 22 05:42:52.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-7199 delete e2e-test-crd-publish-openapi-2347-crds test-cr'
Jun 22 05:42:52.902: INFO: stderr: ""
Jun 22 05:42:52.902: INFO: stdout: "e2e-test-crd-publish-openapi-2347-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 22 05:42:52.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 explain e2e-test-crd-publish-openapi-2347-crds'
Jun 22 05:42:53.262: INFO: stderr: ""
Jun 22 05:42:53.262: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2347-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:42:56.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7199" for this suite.

• [SLOW TEST:14.241 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":102,"skipped":1922,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:42:57.000: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1297
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jun 22 05:42:57.170: INFO: namespace kubectl-1297
Jun 22 05:42:57.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-1297'
Jun 22 05:42:57.488: INFO: stderr: ""
Jun 22 05:42:57.488: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jun 22 05:42:58.494: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 05:42:58.494: INFO: Found 0 / 1
Jun 22 05:42:59.497: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 05:42:59.497: INFO: Found 1 / 1
Jun 22 05:42:59.497: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 05:42:59.504: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 05:42:59.504: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 05:42:59.504: INFO: wait on agnhost-master startup in kubectl-1297 
Jun 22 05:42:59.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 logs agnhost-master-6cq8z agnhost-master --namespace=kubectl-1297'
Jun 22 05:42:59.649: INFO: stderr: ""
Jun 22 05:42:59.649: INFO: stdout: "Paused\n"
STEP: exposing RC
Jun 22 05:42:59.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1297'
Jun 22 05:42:59.820: INFO: stderr: ""
Jun 22 05:42:59.820: INFO: stdout: "service/rm2 exposed\n"
Jun 22 05:42:59.830: INFO: Service rm2 in namespace kubectl-1297 found.
STEP: exposing service
Jun 22 05:43:01.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1297'
Jun 22 05:43:01.988: INFO: stderr: ""
Jun 22 05:43:01.988: INFO: stdout: "service/rm3 exposed\n"
Jun 22 05:43:01.995: INFO: Service rm3 in namespace kubectl-1297 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:43:04.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1297" for this suite.

• [SLOW TEST:7.024 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1188
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":103,"skipped":1929,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:43:04.024: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 22 05:43:10.281: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 05:43:10.295: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 05:43:12.295: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 05:43:12.300: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 05:43:14.296: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 05:43:14.305: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 05:43:16.295: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 05:43:16.304: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 05:43:18.295: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 05:43:18.303: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 05:43:20.295: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 05:43:20.304: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 05:43:22.295: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 05:43:22.305: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 05:43:24.295: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 05:43:24.303: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:43:24.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1644" for this suite.

• [SLOW TEST:20.295 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":104,"skipped":1938,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:43:24.320: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:43:26.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7071" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":105,"skipped":1949,"failed":0}

------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:43:26.561: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5266
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-7d4ef3f6-f0c7-47be-b50f-04acd181fafd
STEP: Creating secret with name s-test-opt-upd-a8680c76-be78-453e-81b3-3aab217daf30
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7d4ef3f6-f0c7-47be-b50f-04acd181fafd
STEP: Updating secret s-test-opt-upd-a8680c76-be78-453e-81b3-3aab217daf30
STEP: Creating secret with name s-test-opt-create-7cea1c58-e9bf-43a5-a53e-412c23a72e38
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:43:30.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5266" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":106,"skipped":1949,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:43:30.955: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9311
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-e2887e2e-92a8-4adf-8849-404a10ebb660
STEP: Creating a pod to test consume secrets
Jun 22 05:43:31.151: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b8370d0a-5fce-426e-a6b6-cea50b27da7b" in namespace "projected-9311" to be "success or failure"
Jun 22 05:43:31.156: INFO: Pod "pod-projected-secrets-b8370d0a-5fce-426e-a6b6-cea50b27da7b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.793768ms
Jun 22 05:43:33.162: INFO: Pod "pod-projected-secrets-b8370d0a-5fce-426e-a6b6-cea50b27da7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011276186s
Jun 22 05:43:35.170: INFO: Pod "pod-projected-secrets-b8370d0a-5fce-426e-a6b6-cea50b27da7b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01860885s
Jun 22 05:43:37.178: INFO: Pod "pod-projected-secrets-b8370d0a-5fce-426e-a6b6-cea50b27da7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026696701s
STEP: Saw pod success
Jun 22 05:43:37.178: INFO: Pod "pod-projected-secrets-b8370d0a-5fce-426e-a6b6-cea50b27da7b" satisfied condition "success or failure"
Jun 22 05:43:37.185: INFO: Trying to get logs from node node1 pod pod-projected-secrets-b8370d0a-5fce-426e-a6b6-cea50b27da7b container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 05:43:37.237: INFO: Waiting for pod pod-projected-secrets-b8370d0a-5fce-426e-a6b6-cea50b27da7b to disappear
Jun 22 05:43:37.244: INFO: Pod pod-projected-secrets-b8370d0a-5fce-426e-a6b6-cea50b27da7b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:43:37.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9311" for this suite.

• [SLOW TEST:6.311 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":107,"skipped":1958,"failed":0}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:43:37.267: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8763
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:44:00.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8763" for this suite.

• [SLOW TEST:23.558 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":108,"skipped":1961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:44:00.827: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-c1911324-0230-4704-8f5e-93a8aaae8dc1
STEP: Creating a pod to test consume secrets
Jun 22 05:44:01.040: INFO: Waiting up to 5m0s for pod "pod-secrets-8826696c-d61a-45c6-bd64-14b556300e7a" in namespace "secrets-3826" to be "success or failure"
Jun 22 05:44:01.044: INFO: Pod "pod-secrets-8826696c-d61a-45c6-bd64-14b556300e7a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.856019ms
Jun 22 05:44:03.049: INFO: Pod "pod-secrets-8826696c-d61a-45c6-bd64-14b556300e7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009262954s
STEP: Saw pod success
Jun 22 05:44:03.049: INFO: Pod "pod-secrets-8826696c-d61a-45c6-bd64-14b556300e7a" satisfied condition "success or failure"
Jun 22 05:44:03.055: INFO: Trying to get logs from node node4 pod pod-secrets-8826696c-d61a-45c6-bd64-14b556300e7a container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 05:44:03.088: INFO: Waiting for pod pod-secrets-8826696c-d61a-45c6-bd64-14b556300e7a to disappear
Jun 22 05:44:03.093: INFO: Pod pod-secrets-8826696c-d61a-45c6-bd64-14b556300e7a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:44:03.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3826" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":109,"skipped":2020,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:44:03.108: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6992
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6992.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6992.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6992.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6992.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6992.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6992.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 05:44:05.382: INFO: DNS probes using dns-6992/dns-test-86fbf3c7-35bd-4d86-8392-b952bd94843e succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:44:05.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6992" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":110,"skipped":2023,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:44:05.451: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:44:16.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9652" for this suite.

• [SLOW TEST:11.272 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":111,"skipped":2035,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:44:16.723: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4638
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1790
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 05:44:16.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-4638'
Jun 22 05:44:17.043: INFO: stderr: ""
Jun 22 05:44:17.043: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun 22 05:44:22.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pod e2e-test-httpd-pod --namespace=kubectl-4638 -o json'
Jun 22 05:44:22.221: INFO: stderr: ""
Jun 22 05:44:22.221: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.41.130.173/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.41.130.173/32\"\n        },\n        \"creationTimestamp\": \"2020-06-22T05:44:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4638\",\n        \"resourceVersion\": \"21790\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4638/pods/e2e-test-httpd-pod\",\n        \"uid\": \"a66929a5-5d12-4d01-999d-3b2eaea76e6b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-w4vxb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node4\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 30\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 30\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-w4vxb\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-w4vxb\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-22T05:44:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-22T05:44:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-22T05:44:18Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-22T05:44:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://a105f53f4dfebca935a859e08b65c3e23f322eb13d214640c52e4ac642bcd85a\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-06-22T05:44:18Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.144\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.41.130.173\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.41.130.173\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-06-22T05:44:17Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 22 05:44:22.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 replace -f - --namespace=kubectl-4638'
Jun 22 05:44:22.494: INFO: stderr: ""
Jun 22 05:44:22.494: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1795
Jun 22 05:44:22.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete pods e2e-test-httpd-pod --namespace=kubectl-4638'
Jun 22 05:44:33.511: INFO: stderr: ""
Jun 22 05:44:33.511: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:44:33.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4638" for this suite.

• [SLOW TEST:16.805 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1786
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":112,"skipped":2050,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:44:33.528: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5553
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 05:44:34.280: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 05:44:37.325: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:44:47.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5553" for this suite.
STEP: Destroying namespace "webhook-5553-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.162 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":113,"skipped":2054,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:44:47.690: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4365.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4365.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4365.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4365.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4365.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4365.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 05:44:51.975: INFO: DNS probes using dns-4365/dns-test-09cb5c8e-db5e-4577-b310-6ec8fadbf478 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:44:51.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4365" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":114,"skipped":2061,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:44:52.023: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8320
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 05:44:54.243: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:44:54.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8320" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":115,"skipped":2074,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:44:54.295: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun 22 05:44:54.470: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 05:44:54.496: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 05:44:54.502: INFO: 
Logging pods the kubelet thinks is on node node1 before test
Jun 22 05:44:54.520: INFO: coredns-ddbf59985-7mcct from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container coredns ready: true, restart count 0
Jun 22 05:44:54.520: INFO: cocktail-cluster-api-69794f5b78-9htrk from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container cluster-api ready: true, restart count 1
Jun 22 05:44:54.520: INFO: metrics-server-76b7895b66-f4lch from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 05:44:54.520: INFO: cocktail-alarm-collector-6cfd6bbb9c-kl9zt from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container collector ready: true, restart count 0
Jun 22 05:44:54.520: INFO: kube-proxy-qbghn from kube-system started at 2020-06-22 04:48:30 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 05:44:54.520: INFO: kube-controller-manager-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 05:44:54.520: INFO: kube-scheduler-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 05:44:54.520: INFO: calico-node-qkngx from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 05:44:54.520: INFO: haproxy-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container haproxy ready: true, restart count 1
Jun 22 05:44:54.520: INFO: calico-kube-controllers-77c4b7448-pvlhp from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 05:44:54.520: INFO: cocktail-api-cmdb-0 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 05:44:54.520: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-kwffd from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:44:54.520: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 05:44:54.520: INFO: kube-apiserver-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 05:44:54.520: INFO: cocktail-batch-server-75c78755c5-c94nd from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container batch ready: true, restart count 0
Jun 22 05:44:54.520: INFO: cocktail-monitoring-76bc9fcf6f-r9tw9 from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.520: INFO: 	Container monitoring ready: true, restart count 0
Jun 22 05:44:54.520: INFO: 
Logging pods the kubelet thinks is on node node2 before test
Jun 22 05:44:54.548: INFO: haproxy-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 05:44:54.548: INFO: kube-scheduler-node2 from kube-system started at 2020-06-22 04:48:10 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 05:44:54.548: INFO: cocktail-dashboard-queue-845dd499b5-zzxrl from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container dashboard-queue ready: true, restart count 0
Jun 22 05:44:54.548: INFO: cocktail-api-server-84467f9bf8-8h74n from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container api-server ready: true, restart count 0
Jun 22 05:44:54.548: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-jrq4m from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:44:54.548: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 05:44:54.548: INFO: addon-manager-5bb9bbcdcc-j2qhp from cocktail-addon started at 2020-06-22 04:49:54 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container addon-manager ready: true, restart count 0
Jun 22 05:44:54.548: INFO: local-storage-nfs-client-provisioner-745d66655-5qmjz from cocktail-addon started at 2020-06-22 04:50:09 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container nfs-client-provisioner ready: true, restart count 0
Jun 22 05:44:54.548: INFO: cocktail-cluster-health-checker-7456f8c49-x62nj from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container checker ready: true, restart count 0
Jun 22 05:44:54.548: INFO: cocktail-dashboard-session-7fb7444f46-xcm2x from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container dashboard-session ready: true, restart count 0
Jun 22 05:44:54.548: INFO: cocktail-metric-collector-5987649879-49kjv from cocktail-system started at 2020-06-22 04:50:15 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container statcollector ready: true, restart count 0
Jun 22 05:44:54.548: INFO: cocktail-dashboard-6bd44cf6c8-mmhlx from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container cocktail-dashboard ready: true, restart count 0
Jun 22 05:44:54.548: INFO: kube-apiserver-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 05:44:54.548: INFO: kube-controller-manager-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 05:44:54.548: INFO: calico-node-xt4w8 from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 05:44:54.548: INFO: kube-proxy-chh9k from kube-system started at 2020-06-22 04:48:19 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 05:44:54.548: INFO: cocktail-build-queue-678588c57d-h5dd4 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container nats-streaming ready: true, restart count 0
Jun 22 05:44:54.548: INFO: cocktail-api-cmdb-2 from cocktail-system started at 2020-06-22 04:52:57 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.548: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 05:44:54.548: INFO: 
Logging pods the kubelet thinks is on node node3 before test
Jun 22 05:44:54.577: INFO: kube-scheduler-node3 from kube-system started at 2020-06-22 04:48:05 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 05:44:54.577: INFO: cocktail-api-cmdb-1 from cocktail-system started at 2020-06-22 04:51:32 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 05:44:54.577: INFO: haproxy-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 05:44:54.577: INFO: kube-proxy-sb9hq from kube-system started at 2020-06-22 04:48:28 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 05:44:54.577: INFO: cocktail-package-75488df5f8-7p55f from cocktail-system started at 2020-06-22 04:50:13 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container package ready: true, restart count 1
Jun 22 05:44:54.577: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-88h64 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:44:54.577: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 05:44:54.577: INFO: kube-apiserver-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 05:44:54.577: INFO: metrics-server-76b7895b66-nz47p from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 05:44:54.577: INFO: cocktail-build-api-7968df5d66-kqcnw from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container build-api ready: true, restart count 0
Jun 22 05:44:54.577: INFO: kube-controller-manager-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 05:44:54.577: INFO: calico-node-lhxv5 from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 05:44:54.577: INFO: coredns-ddbf59985-ld84l from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container coredns ready: true, restart count 0
Jun 22 05:44:54.577: INFO: cocktail-monitoring-tsdb-799959dd7c-bg2nj from cocktail-system started at 2020-06-22 04:50:15 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container db ready: true, restart count 0
Jun 22 05:44:54.577: INFO: cocktail-dashboard-proxy-6557bc4fbf-fh5f9 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.577: INFO: 	Container dashboard-proxy-01 ready: true, restart count 0
Jun 22 05:44:54.577: INFO: 
Logging pods the kubelet thinks is on node node4 before test
Jun 22 05:44:54.592: INFO: kube-proxy-42p24 from kube-system started at 2020-06-22 04:49:39 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.592: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 05:44:54.592: INFO: haproxy-node4 from kube-system started at 2020-06-22 04:50:03 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.592: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 05:44:54.592: INFO: calico-node-q9h8x from kube-system started at 2020-06-22 04:49:39 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.592: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 05:44:54.592: INFO: sonobuoy from sonobuoy started at 2020-06-22 05:09:07 +0000 UTC (1 container statuses recorded)
Jun 22 05:44:54.592: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 05:44:54.592: INFO: sonobuoy-e2e-job-6934f20e32d94a27 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:44:54.592: INFO: 	Container e2e ready: true, restart count 0
Jun 22 05:44:54.592: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:44:54.592: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-h48z4 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:44:54.592: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:44:54.592: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ec04ec72-98d7-479f-87bb-0aa379ce1718 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-ec04ec72-98d7-479f-87bb-0aa379ce1718 off the node node4
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ec04ec72-98d7-479f-87bb-0aa379ce1718
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:44:58.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4255" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":116,"skipped":2080,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:44:58.750: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-50
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-e44bc5a3-e884-43cf-b320-b4e8b8bbaad7
STEP: Creating a pod to test consume secrets
Jun 22 05:44:58.955: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0cc4ac09-bd22-4f70-8189-73f2fe061b3c" in namespace "projected-50" to be "success or failure"
Jun 22 05:44:58.967: INFO: Pod "pod-projected-secrets-0cc4ac09-bd22-4f70-8189-73f2fe061b3c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.60456ms
Jun 22 05:45:00.972: INFO: Pod "pod-projected-secrets-0cc4ac09-bd22-4f70-8189-73f2fe061b3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017363582s
STEP: Saw pod success
Jun 22 05:45:00.972: INFO: Pod "pod-projected-secrets-0cc4ac09-bd22-4f70-8189-73f2fe061b3c" satisfied condition "success or failure"
Jun 22 05:45:00.978: INFO: Trying to get logs from node node4 pod pod-projected-secrets-0cc4ac09-bd22-4f70-8189-73f2fe061b3c container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 05:45:01.021: INFO: Waiting for pod pod-projected-secrets-0cc4ac09-bd22-4f70-8189-73f2fe061b3c to disappear
Jun 22 05:45:01.026: INFO: Pod pod-projected-secrets-0cc4ac09-bd22-4f70-8189-73f2fe061b3c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:45:01.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-50" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":117,"skipped":2102,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:45:01.045: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4098
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 05:45:01.878: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 05:45:03.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401501, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401501, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401501, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401501, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 05:45:06.942: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:45:06.951: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:45:12.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4098" for this suite.
STEP: Destroying namespace "webhook-4098-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.794 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":118,"skipped":2121,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:45:12.840: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4684
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun 22 05:45:13.054: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 05:45:13.078: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 05:45:13.084: INFO: 
Logging pods the kubelet thinks is on node node1 before test
Jun 22 05:45:13.096: INFO: calico-node-qkngx from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 05:45:13.096: INFO: haproxy-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container haproxy ready: true, restart count 1
Jun 22 05:45:13.096: INFO: calico-kube-controllers-77c4b7448-pvlhp from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 05:45:13.096: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-kwffd from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:45:13.096: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 05:45:13.096: INFO: kube-apiserver-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 05:45:13.096: INFO: cocktail-batch-server-75c78755c5-c94nd from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container batch ready: true, restart count 0
Jun 22 05:45:13.096: INFO: cocktail-monitoring-76bc9fcf6f-r9tw9 from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container monitoring ready: true, restart count 0
Jun 22 05:45:13.096: INFO: cocktail-api-cmdb-0 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 05:45:13.096: INFO: coredns-ddbf59985-7mcct from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container coredns ready: true, restart count 0
Jun 22 05:45:13.096: INFO: cocktail-cluster-api-69794f5b78-9htrk from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container cluster-api ready: true, restart count 1
Jun 22 05:45:13.096: INFO: cocktail-alarm-collector-6cfd6bbb9c-kl9zt from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container collector ready: true, restart count 0
Jun 22 05:45:13.096: INFO: kube-proxy-qbghn from kube-system started at 2020-06-22 04:48:30 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 05:45:13.096: INFO: kube-controller-manager-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 05:45:13.096: INFO: kube-scheduler-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 05:45:13.096: INFO: metrics-server-76b7895b66-f4lch from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.096: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 05:45:13.096: INFO: 
Logging pods the kubelet thinks is on node node2 before test
Jun 22 05:45:13.109: INFO: local-storage-nfs-client-provisioner-745d66655-5qmjz from cocktail-addon started at 2020-06-22 04:50:09 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container nfs-client-provisioner ready: true, restart count 0
Jun 22 05:45:13.109: INFO: cocktail-cluster-health-checker-7456f8c49-x62nj from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container checker ready: true, restart count 0
Jun 22 05:45:13.109: INFO: cocktail-dashboard-session-7fb7444f46-xcm2x from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container dashboard-session ready: true, restart count 0
Jun 22 05:45:13.109: INFO: cocktail-api-server-84467f9bf8-8h74n from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container api-server ready: true, restart count 0
Jun 22 05:45:13.109: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-jrq4m from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:45:13.109: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 05:45:13.109: INFO: addon-manager-5bb9bbcdcc-j2qhp from cocktail-addon started at 2020-06-22 04:49:54 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container addon-manager ready: true, restart count 0
Jun 22 05:45:13.109: INFO: kube-controller-manager-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 05:45:13.109: INFO: calico-node-xt4w8 from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 05:45:13.109: INFO: kube-proxy-chh9k from kube-system started at 2020-06-22 04:48:19 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 05:45:13.109: INFO: cocktail-metric-collector-5987649879-49kjv from cocktail-system started at 2020-06-22 04:50:15 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container statcollector ready: true, restart count 0
Jun 22 05:45:13.109: INFO: cocktail-dashboard-6bd44cf6c8-mmhlx from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container cocktail-dashboard ready: true, restart count 0
Jun 22 05:45:13.109: INFO: kube-apiserver-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 05:45:13.109: INFO: cocktail-api-cmdb-2 from cocktail-system started at 2020-06-22 04:52:57 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 05:45:13.109: INFO: cocktail-build-queue-678588c57d-h5dd4 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container nats-streaming ready: true, restart count 0
Jun 22 05:45:13.109: INFO: kube-scheduler-node2 from kube-system started at 2020-06-22 04:48:10 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 05:45:13.109: INFO: cocktail-dashboard-queue-845dd499b5-zzxrl from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container dashboard-queue ready: true, restart count 0
Jun 22 05:45:13.109: INFO: haproxy-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.109: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 05:45:13.109: INFO: 
Logging pods the kubelet thinks is on node node3 before test
Jun 22 05:45:13.125: INFO: kube-apiserver-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 05:45:13.126: INFO: metrics-server-76b7895b66-nz47p from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 05:45:13.126: INFO: cocktail-build-api-7968df5d66-kqcnw from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container build-api ready: true, restart count 0
Jun 22 05:45:13.126: INFO: cocktail-dashboard-proxy-6557bc4fbf-fh5f9 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container dashboard-proxy-01 ready: true, restart count 0
Jun 22 05:45:13.126: INFO: kube-controller-manager-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 05:45:13.126: INFO: calico-node-lhxv5 from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 05:45:13.126: INFO: coredns-ddbf59985-ld84l from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container coredns ready: true, restart count 0
Jun 22 05:45:13.126: INFO: cocktail-monitoring-tsdb-799959dd7c-bg2nj from cocktail-system started at 2020-06-22 04:50:15 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container db ready: true, restart count 0
Jun 22 05:45:13.126: INFO: kube-scheduler-node3 from kube-system started at 2020-06-22 04:48:05 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 05:45:13.126: INFO: cocktail-api-cmdb-1 from cocktail-system started at 2020-06-22 04:51:32 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 05:45:13.126: INFO: haproxy-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 05:45:13.126: INFO: kube-proxy-sb9hq from kube-system started at 2020-06-22 04:48:28 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 05:45:13.126: INFO: cocktail-package-75488df5f8-7p55f from cocktail-system started at 2020-06-22 04:50:13 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container package ready: true, restart count 1
Jun 22 05:45:13.126: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-88h64 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:45:13.126: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:45:13.126: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 05:45:13.126: INFO: 
Logging pods the kubelet thinks is on node node4 before test
Jun 22 05:45:13.138: INFO: kube-proxy-42p24 from kube-system started at 2020-06-22 04:49:39 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.138: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 05:45:13.138: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-h48z4 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:45:13.138: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 05:45:13.138: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 05:45:13.138: INFO: haproxy-node4 from kube-system started at 2020-06-22 04:50:03 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.138: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 05:45:13.138: INFO: calico-node-q9h8x from kube-system started at 2020-06-22 04:49:39 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.138: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 05:45:13.138: INFO: sonobuoy from sonobuoy started at 2020-06-22 05:09:07 +0000 UTC (1 container statuses recorded)
Jun 22 05:45:13.138: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 05:45:13.138: INFO: sonobuoy-e2e-job-6934f20e32d94a27 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 05:45:13.138: INFO: 	Container e2e ready: true, restart count 0
Jun 22 05:45:13.138: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node node1
STEP: verifying the node has the label node node2
STEP: verifying the node has the label node node3
STEP: verifying the node has the label node node4
Jun 22 05:45:13.255: INFO: Pod addon-manager-5bb9bbcdcc-j2qhp requesting resource cpu=200m on Node node2
Jun 22 05:45:13.255: INFO: Pod local-storage-nfs-client-provisioner-745d66655-5qmjz requesting resource cpu=0m on Node node2
Jun 22 05:45:13.255: INFO: Pod cocktail-alarm-collector-6cfd6bbb9c-kl9zt requesting resource cpu=100m on Node node1
Jun 22 05:45:13.255: INFO: Pod cocktail-api-cmdb-0 requesting resource cpu=500m on Node node1
Jun 22 05:45:13.255: INFO: Pod cocktail-api-cmdb-1 requesting resource cpu=500m on Node node3
Jun 22 05:45:13.255: INFO: Pod cocktail-api-cmdb-2 requesting resource cpu=500m on Node node2
Jun 22 05:45:13.255: INFO: Pod cocktail-api-server-84467f9bf8-8h74n requesting resource cpu=1500m on Node node2
Jun 22 05:45:13.255: INFO: Pod cocktail-batch-server-75c78755c5-c94nd requesting resource cpu=100m on Node node1
Jun 22 05:45:13.255: INFO: Pod cocktail-build-api-7968df5d66-kqcnw requesting resource cpu=100m on Node node3
Jun 22 05:45:13.256: INFO: Pod cocktail-build-queue-678588c57d-h5dd4 requesting resource cpu=0m on Node node2
Jun 22 05:45:13.256: INFO: Pod cocktail-cluster-api-69794f5b78-9htrk requesting resource cpu=100m on Node node1
Jun 22 05:45:13.256: INFO: Pod cocktail-cluster-health-checker-7456f8c49-x62nj requesting resource cpu=50m on Node node2
Jun 22 05:45:13.256: INFO: Pod cocktail-dashboard-6bd44cf6c8-mmhlx requesting resource cpu=300m on Node node2
Jun 22 05:45:13.256: INFO: Pod cocktail-dashboard-proxy-6557bc4fbf-fh5f9 requesting resource cpu=200m on Node node3
Jun 22 05:45:13.256: INFO: Pod cocktail-dashboard-queue-845dd499b5-zzxrl requesting resource cpu=100m on Node node2
Jun 22 05:45:13.256: INFO: Pod cocktail-dashboard-session-7fb7444f46-xcm2x requesting resource cpu=100m on Node node2
Jun 22 05:45:13.256: INFO: Pod cocktail-metric-collector-5987649879-49kjv requesting resource cpu=100m on Node node2
Jun 22 05:45:13.256: INFO: Pod cocktail-monitoring-76bc9fcf6f-r9tw9 requesting resource cpu=150m on Node node1
Jun 22 05:45:13.256: INFO: Pod cocktail-monitoring-tsdb-799959dd7c-bg2nj requesting resource cpu=1000m on Node node3
Jun 22 05:45:13.256: INFO: Pod cocktail-package-75488df5f8-7p55f requesting resource cpu=1000m on Node node3
Jun 22 05:45:13.256: INFO: Pod calico-kube-controllers-77c4b7448-pvlhp requesting resource cpu=0m on Node node1
Jun 22 05:45:13.256: INFO: Pod calico-node-lhxv5 requesting resource cpu=250m on Node node3
Jun 22 05:45:13.256: INFO: Pod calico-node-q9h8x requesting resource cpu=250m on Node node4
Jun 22 05:45:13.256: INFO: Pod calico-node-qkngx requesting resource cpu=250m on Node node1
Jun 22 05:45:13.256: INFO: Pod calico-node-xt4w8 requesting resource cpu=250m on Node node2
Jun 22 05:45:13.256: INFO: Pod coredns-ddbf59985-7mcct requesting resource cpu=100m on Node node1
Jun 22 05:45:13.256: INFO: Pod coredns-ddbf59985-ld84l requesting resource cpu=100m on Node node3
Jun 22 05:45:13.256: INFO: Pod haproxy-node1 requesting resource cpu=25m on Node node1
Jun 22 05:45:13.256: INFO: Pod haproxy-node2 requesting resource cpu=25m on Node node2
Jun 22 05:45:13.256: INFO: Pod haproxy-node3 requesting resource cpu=25m on Node node3
Jun 22 05:45:13.256: INFO: Pod haproxy-node4 requesting resource cpu=25m on Node node4
Jun 22 05:45:13.256: INFO: Pod kube-apiserver-node1 requesting resource cpu=250m on Node node1
Jun 22 05:45:13.256: INFO: Pod kube-apiserver-node2 requesting resource cpu=250m on Node node2
Jun 22 05:45:13.256: INFO: Pod kube-apiserver-node3 requesting resource cpu=250m on Node node3
Jun 22 05:45:13.256: INFO: Pod kube-controller-manager-node1 requesting resource cpu=200m on Node node1
Jun 22 05:45:13.256: INFO: Pod kube-controller-manager-node2 requesting resource cpu=200m on Node node2
Jun 22 05:45:13.256: INFO: Pod kube-controller-manager-node3 requesting resource cpu=200m on Node node3
Jun 22 05:45:13.256: INFO: Pod kube-proxy-42p24 requesting resource cpu=0m on Node node4
Jun 22 05:45:13.256: INFO: Pod kube-proxy-chh9k requesting resource cpu=0m on Node node2
Jun 22 05:45:13.256: INFO: Pod kube-proxy-qbghn requesting resource cpu=0m on Node node1
Jun 22 05:45:13.256: INFO: Pod kube-proxy-sb9hq requesting resource cpu=0m on Node node3
Jun 22 05:45:13.256: INFO: Pod kube-scheduler-node1 requesting resource cpu=100m on Node node1
Jun 22 05:45:13.256: INFO: Pod kube-scheduler-node2 requesting resource cpu=100m on Node node2
Jun 22 05:45:13.256: INFO: Pod kube-scheduler-node3 requesting resource cpu=100m on Node node3
Jun 22 05:45:13.256: INFO: Pod metrics-server-76b7895b66-f4lch requesting resource cpu=0m on Node node1
Jun 22 05:45:13.256: INFO: Pod metrics-server-76b7895b66-nz47p requesting resource cpu=0m on Node node3
Jun 22 05:45:13.256: INFO: Pod sonobuoy requesting resource cpu=0m on Node node4
Jun 22 05:45:13.256: INFO: Pod sonobuoy-e2e-job-6934f20e32d94a27 requesting resource cpu=0m on Node node4
Jun 22 05:45:13.256: INFO: Pod sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-88h64 requesting resource cpu=0m on Node node3
Jun 22 05:45:13.256: INFO: Pod sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-h48z4 requesting resource cpu=0m on Node node4
Jun 22 05:45:13.256: INFO: Pod sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-jrq4m requesting resource cpu=0m on Node node2
Jun 22 05:45:13.256: INFO: Pod sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-kwffd requesting resource cpu=0m on Node node1
STEP: Starting Pods to consume most of the cluster CPU.
Jun 22 05:45:13.256: INFO: Creating a pod which consumes cpu=2887m on Node node1
Jun 22 05:45:13.271: INFO: Creating a pod which consumes cpu=1627m on Node node2
Jun 22 05:45:13.282: INFO: Creating a pod which consumes cpu=1592m on Node node3
Jun 22 05:45:13.297: INFO: Creating a pod which consumes cpu=2607m on Node node4
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62fb9330-e21d-4b63-9963-cc2b65b46e5a.161ac7729bb97ed2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4684/filler-pod-62fb9330-e21d-4b63-9963-cc2b65b46e5a to node1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62fb9330-e21d-4b63-9963-cc2b65b46e5a.161ac772d6b08f09], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62fb9330-e21d-4b63-9963-cc2b65b46e5a.161ac7734e239b17], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62fb9330-e21d-4b63-9963-cc2b65b46e5a.161ac7735185987b], Reason = [Created], Message = [Created container filler-pod-62fb9330-e21d-4b63-9963-cc2b65b46e5a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62fb9330-e21d-4b63-9963-cc2b65b46e5a.161ac7735cb7a910], Reason = [Started], Message = [Started container filler-pod-62fb9330-e21d-4b63-9963-cc2b65b46e5a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b7cd4d3a-7f22-463d-9a0f-9a8c1ebd2d01.161ac7729e001b20], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4684/filler-pod-b7cd4d3a-7f22-463d-9a0f-9a8c1ebd2d01 to node4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b7cd4d3a-7f22-463d-9a0f-9a8c1ebd2d01.161ac772dfbbd2fa], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b7cd4d3a-7f22-463d-9a0f-9a8c1ebd2d01.161ac772e3a522cf], Reason = [Created], Message = [Created container filler-pod-b7cd4d3a-7f22-463d-9a0f-9a8c1ebd2d01]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b7cd4d3a-7f22-463d-9a0f-9a8c1ebd2d01.161ac772eee84451], Reason = [Started], Message = [Started container filler-pod-b7cd4d3a-7f22-463d-9a0f-9a8c1ebd2d01]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9c71aa5-90d4-49bc-86c5-45d93ff4d5ea.161ac7729dacd864], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4684/filler-pod-d9c71aa5-90d4-49bc-86c5-45d93ff4d5ea to node3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9c71aa5-90d4-49bc-86c5-45d93ff4d5ea.161ac772e0d04a52], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9c71aa5-90d4-49bc-86c5-45d93ff4d5ea.161ac7735825b373], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9c71aa5-90d4-49bc-86c5-45d93ff4d5ea.161ac7735c97a088], Reason = [Created], Message = [Created container filler-pod-d9c71aa5-90d4-49bc-86c5-45d93ff4d5ea]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9c71aa5-90d4-49bc-86c5-45d93ff4d5ea.161ac77368738bb6], Reason = [Started], Message = [Started container filler-pod-d9c71aa5-90d4-49bc-86c5-45d93ff4d5ea]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-de8f945c-5255-4b62-8eef-a5f1ecfc73f1.161ac7729c64a280], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4684/filler-pod-de8f945c-5255-4b62-8eef-a5f1ecfc73f1 to node2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-de8f945c-5255-4b62-8eef-a5f1ecfc73f1.161ac772e1c407c8], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-de8f945c-5255-4b62-8eef-a5f1ecfc73f1.161ac7735657d42d], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-de8f945c-5255-4b62-8eef-a5f1ecfc73f1.161ac7735b1437a5], Reason = [Created], Message = [Created container filler-pod-de8f945c-5255-4b62-8eef-a5f1ecfc73f1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-de8f945c-5255-4b62-8eef-a5f1ecfc73f1.161ac773679e6aad], Reason = [Started], Message = [Started container filler-pod-de8f945c-5255-4b62-8eef-a5f1ecfc73f1]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.161ac77407f65294], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node node4
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node node1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node node2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node node3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:45:20.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4684" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:7.671 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":119,"skipped":2142,"failed":0}
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:45:20.511: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5254
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 22 05:45:25.741: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:45:26.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5254" for this suite.

• [SLOW TEST:6.299 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":120,"skipped":2142,"failed":0}
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:45:26.811: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2840
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 22 05:45:29.574: INFO: Successfully updated pod "pod-update-f3c86b29-11b6-42ea-8c62-c74f5ed59b2a"
STEP: verifying the updated pod is in kubernetes
Jun 22 05:45:29.585: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:45:29.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2840" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":121,"skipped":2142,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:45:29.604: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5501
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-ffcb07ac-01b9-40d4-bcb0-b2ee77af1f10
STEP: Creating a pod to test consume secrets
Jun 22 05:45:29.792: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c6dcb5f0-8d61-4912-b5f7-b8328b43a5e6" in namespace "projected-5501" to be "success or failure"
Jun 22 05:45:29.798: INFO: Pod "pod-projected-secrets-c6dcb5f0-8d61-4912-b5f7-b8328b43a5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.272029ms
Jun 22 05:45:31.807: INFO: Pod "pod-projected-secrets-c6dcb5f0-8d61-4912-b5f7-b8328b43a5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015164106s
Jun 22 05:45:33.817: INFO: Pod "pod-projected-secrets-c6dcb5f0-8d61-4912-b5f7-b8328b43a5e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025282607s
STEP: Saw pod success
Jun 22 05:45:33.817: INFO: Pod "pod-projected-secrets-c6dcb5f0-8d61-4912-b5f7-b8328b43a5e6" satisfied condition "success or failure"
Jun 22 05:45:33.823: INFO: Trying to get logs from node node4 pod pod-projected-secrets-c6dcb5f0-8d61-4912-b5f7-b8328b43a5e6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 05:45:33.861: INFO: Waiting for pod pod-projected-secrets-c6dcb5f0-8d61-4912-b5f7-b8328b43a5e6 to disappear
Jun 22 05:45:33.867: INFO: Pod pod-projected-secrets-c6dcb5f0-8d61-4912-b5f7-b8328b43a5e6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:45:33.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5501" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":122,"skipped":2162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:45:33.888: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 05:45:34.942: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 05:45:36.960: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401534, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401534, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401534, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401534, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 05:45:39.999: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:45:40.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9388" for this suite.
STEP: Destroying namespace "webhook-9388-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.266 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":123,"skipped":2193,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:45:40.154: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1626
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-dbf217ac-cd7e-446c-a65a-b7bcc54e912b
STEP: Creating secret with name secret-projected-all-test-volume-d1d75b05-75e8-484e-a0ba-049d680dff47
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 22 05:45:40.410: INFO: Waiting up to 5m0s for pod "projected-volume-8d267034-e341-43af-a142-1f25053f5522" in namespace "projected-1626" to be "success or failure"
Jun 22 05:45:40.421: INFO: Pod "projected-volume-8d267034-e341-43af-a142-1f25053f5522": Phase="Pending", Reason="", readiness=false. Elapsed: 10.996327ms
Jun 22 05:45:42.427: INFO: Pod "projected-volume-8d267034-e341-43af-a142-1f25053f5522": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016536554s
STEP: Saw pod success
Jun 22 05:45:42.427: INFO: Pod "projected-volume-8d267034-e341-43af-a142-1f25053f5522" satisfied condition "success or failure"
Jun 22 05:45:42.433: INFO: Trying to get logs from node node4 pod projected-volume-8d267034-e341-43af-a142-1f25053f5522 container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 22 05:45:42.469: INFO: Waiting for pod projected-volume-8d267034-e341-43af-a142-1f25053f5522 to disappear
Jun 22 05:45:42.475: INFO: Pod projected-volume-8d267034-e341-43af-a142-1f25053f5522 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:45:42.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1626" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":124,"skipped":2230,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:45:42.490: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5665
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 22 05:45:42.670: INFO: Waiting up to 5m0s for pod "pod-897c2f65-ca17-4b8f-9a0c-54a301e0a022" in namespace "emptydir-5665" to be "success or failure"
Jun 22 05:45:42.676: INFO: Pod "pod-897c2f65-ca17-4b8f-9a0c-54a301e0a022": Phase="Pending", Reason="", readiness=false. Elapsed: 5.509944ms
Jun 22 05:45:44.683: INFO: Pod "pod-897c2f65-ca17-4b8f-9a0c-54a301e0a022": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013375549s
STEP: Saw pod success
Jun 22 05:45:44.683: INFO: Pod "pod-897c2f65-ca17-4b8f-9a0c-54a301e0a022" satisfied condition "success or failure"
Jun 22 05:45:44.688: INFO: Trying to get logs from node node4 pod pod-897c2f65-ca17-4b8f-9a0c-54a301e0a022 container test-container: <nil>
STEP: delete the pod
Jun 22 05:45:44.725: INFO: Waiting for pod pod-897c2f65-ca17-4b8f-9a0c-54a301e0a022 to disappear
Jun 22 05:45:44.733: INFO: Pod pod-897c2f65-ca17-4b8f-9a0c-54a301e0a022 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:45:44.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5665" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":125,"skipped":2244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:45:44.757: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3315
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:45:46.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3315" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":126,"skipped":2276,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:45:46.995: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3134
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-9ec65553-740f-4b70-a452-2849dc335486 in namespace container-probe-3134
Jun 22 05:45:49.192: INFO: Started pod busybox-9ec65553-740f-4b70-a452-2849dc335486 in namespace container-probe-3134
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 05:45:49.198: INFO: Initial restart count of pod busybox-9ec65553-740f-4b70-a452-2849dc335486 is 0
Jun 22 05:46:43.445: INFO: Restart count of pod container-probe-3134/busybox-9ec65553-740f-4b70-a452-2849dc335486 is now 1 (54.247504135s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:46:43.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3134" for this suite.

• [SLOW TEST:56.502 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":127,"skipped":2285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:46:43.498: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:46:43.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8140" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":128,"skipped":2311,"failed":0}

------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:46:43.693: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2133
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-5412b0a2-8401-41ea-b10f-720268f76847 in namespace container-probe-2133
Jun 22 05:46:47.913: INFO: Started pod busybox-5412b0a2-8401-41ea-b10f-720268f76847 in namespace container-probe-2133
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 05:46:47.917: INFO: Initial restart count of pod busybox-5412b0a2-8401-41ea-b10f-720268f76847 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:50:48.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2133" for this suite.

• [SLOW TEST:245.233 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":129,"skipped":2311,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:50:48.926: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 22 05:50:55.183: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:50:55.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0622 05:50:55.183414      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-4508" for this suite.

• [SLOW TEST:6.292 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":130,"skipped":2325,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:50:55.218: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2781
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-2781/configmap-test-7ae788c6-4d05-4541-bb26-a6107e79f9af
STEP: Creating a pod to test consume configMaps
Jun 22 05:50:55.588: INFO: Waiting up to 5m0s for pod "pod-configmaps-096f75ab-f88d-4f2e-b52e-8557b71b01e2" in namespace "configmap-2781" to be "success or failure"
Jun 22 05:50:55.614: INFO: Pod "pod-configmaps-096f75ab-f88d-4f2e-b52e-8557b71b01e2": Phase="Pending", Reason="", readiness=false. Elapsed: 26.20834ms
Jun 22 05:50:57.623: INFO: Pod "pod-configmaps-096f75ab-f88d-4f2e-b52e-8557b71b01e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034616407s
Jun 22 05:50:59.632: INFO: Pod "pod-configmaps-096f75ab-f88d-4f2e-b52e-8557b71b01e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044215087s
STEP: Saw pod success
Jun 22 05:50:59.632: INFO: Pod "pod-configmaps-096f75ab-f88d-4f2e-b52e-8557b71b01e2" satisfied condition "success or failure"
Jun 22 05:50:59.638: INFO: Trying to get logs from node node4 pod pod-configmaps-096f75ab-f88d-4f2e-b52e-8557b71b01e2 container env-test: <nil>
STEP: delete the pod
Jun 22 05:50:59.692: INFO: Waiting for pod pod-configmaps-096f75ab-f88d-4f2e-b52e-8557b71b01e2 to disappear
Jun 22 05:50:59.696: INFO: Pod pod-configmaps-096f75ab-f88d-4f2e-b52e-8557b71b01e2 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:50:59.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2781" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":131,"skipped":2331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:50:59.715: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-415
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 22 05:50:59.907: INFO: Waiting up to 5m0s for pod "pod-96c85a05-81f4-42f7-9ead-181aaeca378b" in namespace "emptydir-415" to be "success or failure"
Jun 22 05:50:59.916: INFO: Pod "pod-96c85a05-81f4-42f7-9ead-181aaeca378b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.186749ms
Jun 22 05:51:01.923: INFO: Pod "pod-96c85a05-81f4-42f7-9ead-181aaeca378b": Phase="Running", Reason="", readiness=true. Elapsed: 2.01540884s
Jun 22 05:51:03.931: INFO: Pod "pod-96c85a05-81f4-42f7-9ead-181aaeca378b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023727885s
STEP: Saw pod success
Jun 22 05:51:03.931: INFO: Pod "pod-96c85a05-81f4-42f7-9ead-181aaeca378b" satisfied condition "success or failure"
Jun 22 05:51:03.936: INFO: Trying to get logs from node node4 pod pod-96c85a05-81f4-42f7-9ead-181aaeca378b container test-container: <nil>
STEP: delete the pod
Jun 22 05:51:03.973: INFO: Waiting for pod pod-96c85a05-81f4-42f7-9ead-181aaeca378b to disappear
Jun 22 05:51:03.982: INFO: Pod pod-96c85a05-81f4-42f7-9ead-181aaeca378b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:51:03.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-415" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":132,"skipped":2359,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:51:03.999: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 05:51:04.789: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 05:51:06.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401864, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401864, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401864, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401864, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 05:51:09.843: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:51:09.849: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2287-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:51:16.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8157" for this suite.
STEP: Destroying namespace "webhook-8157-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.244 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":133,"skipped":2377,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:51:16.244: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2026
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-2026
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 05:51:16.441: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 05:51:38.638: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.37.149.35:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2026 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:51:38.638: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:51:38.808: INFO: Found all expected endpoints: [netserver-0]
Jun 22 05:51:38.813: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.37.11.18:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2026 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:51:38.814: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:51:38.977: INFO: Found all expected endpoints: [netserver-1]
Jun 22 05:51:38.984: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.135.23:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2026 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:51:38.984: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:51:39.150: INFO: Found all expected endpoints: [netserver-2]
Jun 22 05:51:39.155: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.41.130.135:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2026 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:51:39.155: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:51:39.321: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:51:39.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2026" for this suite.

• [SLOW TEST:23.095 seconds]
[sig-network] Networking
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":134,"skipped":2386,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:51:39.339: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:51:39.508: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 22 05:51:39.526: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 22 05:51:44.541: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 05:51:44.541: INFO: Creating deployment "test-rolling-update-deployment"
Jun 22 05:51:44.552: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 22 05:51:44.564: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 22 05:51:46.576: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 22 05:51:46.582: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401904, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401904, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401904, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728401904, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 05:51:48.587: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun 22 05:51:48.607: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5850 /apis/apps/v1/namespaces/deployment-5850/deployments/test-rolling-update-deployment c3a1b7dd-005d-4f59-9aff-a86235c5875e 25144 1 2020-06-22 05:51:44 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003c087c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-22 05:51:44 +0000 UTC,LastTransitionTime:2020-06-22 05:51:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-06-22 05:51:47 +0000 UTC,LastTransitionTime:2020-06-22 05:51:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 05:51:48.613: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-5850 /apis/apps/v1/namespaces/deployment-5850/replicasets/test-rolling-update-deployment-67cf4f6444 da8f19d8-611f-4be9-87b2-045704913c63 25134 1 2020-06-22 05:51:44 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment c3a1b7dd-005d-4f59-9aff-a86235c5875e 0xc003c08c87 0xc003c08c88}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003c08cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 05:51:48.613: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 22 05:51:48.613: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5850 /apis/apps/v1/namespaces/deployment-5850/replicasets/test-rolling-update-controller af542456-a044-4716-9763-0342abb5eabb 25143 2 2020-06-22 05:51:39 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment c3a1b7dd-005d-4f59-9aff-a86235c5875e 0xc003c08bb7 0xc003c08bb8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c08c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 05:51:48.619: INFO: Pod "test-rolling-update-deployment-67cf4f6444-qrdn5" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-qrdn5 test-rolling-update-deployment-67cf4f6444- deployment-5850 /api/v1/namespaces/deployment-5850/pods/test-rolling-update-deployment-67cf4f6444-qrdn5 978b95a0-cfb2-47ff-b1d6-99ac3fd976b7 25133 0 2020-06-22 05:51:44 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[cni.projectcalico.org/podIP:10.37.149.36/32 cni.projectcalico.org/podIPs:10.37.149.36/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 da8f19d8-611f-4be9-87b2-045704913c63 0xc003c091a7 0xc003c091a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fg5cn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fg5cn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fg5cn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:51:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:51:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:51:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:51:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.141,PodIP:10.37.149.36,StartTime:2020-06-22 05:51:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 05:51:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://d6933604b6e0c79f4b9738c9c83e83956614de0c4e5997d579ed1115f15268a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.37.149.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:51:48.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5850" for this suite.

• [SLOW TEST:9.297 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":135,"skipped":2390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:51:48.636: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2474
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:52:04.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2474" for this suite.

• [SLOW TEST:16.381 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":136,"skipped":2421,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:52:05.018: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0622 05:52:15.314904      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 22 05:52:15.315: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:52:15.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4576" for this suite.

• [SLOW TEST:10.311 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":137,"skipped":2435,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:52:15.329: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2777
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Jun 22 05:52:15.512: INFO: Waiting up to 5m0s for pod "client-containers-2bee78d4-7583-476d-a276-31278efc69f5" in namespace "containers-2777" to be "success or failure"
Jun 22 05:52:15.516: INFO: Pod "client-containers-2bee78d4-7583-476d-a276-31278efc69f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072793ms
Jun 22 05:52:17.523: INFO: Pod "client-containers-2bee78d4-7583-476d-a276-31278efc69f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011352957s
Jun 22 05:52:19.534: INFO: Pod "client-containers-2bee78d4-7583-476d-a276-31278efc69f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022229904s
STEP: Saw pod success
Jun 22 05:52:19.534: INFO: Pod "client-containers-2bee78d4-7583-476d-a276-31278efc69f5" satisfied condition "success or failure"
Jun 22 05:52:19.538: INFO: Trying to get logs from node node4 pod client-containers-2bee78d4-7583-476d-a276-31278efc69f5 container test-container: <nil>
STEP: delete the pod
Jun 22 05:52:19.573: INFO: Waiting for pod client-containers-2bee78d4-7583-476d-a276-31278efc69f5 to disappear
Jun 22 05:52:19.580: INFO: Pod client-containers-2bee78d4-7583-476d-a276-31278efc69f5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:52:19.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2777" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":138,"skipped":2449,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:52:19.606: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5979
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:52:19.788: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:52:25.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5979" for this suite.

• [SLOW TEST:5.799 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":139,"skipped":2458,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:52:25.405: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8060
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Jun 22 05:52:25.629: INFO: Waiting up to 5m0s for pod "var-expansion-3fac392a-e355-4a53-97a1-64ba06d590e4" in namespace "var-expansion-8060" to be "success or failure"
Jun 22 05:52:25.633: INFO: Pod "var-expansion-3fac392a-e355-4a53-97a1-64ba06d590e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.347561ms
Jun 22 05:52:27.640: INFO: Pod "var-expansion-3fac392a-e355-4a53-97a1-64ba06d590e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010571611s
Jun 22 05:52:29.649: INFO: Pod "var-expansion-3fac392a-e355-4a53-97a1-64ba06d590e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020238214s
STEP: Saw pod success
Jun 22 05:52:29.649: INFO: Pod "var-expansion-3fac392a-e355-4a53-97a1-64ba06d590e4" satisfied condition "success or failure"
Jun 22 05:52:29.656: INFO: Trying to get logs from node node4 pod var-expansion-3fac392a-e355-4a53-97a1-64ba06d590e4 container dapi-container: <nil>
STEP: delete the pod
Jun 22 05:52:29.691: INFO: Waiting for pod var-expansion-3fac392a-e355-4a53-97a1-64ba06d590e4 to disappear
Jun 22 05:52:29.695: INFO: Pod var-expansion-3fac392a-e355-4a53-97a1-64ba06d590e4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:52:29.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8060" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":140,"skipped":2463,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:52:29.711: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6026
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:52:29.882: INFO: Creating deployment "webserver-deployment"
Jun 22 05:52:29.893: INFO: Waiting for observed generation 1
Jun 22 05:52:31.932: INFO: Waiting for all required pods to come up
Jun 22 05:52:31.939: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 22 05:52:33.978: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 22 05:52:33.990: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 22 05:52:34.002: INFO: Updating deployment webserver-deployment
Jun 22 05:52:34.002: INFO: Waiting for observed generation 2
Jun 22 05:52:36.015: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 22 05:52:36.021: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 22 05:52:36.026: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 22 05:52:36.041: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 22 05:52:36.041: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 22 05:52:36.047: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 22 05:52:36.057: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 22 05:52:36.057: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 22 05:52:36.075: INFO: Updating deployment webserver-deployment
Jun 22 05:52:36.075: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 22 05:52:36.090: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 22 05:52:36.101: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun 22 05:52:36.135: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6026 /apis/apps/v1/namespaces/deployment-6026/deployments/webserver-deployment efeb6cfd-a8ce-43c6-8a89-249065215bd6 26084 3 2020-06-22 05:52:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0030e1f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-06-22 05:52:34 +0000 UTC,LastTransitionTime:2020-06-22 05:52:29 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-06-22 05:52:36 +0000 UTC,LastTransitionTime:2020-06-22 05:52:36 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 22 05:52:36.167: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-6026 /apis/apps/v1/namespaces/deployment-6026/replicasets/webserver-deployment-c7997dcc8 b4344e27-6ea4-4b85-b43c-37ccd6fd2918 26079 3 2020-06-22 05:52:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment efeb6cfd-a8ce-43c6-8a89-249065215bd6 0xc0023b5367 0xc0023b5368}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0023b53d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 05:52:36.167: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 22 05:52:36.167: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-6026 /apis/apps/v1/namespaces/deployment-6026/replicasets/webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 26076 3 2020-06-22 05:52:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment efeb6cfd-a8ce-43c6-8a89-249065215bd6 0xc0023b52a7 0xc0023b52a8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0023b5308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 22 05:52:36.285: INFO: Pod "webserver-deployment-595b5b9587-5lzqs" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5lzqs webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-5lzqs b7d9fcbe-019e-4373-af44-a406b8da74dd 25943 0 2020-06-22 05:52:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.37.149.41/32 cni.projectcalico.org/podIPs:10.37.149.41/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc0023b58d7 0xc0023b58d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.141,PodIP:10.37.149.41,StartTime:2020-06-22 05:52:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 05:52:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2559bca7c63709e2b9277fd5f47d4362f660498a369d9de4f382dabcd6c7adfa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.37.149.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.285: INFO: Pod "webserver-deployment-595b5b9587-bhw6c" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bhw6c webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-bhw6c c9ba6bf3-8d66-4cb8-8eb5-f2f0173c3add 25936 0 2020-06-22 05:52:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.41.130.144/32 cni.projectcalico.org/podIPs:10.41.130.144/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc0023b5a77 0xc0023b5a78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.144,PodIP:10.41.130.144,StartTime:2020-06-22 05:52:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 05:52:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://cf8160ac36675d9b765292716d45c89602dad293a5fe9b200ebbe7d9641ba446,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.41.130.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.285: INFO: Pod "webserver-deployment-595b5b9587-bm7ht" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bm7ht webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-bm7ht 60428400-7add-49ae-ac7a-107816cecf25 26095 0 2020-06-22 05:52:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc0023b5bf7 0xc0023b5bf8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.285: INFO: Pod "webserver-deployment-595b5b9587-cpgtj" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cpgtj webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-cpgtj cad8b9de-1456-4d81-b4f2-286ae0a47a62 25960 0 2020-06-22 05:52:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.42.135.27/32 cni.projectcalico.org/podIPs:10.42.135.27/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc0023b5d07 0xc0023b5d08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.143,PodIP:10.42.135.27,StartTime:2020-06-22 05:52:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 05:52:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f0b9b08aa1b7c35438e051c26221bec1e4d92e145cfbe38cc18490a23ba43f55,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.135.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.286: INFO: Pod "webserver-deployment-595b5b9587-fl92l" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fl92l webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-fl92l 7525e1cd-fb99-44ce-854f-26d4443f7509 26100 0 2020-06-22 05:52:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc0023b5e87 0xc0023b5e88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.286: INFO: Pod "webserver-deployment-595b5b9587-ftgj8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ftgj8 webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-ftgj8 72951fe9-00e4-4504-a0de-6d3f341ccddc 26090 0 2020-06-22 05:52:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc0023b5f77 0xc0023b5f78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.286: INFO: Pod "webserver-deployment-595b5b9587-hgthv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hgthv webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-hgthv 28925df2-df18-4dec-9b8b-85f7c6476c4e 25957 0 2020-06-22 05:52:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.42.135.28/32 cni.projectcalico.org/podIPs:10.42.135.28/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc0029100c0 0xc0029100c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.143,PodIP:10.42.135.28,StartTime:2020-06-22 05:52:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 05:52:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a15f7342d5822d72463c51b3fffe6ca2f22d959f011a78eb19cabbc31bd56f03,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.135.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.287: INFO: Pod "webserver-deployment-595b5b9587-nfv9q" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nfv9q webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-nfv9q b9e6af6a-8b19-4c97-98d8-71cf6392ed4c 26101 0 2020-06-22 05:52:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc002910237 0xc002910238}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.144,PodIP:,StartTime:2020-06-22 05:52:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.287: INFO: Pod "webserver-deployment-595b5b9587-p97gm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p97gm webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-p97gm 395a4950-8dc2-4f23-8f36-c87776b5445a 26096 0 2020-06-22 05:52:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc002910397 0xc002910398}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.287: INFO: Pod "webserver-deployment-595b5b9587-p9hxp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p9hxp webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-p9hxp e1acf8fb-3cc8-4ac0-8783-9bb8be05b4f3 25950 0 2020-06-22 05:52:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.37.11.20/32 cni.projectcalico.org/podIPs:10.37.11.20/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc0029104c7 0xc0029104c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.142,PodIP:10.37.11.20,StartTime:2020-06-22 05:52:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 05:52:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d75beb18bbfb1b943fa6320eaff8fae1f247904be50e930cbadeb74195531b74,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.37.11.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.288: INFO: Pod "webserver-deployment-595b5b9587-q2d58" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-q2d58 webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-q2d58 2fece515-1a04-4571-a194-1fb277cced6d 25947 0 2020-06-22 05:52:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.37.149.40/32 cni.projectcalico.org/podIPs:10.37.149.40/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc002910660 0xc002910661}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.141,PodIP:10.37.149.40,StartTime:2020-06-22 05:52:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 05:52:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3c301b276564ab6f0fb82f72056fd5abdedb2337bd81ccbe7122f9cfd55fa580,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.37.149.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.288: INFO: Pod "webserver-deployment-595b5b9587-smdnf" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-smdnf webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-smdnf 200adcfc-b948-4fa7-a4eb-04c3bef88468 25963 0 2020-06-22 05:52:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.42.135.29/32 cni.projectcalico.org/podIPs:10.42.135.29/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc002910b27 0xc002910b28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.143,PodIP:10.42.135.29,StartTime:2020-06-22 05:52:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 05:52:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ac4d08056d95f30a36fc39155f03ca7dc3300e6bb59efa3f80c0e4acd03c79a3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.135.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.288: INFO: Pod "webserver-deployment-595b5b9587-stq5k" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-stq5k webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-stq5k a33ca460-f99e-4f91-8611-02607ff73217 26094 0 2020-06-22 05:52:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc002910ea7 0xc002910ea8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.288: INFO: Pod "webserver-deployment-595b5b9587-tgkn2" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tgkn2 webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-tgkn2 94d30cbe-6267-4e08-aca0-6838502392ef 25939 0 2020-06-22 05:52:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.41.130.145/32 cni.projectcalico.org/podIPs:10.41.130.145/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc002910fb7 0xc002910fb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.144,PodIP:10.41.130.145,StartTime:2020-06-22 05:52:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 05:52:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1f919ee2df8b7603b31579d14736d1c13792ef126db7f7a42c06577d58020989,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.41.130.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.289: INFO: Pod "webserver-deployment-595b5b9587-wljb9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wljb9 webserver-deployment-595b5b9587- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-595b5b9587-wljb9 85bb0faf-3870-4e1a-967a-26395da16257 26087 0 2020-06-22 05:52:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c0149445-964e-4936-a656-78d88db717bc 0xc002911137 0xc002911138}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.289: INFO: Pod "webserver-deployment-c7997dcc8-26m4b" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-26m4b webserver-deployment-c7997dcc8- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-c7997dcc8-26m4b 4e1e99ea-75e0-4701-bad7-311f24a44d1c 26064 0 2020-06-22 05:52:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.41.130.147/32 cni.projectcalico.org/podIPs:10.41.130.147/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 b4344e27-6ea4-4b85-b43c-37ccd6fd2918 0xc002911270 0xc002911271}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.144,PodIP:,StartTime:2020-06-22 05:52:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.301: INFO: Pod "webserver-deployment-c7997dcc8-5xx6w" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5xx6w webserver-deployment-c7997dcc8- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-c7997dcc8-5xx6w 6b490697-ddd1-4f92-8b93-184680eef397 26060 0 2020-06-22 05:52:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.37.149.44/32 cni.projectcalico.org/podIPs:10.37.149.44/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 b4344e27-6ea4-4b85-b43c-37ccd6fd2918 0xc002911407 0xc002911408}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.141,PodIP:,StartTime:2020-06-22 05:52:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.301: INFO: Pod "webserver-deployment-c7997dcc8-8pr8q" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8pr8q webserver-deployment-c7997dcc8- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-c7997dcc8-8pr8q 821e84dd-c837-4ce5-9f23-710d6da8bf46 26052 0 2020-06-22 05:52:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.37.149.43/32 cni.projectcalico.org/podIPs:10.37.149.43/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 b4344e27-6ea4-4b85-b43c-37ccd6fd2918 0xc0029115a7 0xc0029115a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.141,PodIP:,StartTime:2020-06-22 05:52:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.301: INFO: Pod "webserver-deployment-c7997dcc8-ckp69" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ckp69 webserver-deployment-c7997dcc8- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-c7997dcc8-ckp69 0c00feb1-06b5-4882-a054-297513939067 26057 0 2020-06-22 05:52:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.42.135.30/32 cni.projectcalico.org/podIPs:10.42.135.30/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 b4344e27-6ea4-4b85-b43c-37ccd6fd2918 0xc0029118b7 0xc0029118b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.143,PodIP:,StartTime:2020-06-22 05:52:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.301: INFO: Pod "webserver-deployment-c7997dcc8-hmsw2" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hmsw2 webserver-deployment-c7997dcc8- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-c7997dcc8-hmsw2 abd32405-2c72-4db6-a9ae-14b5eca23071 26055 0 2020-06-22 05:52:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.41.130.146/32 cni.projectcalico.org/podIPs:10.41.130.146/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 b4344e27-6ea4-4b85-b43c-37ccd6fd2918 0xc002911df7 0xc002911df8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.144,PodIP:,StartTime:2020-06-22 05:52:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.302: INFO: Pod "webserver-deployment-c7997dcc8-m7fq4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-m7fq4 webserver-deployment-c7997dcc8- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-c7997dcc8-m7fq4 f8fba26f-a9ac-4083-bfda-feb763fdabe0 26099 0 2020-06-22 05:52:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 b4344e27-6ea4-4b85-b43c-37ccd6fd2918 0xc002911f77 0xc002911f78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.302: INFO: Pod "webserver-deployment-c7997dcc8-v4jcp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-v4jcp webserver-deployment-c7997dcc8- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-c7997dcc8-v4jcp c76f7212-6cd6-4083-abeb-20cfdcff69ef 26097 0 2020-06-22 05:52:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 b4344e27-6ea4-4b85-b43c-37ccd6fd2918 0xc00314c077 0xc00314c078}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.302: INFO: Pod "webserver-deployment-c7997dcc8-vsn2d" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vsn2d webserver-deployment-c7997dcc8- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-c7997dcc8-vsn2d c435ab35-ebf0-4103-954d-e8c7a6365f2e 26098 0 2020-06-22 05:52:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 b4344e27-6ea4-4b85-b43c-37ccd6fd2918 0xc00314c1b0 0xc00314c1b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 05:52:36.302: INFO: Pod "webserver-deployment-c7997dcc8-wd6cj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wd6cj webserver-deployment-c7997dcc8- deployment-6026 /api/v1/namespaces/deployment-6026/pods/webserver-deployment-c7997dcc8-wd6cj 7c53eafe-313c-4374-8fbf-0642200da6ea 26089 0 2020-06-22 05:52:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 b4344e27-6ea4-4b85-b43c-37ccd6fd2918 0xc00314c2d0 0xc00314c2d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrnfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:52:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:52:36.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6026" for this suite.

• [SLOW TEST:6.675 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":141,"skipped":2477,"failed":0}
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:52:36.387: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:52:36.947: INFO: (0) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 298.871334ms)
Jun 22 05:52:37.037: INFO: (1) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 90.173369ms)
Jun 22 05:52:37.057: INFO: (2) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 20.563544ms)
Jun 22 05:52:37.078: INFO: (3) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 21.002953ms)
Jun 22 05:52:37.093: INFO: (4) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 14.839885ms)
Jun 22 05:52:37.116: INFO: (5) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 22.80518ms)
Jun 22 05:52:37.142: INFO: (6) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 25.2074ms)
Jun 22 05:52:37.188: INFO: (7) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 46.907758ms)
Jun 22 05:52:37.239: INFO: (8) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 50.360378ms)
Jun 22 05:52:37.270: INFO: (9) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 30.679583ms)
Jun 22 05:52:37.293: INFO: (10) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 22.80477ms)
Jun 22 05:52:37.310: INFO: (11) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 17.336727ms)
Jun 22 05:52:37.353: INFO: (12) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 42.576528ms)
Jun 22 05:52:37.383: INFO: (13) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 30.329204ms)
Jun 22 05:52:37.426: INFO: (14) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 43.483647ms)
Jun 22 05:52:37.461: INFO: (15) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 34.084053ms)
Jun 22 05:52:37.484: INFO: (16) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 23.74044ms)
Jun 22 05:52:37.499: INFO: (17) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 14.09248ms)
Jun 22 05:52:37.529: INFO: (18) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 29.896375ms)
Jun 22 05:52:37.546: INFO: (19) /api/v1/nodes/node4:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 17.125933ms)
[AfterEach] version v1
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:52:37.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7481" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":142,"skipped":2482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:52:37.592: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5536
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:52:38.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5536" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":143,"skipped":2504,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:52:38.459: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4944
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Jun 22 05:52:38.874: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Jun 22 05:52:38.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-4944'
Jun 22 05:52:39.617: INFO: stderr: ""
Jun 22 05:52:39.617: INFO: stdout: "service/agnhost-slave created\n"
Jun 22 05:52:39.618: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Jun 22 05:52:39.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-4944'
Jun 22 05:52:40.159: INFO: stderr: ""
Jun 22 05:52:40.159: INFO: stdout: "service/agnhost-master created\n"
Jun 22 05:52:40.160: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 22 05:52:40.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-4944'
Jun 22 05:52:40.532: INFO: stderr: ""
Jun 22 05:52:40.532: INFO: stdout: "service/frontend created\n"
Jun 22 05:52:40.532: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun 22 05:52:40.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-4944'
Jun 22 05:52:40.913: INFO: stderr: ""
Jun 22 05:52:40.913: INFO: stdout: "deployment.apps/frontend created\n"
Jun 22 05:52:40.913: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 22 05:52:40.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-4944'
Jun 22 05:52:41.305: INFO: stderr: ""
Jun 22 05:52:41.305: INFO: stdout: "deployment.apps/agnhost-master created\n"
Jun 22 05:52:41.306: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 22 05:52:41.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-4944'
Jun 22 05:52:41.719: INFO: stderr: ""
Jun 22 05:52:41.719: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Jun 22 05:52:41.719: INFO: Waiting for all frontend pods to be Running.
Jun 22 05:52:46.775: INFO: Waiting for frontend to serve content.
Jun 22 05:52:47.258: INFO: Trying to add a new entry to the guestbook.
Jun 22 05:52:47.483: INFO: Verifying that added entry can be retrieved.
Jun 22 05:52:47.605: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Jun 22 05:52:52.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete --grace-period=0 --force -f - --namespace=kubectl-4944'
Jun 22 05:52:52.946: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 05:52:52.946: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 05:52:52.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete --grace-period=0 --force -f - --namespace=kubectl-4944'
Jun 22 05:52:53.197: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 05:52:53.197: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 05:52:53.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete --grace-period=0 --force -f - --namespace=kubectl-4944'
Jun 22 05:52:53.412: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 05:52:53.412: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 05:52:53.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete --grace-period=0 --force -f - --namespace=kubectl-4944'
Jun 22 05:52:53.574: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 05:52:53.574: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 05:52:53.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete --grace-period=0 --force -f - --namespace=kubectl-4944'
Jun 22 05:52:53.779: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 05:52:53.779: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 05:52:53.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete --grace-period=0 --force -f - --namespace=kubectl-4944'
Jun 22 05:52:53.957: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 05:52:53.957: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:52:53.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4944" for this suite.

• [SLOW TEST:15.528 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:380
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":144,"skipped":2507,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:52:53.988: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7797
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-e082f043-8769-49fd-b4e7-a5e1d85c40e2
STEP: Creating a pod to test consume secrets
Jun 22 05:52:54.308: INFO: Waiting up to 5m0s for pod "pod-secrets-ef7ec20b-dc49-486e-b0de-420ececf4b30" in namespace "secrets-7797" to be "success or failure"
Jun 22 05:52:54.320: INFO: Pod "pod-secrets-ef7ec20b-dc49-486e-b0de-420ececf4b30": Phase="Pending", Reason="", readiness=false. Elapsed: 11.6285ms
Jun 22 05:52:56.326: INFO: Pod "pod-secrets-ef7ec20b-dc49-486e-b0de-420ececf4b30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01734581s
STEP: Saw pod success
Jun 22 05:52:56.326: INFO: Pod "pod-secrets-ef7ec20b-dc49-486e-b0de-420ececf4b30" satisfied condition "success or failure"
Jun 22 05:52:56.333: INFO: Trying to get logs from node node4 pod pod-secrets-ef7ec20b-dc49-486e-b0de-420ececf4b30 container secret-env-test: <nil>
STEP: delete the pod
Jun 22 05:52:56.376: INFO: Waiting for pod pod-secrets-ef7ec20b-dc49-486e-b0de-420ececf4b30 to disappear
Jun 22 05:52:56.384: INFO: Pod pod-secrets-ef7ec20b-dc49-486e-b0de-420ececf4b30 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:52:56.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7797" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":145,"skipped":2510,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:52:56.410: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8086
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jun 22 05:52:56.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-8086'
Jun 22 05:52:56.897: INFO: stderr: ""
Jun 22 05:52:56.898: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jun 22 05:52:57.904: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 05:52:57.904: INFO: Found 0 / 1
Jun 22 05:52:58.911: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 05:52:58.911: INFO: Found 1 / 1
Jun 22 05:52:58.911: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 22 05:52:58.917: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 05:52:58.917: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 05:52:58.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 patch pod agnhost-master-m4jxd --namespace=kubectl-8086 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 22 05:52:59.040: INFO: stderr: ""
Jun 22 05:52:59.040: INFO: stdout: "pod/agnhost-master-m4jxd patched\n"
STEP: checking annotations
Jun 22 05:52:59.049: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 05:52:59.049: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:52:59.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8086" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":146,"skipped":2515,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:52:59.074: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun 22 05:52:59.279: INFO: Waiting up to 5m0s for pod "downward-api-7177c073-87ba-4880-bd08-0c7c633b2025" in namespace "downward-api-4623" to be "success or failure"
Jun 22 05:52:59.288: INFO: Pod "downward-api-7177c073-87ba-4880-bd08-0c7c633b2025": Phase="Pending", Reason="", readiness=false. Elapsed: 8.801891ms
Jun 22 05:53:01.296: INFO: Pod "downward-api-7177c073-87ba-4880-bd08-0c7c633b2025": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016038893s
STEP: Saw pod success
Jun 22 05:53:01.296: INFO: Pod "downward-api-7177c073-87ba-4880-bd08-0c7c633b2025" satisfied condition "success or failure"
Jun 22 05:53:01.305: INFO: Trying to get logs from node node4 pod downward-api-7177c073-87ba-4880-bd08-0c7c633b2025 container dapi-container: <nil>
STEP: delete the pod
Jun 22 05:53:01.369: INFO: Waiting for pod downward-api-7177c073-87ba-4880-bd08-0c7c633b2025 to disappear
Jun 22 05:53:01.375: INFO: Pod downward-api-7177c073-87ba-4880-bd08-0c7c633b2025 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:53:01.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4623" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":147,"skipped":2527,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:53:01.395: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-392
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Jun 22 05:53:01.584: INFO: Waiting up to 5m0s for pod "var-expansion-86cb23ff-da06-4836-ba9c-c0139790d9b5" in namespace "var-expansion-392" to be "success or failure"
Jun 22 05:53:01.590: INFO: Pod "var-expansion-86cb23ff-da06-4836-ba9c-c0139790d9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.532194ms
Jun 22 05:53:03.604: INFO: Pod "var-expansion-86cb23ff-da06-4836-ba9c-c0139790d9b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019482175s
STEP: Saw pod success
Jun 22 05:53:03.604: INFO: Pod "var-expansion-86cb23ff-da06-4836-ba9c-c0139790d9b5" satisfied condition "success or failure"
Jun 22 05:53:03.613: INFO: Trying to get logs from node node4 pod var-expansion-86cb23ff-da06-4836-ba9c-c0139790d9b5 container dapi-container: <nil>
STEP: delete the pod
Jun 22 05:53:03.655: INFO: Waiting for pod var-expansion-86cb23ff-da06-4836-ba9c-c0139790d9b5 to disappear
Jun 22 05:53:03.659: INFO: Pod var-expansion-86cb23ff-da06-4836-ba9c-c0139790d9b5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:53:03.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-392" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":148,"skipped":2532,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:53:03.677: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5213
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-5213
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 05:53:03.854: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 05:53:28.032: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.41.130.167:8080/dial?request=hostname&protocol=http&host=10.37.149.55&port=8080&tries=1'] Namespace:pod-network-test-5213 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:53:28.032: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:53:28.210: INFO: Waiting for responses: map[]
Jun 22 05:53:28.216: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.41.130.167:8080/dial?request=hostname&protocol=http&host=10.37.11.26&port=8080&tries=1'] Namespace:pod-network-test-5213 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:53:28.216: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:53:28.379: INFO: Waiting for responses: map[]
Jun 22 05:53:28.386: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.41.130.167:8080/dial?request=hostname&protocol=http&host=10.42.135.34&port=8080&tries=1'] Namespace:pod-network-test-5213 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:53:28.386: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:53:28.548: INFO: Waiting for responses: map[]
Jun 22 05:53:28.564: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.41.130.167:8080/dial?request=hostname&protocol=http&host=10.41.130.165&port=8080&tries=1'] Namespace:pod-network-test-5213 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:53:28.564: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 05:53:28.722: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:53:28.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5213" for this suite.

• [SLOW TEST:25.064 seconds]
[sig-network] Networking
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":149,"skipped":2537,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:53:28.741: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-jfrc
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 05:53:28.934: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-jfrc" in namespace "subpath-9814" to be "success or failure"
Jun 22 05:53:28.944: INFO: Pod "pod-subpath-test-projected-jfrc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.924805ms
Jun 22 05:53:30.951: INFO: Pod "pod-subpath-test-projected-jfrc": Phase="Running", Reason="", readiness=true. Elapsed: 2.016915062s
Jun 22 05:53:32.962: INFO: Pod "pod-subpath-test-projected-jfrc": Phase="Running", Reason="", readiness=true. Elapsed: 4.027962553s
Jun 22 05:53:34.978: INFO: Pod "pod-subpath-test-projected-jfrc": Phase="Running", Reason="", readiness=true. Elapsed: 6.044313653s
Jun 22 05:53:36.983: INFO: Pod "pod-subpath-test-projected-jfrc": Phase="Running", Reason="", readiness=true. Elapsed: 8.04935755s
Jun 22 05:53:38.993: INFO: Pod "pod-subpath-test-projected-jfrc": Phase="Running", Reason="", readiness=true. Elapsed: 10.059437161s
Jun 22 05:53:41.003: INFO: Pod "pod-subpath-test-projected-jfrc": Phase="Running", Reason="", readiness=true. Elapsed: 12.069022262s
Jun 22 05:53:43.012: INFO: Pod "pod-subpath-test-projected-jfrc": Phase="Running", Reason="", readiness=true. Elapsed: 14.077809996s
Jun 22 05:53:45.017: INFO: Pod "pod-subpath-test-projected-jfrc": Phase="Running", Reason="", readiness=true. Elapsed: 16.083176559s
Jun 22 05:53:47.025: INFO: Pod "pod-subpath-test-projected-jfrc": Phase="Running", Reason="", readiness=true. Elapsed: 18.091170198s
Jun 22 05:53:49.035: INFO: Pod "pod-subpath-test-projected-jfrc": Phase="Running", Reason="", readiness=true. Elapsed: 20.101547694s
Jun 22 05:53:51.047: INFO: Pod "pod-subpath-test-projected-jfrc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.112911602s
STEP: Saw pod success
Jun 22 05:53:51.047: INFO: Pod "pod-subpath-test-projected-jfrc" satisfied condition "success or failure"
Jun 22 05:53:51.053: INFO: Trying to get logs from node node4 pod pod-subpath-test-projected-jfrc container test-container-subpath-projected-jfrc: <nil>
STEP: delete the pod
Jun 22 05:53:51.088: INFO: Waiting for pod pod-subpath-test-projected-jfrc to disappear
Jun 22 05:53:51.095: INFO: Pod pod-subpath-test-projected-jfrc no longer exists
STEP: Deleting pod pod-subpath-test-projected-jfrc
Jun 22 05:53:51.095: INFO: Deleting pod "pod-subpath-test-projected-jfrc" in namespace "subpath-9814"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:53:51.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9814" for this suite.

• [SLOW TEST:22.372 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":150,"skipped":2586,"failed":0}
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:53:51.114: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-4e8eccb2-e4ee-4f30-a722-7a2322dff38b
Jun 22 05:53:51.286: INFO: Pod name my-hostname-basic-4e8eccb2-e4ee-4f30-a722-7a2322dff38b: Found 0 pods out of 1
Jun 22 05:53:56.293: INFO: Pod name my-hostname-basic-4e8eccb2-e4ee-4f30-a722-7a2322dff38b: Found 1 pods out of 1
Jun 22 05:53:56.293: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4e8eccb2-e4ee-4f30-a722-7a2322dff38b" are running
Jun 22 05:53:56.302: INFO: Pod "my-hostname-basic-4e8eccb2-e4ee-4f30-a722-7a2322dff38b-5q7jf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 05:53:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 05:53:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 05:53:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 05:53:51 +0000 UTC Reason: Message:}])
Jun 22 05:53:56.302: INFO: Trying to dial the pod
Jun 22 05:54:01.322: INFO: Controller my-hostname-basic-4e8eccb2-e4ee-4f30-a722-7a2322dff38b: Got expected result from replica 1 [my-hostname-basic-4e8eccb2-e4ee-4f30-a722-7a2322dff38b-5q7jf]: "my-hostname-basic-4e8eccb2-e4ee-4f30-a722-7a2322dff38b-5q7jf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:54:01.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5199" for this suite.

• [SLOW TEST:10.224 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":151,"skipped":2586,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:54:01.338: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2606
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 05:54:01.516: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 22 05:54:06.527: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 05:54:06.527: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun 22 05:54:08.589: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2606 /apis/apps/v1/namespaces/deployment-2606/deployments/test-cleanup-deployment f83528ad-0d83-4415-a20d-953e7a4767d9 27650 1 2020-06-22 05:54:06 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000a28228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-22 05:54:06 +0000 UTC,LastTransitionTime:2020-06-22 05:54:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-55ffc6b7b6" has successfully progressed.,LastUpdateTime:2020-06-22 05:54:07 +0000 UTC,LastTransitionTime:2020-06-22 05:54:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 05:54:08.595: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-2606 /apis/apps/v1/namespaces/deployment-2606/replicasets/test-cleanup-deployment-55ffc6b7b6 004e63a4-49ea-46f3-8db4-e2144ef1e819 27639 1 2020-06-22 05:54:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment f83528ad-0d83-4415-a20d-953e7a4767d9 0xc000a28847 0xc000a28848}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000a288b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 05:54:08.603: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-qqnfw" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-qqnfw test-cleanup-deployment-55ffc6b7b6- deployment-2606 /api/v1/namespaces/deployment-2606/pods/test-cleanup-deployment-55ffc6b7b6-qqnfw e132035d-e503-43c0-b41e-752092cbfdfb 27638 0 2020-06-22 05:54:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[cni.projectcalico.org/podIP:10.41.130.170/32 cni.projectcalico.org/podIPs:10.41.130.170/32] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 004e63a4-49ea-46f3-8db4-e2144ef1e819 0xc000a29067 0xc000a29068}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pvqfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pvqfj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pvqfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:54:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:54:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:54:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 05:54:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.144,PodIP:10.41.130.170,StartTime:2020-06-22 05:54:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 05:54:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://07ec9d7fab33c393071d327bffe2802d0ce6727767f0196293541d6cbeeae4b6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.41.130.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:54:08.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2606" for this suite.

• [SLOW TEST:7.282 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":152,"skipped":2607,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:54:08.620: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3034
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 22 05:54:08.826: INFO: Waiting up to 5m0s for pod "pod-c99fc982-8caf-4ed5-a4e5-52b7ee627f50" in namespace "emptydir-3034" to be "success or failure"
Jun 22 05:54:08.833: INFO: Pod "pod-c99fc982-8caf-4ed5-a4e5-52b7ee627f50": Phase="Pending", Reason="", readiness=false. Elapsed: 7.076106ms
Jun 22 05:54:10.837: INFO: Pod "pod-c99fc982-8caf-4ed5-a4e5-52b7ee627f50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011615753s
Jun 22 05:54:12.843: INFO: Pod "pod-c99fc982-8caf-4ed5-a4e5-52b7ee627f50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017065498s
STEP: Saw pod success
Jun 22 05:54:12.843: INFO: Pod "pod-c99fc982-8caf-4ed5-a4e5-52b7ee627f50" satisfied condition "success or failure"
Jun 22 05:54:12.857: INFO: Trying to get logs from node node4 pod pod-c99fc982-8caf-4ed5-a4e5-52b7ee627f50 container test-container: <nil>
STEP: delete the pod
Jun 22 05:54:12.917: INFO: Waiting for pod pod-c99fc982-8caf-4ed5-a4e5-52b7ee627f50 to disappear
Jun 22 05:54:12.922: INFO: Pod pod-c99fc982-8caf-4ed5-a4e5-52b7ee627f50 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:54:12.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3034" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":153,"skipped":2614,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:54:12.939: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-280
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-a659e5f0-38a0-4ba8-b224-f0b7233a5cf0
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:54:17.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-280" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":154,"skipped":2622,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:54:17.243: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-593
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 22 05:54:17.851: INFO: Pod name wrapped-volume-race-27e868d7-5d89-4918-9820-3e026482cb5b: Found 0 pods out of 5
Jun 22 05:54:22.862: INFO: Pod name wrapped-volume-race-27e868d7-5d89-4918-9820-3e026482cb5b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-27e868d7-5d89-4918-9820-3e026482cb5b in namespace emptydir-wrapper-593, will wait for the garbage collector to delete the pods
Jun 22 05:54:32.980: INFO: Deleting ReplicationController wrapped-volume-race-27e868d7-5d89-4918-9820-3e026482cb5b took: 18.929539ms
Jun 22 05:54:33.481: INFO: Terminating ReplicationController wrapped-volume-race-27e868d7-5d89-4918-9820-3e026482cb5b pods took: 500.219227ms
STEP: Creating RC which spawns configmap-volume pods
Jun 22 05:54:43.303: INFO: Pod name wrapped-volume-race-278c88e4-e31f-4626-aefd-0ec8e9aa3982: Found 0 pods out of 5
Jun 22 05:54:48.316: INFO: Pod name wrapped-volume-race-278c88e4-e31f-4626-aefd-0ec8e9aa3982: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-278c88e4-e31f-4626-aefd-0ec8e9aa3982 in namespace emptydir-wrapper-593, will wait for the garbage collector to delete the pods
Jun 22 05:55:00.435: INFO: Deleting ReplicationController wrapped-volume-race-278c88e4-e31f-4626-aefd-0ec8e9aa3982 took: 15.233035ms
Jun 22 05:55:00.836: INFO: Terminating ReplicationController wrapped-volume-race-278c88e4-e31f-4626-aefd-0ec8e9aa3982 pods took: 400.244923ms
STEP: Creating RC which spawns configmap-volume pods
Jun 22 05:55:06.567: INFO: Pod name wrapped-volume-race-dbccdc64-9d93-4a16-a9b3-79de2e01fa7d: Found 0 pods out of 5
Jun 22 05:55:11.592: INFO: Pod name wrapped-volume-race-dbccdc64-9d93-4a16-a9b3-79de2e01fa7d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-dbccdc64-9d93-4a16-a9b3-79de2e01fa7d in namespace emptydir-wrapper-593, will wait for the garbage collector to delete the pods
Jun 22 05:55:21.705: INFO: Deleting ReplicationController wrapped-volume-race-dbccdc64-9d93-4a16-a9b3-79de2e01fa7d took: 16.46534ms
Jun 22 05:55:22.205: INFO: Terminating ReplicationController wrapped-volume-race-dbccdc64-9d93-4a16-a9b3-79de2e01fa7d pods took: 500.382118ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:55:29.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-593" for this suite.

• [SLOW TEST:71.905 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":155,"skipped":2623,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:55:29.149: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7385
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-2dbd3aad-2ff8-4735-9c58-93f74ca7fa08
STEP: Creating a pod to test consume configMaps
Jun 22 05:55:29.336: INFO: Waiting up to 5m0s for pod "pod-configmaps-21f6d2fd-0dd9-430c-a5d5-f73c8e055057" in namespace "configmap-7385" to be "success or failure"
Jun 22 05:55:29.348: INFO: Pod "pod-configmaps-21f6d2fd-0dd9-430c-a5d5-f73c8e055057": Phase="Pending", Reason="", readiness=false. Elapsed: 11.080378ms
Jun 22 05:55:31.356: INFO: Pod "pod-configmaps-21f6d2fd-0dd9-430c-a5d5-f73c8e055057": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019893283s
STEP: Saw pod success
Jun 22 05:55:31.356: INFO: Pod "pod-configmaps-21f6d2fd-0dd9-430c-a5d5-f73c8e055057" satisfied condition "success or failure"
Jun 22 05:55:31.362: INFO: Trying to get logs from node node4 pod pod-configmaps-21f6d2fd-0dd9-430c-a5d5-f73c8e055057 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 05:55:31.400: INFO: Waiting for pod pod-configmaps-21f6d2fd-0dd9-430c-a5d5-f73c8e055057 to disappear
Jun 22 05:55:31.407: INFO: Pod pod-configmaps-21f6d2fd-0dd9-430c-a5d5-f73c8e055057 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:55:31.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7385" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":156,"skipped":2632,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:55:31.431: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2242
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jun 22 05:55:32.707: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0622 05:55:32.707822      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 22 05:55:32.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2242" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":157,"skipped":2633,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:55:32.736: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7070
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-61bc46cf-4aba-4989-85d5-5d48626dc3f9 in namespace container-probe-7070
Jun 22 05:55:34.959: INFO: Started pod liveness-61bc46cf-4aba-4989-85d5-5d48626dc3f9 in namespace container-probe-7070
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 05:55:34.966: INFO: Initial restart count of pod liveness-61bc46cf-4aba-4989-85d5-5d48626dc3f9 is 0
Jun 22 05:55:55.045: INFO: Restart count of pod container-probe-7070/liveness-61bc46cf-4aba-4989-85d5-5d48626dc3f9 is now 1 (20.079003329s elapsed)
Jun 22 05:56:15.126: INFO: Restart count of pod container-probe-7070/liveness-61bc46cf-4aba-4989-85d5-5d48626dc3f9 is now 2 (40.159930348s elapsed)
Jun 22 05:56:35.198: INFO: Restart count of pod container-probe-7070/liveness-61bc46cf-4aba-4989-85d5-5d48626dc3f9 is now 3 (1m0.232025735s elapsed)
Jun 22 05:56:55.279: INFO: Restart count of pod container-probe-7070/liveness-61bc46cf-4aba-4989-85d5-5d48626dc3f9 is now 4 (1m20.312904923s elapsed)
Jun 22 05:57:57.555: INFO: Restart count of pod container-probe-7070/liveness-61bc46cf-4aba-4989-85d5-5d48626dc3f9 is now 5 (2m22.589606843s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 05:57:57.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7070" for this suite.

• [SLOW TEST:144.874 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":158,"skipped":2634,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 05:57:57.610: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-7755
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-7755
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7755
Jun 22 05:57:57.835: INFO: Found 0 stateful pods, waiting for 1
Jun 22 05:58:07.848: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 22 05:58:07.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 05:58:08.168: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 05:58:08.168: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 05:58:08.168: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 05:58:08.174: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 22 05:58:18.182: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 05:58:18.182: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 05:58:18.213: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jun 22 05:58:18.213: INFO: ss-0  node4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  }]
Jun 22 05:58:18.213: INFO: 
Jun 22 05:58:18.213: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 22 05:58:19.227: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989254537s
Jun 22 05:58:20.239: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97538377s
Jun 22 05:58:21.250: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.961164054s
Jun 22 05:58:22.263: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.952139377s
Jun 22 05:58:23.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.940724071s
Jun 22 05:58:24.282: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.927979366s
Jun 22 05:58:25.291: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.920835147s
Jun 22 05:58:26.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.909480211s
Jun 22 05:58:27.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 903.212731ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7755
Jun 22 05:58:28.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 05:58:28.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 05:58:28.609: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 05:58:28.609: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 05:58:28.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 05:58:28.956: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 22 05:58:28.956: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 05:58:28.956: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 05:58:28.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 05:58:29.302: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 22 05:58:29.302: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 05:58:29.302: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 05:58:29.311: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jun 22 05:58:39.322: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 05:58:39.322: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 05:58:39.322: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 22 05:58:39.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 05:58:39.679: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 05:58:39.679: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 05:58:39.679: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 05:58:39.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 05:58:39.975: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 05:58:39.975: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 05:58:39.975: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 05:58:39.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 05:58:40.304: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 05:58:40.304: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 05:58:40.304: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 05:58:40.304: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 05:58:40.309: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 22 05:58:50.324: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 05:58:50.324: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 05:58:50.324: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 05:58:50.355: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jun 22 05:58:50.355: INFO: ss-0  node4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  }]
Jun 22 05:58:50.355: INFO: ss-1  node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  }]
Jun 22 05:58:50.355: INFO: ss-2  node3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  }]
Jun 22 05:58:50.355: INFO: 
Jun 22 05:58:50.355: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 05:58:51.366: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jun 22 05:58:51.366: INFO: ss-0  node4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  }]
Jun 22 05:58:51.366: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  }]
Jun 22 05:58:51.366: INFO: ss-2  node3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  }]
Jun 22 05:58:51.366: INFO: 
Jun 22 05:58:51.366: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 05:58:52.376: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jun 22 05:58:52.376: INFO: ss-0  node4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  }]
Jun 22 05:58:52.376: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  }]
Jun 22 05:58:52.376: INFO: ss-2  node3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  }]
Jun 22 05:58:52.376: INFO: 
Jun 22 05:58:52.376: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 05:58:53.382: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jun 22 05:58:53.382: INFO: ss-0  node4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  }]
Jun 22 05:58:53.382: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  }]
Jun 22 05:58:53.382: INFO: 
Jun 22 05:58:53.382: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 05:58:54.391: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jun 22 05:58:54.391: INFO: ss-0  node4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  }]
Jun 22 05:58:54.391: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  }]
Jun 22 05:58:54.391: INFO: 
Jun 22 05:58:54.391: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 05:58:55.401: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jun 22 05:58:55.402: INFO: ss-0  node4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  }]
Jun 22 05:58:55.402: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  }]
Jun 22 05:58:55.402: INFO: 
Jun 22 05:58:55.402: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 05:58:56.411: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jun 22 05:58:56.411: INFO: ss-0  node4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  }]
Jun 22 05:58:56.411: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  }]
Jun 22 05:58:56.411: INFO: 
Jun 22 05:58:56.411: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 05:58:57.420: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jun 22 05:58:57.420: INFO: ss-0  node4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  }]
Jun 22 05:58:57.420: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  }]
Jun 22 05:58:57.420: INFO: 
Jun 22 05:58:57.420: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 05:58:58.430: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jun 22 05:58:58.430: INFO: ss-0  node4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  }]
Jun 22 05:58:58.430: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  }]
Jun 22 05:58:58.430: INFO: 
Jun 22 05:58:58.430: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 05:58:59.442: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jun 22 05:58:59.442: INFO: ss-0  node4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:57:57 +0000 UTC  }]
Jun 22 05:58:59.442: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 05:58:18 +0000 UTC  }]
Jun 22 05:58:59.442: INFO: 
Jun 22 05:58:59.442: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7755
Jun 22 05:59:00.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 05:59:00.624: INFO: rc: 1
Jun 22 05:59:00.624: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jun 22 05:59:10.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 05:59:10.751: INFO: rc: 1
Jun 22 05:59:10.751: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 05:59:20.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 05:59:20.862: INFO: rc: 1
Jun 22 05:59:20.862: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 05:59:30.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 05:59:30.986: INFO: rc: 1
Jun 22 05:59:30.986: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 05:59:41.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 05:59:41.212: INFO: rc: 1
Jun 22 05:59:41.212: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 05:59:51.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 05:59:51.337: INFO: rc: 1
Jun 22 05:59:51.337: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:00:01.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:00:01.469: INFO: rc: 1
Jun 22 06:00:01.469: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:00:11.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:00:11.607: INFO: rc: 1
Jun 22 06:00:11.607: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:00:21.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:00:21.733: INFO: rc: 1
Jun 22 06:00:21.733: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:00:31.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:00:31.866: INFO: rc: 1
Jun 22 06:00:31.866: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:00:41.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:00:42.001: INFO: rc: 1
Jun 22 06:00:42.001: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:00:52.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:00:52.136: INFO: rc: 1
Jun 22 06:00:52.136: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:01:02.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:01:02.279: INFO: rc: 1
Jun 22 06:01:02.279: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:01:12.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:01:12.404: INFO: rc: 1
Jun 22 06:01:12.404: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:01:22.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:01:22.537: INFO: rc: 1
Jun 22 06:01:22.537: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:01:32.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:01:32.655: INFO: rc: 1
Jun 22 06:01:32.655: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:01:42.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:01:42.775: INFO: rc: 1
Jun 22 06:01:42.775: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:01:52.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:01:52.907: INFO: rc: 1
Jun 22 06:01:52.907: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:02:02.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:02:03.045: INFO: rc: 1
Jun 22 06:02:03.045: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:02:13.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:02:13.172: INFO: rc: 1
Jun 22 06:02:13.172: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:02:23.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:02:23.311: INFO: rc: 1
Jun 22 06:02:23.311: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:02:33.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:02:33.440: INFO: rc: 1
Jun 22 06:02:33.440: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:02:43.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:02:43.577: INFO: rc: 1
Jun 22 06:02:43.577: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:02:53.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:02:53.829: INFO: rc: 1
Jun 22 06:02:53.829: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:03:03.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:03:03.950: INFO: rc: 1
Jun 22 06:03:03.950: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:03:13.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:03:14.071: INFO: rc: 1
Jun 22 06:03:14.071: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:03:24.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:03:24.186: INFO: rc: 1
Jun 22 06:03:24.186: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:03:34.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:03:34.311: INFO: rc: 1
Jun 22 06:03:34.311: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:03:44.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:03:44.449: INFO: rc: 1
Jun 22 06:03:44.449: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:03:54.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:03:54.576: INFO: rc: 1
Jun 22 06:03:54.576: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 06:04:04.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-7755 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:04:04.710: INFO: rc: 1
Jun 22 06:04:04.710: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Jun 22 06:04:04.710: INFO: Scaling statefulset ss to 0
Jun 22 06:04:04.727: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun 22 06:04:04.732: INFO: Deleting all statefulset in ns statefulset-7755
Jun 22 06:04:04.737: INFO: Scaling statefulset ss to 0
Jun 22 06:04:04.754: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 06:04:04.759: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:04:04.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7755" for this suite.

• [SLOW TEST:367.200 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":159,"skipped":2639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:04:04.811: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-81
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-a0c63f94-0a2c-47cb-b04f-e5d862115ee0
STEP: Creating configMap with name cm-test-opt-upd-07b1dc86-8829-49d9-8d36-185206630151
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a0c63f94-0a2c-47cb-b04f-e5d862115ee0
STEP: Updating configmap cm-test-opt-upd-07b1dc86-8829-49d9-8d36-185206630151
STEP: Creating configMap with name cm-test-opt-create-5d0c0c94-cd2f-4b6c-b51b-aa672a25c436
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:04:09.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-81" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":160,"skipped":2687,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:04:09.246: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7962
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jun 22 06:04:09.448: INFO: Created pod &Pod{ObjectMeta:{dns-7962  dns-7962 /api/v1/namespaces/dns-7962/pods/dns-7962 f40e231f-89ea-4b9a-9807-e196cec1ceb0 31535 0 2020-06-22 06:04:09 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mhrts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mhrts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mhrts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Jun 22 06:04:11.465: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7962 PodName:dns-7962 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 06:04:11.465: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Verifying customized DNS server is configured on pod...
Jun 22 06:04:11.665: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7962 PodName:dns-7962 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 06:04:11.665: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 06:04:11.839: INFO: Deleting pod dns-7962...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:04:11.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7962" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":161,"skipped":2697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:04:11.896: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8066
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8066.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8066.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8066.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8066.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8066.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8066.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8066.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 154.84.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.84.154_udp@PTR;check="$$(dig +tcp +noall +answer +search 154.84.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.84.154_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8066.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8066.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8066.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8066.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8066.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8066.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8066.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 154.84.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.84.154_udp@PTR;check="$$(dig +tcp +noall +answer +search 154.84.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.84.154_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 06:04:36.184: INFO: Unable to read wheezy_udp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:36.196: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:36.204: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:36.218: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:36.285: INFO: Unable to read jessie_udp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:36.290: INFO: Unable to read jessie_tcp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:36.299: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:36.307: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:36.349: INFO: Lookups using dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e failed for: [wheezy_udp@dns-test-service.dns-8066.svc.cluster.local wheezy_tcp@dns-test-service.dns-8066.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local jessie_udp@dns-test-service.dns-8066.svc.cluster.local jessie_tcp@dns-test-service.dns-8066.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local]

Jun 22 06:04:41.357: INFO: Unable to read wheezy_udp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:41.364: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:41.385: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:41.391: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:41.446: INFO: Unable to read jessie_udp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:41.454: INFO: Unable to read jessie_tcp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:41.459: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:41.466: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:41.520: INFO: Lookups using dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e failed for: [wheezy_udp@dns-test-service.dns-8066.svc.cluster.local wheezy_tcp@dns-test-service.dns-8066.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local jessie_udp@dns-test-service.dns-8066.svc.cluster.local jessie_tcp@dns-test-service.dns-8066.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local]

Jun 22 06:04:46.359: INFO: Unable to read wheezy_udp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:46.366: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:46.373: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:46.380: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:46.438: INFO: Unable to read jessie_udp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:46.449: INFO: Unable to read jessie_tcp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:46.471: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:46.477: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:46.538: INFO: Lookups using dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e failed for: [wheezy_udp@dns-test-service.dns-8066.svc.cluster.local wheezy_tcp@dns-test-service.dns-8066.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local jessie_udp@dns-test-service.dns-8066.svc.cluster.local jessie_tcp@dns-test-service.dns-8066.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8066.svc.cluster.local]

Jun 22 06:04:51.366: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:51.435: INFO: Unable to read jessie_tcp@dns-test-service.dns-8066.svc.cluster.local from pod dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e: the server could not find the requested resource (get pods dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e)
Jun 22 06:04:51.505: INFO: Lookups using dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e failed for: [wheezy_tcp@dns-test-service.dns-8066.svc.cluster.local jessie_tcp@dns-test-service.dns-8066.svc.cluster.local]

Jun 22 06:04:56.499: INFO: DNS probes using dns-8066/dns-test-c791e872-0d73-42b0-807d-2de55f1dc94e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:04:56.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8066" for this suite.

• [SLOW TEST:44.882 seconds]
[sig-network] DNS
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":162,"skipped":2749,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:04:56.779: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1907
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1907
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1907
STEP: creating replication controller externalsvc in namespace services-1907
I0622 06:04:57.059388      22 runners.go:189] Created replication controller with name: externalsvc, namespace: services-1907, replica count: 2
I0622 06:05:00.110352      22 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun 22 06:05:00.152: INFO: Creating new exec pod
Jun 22 06:05:02.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=services-1907 execpodkcxv8 -- /bin/sh -x -c nslookup clusterip-service'
Jun 22 06:05:02.553: INFO: stderr: "+ nslookup clusterip-service\n"
Jun 22 06:05:02.553: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-1907.svc.cluster.local\tcanonical name = externalsvc.services-1907.svc.cluster.local.\nName:\texternalsvc.services-1907.svc.cluster.local\nAddress: 10.101.165.103\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1907, will wait for the garbage collector to delete the pods
Jun 22 06:05:02.629: INFO: Deleting ReplicationController externalsvc took: 18.280924ms
Jun 22 06:05:03.030: INFO: Terminating ReplicationController externalsvc pods took: 400.41948ms
Jun 22 06:05:13.600: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:05:13.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1907" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:16.909 seconds]
[sig-network] Services
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":163,"skipped":2779,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:05:13.688: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-822
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun 22 06:05:13.907: INFO: Waiting up to 5m0s for pod "downward-api-06b26921-243a-4a67-acaa-44b02b97a051" in namespace "downward-api-822" to be "success or failure"
Jun 22 06:05:13.928: INFO: Pod "downward-api-06b26921-243a-4a67-acaa-44b02b97a051": Phase="Pending", Reason="", readiness=false. Elapsed: 21.01378ms
Jun 22 06:05:15.937: INFO: Pod "downward-api-06b26921-243a-4a67-acaa-44b02b97a051": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030177493s
STEP: Saw pod success
Jun 22 06:05:15.937: INFO: Pod "downward-api-06b26921-243a-4a67-acaa-44b02b97a051" satisfied condition "success or failure"
Jun 22 06:05:15.944: INFO: Trying to get logs from node node4 pod downward-api-06b26921-243a-4a67-acaa-44b02b97a051 container dapi-container: <nil>
STEP: delete the pod
Jun 22 06:05:15.984: INFO: Waiting for pod downward-api-06b26921-243a-4a67-acaa-44b02b97a051 to disappear
Jun 22 06:05:15.998: INFO: Pod downward-api-06b26921-243a-4a67-acaa-44b02b97a051 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:05:15.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-822" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":164,"skipped":2800,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:05:16.021: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4597
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-8fc98d9a-0359-4c28-9138-b3a3e71602b7
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:05:16.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4597" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":165,"skipped":2829,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:05:16.241: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6294
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-cvjf
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 06:05:16.447: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cvjf" in namespace "subpath-6294" to be "success or failure"
Jun 22 06:05:16.454: INFO: Pod "pod-subpath-test-configmap-cvjf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.992804ms
Jun 22 06:05:18.460: INFO: Pod "pod-subpath-test-configmap-cvjf": Phase="Running", Reason="", readiness=true. Elapsed: 2.012652224s
Jun 22 06:05:20.468: INFO: Pod "pod-subpath-test-configmap-cvjf": Phase="Running", Reason="", readiness=true. Elapsed: 4.021110682s
Jun 22 06:05:22.478: INFO: Pod "pod-subpath-test-configmap-cvjf": Phase="Running", Reason="", readiness=true. Elapsed: 6.031285445s
Jun 22 06:05:24.487: INFO: Pod "pod-subpath-test-configmap-cvjf": Phase="Running", Reason="", readiness=true. Elapsed: 8.039536649s
Jun 22 06:05:26.495: INFO: Pod "pod-subpath-test-configmap-cvjf": Phase="Running", Reason="", readiness=true. Elapsed: 10.047682641s
Jun 22 06:05:28.504: INFO: Pod "pod-subpath-test-configmap-cvjf": Phase="Running", Reason="", readiness=true. Elapsed: 12.05663623s
Jun 22 06:05:30.511: INFO: Pod "pod-subpath-test-configmap-cvjf": Phase="Running", Reason="", readiness=true. Elapsed: 14.064440884s
Jun 22 06:05:32.517: INFO: Pod "pod-subpath-test-configmap-cvjf": Phase="Running", Reason="", readiness=true. Elapsed: 16.069806849s
Jun 22 06:05:34.524: INFO: Pod "pod-subpath-test-configmap-cvjf": Phase="Running", Reason="", readiness=true. Elapsed: 18.076686074s
Jun 22 06:05:36.534: INFO: Pod "pod-subpath-test-configmap-cvjf": Phase="Running", Reason="", readiness=true. Elapsed: 20.087415829s
Jun 22 06:05:38.558: INFO: Pod "pod-subpath-test-configmap-cvjf": Phase="Running", Reason="", readiness=true. Elapsed: 22.111207391s
Jun 22 06:05:40.565: INFO: Pod "pod-subpath-test-configmap-cvjf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.118264061s
STEP: Saw pod success
Jun 22 06:05:40.565: INFO: Pod "pod-subpath-test-configmap-cvjf" satisfied condition "success or failure"
Jun 22 06:05:40.576: INFO: Trying to get logs from node node4 pod pod-subpath-test-configmap-cvjf container test-container-subpath-configmap-cvjf: <nil>
STEP: delete the pod
Jun 22 06:05:40.619: INFO: Waiting for pod pod-subpath-test-configmap-cvjf to disappear
Jun 22 06:05:40.626: INFO: Pod pod-subpath-test-configmap-cvjf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-cvjf
Jun 22 06:05:40.626: INFO: Deleting pod "pod-subpath-test-configmap-cvjf" in namespace "subpath-6294"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:05:40.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6294" for this suite.

• [SLOW TEST:24.410 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":166,"skipped":2840,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:05:40.651: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7112
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 22 06:05:40.847: INFO: Waiting up to 5m0s for pod "pod-7e72d077-cd5f-4ad8-ba9c-88d81986baf5" in namespace "emptydir-7112" to be "success or failure"
Jun 22 06:05:40.854: INFO: Pod "pod-7e72d077-cd5f-4ad8-ba9c-88d81986baf5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.209487ms
Jun 22 06:05:42.860: INFO: Pod "pod-7e72d077-cd5f-4ad8-ba9c-88d81986baf5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012314627s
STEP: Saw pod success
Jun 22 06:05:42.860: INFO: Pod "pod-7e72d077-cd5f-4ad8-ba9c-88d81986baf5" satisfied condition "success or failure"
Jun 22 06:05:42.865: INFO: Trying to get logs from node node4 pod pod-7e72d077-cd5f-4ad8-ba9c-88d81986baf5 container test-container: <nil>
STEP: delete the pod
Jun 22 06:05:42.901: INFO: Waiting for pod pod-7e72d077-cd5f-4ad8-ba9c-88d81986baf5 to disappear
Jun 22 06:05:42.907: INFO: Pod pod-7e72d077-cd5f-4ad8-ba9c-88d81986baf5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:05:42.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7112" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":167,"skipped":2842,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:05:42.925: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9531
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jun 22 06:05:43.110: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:05:46.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9531" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":168,"skipped":2846,"failed":0}

------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:05:46.331: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-368
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jun 22 06:05:46.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-368'
Jun 22 06:05:46.922: INFO: stderr: ""
Jun 22 06:05:46.922: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 06:05:46.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-368'
Jun 22 06:05:47.082: INFO: stderr: ""
Jun 22 06:05:47.082: INFO: stdout: "update-demo-nautilus-qwmr4 update-demo-nautilus-wsm2s "
Jun 22 06:05:47.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-qwmr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-368'
Jun 22 06:05:47.228: INFO: stderr: ""
Jun 22 06:05:47.228: INFO: stdout: ""
Jun 22 06:05:47.228: INFO: update-demo-nautilus-qwmr4 is created but not running
Jun 22 06:05:52.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-368'
Jun 22 06:05:52.355: INFO: stderr: ""
Jun 22 06:05:52.355: INFO: stdout: "update-demo-nautilus-qwmr4 update-demo-nautilus-wsm2s "
Jun 22 06:05:52.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-qwmr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-368'
Jun 22 06:05:52.472: INFO: stderr: ""
Jun 22 06:05:52.472: INFO: stdout: "true"
Jun 22 06:05:52.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-qwmr4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-368'
Jun 22 06:05:52.577: INFO: stderr: ""
Jun 22 06:05:52.577: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 06:05:52.577: INFO: validating pod update-demo-nautilus-qwmr4
Jun 22 06:05:52.588: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 06:05:52.588: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 06:05:52.588: INFO: update-demo-nautilus-qwmr4 is verified up and running
Jun 22 06:05:52.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-wsm2s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-368'
Jun 22 06:05:52.712: INFO: stderr: ""
Jun 22 06:05:52.712: INFO: stdout: "true"
Jun 22 06:05:52.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods update-demo-nautilus-wsm2s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-368'
Jun 22 06:05:52.880: INFO: stderr: ""
Jun 22 06:05:52.880: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 06:05:52.880: INFO: validating pod update-demo-nautilus-wsm2s
Jun 22 06:05:52.889: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 06:05:52.889: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 06:05:52.889: INFO: update-demo-nautilus-wsm2s is verified up and running
STEP: using delete to clean up resources
Jun 22 06:05:52.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete --grace-period=0 --force -f - --namespace=kubectl-368'
Jun 22 06:05:53.020: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 06:05:53.020: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 22 06:05:53.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-368'
Jun 22 06:05:53.180: INFO: stderr: "No resources found in kubectl-368 namespace.\n"
Jun 22 06:05:53.180: INFO: stdout: ""
Jun 22 06:05:53.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -l name=update-demo --namespace=kubectl-368 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 06:05:53.308: INFO: stderr: ""
Jun 22 06:05:53.308: INFO: stdout: "update-demo-nautilus-qwmr4\nupdate-demo-nautilus-wsm2s\n"
Jun 22 06:05:53.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-368'
Jun 22 06:05:53.997: INFO: stderr: "No resources found in kubectl-368 namespace.\n"
Jun 22 06:05:53.997: INFO: stdout: ""
Jun 22 06:05:53.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -l name=update-demo --namespace=kubectl-368 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 06:05:54.112: INFO: stderr: ""
Jun 22 06:05:54.112: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:05:54.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-368" for this suite.

• [SLOW TEST:7.797 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":169,"skipped":2846,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:05:54.129: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9660
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jun 22 06:05:58.900: INFO: Successfully updated pod "annotationupdate0f58dc2d-60c6-4159-af8f-1eadfc067b33"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:06:00.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9660" for this suite.

• [SLOW TEST:6.839 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":170,"skipped":2852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:06:00.970: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6612
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-c5d0c869-69d7-44e6-ade1-0e6ecfcaacfa
STEP: Creating a pod to test consume secrets
Jun 22 06:06:01.189: INFO: Waiting up to 5m0s for pod "pod-secrets-06737fe7-81f2-4da2-af88-0f64db4b1c42" in namespace "secrets-6612" to be "success or failure"
Jun 22 06:06:01.196: INFO: Pod "pod-secrets-06737fe7-81f2-4da2-af88-0f64db4b1c42": Phase="Pending", Reason="", readiness=false. Elapsed: 7.090586ms
Jun 22 06:06:03.205: INFO: Pod "pod-secrets-06737fe7-81f2-4da2-af88-0f64db4b1c42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016766849s
STEP: Saw pod success
Jun 22 06:06:03.205: INFO: Pod "pod-secrets-06737fe7-81f2-4da2-af88-0f64db4b1c42" satisfied condition "success or failure"
Jun 22 06:06:03.210: INFO: Trying to get logs from node node4 pod pod-secrets-06737fe7-81f2-4da2-af88-0f64db4b1c42 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 06:06:03.251: INFO: Waiting for pod pod-secrets-06737fe7-81f2-4da2-af88-0f64db4b1c42 to disappear
Jun 22 06:06:03.257: INFO: Pod pod-secrets-06737fe7-81f2-4da2-af88-0f64db4b1c42 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:06:03.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6612" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":171,"skipped":2915,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:06:03.277: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 22 06:06:03.480: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3119 /api/v1/namespaces/watch-3119/configmaps/e2e-watch-test-watch-closed e20024e7-38e7-4b8f-a7ad-1ccd423d43b5 32536 0 2020-06-22 06:06:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 06:06:03.481: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3119 /api/v1/namespaces/watch-3119/configmaps/e2e-watch-test-watch-closed e20024e7-38e7-4b8f-a7ad-1ccd423d43b5 32537 0 2020-06-22 06:06:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 22 06:06:03.512: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3119 /api/v1/namespaces/watch-3119/configmaps/e2e-watch-test-watch-closed e20024e7-38e7-4b8f-a7ad-1ccd423d43b5 32538 0 2020-06-22 06:06:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 06:06:03.512: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3119 /api/v1/namespaces/watch-3119/configmaps/e2e-watch-test-watch-closed e20024e7-38e7-4b8f-a7ad-1ccd423d43b5 32540 0 2020-06-22 06:06:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:06:03.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3119" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":172,"skipped":2918,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:06:03.543: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 22 06:06:09.787: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 06:06:09.800: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 06:06:11.800: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 06:06:11.806: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 06:06:13.800: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 06:06:13.811: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:06:13.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4023" for this suite.

• [SLOW TEST:10.302 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":173,"skipped":2921,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:06:13.846: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:06:14.016: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:06:16.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6454" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":174,"skipped":2947,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:06:16.238: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:06:16.422: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9dab0f81-2a59-49b8-aec8-30bb0c10542c" in namespace "projected-7909" to be "success or failure"
Jun 22 06:06:16.431: INFO: Pod "downwardapi-volume-9dab0f81-2a59-49b8-aec8-30bb0c10542c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.142077ms
Jun 22 06:06:18.440: INFO: Pod "downwardapi-volume-9dab0f81-2a59-49b8-aec8-30bb0c10542c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018051015s
Jun 22 06:06:20.448: INFO: Pod "downwardapi-volume-9dab0f81-2a59-49b8-aec8-30bb0c10542c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025644584s
STEP: Saw pod success
Jun 22 06:06:20.448: INFO: Pod "downwardapi-volume-9dab0f81-2a59-49b8-aec8-30bb0c10542c" satisfied condition "success or failure"
Jun 22 06:06:20.455: INFO: Trying to get logs from node node4 pod downwardapi-volume-9dab0f81-2a59-49b8-aec8-30bb0c10542c container client-container: <nil>
STEP: delete the pod
Jun 22 06:06:20.501: INFO: Waiting for pod downwardapi-volume-9dab0f81-2a59-49b8-aec8-30bb0c10542c to disappear
Jun 22 06:06:20.515: INFO: Pod downwardapi-volume-9dab0f81-2a59-49b8-aec8-30bb0c10542c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:06:20.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7909" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":175,"skipped":2963,"failed":0}
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:06:20.531: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-8877
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:06:20.706: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8877
I0622 06:06:20.725462      22 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8877, replica count: 1
I0622 06:06:21.776316      22 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 06:06:22.776548      22 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 06:06:23.777385      22 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 06:06:23.900: INFO: Created: latency-svc-plhqr
Jun 22 06:06:23.910: INFO: Got endpoints: latency-svc-plhqr [32.550627ms]
Jun 22 06:06:23.958: INFO: Created: latency-svc-v9cm8
Jun 22 06:06:23.966: INFO: Got endpoints: latency-svc-v9cm8 [55.803264ms]
Jun 22 06:06:23.985: INFO: Created: latency-svc-5qkd2
Jun 22 06:06:23.992: INFO: Got endpoints: latency-svc-5qkd2 [80.759935ms]
Jun 22 06:06:24.013: INFO: Created: latency-svc-jjxmx
Jun 22 06:06:24.030: INFO: Got endpoints: latency-svc-jjxmx [117.290875ms]
Jun 22 06:06:24.060: INFO: Created: latency-svc-87rqg
Jun 22 06:06:24.077: INFO: Got endpoints: latency-svc-87rqg [164.196547ms]
Jun 22 06:06:24.088: INFO: Created: latency-svc-6d8ng
Jun 22 06:06:24.096: INFO: Got endpoints: latency-svc-6d8ng [184.381789ms]
Jun 22 06:06:24.117: INFO: Created: latency-svc-m2gvk
Jun 22 06:06:24.141: INFO: Got endpoints: latency-svc-m2gvk [227.186938ms]
Jun 22 06:06:24.169: INFO: Created: latency-svc-v2xqj
Jun 22 06:06:24.188: INFO: Got endpoints: latency-svc-v2xqj [276.854106ms]
Jun 22 06:06:24.203: INFO: Created: latency-svc-mmrpz
Jun 22 06:06:24.218: INFO: Got endpoints: latency-svc-mmrpz [306.344202ms]
Jun 22 06:06:24.226: INFO: Created: latency-svc-bklmw
Jun 22 06:06:24.236: INFO: Got endpoints: latency-svc-bklmw [322.273338ms]
Jun 22 06:06:24.256: INFO: Created: latency-svc-t8pgb
Jun 22 06:06:24.284: INFO: Got endpoints: latency-svc-t8pgb [370.70795ms]
Jun 22 06:06:24.285: INFO: Created: latency-svc-gbt2h
Jun 22 06:06:24.295: INFO: Got endpoints: latency-svc-gbt2h [381.133244ms]
Jun 22 06:06:24.332: INFO: Created: latency-svc-r8s42
Jun 22 06:06:24.340: INFO: Got endpoints: latency-svc-r8s42 [426.31554ms]
Jun 22 06:06:24.378: INFO: Created: latency-svc-t8lx7
Jun 22 06:06:24.397: INFO: Got endpoints: latency-svc-t8lx7 [483.30568ms]
Jun 22 06:06:24.431: INFO: Created: latency-svc-mgfpz
Jun 22 06:06:24.453: INFO: Got endpoints: latency-svc-mgfpz [539.699186ms]
Jun 22 06:06:24.463: INFO: Created: latency-svc-zs6dp
Jun 22 06:06:24.475: INFO: Got endpoints: latency-svc-zs6dp [561.209296ms]
Jun 22 06:06:24.544: INFO: Created: latency-svc-rgvqq
Jun 22 06:06:24.553: INFO: Got endpoints: latency-svc-rgvqq [586.421293ms]
Jun 22 06:06:24.613: INFO: Created: latency-svc-sgt6j
Jun 22 06:06:24.643: INFO: Got endpoints: latency-svc-sgt6j [650.560059ms]
Jun 22 06:06:24.643: INFO: Created: latency-svc-fq5q4
Jun 22 06:06:24.653: INFO: Got endpoints: latency-svc-fq5q4 [623.220778ms]
Jun 22 06:06:24.689: INFO: Created: latency-svc-8hwlz
Jun 22 06:06:24.697: INFO: Got endpoints: latency-svc-8hwlz [620.165954ms]
Jun 22 06:06:24.730: INFO: Created: latency-svc-d79kx
Jun 22 06:06:24.748: INFO: Got endpoints: latency-svc-d79kx [651.58635ms]
Jun 22 06:06:24.758: INFO: Created: latency-svc-nbrjz
Jun 22 06:06:24.766: INFO: Got endpoints: latency-svc-nbrjz [625.402732ms]
Jun 22 06:06:24.808: INFO: Created: latency-svc-dm24t
Jun 22 06:06:24.824: INFO: Got endpoints: latency-svc-dm24t [635.621542ms]
Jun 22 06:06:24.841: INFO: Created: latency-svc-kkd2f
Jun 22 06:06:24.868: INFO: Got endpoints: latency-svc-kkd2f [650.676041ms]
Jun 22 06:06:24.910: INFO: Created: latency-svc-l8hf9
Jun 22 06:06:24.923: INFO: Got endpoints: latency-svc-l8hf9 [687.639488ms]
Jun 22 06:06:24.934: INFO: Created: latency-svc-txfrk
Jun 22 06:06:24.977: INFO: Got endpoints: latency-svc-txfrk [693.296674ms]
Jun 22 06:06:24.980: INFO: Created: latency-svc-ph6z4
Jun 22 06:06:24.998: INFO: Got endpoints: latency-svc-ph6z4 [702.991123ms]
Jun 22 06:06:25.019: INFO: Created: latency-svc-gwv7g
Jun 22 06:06:25.023: INFO: Got endpoints: latency-svc-gwv7g [683.701098ms]
Jun 22 06:06:25.045: INFO: Created: latency-svc-nnb8c
Jun 22 06:06:25.091: INFO: Got endpoints: latency-svc-nnb8c [693.367205ms]
Jun 22 06:06:25.099: INFO: Created: latency-svc-58bxg
Jun 22 06:06:25.118: INFO: Got endpoints: latency-svc-58bxg [665.236139ms]
Jun 22 06:06:25.126: INFO: Created: latency-svc-h5t6m
Jun 22 06:06:25.141: INFO: Got endpoints: latency-svc-h5t6m [666.281401ms]
Jun 22 06:06:25.167: INFO: Created: latency-svc-6bmdw
Jun 22 06:06:25.207: INFO: Created: latency-svc-54gnc
Jun 22 06:06:25.207: INFO: Got endpoints: latency-svc-6bmdw [653.761353ms]
Jun 22 06:06:25.219: INFO: Got endpoints: latency-svc-54gnc [575.640452ms]
Jun 22 06:06:25.236: INFO: Created: latency-svc-5hmrb
Jun 22 06:06:25.250: INFO: Got endpoints: latency-svc-5hmrb [596.831007ms]
Jun 22 06:06:25.290: INFO: Created: latency-svc-v2jf5
Jun 22 06:06:25.327: INFO: Got endpoints: latency-svc-v2jf5 [629.754771ms]
Jun 22 06:06:25.339: INFO: Created: latency-svc-9mp9j
Jun 22 06:06:25.361: INFO: Got endpoints: latency-svc-9mp9j [613.418856ms]
Jun 22 06:06:25.387: INFO: Created: latency-svc-kmhsg
Jun 22 06:06:25.418: INFO: Got endpoints: latency-svc-kmhsg [651.457816ms]
Jun 22 06:06:25.453: INFO: Created: latency-svc-d4v97
Jun 22 06:06:25.489: INFO: Got endpoints: latency-svc-d4v97 [664.488484ms]
Jun 22 06:06:25.518: INFO: Created: latency-svc-sqqcp
Jun 22 06:06:25.531: INFO: Got endpoints: latency-svc-sqqcp [662.086464ms]
Jun 22 06:06:25.541: INFO: Created: latency-svc-w9c9h
Jun 22 06:06:25.575: INFO: Got endpoints: latency-svc-w9c9h [651.556869ms]
Jun 22 06:06:25.597: INFO: Created: latency-svc-vmbfb
Jun 22 06:06:25.605: INFO: Got endpoints: latency-svc-vmbfb [627.595287ms]
Jun 22 06:06:25.638: INFO: Created: latency-svc-hlnrl
Jun 22 06:06:25.651: INFO: Got endpoints: latency-svc-hlnrl [653.008857ms]
Jun 22 06:06:25.663: INFO: Created: latency-svc-jflhc
Jun 22 06:06:25.689: INFO: Got endpoints: latency-svc-jflhc [665.209559ms]
Jun 22 06:06:25.717: INFO: Created: latency-svc-68btd
Jun 22 06:06:25.741: INFO: Got endpoints: latency-svc-68btd [650.315172ms]
Jun 22 06:06:25.769: INFO: Created: latency-svc-7kdnr
Jun 22 06:06:25.820: INFO: Got endpoints: latency-svc-7kdnr [701.968042ms]
Jun 22 06:06:25.849: INFO: Created: latency-svc-2kk48
Jun 22 06:06:25.876: INFO: Got endpoints: latency-svc-2kk48 [734.433207ms]
Jun 22 06:06:25.901: INFO: Created: latency-svc-mrtkw
Jun 22 06:06:25.934: INFO: Got endpoints: latency-svc-mrtkw [727.696779ms]
Jun 22 06:06:25.945: INFO: Created: latency-svc-579vz
Jun 22 06:06:25.967: INFO: Got endpoints: latency-svc-579vz [747.819313ms]
Jun 22 06:06:25.990: INFO: Created: latency-svc-hzpzb
Jun 22 06:06:26.031: INFO: Got endpoints: latency-svc-hzpzb [780.162985ms]
Jun 22 06:06:26.070: INFO: Created: latency-svc-bljvx
Jun 22 06:06:26.098: INFO: Got endpoints: latency-svc-bljvx [770.968037ms]
Jun 22 06:06:26.104: INFO: Created: latency-svc-8qlgv
Jun 22 06:06:26.117: INFO: Got endpoints: latency-svc-8qlgv [755.313145ms]
Jun 22 06:06:26.133: INFO: Created: latency-svc-zr8wb
Jun 22 06:06:26.148: INFO: Got endpoints: latency-svc-zr8wb [730.534418ms]
Jun 22 06:06:26.173: INFO: Created: latency-svc-7wlxq
Jun 22 06:06:26.215: INFO: Created: latency-svc-22h98
Jun 22 06:06:26.216: INFO: Got endpoints: latency-svc-7wlxq [727.501695ms]
Jun 22 06:06:26.241: INFO: Got endpoints: latency-svc-22h98 [710.238121ms]
Jun 22 06:06:26.263: INFO: Created: latency-svc-4n4km
Jun 22 06:06:26.304: INFO: Got endpoints: latency-svc-4n4km [729.051538ms]
Jun 22 06:06:26.317: INFO: Created: latency-svc-lvvqb
Jun 22 06:06:26.329: INFO: Got endpoints: latency-svc-lvvqb [723.81681ms]
Jun 22 06:06:26.353: INFO: Created: latency-svc-57m56
Jun 22 06:06:26.359: INFO: Got endpoints: latency-svc-57m56 [707.824533ms]
Jun 22 06:06:26.369: INFO: Created: latency-svc-jg2l2
Jun 22 06:06:26.417: INFO: Got endpoints: latency-svc-jg2l2 [727.983335ms]
Jun 22 06:06:26.452: INFO: Created: latency-svc-4csp7
Jun 22 06:06:26.467: INFO: Got endpoints: latency-svc-4csp7 [725.579167ms]
Jun 22 06:06:26.489: INFO: Created: latency-svc-8bfdl
Jun 22 06:06:26.511: INFO: Got endpoints: latency-svc-8bfdl [690.325043ms]
Jun 22 06:06:26.518: INFO: Created: latency-svc-7jxsr
Jun 22 06:06:26.559: INFO: Got endpoints: latency-svc-7jxsr [683.049895ms]
Jun 22 06:06:26.593: INFO: Created: latency-svc-cj9zl
Jun 22 06:06:26.618: INFO: Got endpoints: latency-svc-cj9zl [683.977694ms]
Jun 22 06:06:26.636: INFO: Created: latency-svc-hm8p8
Jun 22 06:06:26.650: INFO: Got endpoints: latency-svc-hm8p8 [682.932412ms]
Jun 22 06:06:26.683: INFO: Created: latency-svc-b77b2
Jun 22 06:06:26.700: INFO: Got endpoints: latency-svc-b77b2 [669.472555ms]
Jun 22 06:06:26.716: INFO: Created: latency-svc-wxh72
Jun 22 06:06:26.734: INFO: Got endpoints: latency-svc-wxh72 [635.051049ms]
Jun 22 06:06:26.759: INFO: Created: latency-svc-tgb5s
Jun 22 06:06:26.797: INFO: Got endpoints: latency-svc-tgb5s [679.846599ms]
Jun 22 06:06:26.803: INFO: Created: latency-svc-qjwm2
Jun 22 06:06:26.821: INFO: Got endpoints: latency-svc-qjwm2 [672.420716ms]
Jun 22 06:06:26.867: INFO: Created: latency-svc-c4gjl
Jun 22 06:06:26.881: INFO: Got endpoints: latency-svc-c4gjl [665.081326ms]
Jun 22 06:06:26.912: INFO: Created: latency-svc-q8rv2
Jun 22 06:06:26.919: INFO: Got endpoints: latency-svc-q8rv2 [677.858587ms]
Jun 22 06:06:26.967: INFO: Created: latency-svc-x4zn4
Jun 22 06:06:26.995: INFO: Got endpoints: latency-svc-x4zn4 [690.265212ms]
Jun 22 06:06:27.025: INFO: Created: latency-svc-pb22v
Jun 22 06:06:27.041: INFO: Got endpoints: latency-svc-pb22v [712.400406ms]
Jun 22 06:06:27.052: INFO: Created: latency-svc-lmps5
Jun 22 06:06:27.064: INFO: Got endpoints: latency-svc-lmps5 [704.833311ms]
Jun 22 06:06:27.080: INFO: Created: latency-svc-wc9mv
Jun 22 06:06:27.111: INFO: Got endpoints: latency-svc-wc9mv [694.286325ms]
Jun 22 06:06:27.172: INFO: Created: latency-svc-vl2v2
Jun 22 06:06:27.181: INFO: Got endpoints: latency-svc-vl2v2 [714.380527ms]
Jun 22 06:06:27.198: INFO: Created: latency-svc-plfcn
Jun 22 06:06:27.221: INFO: Created: latency-svc-jxnmx
Jun 22 06:06:27.240: INFO: Got endpoints: latency-svc-plfcn [729.553137ms]
Jun 22 06:06:27.267: INFO: Got endpoints: latency-svc-jxnmx [708.449704ms]
Jun 22 06:06:27.299: INFO: Created: latency-svc-hbbxn
Jun 22 06:06:27.320: INFO: Got endpoints: latency-svc-hbbxn [701.529933ms]
Jun 22 06:06:27.370: INFO: Created: latency-svc-hsnbz
Jun 22 06:06:27.379: INFO: Got endpoints: latency-svc-hsnbz [728.739511ms]
Jun 22 06:06:27.400: INFO: Created: latency-svc-kbd8d
Jun 22 06:06:27.425: INFO: Got endpoints: latency-svc-kbd8d [724.470473ms]
Jun 22 06:06:27.445: INFO: Created: latency-svc-mqmwz
Jun 22 06:06:27.467: INFO: Got endpoints: latency-svc-mqmwz [733.57307ms]
Jun 22 06:06:27.484: INFO: Created: latency-svc-ln5pk
Jun 22 06:06:27.497: INFO: Got endpoints: latency-svc-ln5pk [700.190665ms]
Jun 22 06:06:27.520: INFO: Created: latency-svc-v7pxr
Jun 22 06:06:27.553: INFO: Got endpoints: latency-svc-v7pxr [731.707982ms]
Jun 22 06:06:27.590: INFO: Created: latency-svc-9vrdq
Jun 22 06:06:27.608: INFO: Got endpoints: latency-svc-9vrdq [726.640058ms]
Jun 22 06:06:27.618: INFO: Created: latency-svc-hrzfg
Jun 22 06:06:27.638: INFO: Got endpoints: latency-svc-hrzfg [718.954561ms]
Jun 22 06:06:27.644: INFO: Created: latency-svc-nt5l9
Jun 22 06:06:27.690: INFO: Got endpoints: latency-svc-nt5l9 [694.908188ms]
Jun 22 06:06:27.706: INFO: Created: latency-svc-j9mss
Jun 22 06:06:27.731: INFO: Got endpoints: latency-svc-j9mss [690.224782ms]
Jun 22 06:06:27.736: INFO: Created: latency-svc-bdh29
Jun 22 06:06:27.753: INFO: Got endpoints: latency-svc-bdh29 [689.438506ms]
Jun 22 06:06:27.762: INFO: Created: latency-svc-l8zgj
Jun 22 06:06:27.789: INFO: Got endpoints: latency-svc-l8zgj [677.473039ms]
Jun 22 06:06:27.799: INFO: Created: latency-svc-gm4q6
Jun 22 06:06:27.829: INFO: Got endpoints: latency-svc-gm4q6 [647.809501ms]
Jun 22 06:06:27.925: INFO: Created: latency-svc-4gl8h
Jun 22 06:06:27.925: INFO: Created: latency-svc-zxq7l
Jun 22 06:06:27.925: INFO: Got endpoints: latency-svc-zxq7l [684.354611ms]
Jun 22 06:06:27.953: INFO: Got endpoints: latency-svc-4gl8h [685.382283ms]
Jun 22 06:06:28.074: INFO: Created: latency-svc-p8zz9
Jun 22 06:06:28.090: INFO: Got endpoints: latency-svc-p8zz9 [770.111549ms]
Jun 22 06:06:28.112: INFO: Created: latency-svc-d7hnw
Jun 22 06:06:28.118: INFO: Got endpoints: latency-svc-d7hnw [737.568862ms]
Jun 22 06:06:28.164: INFO: Created: latency-svc-hk9fr
Jun 22 06:06:28.192: INFO: Got endpoints: latency-svc-hk9fr [767.558167ms]
Jun 22 06:06:28.211: INFO: Created: latency-svc-dcv54
Jun 22 06:06:28.230: INFO: Got endpoints: latency-svc-dcv54 [763.143156ms]
Jun 22 06:06:28.245: INFO: Created: latency-svc-d6w9k
Jun 22 06:06:28.274: INFO: Got endpoints: latency-svc-d6w9k [776.688954ms]
Jun 22 06:06:28.282: INFO: Created: latency-svc-v5bpm
Jun 22 06:06:28.324: INFO: Got endpoints: latency-svc-v5bpm [771.341424ms]
Jun 22 06:06:28.351: INFO: Created: latency-svc-jlfs9
Jun 22 06:06:28.361: INFO: Got endpoints: latency-svc-jlfs9 [753.416396ms]
Jun 22 06:06:28.398: INFO: Created: latency-svc-bgvz9
Jun 22 06:06:28.429: INFO: Got endpoints: latency-svc-bgvz9 [790.19197ms]
Jun 22 06:06:28.440: INFO: Created: latency-svc-6qp49
Jun 22 06:06:28.455: INFO: Got endpoints: latency-svc-6qp49 [765.7748ms]
Jun 22 06:06:28.491: INFO: Created: latency-svc-gltwc
Jun 22 06:06:28.522: INFO: Got endpoints: latency-svc-gltwc [790.002376ms]
Jun 22 06:06:28.554: INFO: Created: latency-svc-xv4n7
Jun 22 06:06:28.584: INFO: Created: latency-svc-qwdm5
Jun 22 06:06:28.599: INFO: Got endpoints: latency-svc-xv4n7 [845.341351ms]
Jun 22 06:06:28.600: INFO: Got endpoints: latency-svc-qwdm5 [811.513147ms]
Jun 22 06:06:28.639: INFO: Created: latency-svc-f94f6
Jun 22 06:06:28.667: INFO: Got endpoints: latency-svc-f94f6 [837.762857ms]
Jun 22 06:06:28.681: INFO: Created: latency-svc-68dp9
Jun 22 06:06:28.711: INFO: Created: latency-svc-24tgv
Jun 22 06:06:28.723: INFO: Got endpoints: latency-svc-24tgv [769.69488ms]
Jun 22 06:06:28.723: INFO: Got endpoints: latency-svc-68dp9 [798.48783ms]
Jun 22 06:06:28.756: INFO: Created: latency-svc-wbkmm
Jun 22 06:06:28.789: INFO: Got endpoints: latency-svc-wbkmm [698.94313ms]
Jun 22 06:06:28.800: INFO: Created: latency-svc-fl6kl
Jun 22 06:06:28.803: INFO: Got endpoints: latency-svc-fl6kl [685.623987ms]
Jun 22 06:06:28.825: INFO: Created: latency-svc-2bmqg
Jun 22 06:06:28.844: INFO: Got endpoints: latency-svc-2bmqg [650.456716ms]
Jun 22 06:06:28.888: INFO: Created: latency-svc-4kb55
Jun 22 06:06:28.914: INFO: Got endpoints: latency-svc-4kb55 [684.045623ms]
Jun 22 06:06:28.926: INFO: Created: latency-svc-b5d2k
Jun 22 06:06:28.926: INFO: Got endpoints: latency-svc-b5d2k [652.251552ms]
Jun 22 06:06:28.948: INFO: Created: latency-svc-vvrlw
Jun 22 06:06:28.965: INFO: Got endpoints: latency-svc-vvrlw [641.263227ms]
Jun 22 06:06:29.011: INFO: Created: latency-svc-dw9hh
Jun 22 06:06:29.035: INFO: Got endpoints: latency-svc-dw9hh [673.632461ms]
Jun 22 06:06:29.037: INFO: Created: latency-svc-48lwx
Jun 22 06:06:29.059: INFO: Got endpoints: latency-svc-48lwx [630.521867ms]
Jun 22 06:06:29.073: INFO: Created: latency-svc-b5zmz
Jun 22 06:06:29.097: INFO: Got endpoints: latency-svc-b5zmz [641.730286ms]
Jun 22 06:06:29.128: INFO: Created: latency-svc-rdc4w
Jun 22 06:06:29.158: INFO: Got endpoints: latency-svc-rdc4w [636.673763ms]
Jun 22 06:06:29.164: INFO: Created: latency-svc-wklp9
Jun 22 06:06:29.187: INFO: Got endpoints: latency-svc-wklp9 [587.899683ms]
Jun 22 06:06:29.195: INFO: Created: latency-svc-xnj7t
Jun 22 06:06:29.236: INFO: Got endpoints: latency-svc-xnj7t [635.355497ms]
Jun 22 06:06:29.277: INFO: Created: latency-svc-24nm2
Jun 22 06:06:29.297: INFO: Got endpoints: latency-svc-24nm2 [629.812172ms]
Jun 22 06:06:29.312: INFO: Created: latency-svc-9qrf8
Jun 22 06:06:29.329: INFO: Got endpoints: latency-svc-9qrf8 [606.639617ms]
Jun 22 06:06:29.337: INFO: Created: latency-svc-b5lxk
Jun 22 06:06:29.360: INFO: Got endpoints: latency-svc-b5lxk [636.409128ms]
Jun 22 06:06:29.414: INFO: Created: latency-svc-svmsv
Jun 22 06:06:29.418: INFO: Got endpoints: latency-svc-svmsv [629.087398ms]
Jun 22 06:06:29.450: INFO: Created: latency-svc-rcxb7
Jun 22 06:06:29.494: INFO: Got endpoints: latency-svc-rcxb7 [690.256682ms]
Jun 22 06:06:29.495: INFO: Created: latency-svc-kszmf
Jun 22 06:06:29.498: INFO: Got endpoints: latency-svc-kszmf [654.004738ms]
Jun 22 06:06:29.537: INFO: Created: latency-svc-lgv8q
Jun 22 06:06:29.549: INFO: Got endpoints: latency-svc-lgv8q [634.445627ms]
Jun 22 06:06:29.601: INFO: Created: latency-svc-kkrrx
Jun 22 06:06:29.609: INFO: Got endpoints: latency-svc-kkrrx [682.298979ms]
Jun 22 06:06:29.648: INFO: Created: latency-svc-mnnsd
Jun 22 06:06:29.661: INFO: Got endpoints: latency-svc-mnnsd [696.061211ms]
Jun 22 06:06:29.699: INFO: Created: latency-svc-8k6r8
Jun 22 06:06:29.715: INFO: Got endpoints: latency-svc-8k6r8 [679.557882ms]
Jun 22 06:06:29.733: INFO: Created: latency-svc-jpmlh
Jun 22 06:06:29.780: INFO: Got endpoints: latency-svc-jpmlh [720.554273ms]
Jun 22 06:06:29.796: INFO: Created: latency-svc-zdd4s
Jun 22 06:06:29.804: INFO: Got endpoints: latency-svc-zdd4s [88.898243ms]
Jun 22 06:06:29.842: INFO: Created: latency-svc-p86xv
Jun 22 06:06:29.874: INFO: Got endpoints: latency-svc-p86xv [776.955269ms]
Jun 22 06:06:29.920: INFO: Created: latency-svc-2mrqn
Jun 22 06:06:29.920: INFO: Got endpoints: latency-svc-2mrqn [761.308318ms]
Jun 22 06:06:29.965: INFO: Created: latency-svc-fqgt6
Jun 22 06:06:29.981: INFO: Got endpoints: latency-svc-fqgt6 [793.876016ms]
Jun 22 06:06:30.022: INFO: Created: latency-svc-xkx9j
Jun 22 06:06:30.038: INFO: Got endpoints: latency-svc-xkx9j [802.643266ms]
Jun 22 06:06:30.074: INFO: Created: latency-svc-l76zp
Jun 22 06:06:30.086: INFO: Got endpoints: latency-svc-l76zp [789.60082ms]
Jun 22 06:06:30.103: INFO: Created: latency-svc-zljtk
Jun 22 06:06:30.155: INFO: Got endpoints: latency-svc-zljtk [825.355832ms]
Jun 22 06:06:30.184: INFO: Created: latency-svc-7twnj
Jun 22 06:06:30.203: INFO: Got endpoints: latency-svc-7twnj [842.786968ms]
Jun 22 06:06:30.227: INFO: Created: latency-svc-grfv5
Jun 22 06:06:30.229: INFO: Got endpoints: latency-svc-grfv5 [810.413766ms]
Jun 22 06:06:30.246: INFO: Created: latency-svc-4sxrs
Jun 22 06:06:30.277: INFO: Got endpoints: latency-svc-4sxrs [783.094875ms]
Jun 22 06:06:30.305: INFO: Created: latency-svc-wsj8x
Jun 22 06:06:30.320: INFO: Got endpoints: latency-svc-wsj8x [821.816539ms]
Jun 22 06:06:30.342: INFO: Created: latency-svc-ssfcd
Jun 22 06:06:30.362: INFO: Got endpoints: latency-svc-ssfcd [813.157712ms]
Jun 22 06:06:30.404: INFO: Created: latency-svc-6gfrs
Jun 22 06:06:30.404: INFO: Got endpoints: latency-svc-6gfrs [795.678832ms]
Jun 22 06:06:30.429: INFO: Created: latency-svc-zg6xk
Jun 22 06:06:30.436: INFO: Got endpoints: latency-svc-zg6xk [774.900459ms]
Jun 22 06:06:30.483: INFO: Created: latency-svc-szb4m
Jun 22 06:06:30.485: INFO: Got endpoints: latency-svc-szb4m [705.340001ms]
Jun 22 06:06:30.534: INFO: Created: latency-svc-77hr4
Jun 22 06:06:30.544: INFO: Got endpoints: latency-svc-77hr4 [739.91051ms]
Jun 22 06:06:30.586: INFO: Created: latency-svc-8mr8q
Jun 22 06:06:30.670: INFO: Got endpoints: latency-svc-8mr8q [795.424219ms]
Jun 22 06:06:30.705: INFO: Created: latency-svc-2grh4
Jun 22 06:06:30.705: INFO: Got endpoints: latency-svc-2grh4 [785.148406ms]
Jun 22 06:06:30.724: INFO: Created: latency-svc-qtfl8
Jun 22 06:06:30.739: INFO: Got endpoints: latency-svc-qtfl8 [758.846319ms]
Jun 22 06:06:30.754: INFO: Created: latency-svc-wnm4r
Jun 22 06:06:30.771: INFO: Got endpoints: latency-svc-wnm4r [732.768184ms]
Jun 22 06:06:30.793: INFO: Created: latency-svc-nxqjb
Jun 22 06:06:30.808: INFO: Got endpoints: latency-svc-nxqjb [721.298978ms]
Jun 22 06:06:30.827: INFO: Created: latency-svc-s8f67
Jun 22 06:06:30.853: INFO: Got endpoints: latency-svc-s8f67 [697.98894ms]
Jun 22 06:06:30.888: INFO: Created: latency-svc-xm4t8
Jun 22 06:06:30.911: INFO: Created: latency-svc-lvc5p
Jun 22 06:06:30.936: INFO: Got endpoints: latency-svc-lvc5p [707.107828ms]
Jun 22 06:06:30.937: INFO: Got endpoints: latency-svc-xm4t8 [733.870256ms]
Jun 22 06:06:30.943: INFO: Created: latency-svc-dhswv
Jun 22 06:06:30.954: INFO: Got endpoints: latency-svc-dhswv [677.645653ms]
Jun 22 06:06:30.979: INFO: Created: latency-svc-gfzz4
Jun 22 06:06:31.012: INFO: Got endpoints: latency-svc-gfzz4 [692.581789ms]
Jun 22 06:06:31.027: INFO: Created: latency-svc-hmwfg
Jun 22 06:06:31.052: INFO: Got endpoints: latency-svc-hmwfg [689.621089ms]
Jun 22 06:06:31.072: INFO: Created: latency-svc-6j6w7
Jun 22 06:06:31.078: INFO: Got endpoints: latency-svc-6j6w7 [674.020699ms]
Jun 22 06:06:31.108: INFO: Created: latency-svc-hcz5s
Jun 22 06:06:31.128: INFO: Got endpoints: latency-svc-hcz5s [692.040028ms]
Jun 22 06:06:31.144: INFO: Created: latency-svc-hqdkh
Jun 22 06:06:31.174: INFO: Got endpoints: latency-svc-hqdkh [688.819482ms]
Jun 22 06:06:31.186: INFO: Created: latency-svc-k9qff
Jun 22 06:06:31.196: INFO: Got endpoints: latency-svc-k9qff [652.338084ms]
Jun 22 06:06:31.224: INFO: Created: latency-svc-4hqm7
Jun 22 06:06:31.241: INFO: Got endpoints: latency-svc-4hqm7 [571.607498ms]
Jun 22 06:06:31.270: INFO: Created: latency-svc-s5qzg
Jun 22 06:06:31.308: INFO: Created: latency-svc-5h8jp
Jun 22 06:06:31.309: INFO: Got endpoints: latency-svc-s5qzg [603.8288ms]
Jun 22 06:06:31.315: INFO: Got endpoints: latency-svc-5h8jp [575.532039ms]
Jun 22 06:06:31.350: INFO: Created: latency-svc-hqftq
Jun 22 06:06:31.365: INFO: Got endpoints: latency-svc-hqftq [593.788265ms]
Jun 22 06:06:31.398: INFO: Created: latency-svc-trntv
Jun 22 06:06:31.426: INFO: Got endpoints: latency-svc-trntv [618.393608ms]
Jun 22 06:06:31.431: INFO: Created: latency-svc-ssshg
Jun 22 06:06:31.462: INFO: Got endpoints: latency-svc-ssshg [609.314962ms]
Jun 22 06:06:31.476: INFO: Created: latency-svc-ls5zc
Jun 22 06:06:31.491: INFO: Got endpoints: latency-svc-ls5zc [554.810854ms]
Jun 22 06:06:31.522: INFO: Created: latency-svc-fjlzg
Jun 22 06:06:31.527: INFO: Got endpoints: latency-svc-fjlzg [590.642979ms]
Jun 22 06:06:31.569: INFO: Created: latency-svc-t47hd
Jun 22 06:06:31.592: INFO: Got endpoints: latency-svc-t47hd [637.252735ms]
Jun 22 06:06:31.622: INFO: Created: latency-svc-5hqrf
Jun 22 06:06:31.637: INFO: Got endpoints: latency-svc-5hqrf [624.825011ms]
Jun 22 06:06:31.659: INFO: Created: latency-svc-c2z2g
Jun 22 06:06:31.688: INFO: Got endpoints: latency-svc-c2z2g [635.747535ms]
Jun 22 06:06:31.693: INFO: Created: latency-svc-99dbt
Jun 22 06:06:31.712: INFO: Got endpoints: latency-svc-99dbt [634.037729ms]
Jun 22 06:06:31.745: INFO: Created: latency-svc-xl7rj
Jun 22 06:06:31.793: INFO: Created: latency-svc-9q8j5
Jun 22 06:06:31.821: INFO: Got endpoints: latency-svc-9q8j5 [647.410824ms]
Jun 22 06:06:31.822: INFO: Got endpoints: latency-svc-xl7rj [693.811275ms]
Jun 22 06:06:31.829: INFO: Created: latency-svc-llp96
Jun 22 06:06:31.841: INFO: Got endpoints: latency-svc-llp96 [644.716729ms]
Jun 22 06:06:31.864: INFO: Created: latency-svc-7ph2t
Jun 22 06:06:31.879: INFO: Got endpoints: latency-svc-7ph2t [637.695905ms]
Jun 22 06:06:31.897: INFO: Created: latency-svc-fm6mr
Jun 22 06:06:31.905: INFO: Got endpoints: latency-svc-fm6mr [596.265525ms]
Jun 22 06:06:31.921: INFO: Created: latency-svc-xmsr4
Jun 22 06:06:31.954: INFO: Got endpoints: latency-svc-xmsr4 [639.123522ms]
Jun 22 06:06:31.997: INFO: Created: latency-svc-csvw9
Jun 22 06:06:32.010: INFO: Got endpoints: latency-svc-csvw9 [644.681397ms]
Jun 22 06:06:32.017: INFO: Created: latency-svc-97592
Jun 22 06:06:32.023: INFO: Got endpoints: latency-svc-97592 [596.724425ms]
Jun 22 06:06:32.044: INFO: Created: latency-svc-fqn7r
Jun 22 06:06:32.049: INFO: Got endpoints: latency-svc-fqn7r [586.464953ms]
Jun 22 06:06:32.103: INFO: Created: latency-svc-mc9bg
Jun 22 06:06:32.118: INFO: Got endpoints: latency-svc-mc9bg [627.318142ms]
Jun 22 06:06:32.135: INFO: Created: latency-svc-rcw82
Jun 22 06:06:32.135: INFO: Got endpoints: latency-svc-rcw82 [607.358723ms]
Jun 22 06:06:32.157: INFO: Created: latency-svc-kfvtd
Jun 22 06:06:32.164: INFO: Got endpoints: latency-svc-kfvtd [572.011388ms]
Jun 22 06:06:32.230: INFO: Created: latency-svc-xxxpx
Jun 22 06:06:32.240: INFO: Got endpoints: latency-svc-xxxpx [602.878909ms]
Jun 22 06:06:32.265: INFO: Created: latency-svc-kpcgd
Jun 22 06:06:32.286: INFO: Got endpoints: latency-svc-kpcgd [598.603732ms]
Jun 22 06:06:32.336: INFO: Created: latency-svc-jgs7f
Jun 22 06:06:32.394: INFO: Got endpoints: latency-svc-jgs7f [681.822159ms]
Jun 22 06:06:32.428: INFO: Created: latency-svc-kwnvv
Jun 22 06:06:32.460: INFO: Got endpoints: latency-svc-kwnvv [638.069432ms]
Jun 22 06:06:32.468: INFO: Created: latency-svc-twq4x
Jun 22 06:06:32.498: INFO: Created: latency-svc-99692
Jun 22 06:06:32.498: INFO: Got endpoints: latency-svc-twq4x [676.728634ms]
Jun 22 06:06:32.532: INFO: Got endpoints: latency-svc-99692 [690.451776ms]
Jun 22 06:06:32.547: INFO: Created: latency-svc-tlgln
Jun 22 06:06:32.582: INFO: Got endpoints: latency-svc-tlgln [703.315009ms]
Jun 22 06:06:32.610: INFO: Created: latency-svc-7ckwl
Jun 22 06:06:32.630: INFO: Got endpoints: latency-svc-7ckwl [724.709388ms]
Jun 22 06:06:32.634: INFO: Created: latency-svc-dtfpr
Jun 22 06:06:32.664: INFO: Got endpoints: latency-svc-dtfpr [709.22911ms]
Jun 22 06:06:32.703: INFO: Created: latency-svc-m6f9t
Jun 22 06:06:32.716: INFO: Got endpoints: latency-svc-m6f9t [705.960394ms]
Jun 22 06:06:32.759: INFO: Created: latency-svc-pvfq8
Jun 22 06:06:32.763: INFO: Got endpoints: latency-svc-pvfq8 [739.514422ms]
Jun 22 06:06:32.795: INFO: Created: latency-svc-d6z84
Jun 22 06:06:32.806: INFO: Got endpoints: latency-svc-d6z84 [756.95998ms]
Jun 22 06:06:32.855: INFO: Created: latency-svc-fr6nw
Jun 22 06:06:32.869: INFO: Got endpoints: latency-svc-fr6nw [750.162029ms]
Jun 22 06:06:32.893: INFO: Created: latency-svc-77b66
Jun 22 06:06:32.893: INFO: Got endpoints: latency-svc-77b66 [758.575812ms]
Jun 22 06:06:32.920: INFO: Created: latency-svc-nj7nm
Jun 22 06:06:32.941: INFO: Got endpoints: latency-svc-nj7nm [777.107183ms]
Jun 22 06:06:32.996: INFO: Created: latency-svc-lgm69
Jun 22 06:06:33.000: INFO: Created: latency-svc-kq69r
Jun 22 06:06:33.000: INFO: Got endpoints: latency-svc-lgm69 [760.126484ms]
Jun 22 06:06:33.010: INFO: Got endpoints: latency-svc-kq69r [723.098905ms]
Jun 22 06:06:33.010: INFO: Latencies: [55.803264ms 80.759935ms 88.898243ms 117.290875ms 164.196547ms 184.381789ms 227.186938ms 276.854106ms 306.344202ms 322.273338ms 370.70795ms 381.133244ms 426.31554ms 483.30568ms 539.699186ms 554.810854ms 561.209296ms 571.607498ms 572.011388ms 575.532039ms 575.640452ms 586.421293ms 586.464953ms 587.899683ms 590.642979ms 593.788265ms 596.265525ms 596.724425ms 596.831007ms 598.603732ms 602.878909ms 603.8288ms 606.639617ms 607.358723ms 609.314962ms 613.418856ms 618.393608ms 620.165954ms 623.220778ms 624.825011ms 625.402732ms 627.318142ms 627.595287ms 629.087398ms 629.754771ms 629.812172ms 630.521867ms 634.037729ms 634.445627ms 635.051049ms 635.355497ms 635.621542ms 635.747535ms 636.409128ms 636.673763ms 637.252735ms 637.695905ms 638.069432ms 639.123522ms 641.263227ms 641.730286ms 644.681397ms 644.716729ms 647.410824ms 647.809501ms 650.315172ms 650.456716ms 650.560059ms 650.676041ms 651.457816ms 651.556869ms 651.58635ms 652.251552ms 652.338084ms 653.008857ms 653.761353ms 654.004738ms 662.086464ms 664.488484ms 665.081326ms 665.209559ms 665.236139ms 666.281401ms 669.472555ms 672.420716ms 673.632461ms 674.020699ms 676.728634ms 677.473039ms 677.645653ms 677.858587ms 679.557882ms 679.846599ms 681.822159ms 682.298979ms 682.932412ms 683.049895ms 683.701098ms 683.977694ms 684.045623ms 684.354611ms 685.382283ms 685.623987ms 687.639488ms 688.819482ms 689.438506ms 689.621089ms 690.224782ms 690.256682ms 690.265212ms 690.325043ms 690.451776ms 692.040028ms 692.581789ms 693.296674ms 693.367205ms 693.811275ms 694.286325ms 694.908188ms 696.061211ms 697.98894ms 698.94313ms 700.190665ms 701.529933ms 701.968042ms 702.991123ms 703.315009ms 704.833311ms 705.340001ms 705.960394ms 707.107828ms 707.824533ms 708.449704ms 709.22911ms 710.238121ms 712.400406ms 714.380527ms 718.954561ms 720.554273ms 721.298978ms 723.098905ms 723.81681ms 724.470473ms 724.709388ms 725.579167ms 726.640058ms 727.501695ms 727.696779ms 727.983335ms 728.739511ms 729.051538ms 729.553137ms 730.534418ms 731.707982ms 732.768184ms 733.57307ms 733.870256ms 734.433207ms 737.568862ms 739.514422ms 739.91051ms 747.819313ms 750.162029ms 753.416396ms 755.313145ms 756.95998ms 758.575812ms 758.846319ms 760.126484ms 761.308318ms 763.143156ms 765.7748ms 767.558167ms 769.69488ms 770.111549ms 770.968037ms 771.341424ms 774.900459ms 776.688954ms 776.955269ms 777.107183ms 780.162985ms 783.094875ms 785.148406ms 789.60082ms 790.002376ms 790.19197ms 793.876016ms 795.424219ms 795.678832ms 798.48783ms 802.643266ms 810.413766ms 811.513147ms 813.157712ms 821.816539ms 825.355832ms 837.762857ms 842.786968ms 845.341351ms]
Jun 22 06:06:33.010: INFO: 50 %ile: 684.354611ms
Jun 22 06:06:33.010: INFO: 90 %ile: 777.107183ms
Jun 22 06:06:33.010: INFO: 99 %ile: 842.786968ms
Jun 22 06:06:33.010: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:06:33.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8877" for this suite.

• [SLOW TEST:12.518 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":176,"skipped":2966,"failed":0}
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:06:33.050: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8129
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun 22 06:06:33.262: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Jun 22 06:06:33.850: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 22 06:06:35.960: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:37.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:39.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:41.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:43.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:45.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:47.992: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:49.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:51.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:53.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:55.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:57.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:59.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:07:01.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:07:03.966: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402793, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:07:07.121: INFO: Waited 1.141238738s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:07:07.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8129" for this suite.

• [SLOW TEST:34.728 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":177,"skipped":2966,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:07:07.779: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5400
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 22 06:07:08.130: INFO: Waiting up to 5m0s for pod "pod-86aea8a6-6dba-43fa-98dc-6d5132b904f2" in namespace "emptydir-5400" to be "success or failure"
Jun 22 06:07:08.146: INFO: Pod "pod-86aea8a6-6dba-43fa-98dc-6d5132b904f2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.289994ms
Jun 22 06:07:10.155: INFO: Pod "pod-86aea8a6-6dba-43fa-98dc-6d5132b904f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024699891s
STEP: Saw pod success
Jun 22 06:07:10.155: INFO: Pod "pod-86aea8a6-6dba-43fa-98dc-6d5132b904f2" satisfied condition "success or failure"
Jun 22 06:07:10.168: INFO: Trying to get logs from node node4 pod pod-86aea8a6-6dba-43fa-98dc-6d5132b904f2 container test-container: <nil>
STEP: delete the pod
Jun 22 06:07:10.216: INFO: Waiting for pod pod-86aea8a6-6dba-43fa-98dc-6d5132b904f2 to disappear
Jun 22 06:07:10.224: INFO: Pod pod-86aea8a6-6dba-43fa-98dc-6d5132b904f2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:07:10.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5400" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":178,"skipped":2979,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:07:10.256: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:07:10.453: INFO: Waiting up to 5m0s for pod "downwardapi-volume-48bad1bf-a7ec-46c0-b2c3-e1e30fd9acfa" in namespace "downward-api-799" to be "success or failure"
Jun 22 06:07:10.464: INFO: Pod "downwardapi-volume-48bad1bf-a7ec-46c0-b2c3-e1e30fd9acfa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.537587ms
Jun 22 06:07:12.472: INFO: Pod "downwardapi-volume-48bad1bf-a7ec-46c0-b2c3-e1e30fd9acfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018602667s
STEP: Saw pod success
Jun 22 06:07:12.472: INFO: Pod "downwardapi-volume-48bad1bf-a7ec-46c0-b2c3-e1e30fd9acfa" satisfied condition "success or failure"
Jun 22 06:07:12.477: INFO: Trying to get logs from node node4 pod downwardapi-volume-48bad1bf-a7ec-46c0-b2c3-e1e30fd9acfa container client-container: <nil>
STEP: delete the pod
Jun 22 06:07:12.520: INFO: Waiting for pod downwardapi-volume-48bad1bf-a7ec-46c0-b2c3-e1e30fd9acfa to disappear
Jun 22 06:07:12.526: INFO: Pod downwardapi-volume-48bad1bf-a7ec-46c0-b2c3-e1e30fd9acfa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:07:12.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-799" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":179,"skipped":2990,"failed":0}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:07:12.547: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Jun 22 06:07:12.743: INFO: Waiting up to 5m0s for pod "client-containers-95fb1018-cd26-4dc4-a239-bf88147696a0" in namespace "containers-8315" to be "success or failure"
Jun 22 06:07:12.749: INFO: Pod "client-containers-95fb1018-cd26-4dc4-a239-bf88147696a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.303269ms
Jun 22 06:07:14.760: INFO: Pod "client-containers-95fb1018-cd26-4dc4-a239-bf88147696a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017002334s
STEP: Saw pod success
Jun 22 06:07:14.760: INFO: Pod "client-containers-95fb1018-cd26-4dc4-a239-bf88147696a0" satisfied condition "success or failure"
Jun 22 06:07:14.766: INFO: Trying to get logs from node node4 pod client-containers-95fb1018-cd26-4dc4-a239-bf88147696a0 container test-container: <nil>
STEP: delete the pod
Jun 22 06:07:14.806: INFO: Waiting for pod client-containers-95fb1018-cd26-4dc4-a239-bf88147696a0 to disappear
Jun 22 06:07:14.814: INFO: Pod client-containers-95fb1018-cd26-4dc4-a239-bf88147696a0 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:07:14.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8315" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":180,"skipped":2995,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:07:14.833: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6339
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Jun 22 06:07:15.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 api-versions'
Jun 22 06:07:15.155: INFO: stderr: ""
Jun 22 06:07:15.155: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:07:15.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6339" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":181,"skipped":2998,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:07:15.174: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6468
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-lfml
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 06:07:15.397: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lfml" in namespace "subpath-6468" to be "success or failure"
Jun 22 06:07:15.415: INFO: Pod "pod-subpath-test-configmap-lfml": Phase="Pending", Reason="", readiness=false. Elapsed: 18.08724ms
Jun 22 06:07:17.424: INFO: Pod "pod-subpath-test-configmap-lfml": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027848965s
Jun 22 06:07:19.432: INFO: Pod "pod-subpath-test-configmap-lfml": Phase="Running", Reason="", readiness=true. Elapsed: 4.035780193s
Jun 22 06:07:21.441: INFO: Pod "pod-subpath-test-configmap-lfml": Phase="Running", Reason="", readiness=true. Elapsed: 6.044287462s
Jun 22 06:07:23.447: INFO: Pod "pod-subpath-test-configmap-lfml": Phase="Running", Reason="", readiness=true. Elapsed: 8.049973123s
Jun 22 06:07:25.453: INFO: Pod "pod-subpath-test-configmap-lfml": Phase="Running", Reason="", readiness=true. Elapsed: 10.056580203s
Jun 22 06:07:27.463: INFO: Pod "pod-subpath-test-configmap-lfml": Phase="Running", Reason="", readiness=true. Elapsed: 12.066275186s
Jun 22 06:07:29.471: INFO: Pod "pod-subpath-test-configmap-lfml": Phase="Running", Reason="", readiness=true. Elapsed: 14.074340556s
Jun 22 06:07:31.479: INFO: Pod "pod-subpath-test-configmap-lfml": Phase="Running", Reason="", readiness=true. Elapsed: 16.082386326s
Jun 22 06:07:33.492: INFO: Pod "pod-subpath-test-configmap-lfml": Phase="Running", Reason="", readiness=true. Elapsed: 18.095151202s
Jun 22 06:07:35.501: INFO: Pod "pod-subpath-test-configmap-lfml": Phase="Running", Reason="", readiness=true. Elapsed: 20.104328775s
Jun 22 06:07:37.510: INFO: Pod "pod-subpath-test-configmap-lfml": Phase="Running", Reason="", readiness=true. Elapsed: 22.113257833s
Jun 22 06:07:39.517: INFO: Pod "pod-subpath-test-configmap-lfml": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.120669719s
STEP: Saw pod success
Jun 22 06:07:39.517: INFO: Pod "pod-subpath-test-configmap-lfml" satisfied condition "success or failure"
Jun 22 06:07:39.523: INFO: Trying to get logs from node node4 pod pod-subpath-test-configmap-lfml container test-container-subpath-configmap-lfml: <nil>
STEP: delete the pod
Jun 22 06:07:39.564: INFO: Waiting for pod pod-subpath-test-configmap-lfml to disappear
Jun 22 06:07:39.574: INFO: Pod pod-subpath-test-configmap-lfml no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lfml
Jun 22 06:07:39.574: INFO: Deleting pod "pod-subpath-test-configmap-lfml" in namespace "subpath-6468"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:07:39.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6468" for this suite.

• [SLOW TEST:24.423 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":182,"skipped":3001,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:07:39.597: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4407
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 06:07:40.537: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 06:07:42.563: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402860, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402860, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402860, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402860, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 06:07:45.606: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun 22 06:07:47.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 attach --namespace=webhook-4407 to-be-attached-pod -i -c=container1'
Jun 22 06:07:47.840: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:07:47.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4407" for this suite.
STEP: Destroying namespace "webhook-4407-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.433 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":183,"skipped":3002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:07:48.031: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 22 06:07:48.304: INFO: Waiting up to 5m0s for pod "pod-4609575b-37e4-441d-8e77-a84e4287be57" in namespace "emptydir-102" to be "success or failure"
Jun 22 06:07:48.313: INFO: Pod "pod-4609575b-37e4-441d-8e77-a84e4287be57": Phase="Pending", Reason="", readiness=false. Elapsed: 8.899792ms
Jun 22 06:07:50.322: INFO: Pod "pod-4609575b-37e4-441d-8e77-a84e4287be57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017591685s
Jun 22 06:07:52.333: INFO: Pod "pod-4609575b-37e4-441d-8e77-a84e4287be57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027957392s
STEP: Saw pod success
Jun 22 06:07:52.333: INFO: Pod "pod-4609575b-37e4-441d-8e77-a84e4287be57" satisfied condition "success or failure"
Jun 22 06:07:52.342: INFO: Trying to get logs from node node4 pod pod-4609575b-37e4-441d-8e77-a84e4287be57 container test-container: <nil>
STEP: delete the pod
Jun 22 06:07:52.378: INFO: Waiting for pod pod-4609575b-37e4-441d-8e77-a84e4287be57 to disappear
Jun 22 06:07:52.383: INFO: Pod pod-4609575b-37e4-441d-8e77-a84e4287be57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:07:52.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-102" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":184,"skipped":3026,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:07:52.405: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6272
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6272
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6272
STEP: creating replication controller externalsvc in namespace services-6272
I0622 06:07:52.678660      22 runners.go:189] Created replication controller with name: externalsvc, namespace: services-6272, replica count: 2
I0622 06:07:55.734473      22 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun 22 06:07:55.807: INFO: Creating new exec pod
Jun 22 06:07:59.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=services-6272 execpodr8qpn -- /bin/sh -x -c nslookup nodeport-service'
Jun 22 06:08:00.204: INFO: stderr: "+ nslookup nodeport-service\n"
Jun 22 06:08:00.204: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-6272.svc.cluster.local\tcanonical name = externalsvc.services-6272.svc.cluster.local.\nName:\texternalsvc.services-6272.svc.cluster.local\nAddress: 10.104.10.180\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6272, will wait for the garbage collector to delete the pods
Jun 22 06:08:00.276: INFO: Deleting ReplicationController externalsvc took: 15.764502ms
Jun 22 06:08:00.377: INFO: Terminating ReplicationController externalsvc pods took: 100.266516ms
Jun 22 06:08:05.156: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:08:05.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6272" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:12.829 seconds]
[sig-network] Services
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":185,"skipped":3041,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:08:05.234: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 22 06:08:06.362: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 22 06:08:08.384: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402886, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402886, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402886, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402886, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 06:08:11.474: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:08:11.483: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:08:17.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5728" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:12.150 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":186,"skipped":3050,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:08:17.385: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3889
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-d842c0f4-31b4-44bb-99aa-18707bef15db
STEP: Creating a pod to test consume configMaps
Jun 22 06:08:17.590: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fe60a748-17c0-43b4-8df8-1ce136200372" in namespace "projected-3889" to be "success or failure"
Jun 22 06:08:17.618: INFO: Pod "pod-projected-configmaps-fe60a748-17c0-43b4-8df8-1ce136200372": Phase="Pending", Reason="", readiness=false. Elapsed: 27.251839ms
Jun 22 06:08:19.625: INFO: Pod "pod-projected-configmaps-fe60a748-17c0-43b4-8df8-1ce136200372": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034737247s
STEP: Saw pod success
Jun 22 06:08:19.625: INFO: Pod "pod-projected-configmaps-fe60a748-17c0-43b4-8df8-1ce136200372" satisfied condition "success or failure"
Jun 22 06:08:19.630: INFO: Trying to get logs from node node4 pod pod-projected-configmaps-fe60a748-17c0-43b4-8df8-1ce136200372 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 06:08:19.681: INFO: Waiting for pod pod-projected-configmaps-fe60a748-17c0-43b4-8df8-1ce136200372 to disappear
Jun 22 06:08:19.685: INFO: Pod pod-projected-configmaps-fe60a748-17c0-43b4-8df8-1ce136200372 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:08:19.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3889" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":187,"skipped":3055,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:08:19.708: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1860
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 22 06:08:24.494: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1860 pod-service-account-410df95c-f69f-4b17-9e9b-be469406b2c5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 22 06:08:24.809: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1860 pod-service-account-410df95c-f69f-4b17-9e9b-be469406b2c5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 22 06:08:25.166: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1860 pod-service-account-410df95c-f69f-4b17-9e9b-be469406b2c5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:08:25.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1860" for this suite.

• [SLOW TEST:5.836 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":188,"skipped":3107,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:08:25.544: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:08:25.751: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b247d408-cc80-496b-8108-ff6cdc73d42e" in namespace "projected-6865" to be "success or failure"
Jun 22 06:08:25.759: INFO: Pod "downwardapi-volume-b247d408-cc80-496b-8108-ff6cdc73d42e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.221359ms
Jun 22 06:08:27.766: INFO: Pod "downwardapi-volume-b247d408-cc80-496b-8108-ff6cdc73d42e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014772208s
Jun 22 06:08:29.775: INFO: Pod "downwardapi-volume-b247d408-cc80-496b-8108-ff6cdc73d42e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023243776s
STEP: Saw pod success
Jun 22 06:08:29.775: INFO: Pod "downwardapi-volume-b247d408-cc80-496b-8108-ff6cdc73d42e" satisfied condition "success or failure"
Jun 22 06:08:29.783: INFO: Trying to get logs from node node4 pod downwardapi-volume-b247d408-cc80-496b-8108-ff6cdc73d42e container client-container: <nil>
STEP: delete the pod
Jun 22 06:08:29.826: INFO: Waiting for pod downwardapi-volume-b247d408-cc80-496b-8108-ff6cdc73d42e to disappear
Jun 22 06:08:29.831: INFO: Pod downwardapi-volume-b247d408-cc80-496b-8108-ff6cdc73d42e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:08:29.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6865" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":189,"skipped":3133,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:08:29.852: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-14e11b61-d97b-4f08-8ce9-03bc79e02a77
STEP: Creating a pod to test consume configMaps
Jun 22 06:08:30.089: INFO: Waiting up to 5m0s for pod "pod-configmaps-17b0e878-0b7c-4f6b-a8a8-bc5e275024c9" in namespace "configmap-6188" to be "success or failure"
Jun 22 06:08:30.094: INFO: Pod "pod-configmaps-17b0e878-0b7c-4f6b-a8a8-bc5e275024c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.529042ms
Jun 22 06:08:32.102: INFO: Pod "pod-configmaps-17b0e878-0b7c-4f6b-a8a8-bc5e275024c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013172914s
STEP: Saw pod success
Jun 22 06:08:32.102: INFO: Pod "pod-configmaps-17b0e878-0b7c-4f6b-a8a8-bc5e275024c9" satisfied condition "success or failure"
Jun 22 06:08:32.110: INFO: Trying to get logs from node node4 pod pod-configmaps-17b0e878-0b7c-4f6b-a8a8-bc5e275024c9 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 06:08:32.160: INFO: Waiting for pod pod-configmaps-17b0e878-0b7c-4f6b-a8a8-bc5e275024c9 to disappear
Jun 22 06:08:32.167: INFO: Pod pod-configmaps-17b0e878-0b7c-4f6b-a8a8-bc5e275024c9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:08:32.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6188" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":190,"skipped":3153,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:08:32.186: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-469
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:08:32.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-469" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":191,"skipped":3160,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:08:32.410: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 22 06:08:34.633: INFO: &Pod{ObjectMeta:{send-events-a91cf248-e73f-4255-b1f8-69a76252b23f  events-4103 /api/v1/namespaces/events-4103/pods/send-events-a91cf248-e73f-4255-b1f8-69a76252b23f c87d1114-9b8f-4461-9da2-bb24b7406607 35427 0 2020-06-22 06:08:32 +0000 UTC <nil> <nil> map[name:foo time:585730981] map[cni.projectcalico.org/podIP:10.41.130.146/32 cni.projectcalico.org/podIPs:10.41.130.146/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-njdzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-njdzt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-njdzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 06:08:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 06:08:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 06:08:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 06:08:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.144,PodIP:10.41.130.146,StartTime:2020-06-22 06:08:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 06:08:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://1850f6adf0bf07a4f6c5b17a5d51c1088600d12cc6178876daa3833471e782a2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.41.130.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun 22 06:08:36.640: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 22 06:08:38.654: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:08:38.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4103" for this suite.

• [SLOW TEST:6.286 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":192,"skipped":3173,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:08:38.696: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7107
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Jun 22 06:08:39.424: INFO: created pod pod-service-account-defaultsa
Jun 22 06:08:39.424: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 22 06:08:39.440: INFO: created pod pod-service-account-mountsa
Jun 22 06:08:39.440: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 22 06:08:39.454: INFO: created pod pod-service-account-nomountsa
Jun 22 06:08:39.454: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 22 06:08:39.465: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 22 06:08:39.465: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 22 06:08:39.493: INFO: created pod pod-service-account-mountsa-mountspec
Jun 22 06:08:39.493: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 22 06:08:39.519: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 22 06:08:39.520: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 22 06:08:39.540: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 22 06:08:39.540: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 22 06:08:39.577: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 22 06:08:39.577: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 22 06:08:39.605: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 22 06:08:39.606: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:08:39.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7107" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":193,"skipped":3185,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:08:39.638: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7831
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-7831
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 06:08:39.851: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 06:09:02.197: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.41.130.145:8080/dial?request=hostname&protocol=udp&host=10.37.149.26&port=8081&tries=1'] Namespace:pod-network-test-7831 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 06:09:02.197: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 06:09:02.363: INFO: Waiting for responses: map[]
Jun 22 06:09:02.368: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.41.130.145:8080/dial?request=hostname&protocol=udp&host=10.37.11.27&port=8081&tries=1'] Namespace:pod-network-test-7831 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 06:09:02.368: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 06:09:02.555: INFO: Waiting for responses: map[]
Jun 22 06:09:02.560: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.41.130.145:8080/dial?request=hostname&protocol=udp&host=10.42.135.36&port=8081&tries=1'] Namespace:pod-network-test-7831 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 06:09:02.560: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 06:09:02.734: INFO: Waiting for responses: map[]
Jun 22 06:09:02.742: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.41.130.145:8080/dial?request=hostname&protocol=udp&host=10.41.130.149&port=8081&tries=1'] Namespace:pod-network-test-7831 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 06:09:02.742: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 06:09:02.913: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:09:02.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7831" for this suite.

• [SLOW TEST:23.293 seconds]
[sig-network] Networking
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":194,"skipped":3187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:09:02.932: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3793
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:09:16.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3793" for this suite.

• [SLOW TEST:13.330 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":195,"skipped":3216,"failed":0}
SSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:09:16.262: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2649
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:09:16.463: INFO: Waiting up to 5m0s for pod "busybox-user-65534-40e8295b-ad86-4fdd-821f-ef8f87a75dcd" in namespace "security-context-test-2649" to be "success or failure"
Jun 22 06:09:16.469: INFO: Pod "busybox-user-65534-40e8295b-ad86-4fdd-821f-ef8f87a75dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.730898ms
Jun 22 06:09:18.479: INFO: Pod "busybox-user-65534-40e8295b-ad86-4fdd-821f-ef8f87a75dcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015436401s
Jun 22 06:09:18.479: INFO: Pod "busybox-user-65534-40e8295b-ad86-4fdd-821f-ef8f87a75dcd" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:09:18.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2649" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":196,"skipped":3222,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:09:18.500: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5079
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:09:18.711: INFO: Waiting up to 5m0s for pod "downwardapi-volume-65d1913d-cfcc-494e-bb5f-c0147955de87" in namespace "projected-5079" to be "success or failure"
Jun 22 06:09:18.715: INFO: Pod "downwardapi-volume-65d1913d-cfcc-494e-bb5f-c0147955de87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.307689ms
Jun 22 06:09:20.722: INFO: Pod "downwardapi-volume-65d1913d-cfcc-494e-bb5f-c0147955de87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011281646s
STEP: Saw pod success
Jun 22 06:09:20.722: INFO: Pod "downwardapi-volume-65d1913d-cfcc-494e-bb5f-c0147955de87" satisfied condition "success or failure"
Jun 22 06:09:20.729: INFO: Trying to get logs from node node4 pod downwardapi-volume-65d1913d-cfcc-494e-bb5f-c0147955de87 container client-container: <nil>
STEP: delete the pod
Jun 22 06:09:20.774: INFO: Waiting for pod downwardapi-volume-65d1913d-cfcc-494e-bb5f-c0147955de87 to disappear
Jun 22 06:09:20.794: INFO: Pod downwardapi-volume-65d1913d-cfcc-494e-bb5f-c0147955de87 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:09:20.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5079" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":197,"skipped":3222,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:09:20.816: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-427
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:09:23.096: INFO: Waiting up to 5m0s for pod "client-envvars-2dc37ab9-a8ee-46b1-a6f1-cf2b4e34a761" in namespace "pods-427" to be "success or failure"
Jun 22 06:09:23.107: INFO: Pod "client-envvars-2dc37ab9-a8ee-46b1-a6f1-cf2b4e34a761": Phase="Pending", Reason="", readiness=false. Elapsed: 10.892664ms
Jun 22 06:09:25.117: INFO: Pod "client-envvars-2dc37ab9-a8ee-46b1-a6f1-cf2b4e34a761": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021063636s
STEP: Saw pod success
Jun 22 06:09:25.117: INFO: Pod "client-envvars-2dc37ab9-a8ee-46b1-a6f1-cf2b4e34a761" satisfied condition "success or failure"
Jun 22 06:09:25.124: INFO: Trying to get logs from node node4 pod client-envvars-2dc37ab9-a8ee-46b1-a6f1-cf2b4e34a761 container env3cont: <nil>
STEP: delete the pod
Jun 22 06:09:25.174: INFO: Waiting for pod client-envvars-2dc37ab9-a8ee-46b1-a6f1-cf2b4e34a761 to disappear
Jun 22 06:09:25.179: INFO: Pod client-envvars-2dc37ab9-a8ee-46b1-a6f1-cf2b4e34a761 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:09:25.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-427" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":198,"skipped":3227,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:09:25.199: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4291
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun 22 06:09:25.386: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 06:09:25.411: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 06:09:25.421: INFO: 
Logging pods the kubelet thinks is on node node1 before test
Jun 22 06:09:25.451: INFO: cocktail-batch-server-75c78755c5-c94nd from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container batch ready: true, restart count 0
Jun 22 06:09:25.451: INFO: cocktail-api-cmdb-0 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 06:09:25.451: INFO: cocktail-cluster-api-69794f5b78-9htrk from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container cluster-api ready: true, restart count 1
Jun 22 06:09:25.451: INFO: calico-node-qkngx from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 06:09:25.451: INFO: kube-apiserver-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 06:09:25.451: INFO: cocktail-monitoring-76bc9fcf6f-r9tw9 from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container monitoring ready: true, restart count 0
Jun 22 06:09:25.451: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-kwffd from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 06:09:25.451: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 06:09:25.451: INFO: coredns-ddbf59985-7mcct from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container coredns ready: true, restart count 0
Jun 22 06:09:25.451: INFO: kube-proxy-qbghn from kube-system started at 2020-06-22 04:48:30 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 06:09:25.451: INFO: kube-controller-manager-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 06:09:25.451: INFO: kube-scheduler-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 06:09:25.451: INFO: metrics-server-76b7895b66-f4lch from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 06:09:25.451: INFO: cocktail-alarm-collector-6cfd6bbb9c-kl9zt from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container collector ready: true, restart count 0
Jun 22 06:09:25.451: INFO: haproxy-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container haproxy ready: true, restart count 1
Jun 22 06:09:25.451: INFO: calico-kube-controllers-77c4b7448-pvlhp from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.451: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 06:09:25.451: INFO: 
Logging pods the kubelet thinks is on node node2 before test
Jun 22 06:09:25.487: INFO: cocktail-metric-collector-5987649879-49kjv from cocktail-system started at 2020-06-22 04:50:15 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container statcollector ready: true, restart count 0
Jun 22 06:09:25.487: INFO: cocktail-dashboard-6bd44cf6c8-mmhlx from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container cocktail-dashboard ready: true, restart count 0
Jun 22 06:09:25.487: INFO: kube-apiserver-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 06:09:25.487: INFO: kube-controller-manager-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 06:09:25.487: INFO: calico-node-xt4w8 from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 06:09:25.487: INFO: kube-proxy-chh9k from kube-system started at 2020-06-22 04:48:19 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 06:09:25.487: INFO: cocktail-build-queue-678588c57d-h5dd4 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container nats-streaming ready: true, restart count 0
Jun 22 06:09:25.487: INFO: cocktail-api-cmdb-2 from cocktail-system started at 2020-06-22 04:52:57 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 06:09:25.487: INFO: haproxy-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 06:09:25.487: INFO: kube-scheduler-node2 from kube-system started at 2020-06-22 04:48:10 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 06:09:25.487: INFO: cocktail-dashboard-queue-845dd499b5-zzxrl from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container dashboard-queue ready: true, restart count 0
Jun 22 06:09:25.487: INFO: cocktail-api-server-84467f9bf8-8h74n from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container api-server ready: true, restart count 0
Jun 22 06:09:25.487: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-jrq4m from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 06:09:25.487: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 06:09:25.487: INFO: addon-manager-5bb9bbcdcc-j2qhp from cocktail-addon started at 2020-06-22 04:49:54 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container addon-manager ready: true, restart count 0
Jun 22 06:09:25.487: INFO: local-storage-nfs-client-provisioner-745d66655-5qmjz from cocktail-addon started at 2020-06-22 04:50:09 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container nfs-client-provisioner ready: true, restart count 0
Jun 22 06:09:25.487: INFO: cocktail-cluster-health-checker-7456f8c49-x62nj from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container checker ready: true, restart count 0
Jun 22 06:09:25.487: INFO: cocktail-dashboard-session-7fb7444f46-xcm2x from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.487: INFO: 	Container dashboard-session ready: true, restart count 0
Jun 22 06:09:25.487: INFO: 
Logging pods the kubelet thinks is on node node3 before test
Jun 22 06:09:25.519: INFO: kube-apiserver-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 06:09:25.519: INFO: metrics-server-76b7895b66-nz47p from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 06:09:25.519: INFO: cocktail-build-api-7968df5d66-kqcnw from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container build-api ready: true, restart count 0
Jun 22 06:09:25.519: INFO: kube-controller-manager-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 06:09:25.519: INFO: calico-node-lhxv5 from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 06:09:25.519: INFO: coredns-ddbf59985-ld84l from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container coredns ready: true, restart count 0
Jun 22 06:09:25.519: INFO: cocktail-monitoring-tsdb-799959dd7c-bg2nj from cocktail-system started at 2020-06-22 04:50:15 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container db ready: true, restart count 0
Jun 22 06:09:25.519: INFO: cocktail-dashboard-proxy-6557bc4fbf-fh5f9 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container dashboard-proxy-01 ready: true, restart count 0
Jun 22 06:09:25.519: INFO: kube-scheduler-node3 from kube-system started at 2020-06-22 04:48:05 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 06:09:25.519: INFO: cocktail-api-cmdb-1 from cocktail-system started at 2020-06-22 04:51:32 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 06:09:25.519: INFO: haproxy-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 06:09:25.519: INFO: kube-proxy-sb9hq from kube-system started at 2020-06-22 04:48:28 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 06:09:25.519: INFO: cocktail-package-75488df5f8-7p55f from cocktail-system started at 2020-06-22 04:50:13 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container package ready: true, restart count 1
Jun 22 06:09:25.519: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-88h64 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 06:09:25.519: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 06:09:25.519: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 06:09:25.519: INFO: 
Logging pods the kubelet thinks is on node node4 before test
Jun 22 06:09:25.530: INFO: sonobuoy-e2e-job-6934f20e32d94a27 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 06:09:25.530: INFO: 	Container e2e ready: true, restart count 0
Jun 22 06:09:25.530: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 06:09:25.530: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-h48z4 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 06:09:25.530: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 06:09:25.530: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 06:09:25.530: INFO: kube-proxy-42p24 from kube-system started at 2020-06-22 04:49:39 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.530: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 06:09:25.530: INFO: server-envvars-831a5edc-a645-48aa-bc5f-697a595634a0 from pods-427 started at 2020-06-22 06:09:21 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.530: INFO: 	Container srv ready: true, restart count 0
Jun 22 06:09:25.530: INFO: haproxy-node4 from kube-system started at 2020-06-22 04:50:03 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.530: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 06:09:25.530: INFO: calico-node-q9h8x from kube-system started at 2020-06-22 04:49:39 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.530: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 06:09:25.530: INFO: sonobuoy from sonobuoy started at 2020-06-22 05:09:07 +0000 UTC (1 container statuses recorded)
Jun 22 06:09:25.530: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.161ac8c4bf4ce3fa], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:09:26.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4291" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":199,"skipped":3238,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:09:26.612: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1503
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 06:09:29.852: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:09:29.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1503" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":200,"skipped":3246,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:09:29.931: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8451
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1754
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 06:09:30.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8451'
Jun 22 06:09:30.255: INFO: stderr: ""
Jun 22 06:09:30.255: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1759
Jun 22 06:09:30.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete pods e2e-test-httpd-pod --namespace=kubectl-8451'
Jun 22 06:09:32.514: INFO: stderr: ""
Jun 22 06:09:32.514: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:09:32.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8451" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":201,"skipped":3249,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:09:32.537: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 06:09:33.673: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 22 06:09:35.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402973, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402973, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402973, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728402973, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 06:09:38.741: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:09:38.746: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2977-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:09:45.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5608" for this suite.
STEP: Destroying namespace "webhook-5608-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.750 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":202,"skipped":3268,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:09:45.287: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4880
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jun 22 06:10:25.568: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0622 06:10:25.568078      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:10:25.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4880" for this suite.

• [SLOW TEST:40.300 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":203,"skipped":3282,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:10:25.587: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:10:25.770: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:10:29.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8883" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":204,"skipped":3283,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:10:29.860: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4229
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun 22 06:10:30.063: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 06:10:38.979: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:11:00.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4229" for this suite.

• [SLOW TEST:30.524 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":205,"skipped":3288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:11:00.385: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-456
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 22 06:11:00.583: INFO: Waiting up to 5m0s for pod "pod-1b6b3f0d-6b6d-41d0-be06-3be542b45291" in namespace "emptydir-456" to be "success or failure"
Jun 22 06:11:00.594: INFO: Pod "pod-1b6b3f0d-6b6d-41d0-be06-3be542b45291": Phase="Pending", Reason="", readiness=false. Elapsed: 11.21645ms
Jun 22 06:11:02.609: INFO: Pod "pod-1b6b3f0d-6b6d-41d0-be06-3be542b45291": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026189802s
Jun 22 06:11:04.615: INFO: Pod "pod-1b6b3f0d-6b6d-41d0-be06-3be542b45291": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031988074s
STEP: Saw pod success
Jun 22 06:11:04.615: INFO: Pod "pod-1b6b3f0d-6b6d-41d0-be06-3be542b45291" satisfied condition "success or failure"
Jun 22 06:11:04.623: INFO: Trying to get logs from node node4 pod pod-1b6b3f0d-6b6d-41d0-be06-3be542b45291 container test-container: <nil>
STEP: delete the pod
Jun 22 06:11:04.684: INFO: Waiting for pod pod-1b6b3f0d-6b6d-41d0-be06-3be542b45291 to disappear
Jun 22 06:11:04.694: INFO: Pod pod-1b6b3f0d-6b6d-41d0-be06-3be542b45291 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:11:04.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-456" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":206,"skipped":3322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:11:04.713: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1287
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-9e8c9c21-9e6a-4de4-8765-85a73756f9be
STEP: Creating configMap with name cm-test-opt-upd-7ff05d90-a8ef-485d-a0ef-2c9e5030209e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9e8c9c21-9e6a-4de4-8765-85a73756f9be
STEP: Updating configmap cm-test-opt-upd-7ff05d90-a8ef-485d-a0ef-2c9e5030209e
STEP: Creating configMap with name cm-test-opt-create-f883b842-a9ec-46ea-a43d-b3de96f94a70
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:12:27.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1287" for this suite.

• [SLOW TEST:83.199 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":207,"skipped":3344,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:12:27.913: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7286
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-9cdd6a97-38ba-4dcf-8f13-3c3421832dfa
STEP: Creating a pod to test consume configMaps
Jun 22 06:12:28.130: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b0a143c8-f679-4710-b163-30a7563d7a6a" in namespace "projected-7286" to be "success or failure"
Jun 22 06:12:28.137: INFO: Pod "pod-projected-configmaps-b0a143c8-f679-4710-b163-30a7563d7a6a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.195588ms
Jun 22 06:12:30.145: INFO: Pod "pod-projected-configmaps-b0a143c8-f679-4710-b163-30a7563d7a6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015244787s
STEP: Saw pod success
Jun 22 06:12:30.145: INFO: Pod "pod-projected-configmaps-b0a143c8-f679-4710-b163-30a7563d7a6a" satisfied condition "success or failure"
Jun 22 06:12:30.150: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-b0a143c8-f679-4710-b163-30a7563d7a6a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 06:12:30.219: INFO: Waiting for pod pod-projected-configmaps-b0a143c8-f679-4710-b163-30a7563d7a6a to disappear
Jun 22 06:12:30.224: INFO: Pod pod-projected-configmaps-b0a143c8-f679-4710-b163-30a7563d7a6a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:12:30.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7286" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":208,"skipped":3354,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:12:30.245: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2608
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2608
I0622 06:12:30.517987      22 runners.go:189] Created replication controller with name: externalname-service, namespace: services-2608, replica count: 2
Jun 22 06:12:33.568: INFO: Creating new exec pod
I0622 06:12:33.568619      22 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 06:12:36.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=services-2608 execpod9pwdv -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 22 06:12:36.914: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 22 06:12:36.914: INFO: stdout: ""
Jun 22 06:12:36.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=services-2608 execpod9pwdv -- /bin/sh -x -c nc -zv -t -w 2 10.96.53.162 80'
Jun 22 06:12:37.234: INFO: stderr: "+ nc -zv -t -w 2 10.96.53.162 80\nConnection to 10.96.53.162 80 port [tcp/http] succeeded!\n"
Jun 22 06:12:37.234: INFO: stdout: ""
Jun 22 06:12:37.234: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:12:37.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2608" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.084 seconds]
[sig-network] Services
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":209,"skipped":3363,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:12:37.329: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3209
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9439
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8195
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:12:51.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3209" for this suite.
STEP: Destroying namespace "nsdeletetest-9439" for this suite.
Jun 22 06:12:51.962: INFO: Namespace nsdeletetest-9439 was already deleted
STEP: Destroying namespace "nsdeletetest-8195" for this suite.

• [SLOW TEST:14.653 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":210,"skipped":3373,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:12:51.982: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4798
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-529e67b4-b606-4021-9492-1fa6a83c40f3
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:12:52.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4798" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":211,"skipped":3376,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:12:52.180: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3082
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-dfce89dd-7661-441a-9b63-b5f14f800c10
STEP: Creating a pod to test consume secrets
Jun 22 06:12:52.381: INFO: Waiting up to 5m0s for pod "pod-secrets-536122b4-d4d8-4e29-9902-f1aae0929c33" in namespace "secrets-3082" to be "success or failure"
Jun 22 06:12:52.392: INFO: Pod "pod-secrets-536122b4-d4d8-4e29-9902-f1aae0929c33": Phase="Pending", Reason="", readiness=false. Elapsed: 11.091277ms
Jun 22 06:12:54.397: INFO: Pod "pod-secrets-536122b4-d4d8-4e29-9902-f1aae0929c33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016548133s
STEP: Saw pod success
Jun 22 06:12:54.397: INFO: Pod "pod-secrets-536122b4-d4d8-4e29-9902-f1aae0929c33" satisfied condition "success or failure"
Jun 22 06:12:54.406: INFO: Trying to get logs from node node4 pod pod-secrets-536122b4-d4d8-4e29-9902-f1aae0929c33 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 06:12:54.449: INFO: Waiting for pod pod-secrets-536122b4-d4d8-4e29-9902-f1aae0929c33 to disappear
Jun 22 06:12:54.458: INFO: Pod pod-secrets-536122b4-d4d8-4e29-9902-f1aae0929c33 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:12:54.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3082" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":212,"skipped":3376,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:12:54.533: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3361
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-aba8dc86-1e02-4c26-9eb0-a68880954686
STEP: Creating a pod to test consume secrets
Jun 22 06:12:54.745: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-98500d35-c261-4105-8257-0c00f65ba939" in namespace "projected-3361" to be "success or failure"
Jun 22 06:12:54.752: INFO: Pod "pod-projected-secrets-98500d35-c261-4105-8257-0c00f65ba939": Phase="Pending", Reason="", readiness=false. Elapsed: 6.219368ms
Jun 22 06:12:56.760: INFO: Pod "pod-projected-secrets-98500d35-c261-4105-8257-0c00f65ba939": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014628545s
STEP: Saw pod success
Jun 22 06:12:56.760: INFO: Pod "pod-projected-secrets-98500d35-c261-4105-8257-0c00f65ba939" satisfied condition "success or failure"
Jun 22 06:12:56.773: INFO: Trying to get logs from node node4 pod pod-projected-secrets-98500d35-c261-4105-8257-0c00f65ba939 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 06:12:56.827: INFO: Waiting for pod pod-projected-secrets-98500d35-c261-4105-8257-0c00f65ba939 to disappear
Jun 22 06:12:56.835: INFO: Pod pod-projected-secrets-98500d35-c261-4105-8257-0c00f65ba939 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:12:56.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3361" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":213,"skipped":3404,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:12:56.858: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:12:57.085: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db24d6e0-11c1-4e2a-8083-ded7afd99abb" in namespace "projected-9773" to be "success or failure"
Jun 22 06:12:57.095: INFO: Pod "downwardapi-volume-db24d6e0-11c1-4e2a-8083-ded7afd99abb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.019596ms
Jun 22 06:12:59.103: INFO: Pod "downwardapi-volume-db24d6e0-11c1-4e2a-8083-ded7afd99abb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017161267s
STEP: Saw pod success
Jun 22 06:12:59.103: INFO: Pod "downwardapi-volume-db24d6e0-11c1-4e2a-8083-ded7afd99abb" satisfied condition "success or failure"
Jun 22 06:12:59.109: INFO: Trying to get logs from node node4 pod downwardapi-volume-db24d6e0-11c1-4e2a-8083-ded7afd99abb container client-container: <nil>
STEP: delete the pod
Jun 22 06:12:59.161: INFO: Waiting for pod downwardapi-volume-db24d6e0-11c1-4e2a-8083-ded7afd99abb to disappear
Jun 22 06:12:59.173: INFO: Pod downwardapi-volume-db24d6e0-11c1-4e2a-8083-ded7afd99abb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:12:59.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9773" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":214,"skipped":3414,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:12:59.195: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5608
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-b146821f-991c-469e-a71e-16b324ee2541
STEP: Creating a pod to test consume secrets
Jun 22 06:12:59.395: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-14c07ecb-72be-4dcf-9290-8fbf9a563787" in namespace "projected-5608" to be "success or failure"
Jun 22 06:12:59.405: INFO: Pod "pod-projected-secrets-14c07ecb-72be-4dcf-9290-8fbf9a563787": Phase="Pending", Reason="", readiness=false. Elapsed: 10.076717ms
Jun 22 06:13:01.421: INFO: Pod "pod-projected-secrets-14c07ecb-72be-4dcf-9290-8fbf9a563787": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025538848s
STEP: Saw pod success
Jun 22 06:13:01.421: INFO: Pod "pod-projected-secrets-14c07ecb-72be-4dcf-9290-8fbf9a563787" satisfied condition "success or failure"
Jun 22 06:13:01.430: INFO: Trying to get logs from node node4 pod pod-projected-secrets-14c07ecb-72be-4dcf-9290-8fbf9a563787 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 06:13:01.481: INFO: Waiting for pod pod-projected-secrets-14c07ecb-72be-4dcf-9290-8fbf9a563787 to disappear
Jun 22 06:13:01.490: INFO: Pod pod-projected-secrets-14c07ecb-72be-4dcf-9290-8fbf9a563787 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:13:01.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5608" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":215,"skipped":3416,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:13:01.516: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-973
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:13:06.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-973" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":216,"skipped":3434,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:13:06.479: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3537
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3537.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3537.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 06:13:10.805: INFO: DNS probes using dns-3537/dns-test-293243c3-1758-4d44-b437-5fdcc45bb704 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:13:10.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3537" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":217,"skipped":3478,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:13:10.880: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4366
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 06:13:12.365: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 06:13:14.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403192, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403192, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403192, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403192, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 06:13:17.429: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:13:17.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4366" for this suite.
STEP: Destroying namespace "webhook-4366-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.765 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":218,"skipped":3508,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:13:17.643: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6614
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 06:13:52.197: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:13:52.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6614" for this suite.

• [SLOW TEST:34.601 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":219,"skipped":3530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:13:52.245: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7953
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:13:52.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-673036da-784a-43ab-a722-5b7004b0d489" in namespace "downward-api-7953" to be "success or failure"
Jun 22 06:13:52.447: INFO: Pod "downwardapi-volume-673036da-784a-43ab-a722-5b7004b0d489": Phase="Pending", Reason="", readiness=false. Elapsed: 7.397622ms
Jun 22 06:13:54.457: INFO: Pod "downwardapi-volume-673036da-784a-43ab-a722-5b7004b0d489": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016998233s
Jun 22 06:13:56.464: INFO: Pod "downwardapi-volume-673036da-784a-43ab-a722-5b7004b0d489": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024490111s
STEP: Saw pod success
Jun 22 06:13:56.464: INFO: Pod "downwardapi-volume-673036da-784a-43ab-a722-5b7004b0d489" satisfied condition "success or failure"
Jun 22 06:13:56.471: INFO: Trying to get logs from node node4 pod downwardapi-volume-673036da-784a-43ab-a722-5b7004b0d489 container client-container: <nil>
STEP: delete the pod
Jun 22 06:13:56.521: INFO: Waiting for pod downwardapi-volume-673036da-784a-43ab-a722-5b7004b0d489 to disappear
Jun 22 06:13:56.531: INFO: Pod downwardapi-volume-673036da-784a-43ab-a722-5b7004b0d489 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:13:56.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7953" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":220,"skipped":3567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:13:56.554: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5878
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun 22 06:13:56.741: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun 22 06:14:17.671: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
Jun 22 06:14:26.518: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:14:48.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5878" for this suite.

• [SLOW TEST:51.915 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":221,"skipped":3612,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:14:48.470: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9298
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a2e5d83e-7e3a-48d2-bc90-ef1eba35db6f
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a2e5d83e-7e3a-48d2-bc90-ef1eba35db6f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:16:23.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9298" for this suite.

• [SLOW TEST:95.195 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":222,"skipped":3613,"failed":0}
SSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:16:23.665: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-7398
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:16:23.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-7398" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":223,"skipped":3618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:16:24.014: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:16:28.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8141" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":224,"skipped":3655,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:16:28.246: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:16:28.439: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca517096-14bf-4272-80fa-ca9ef8e522e5" in namespace "downward-api-1822" to be "success or failure"
Jun 22 06:16:28.454: INFO: Pod "downwardapi-volume-ca517096-14bf-4272-80fa-ca9ef8e522e5": Phase="Pending", Reason="", readiness=false. Elapsed: 15.63719ms
Jun 22 06:16:30.467: INFO: Pod "downwardapi-volume-ca517096-14bf-4272-80fa-ca9ef8e522e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028126381s
STEP: Saw pod success
Jun 22 06:16:30.467: INFO: Pod "downwardapi-volume-ca517096-14bf-4272-80fa-ca9ef8e522e5" satisfied condition "success or failure"
Jun 22 06:16:30.472: INFO: Trying to get logs from node node4 pod downwardapi-volume-ca517096-14bf-4272-80fa-ca9ef8e522e5 container client-container: <nil>
STEP: delete the pod
Jun 22 06:16:30.528: INFO: Waiting for pod downwardapi-volume-ca517096-14bf-4272-80fa-ca9ef8e522e5 to disappear
Jun 22 06:16:30.535: INFO: Pod downwardapi-volume-ca517096-14bf-4272-80fa-ca9ef8e522e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:16:30.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1822" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":225,"skipped":3659,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:16:30.570: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:16:30.760: INFO: Creating deployment "test-recreate-deployment"
Jun 22 06:16:30.780: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 22 06:16:30.812: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 22 06:16:32.826: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 22 06:16:32.832: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 22 06:16:32.845: INFO: Updating deployment test-recreate-deployment
Jun 22 06:16:32.845: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun 22 06:16:33.024: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-306 /apis/apps/v1/namespaces/deployment-306/deployments/test-recreate-deployment b39fe8b8-e7c6-45bf-927d-9b39cb5451c7 39346 2 2020-06-22 06:16:30 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001a91a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-06-22 06:16:32 +0000 UTC,LastTransitionTime:2020-06-22 06:16:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-06-22 06:16:33 +0000 UTC,LastTransitionTime:2020-06-22 06:16:30 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 22 06:16:33.033: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-306 /apis/apps/v1/namespaces/deployment-306/replicasets/test-recreate-deployment-5f94c574ff 8fbee40f-f1e7-45df-8750-8ac31305ed71 39345 1 2020-06-22 06:16:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b39fe8b8-e7c6-45bf-927d-9b39cb5451c7 0xc001a91fa7 0xc001a91fa8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00353c048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 06:16:33.033: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 22 06:16:33.033: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-306 /apis/apps/v1/namespaces/deployment-306/replicasets/test-recreate-deployment-799c574856 13c367c8-1606-40ba-9396-c28b2cecd07d 39333 2 2020-06-22 06:16:30 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b39fe8b8-e7c6-45bf-927d-9b39cb5451c7 0xc00353c167 0xc00353c168}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00353c3a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 06:16:33.040: INFO: Pod "test-recreate-deployment-5f94c574ff-2n9t8" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-2n9t8 test-recreate-deployment-5f94c574ff- deployment-306 /api/v1/namespaces/deployment-306/pods/test-recreate-deployment-5f94c574ff-2n9t8 f9c95c5d-d701-454f-9e2f-dc4e8463366b 39344 0 2020-06-22 06:16:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 8fbee40f-f1e7-45df-8750-8ac31305ed71 0xc00353d737 0xc00353d738}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rvmtx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rvmtx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rvmtx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 06:16:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 06:16:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 06:16:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 06:16:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.144,PodIP:,StartTime:2020-06-22 06:16:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:16:33.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-306" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":226,"skipped":3667,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:16:33.057: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9403
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jun 22 06:17:03.846: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:17:03.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0622 06:17:03.846082      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-9403" for this suite.

• [SLOW TEST:30.808 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":227,"skipped":3672,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:17:03.865: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4092
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 22 06:17:04.084: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4092 /api/v1/namespaces/watch-4092/configmaps/e2e-watch-test-label-changed abf115f7-e850-4572-aa16-66cd362bf6cf 39586 0 2020-06-22 06:17:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 06:17:04.084: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4092 /api/v1/namespaces/watch-4092/configmaps/e2e-watch-test-label-changed abf115f7-e850-4572-aa16-66cd362bf6cf 39587 0 2020-06-22 06:17:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 22 06:17:04.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4092 /api/v1/namespaces/watch-4092/configmaps/e2e-watch-test-label-changed abf115f7-e850-4572-aa16-66cd362bf6cf 39588 0 2020-06-22 06:17:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 22 06:17:14.146: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4092 /api/v1/namespaces/watch-4092/configmaps/e2e-watch-test-label-changed abf115f7-e850-4572-aa16-66cd362bf6cf 39661 0 2020-06-22 06:17:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 06:17:14.146: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4092 /api/v1/namespaces/watch-4092/configmaps/e2e-watch-test-label-changed abf115f7-e850-4572-aa16-66cd362bf6cf 39662 0 2020-06-22 06:17:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun 22 06:17:14.146: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4092 /api/v1/namespaces/watch-4092/configmaps/e2e-watch-test-label-changed abf115f7-e850-4572-aa16-66cd362bf6cf 39663 0 2020-06-22 06:17:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:17:14.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4092" for this suite.

• [SLOW TEST:10.302 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":228,"skipped":3686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:17:14.170: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9278
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-hkz4p in namespace proxy-9278
I0622 06:17:14.401798      22 runners.go:189] Created replication controller with name: proxy-service-hkz4p, namespace: proxy-9278, replica count: 1
I0622 06:17:15.452273      22 runners.go:189] proxy-service-hkz4p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 06:17:16.455928      22 runners.go:189] proxy-service-hkz4p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 06:17:17.456699      22 runners.go:189] proxy-service-hkz4p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 06:17:18.458591      22 runners.go:189] proxy-service-hkz4p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 06:17:19.459062      22 runners.go:189] proxy-service-hkz4p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 06:17:20.459340      22 runners.go:189] proxy-service-hkz4p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 06:17:21.462489      22 runners.go:189] proxy-service-hkz4p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 06:17:22.465311      22 runners.go:189] proxy-service-hkz4p Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 06:17:22.474: INFO: setup took 8.12494384s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 22 06:17:22.486: INFO: (0) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 10.517326ms)
Jun 22 06:17:22.486: INFO: (0) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 11.773181ms)
Jun 22 06:17:22.486: INFO: (0) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 11.052606ms)
Jun 22 06:17:22.486: INFO: (0) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 11.163269ms)
Jun 22 06:17:22.486: INFO: (0) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 10.962285ms)
Jun 22 06:17:22.486: INFO: (0) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 10.920464ms)
Jun 22 06:17:22.489: INFO: (0) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 14.401655ms)
Jun 22 06:17:22.492: INFO: (0) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 16.645002ms)
Jun 22 06:17:22.492: INFO: (0) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 16.741443ms)
Jun 22 06:17:22.493: INFO: (0) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 17.668421ms)
Jun 22 06:17:22.493: INFO: (0) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 18.047291ms)
Jun 22 06:17:22.495: INFO: (0) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 19.99121ms)
Jun 22 06:17:22.495: INFO: (0) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 19.365327ms)
Jun 22 06:17:22.500: INFO: (0) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 24.479142ms)
Jun 22 06:17:22.501: INFO: (0) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 26.029774ms)
Jun 22 06:17:22.504: INFO: (0) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 28.925583ms)
Jun 22 06:17:22.512: INFO: (1) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 7.847241ms)
Jun 22 06:17:22.513: INFO: (1) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 8.466145ms)
Jun 22 06:17:22.513: INFO: (1) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 8.73496ms)
Jun 22 06:17:22.513: INFO: (1) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 8.485034ms)
Jun 22 06:17:22.513: INFO: (1) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 8.921923ms)
Jun 22 06:17:22.514: INFO: (1) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 9.881152ms)
Jun 22 06:17:22.515: INFO: (1) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 10.405734ms)
Jun 22 06:17:22.515: INFO: (1) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 10.371192ms)
Jun 22 06:17:22.515: INFO: (1) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 10.881893ms)
Jun 22 06:17:22.519: INFO: (1) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 14.370865ms)
Jun 22 06:17:22.519: INFO: (1) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 14.472277ms)
Jun 22 06:17:22.519: INFO: (1) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 14.760192ms)
Jun 22 06:17:22.519: INFO: (1) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 15.12557ms)
Jun 22 06:17:22.520: INFO: (1) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 16.299375ms)
Jun 22 06:17:22.523: INFO: (1) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 18.238224ms)
Jun 22 06:17:22.523: INFO: (1) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 18.888068ms)
Jun 22 06:17:22.531: INFO: (2) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 7.579645ms)
Jun 22 06:17:22.531: INFO: (2) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 7.79811ms)
Jun 22 06:17:22.531: INFO: (2) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 7.33494ms)
Jun 22 06:17:22.533: INFO: (2) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 9.2369ms)
Jun 22 06:17:22.533: INFO: (2) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 10.165829ms)
Jun 22 06:17:22.535: INFO: (2) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 11.611429ms)
Jun 22 06:17:22.535: INFO: (2) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 11.584548ms)
Jun 22 06:17:22.535: INFO: (2) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 11.702771ms)
Jun 22 06:17:22.536: INFO: (2) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 12.080258ms)
Jun 22 06:17:22.536: INFO: (2) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 12.590968ms)
Jun 22 06:17:22.536: INFO: (2) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 12.225191ms)
Jun 22 06:17:22.539: INFO: (2) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 15.58263ms)
Jun 22 06:17:22.540: INFO: (2) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 16.984758ms)
Jun 22 06:17:22.540: INFO: (2) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 16.532859ms)
Jun 22 06:17:22.541: INFO: (2) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 17.237444ms)
Jun 22 06:17:22.545: INFO: (2) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 21.996771ms)
Jun 22 06:17:22.553: INFO: (3) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 7.354621ms)
Jun 22 06:17:22.563: INFO: (3) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 17.412357ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 17.98931ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 18.324757ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 18.323156ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 18.525141ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 18.239334ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 18.388327ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 18.470889ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 18.376588ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 18.581341ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 18.837507ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 18.719184ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 18.842146ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 18.610682ms)
Jun 22 06:17:22.564: INFO: (3) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 18.638983ms)
Jun 22 06:17:22.574: INFO: (4) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 9.31961ms)
Jun 22 06:17:22.574: INFO: (4) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 8.211008ms)
Jun 22 06:17:22.576: INFO: (4) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 11.169099ms)
Jun 22 06:17:22.577: INFO: (4) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 11.439964ms)
Jun 22 06:17:22.577: INFO: (4) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 12.127818ms)
Jun 22 06:17:22.577: INFO: (4) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 11.181249ms)
Jun 22 06:17:22.577: INFO: (4) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 11.161859ms)
Jun 22 06:17:22.578: INFO: (4) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 12.209181ms)
Jun 22 06:17:22.579: INFO: (4) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 13.077698ms)
Jun 22 06:17:22.579: INFO: (4) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 13.817383ms)
Jun 22 06:17:22.579: INFO: (4) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 13.671491ms)
Jun 22 06:17:22.580: INFO: (4) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 13.921115ms)
Jun 22 06:17:22.580: INFO: (4) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 14.037348ms)
Jun 22 06:17:22.580: INFO: (4) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 14.869426ms)
Jun 22 06:17:22.581: INFO: (4) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 14.668702ms)
Jun 22 06:17:22.581: INFO: (4) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 15.156981ms)
Jun 22 06:17:22.589: INFO: (5) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 7.566686ms)
Jun 22 06:17:22.589: INFO: (5) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 7.976614ms)
Jun 22 06:17:22.590: INFO: (5) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 9.005924ms)
Jun 22 06:17:22.590: INFO: (5) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 9.064246ms)
Jun 22 06:17:22.591: INFO: (5) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 9.871991ms)
Jun 22 06:17:22.592: INFO: (5) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 11.201369ms)
Jun 22 06:17:22.593: INFO: (5) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 11.537696ms)
Jun 22 06:17:22.593: INFO: (5) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 11.531996ms)
Jun 22 06:17:22.594: INFO: (5) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 12.70888ms)
Jun 22 06:17:22.601: INFO: (5) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 19.428288ms)
Jun 22 06:17:22.601: INFO: (5) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 19.858447ms)
Jun 22 06:17:22.601: INFO: (5) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 19.728535ms)
Jun 22 06:17:22.608: INFO: (5) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 26.469303ms)
Jun 22 06:17:22.608: INFO: (5) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 27.194988ms)
Jun 22 06:17:22.608: INFO: (5) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 27.436753ms)
Jun 22 06:17:22.611: INFO: (5) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 29.230399ms)
Jun 22 06:17:22.625: INFO: (6) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 14.19477ms)
Jun 22 06:17:22.625: INFO: (6) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 14.038598ms)
Jun 22 06:17:22.625: INFO: (6) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 13.695671ms)
Jun 22 06:17:22.625: INFO: (6) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 13.708591ms)
Jun 22 06:17:22.625: INFO: (6) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 13.504077ms)
Jun 22 06:17:22.625: INFO: (6) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 13.670671ms)
Jun 22 06:17:22.625: INFO: (6) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 13.703751ms)
Jun 22 06:17:22.625: INFO: (6) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 13.64213ms)
Jun 22 06:17:22.625: INFO: (6) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 13.64775ms)
Jun 22 06:17:22.626: INFO: (6) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 14.440565ms)
Jun 22 06:17:22.626: INFO: (6) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 14.573528ms)
Jun 22 06:17:22.626: INFO: (6) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 14.386175ms)
Jun 22 06:17:22.626: INFO: (6) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 14.557309ms)
Jun 22 06:17:22.627: INFO: (6) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 15.669801ms)
Jun 22 06:17:22.627: INFO: (6) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 15.774183ms)
Jun 22 06:17:22.629: INFO: (6) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 17.763714ms)
Jun 22 06:17:22.637: INFO: (7) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 7.404162ms)
Jun 22 06:17:22.637: INFO: (7) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 7.543775ms)
Jun 22 06:17:22.638: INFO: (7) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 8.637167ms)
Jun 22 06:17:22.639: INFO: (7) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 8.78358ms)
Jun 22 06:17:22.639: INFO: (7) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 9.29953ms)
Jun 22 06:17:22.640: INFO: (7) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 10.063797ms)
Jun 22 06:17:22.640: INFO: (7) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 10.24299ms)
Jun 22 06:17:22.640: INFO: (7) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 10.73522ms)
Jun 22 06:17:22.642: INFO: (7) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 12.421336ms)
Jun 22 06:17:22.642: INFO: (7) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 12.325653ms)
Jun 22 06:17:22.643: INFO: (7) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 12.632599ms)
Jun 22 06:17:22.645: INFO: (7) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 14.928516ms)
Jun 22 06:17:22.645: INFO: (7) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 15.12253ms)
Jun 22 06:17:22.645: INFO: (7) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 15.053299ms)
Jun 22 06:17:22.646: INFO: (7) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 15.831125ms)
Jun 22 06:17:22.647: INFO: (7) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 17.154711ms)
Jun 22 06:17:22.654: INFO: (8) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 6.451643ms)
Jun 22 06:17:22.655: INFO: (8) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 7.665848ms)
Jun 22 06:17:22.663: INFO: (8) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 15.674761ms)
Jun 22 06:17:22.663: INFO: (8) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 15.390726ms)
Jun 22 06:17:22.665: INFO: (8) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 17.593831ms)
Jun 22 06:17:22.665: INFO: (8) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 17.841817ms)
Jun 22 06:17:22.668: INFO: (8) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 21.273847ms)
Jun 22 06:17:22.668: INFO: (8) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 20.912229ms)
Jun 22 06:17:22.668: INFO: (8) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 21.102863ms)
Jun 22 06:17:22.668: INFO: (8) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 20.877207ms)
Jun 22 06:17:22.668: INFO: (8) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 21.038062ms)
Jun 22 06:17:22.669: INFO: (8) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 21.122883ms)
Jun 22 06:17:22.669: INFO: (8) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 21.190615ms)
Jun 22 06:17:22.672: INFO: (8) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 24.869239ms)
Jun 22 06:17:22.672: INFO: (8) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 25.269928ms)
Jun 22 06:17:22.674: INFO: (8) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 26.037693ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 17.983468ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 17.953018ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 18.204613ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 18.0382ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 18.142511ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 17.982918ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 17.974368ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 18.180142ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 18.371186ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 18.167502ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 18.385707ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 18.327885ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 18.365086ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 18.301065ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 18.273674ms)
Jun 22 06:17:22.692: INFO: (9) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 18.295264ms)
Jun 22 06:17:22.698: INFO: (10) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 6.151507ms)
Jun 22 06:17:22.702: INFO: (10) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 9.010945ms)
Jun 22 06:17:22.702: INFO: (10) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 9.006325ms)
Jun 22 06:17:22.702: INFO: (10) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 9.428923ms)
Jun 22 06:17:22.702: INFO: (10) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 8.625737ms)
Jun 22 06:17:22.702: INFO: (10) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 9.705868ms)
Jun 22 06:17:22.702: INFO: (10) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 9.648767ms)
Jun 22 06:17:22.705: INFO: (10) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 11.291232ms)
Jun 22 06:17:22.708: INFO: (10) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 14.256872ms)
Jun 22 06:17:22.708: INFO: (10) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 14.579008ms)
Jun 22 06:17:22.708: INFO: (10) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 14.351464ms)
Jun 22 06:17:22.708: INFO: (10) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 14.825434ms)
Jun 22 06:17:22.710: INFO: (10) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 16.738894ms)
Jun 22 06:17:22.712: INFO: (10) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 18.779165ms)
Jun 22 06:17:22.714: INFO: (10) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 19.99154ms)
Jun 22 06:17:22.714: INFO: (10) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 20.038411ms)
Jun 22 06:17:22.722: INFO: (11) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 7.881391ms)
Jun 22 06:17:22.722: INFO: (11) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 8.447903ms)
Jun 22 06:17:22.722: INFO: (11) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 8.700468ms)
Jun 22 06:17:22.725: INFO: (11) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 10.568157ms)
Jun 22 06:17:22.726: INFO: (11) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 11.610429ms)
Jun 22 06:17:22.726: INFO: (11) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 11.627029ms)
Jun 22 06:17:22.727: INFO: (11) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 12.16873ms)
Jun 22 06:17:22.727: INFO: (11) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 12.67223ms)
Jun 22 06:17:22.728: INFO: (11) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 12.904425ms)
Jun 22 06:17:22.728: INFO: (11) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 14.398225ms)
Jun 22 06:17:22.729: INFO: (11) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 14.097169ms)
Jun 22 06:17:22.729: INFO: (11) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 14.469667ms)
Jun 22 06:17:22.731: INFO: (11) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 16.696072ms)
Jun 22 06:17:22.731: INFO: (11) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 16.325975ms)
Jun 22 06:17:22.731: INFO: (11) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 16.291524ms)
Jun 22 06:17:22.731: INFO: (11) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 16.55426ms)
Jun 22 06:17:22.738: INFO: (12) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 6.32583ms)
Jun 22 06:17:22.739: INFO: (12) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 7.744879ms)
Jun 22 06:17:22.740: INFO: (12) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 9.068807ms)
Jun 22 06:17:22.741: INFO: (12) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 9.824602ms)
Jun 22 06:17:22.741: INFO: (12) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 10.029426ms)
Jun 22 06:17:22.741: INFO: (12) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 10.001536ms)
Jun 22 06:17:22.744: INFO: (12) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 12.368844ms)
Jun 22 06:17:22.744: INFO: (12) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 12.252982ms)
Jun 22 06:17:22.746: INFO: (12) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 14.372835ms)
Jun 22 06:17:22.746: INFO: (12) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 14.258524ms)
Jun 22 06:17:22.746: INFO: (12) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 14.421136ms)
Jun 22 06:17:22.746: INFO: (12) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 14.673012ms)
Jun 22 06:17:22.746: INFO: (12) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 14.439107ms)
Jun 22 06:17:22.746: INFO: (12) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 14.733962ms)
Jun 22 06:17:22.747: INFO: (12) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 15.555349ms)
Jun 22 06:17:22.748: INFO: (12) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 16.615201ms)
Jun 22 06:17:22.754: INFO: (13) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 6.477723ms)
Jun 22 06:17:22.754: INFO: (13) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 6.408832ms)
Jun 22 06:17:22.757: INFO: (13) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 8.551876ms)
Jun 22 06:17:22.757: INFO: (13) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 8.958913ms)
Jun 22 06:17:22.758: INFO: (13) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 10.049587ms)
Jun 22 06:17:22.758: INFO: (13) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 10.010416ms)
Jun 22 06:17:22.758: INFO: (13) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 10.317662ms)
Jun 22 06:17:22.759: INFO: (13) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 11.040947ms)
Jun 22 06:17:22.760: INFO: (13) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 11.875644ms)
Jun 22 06:17:22.760: INFO: (13) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 12.318113ms)
Jun 22 06:17:22.761: INFO: (13) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 12.327843ms)
Jun 22 06:17:22.761: INFO: (13) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 12.384904ms)
Jun 22 06:17:22.761: INFO: (13) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 13.071218ms)
Jun 22 06:17:22.761: INFO: (13) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 13.228992ms)
Jun 22 06:17:22.762: INFO: (13) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 14.180671ms)
Jun 22 06:17:22.766: INFO: (13) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 18.255955ms)
Jun 22 06:17:22.780: INFO: (14) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 13.431086ms)
Jun 22 06:17:22.780: INFO: (14) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 13.323274ms)
Jun 22 06:17:22.780: INFO: (14) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 13.405885ms)
Jun 22 06:17:22.780: INFO: (14) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 13.557968ms)
Jun 22 06:17:22.780: INFO: (14) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 13.67923ms)
Jun 22 06:17:22.780: INFO: (14) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 13.884975ms)
Jun 22 06:17:22.780: INFO: (14) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 13.377505ms)
Jun 22 06:17:22.781: INFO: (14) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 14.684391ms)
Jun 22 06:17:22.781: INFO: (14) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 14.847224ms)
Jun 22 06:17:22.781: INFO: (14) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 14.447136ms)
Jun 22 06:17:22.782: INFO: (14) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 14.982857ms)
Jun 22 06:17:22.782: INFO: (14) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 14.807173ms)
Jun 22 06:17:22.782: INFO: (14) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 15.54886ms)
Jun 22 06:17:22.782: INFO: (14) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 15.685712ms)
Jun 22 06:17:22.783: INFO: (14) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 16.446227ms)
Jun 22 06:17:22.784: INFO: (14) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 17.172512ms)
Jun 22 06:17:22.792: INFO: (15) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 8.078645ms)
Jun 22 06:17:22.792: INFO: (15) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 8.463163ms)
Jun 22 06:17:22.792: INFO: (15) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 8.350311ms)
Jun 22 06:17:22.792: INFO: (15) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 8.549966ms)
Jun 22 06:17:22.792: INFO: (15) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 8.519035ms)
Jun 22 06:17:22.792: INFO: (15) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 8.414102ms)
Jun 22 06:17:22.794: INFO: (15) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 9.637148ms)
Jun 22 06:17:22.794: INFO: (15) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 9.7327ms)
Jun 22 06:17:22.794: INFO: (15) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 9.73947ms)
Jun 22 06:17:22.794: INFO: (15) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 10.453315ms)
Jun 22 06:17:22.796: INFO: (15) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 12.469806ms)
Jun 22 06:17:22.799: INFO: (15) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 14.549198ms)
Jun 22 06:17:22.799: INFO: (15) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 14.860964ms)
Jun 22 06:17:22.799: INFO: (15) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 14.728982ms)
Jun 22 06:17:22.800: INFO: (15) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 15.822234ms)
Jun 22 06:17:22.801: INFO: (15) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 17.472008ms)
Jun 22 06:17:22.809: INFO: (16) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 7.409223ms)
Jun 22 06:17:22.809: INFO: (16) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 7.193288ms)
Jun 22 06:17:22.810: INFO: (16) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 7.858311ms)
Jun 22 06:17:22.811: INFO: (16) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 8.857322ms)
Jun 22 06:17:22.811: INFO: (16) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 9.310342ms)
Jun 22 06:17:22.811: INFO: (16) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 9.583887ms)
Jun 22 06:17:22.811: INFO: (16) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 9.611217ms)
Jun 22 06:17:22.812: INFO: (16) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 10.174029ms)
Jun 22 06:17:22.812: INFO: (16) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 9.924233ms)
Jun 22 06:17:22.813: INFO: (16) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 11.454486ms)
Jun 22 06:17:22.813: INFO: (16) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 11.600739ms)
Jun 22 06:17:22.814: INFO: (16) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 12.14585ms)
Jun 22 06:17:22.817: INFO: (16) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 15.242553ms)
Jun 22 06:17:22.818: INFO: (16) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 15.790394ms)
Jun 22 06:17:22.818: INFO: (16) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 15.853236ms)
Jun 22 06:17:22.819: INFO: (16) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 17.156892ms)
Jun 22 06:17:22.826: INFO: (17) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 6.773489ms)
Jun 22 06:17:22.826: INFO: (17) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 7.133336ms)
Jun 22 06:17:22.826: INFO: (17) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 6.83441ms)
Jun 22 06:17:22.826: INFO: (17) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 7.083014ms)
Jun 22 06:17:22.828: INFO: (17) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 8.748869ms)
Jun 22 06:17:22.828: INFO: (17) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 8.684027ms)
Jun 22 06:17:22.830: INFO: (17) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 11.1808ms)
Jun 22 06:17:22.830: INFO: (17) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 11.142899ms)
Jun 22 06:17:22.830: INFO: (17) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 11.131498ms)
Jun 22 06:17:22.832: INFO: (17) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 13.000356ms)
Jun 22 06:17:22.834: INFO: (17) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 14.393385ms)
Jun 22 06:17:22.834: INFO: (17) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 14.437596ms)
Jun 22 06:17:22.834: INFO: (17) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 14.490157ms)
Jun 22 06:17:22.834: INFO: (17) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 15.233132ms)
Jun 22 06:17:22.836: INFO: (17) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 16.270504ms)
Jun 22 06:17:22.836: INFO: (17) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 16.55959ms)
Jun 22 06:17:22.842: INFO: (18) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 5.35411ms)
Jun 22 06:17:22.843: INFO: (18) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 7.035393ms)
Jun 22 06:17:22.843: INFO: (18) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 7.423982ms)
Jun 22 06:17:22.844: INFO: (18) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 6.224628ms)
Jun 22 06:17:22.844: INFO: (18) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 6.995874ms)
Jun 22 06:17:22.845: INFO: (18) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 6.619626ms)
Jun 22 06:17:22.846: INFO: (18) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 8.552275ms)
Jun 22 06:17:22.846: INFO: (18) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 9.147857ms)
Jun 22 06:17:22.848: INFO: (18) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 11.045366ms)
Jun 22 06:17:22.849: INFO: (18) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 10.296201ms)
Jun 22 06:17:22.849: INFO: (18) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 10.502865ms)
Jun 22 06:17:22.850: INFO: (18) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 12.476186ms)
Jun 22 06:17:22.852: INFO: (18) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 12.765041ms)
Jun 22 06:17:22.852: INFO: (18) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 13.283882ms)
Jun 22 06:17:22.852: INFO: (18) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 13.466405ms)
Jun 22 06:17:22.853: INFO: (18) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 15.002109ms)
Jun 22 06:17:22.860: INFO: (19) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:443/proxy/tlsrewritem... (200; 6.810899ms)
Jun 22 06:17:22.861: INFO: (19) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 7.798249ms)
Jun 22 06:17:22.861: INFO: (19) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">test<... (200; 7.981843ms)
Jun 22 06:17:22.863: INFO: (19) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 9.952904ms)
Jun 22 06:17:22.863: INFO: (19) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:162/proxy/: bar (200; 9.868552ms)
Jun 22 06:17:22.863: INFO: (19) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g:160/proxy/: foo (200; 9.933453ms)
Jun 22 06:17:22.863: INFO: (19) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:460/proxy/: tls baz (200; 10.024125ms)
Jun 22 06:17:22.863: INFO: (19) /api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/proxy-service-hkz4p-zkn8g/proxy/rewriteme">test</a> (200; 9.883602ms)
Jun 22 06:17:22.864: INFO: (19) /api/v1/namespaces/proxy-9278/pods/https:proxy-service-hkz4p-zkn8g:462/proxy/: tls qux (200; 11.003035ms)
Jun 22 06:17:22.868: INFO: (19) /api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9278/pods/http:proxy-service-hkz4p-zkn8g:1080/proxy/rewriteme">... (200; 15.547618ms)
Jun 22 06:17:22.872: INFO: (19) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname1/proxy/: foo (200; 19.390688ms)
Jun 22 06:17:22.874: INFO: (19) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname1/proxy/: tls baz (200; 21.725965ms)
Jun 22 06:17:22.876: INFO: (19) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname2/proxy/: bar (200; 22.911559ms)
Jun 22 06:17:22.876: INFO: (19) /api/v1/namespaces/proxy-9278/services/proxy-service-hkz4p:portname2/proxy/: bar (200; 22.94169ms)
Jun 22 06:17:22.876: INFO: (19) /api/v1/namespaces/proxy-9278/services/https:proxy-service-hkz4p:tlsportname2/proxy/: tls qux (200; 23.106953ms)
Jun 22 06:17:22.879: INFO: (19) /api/v1/namespaces/proxy-9278/services/http:proxy-service-hkz4p:portname1/proxy/: foo (200; 25.986222ms)
STEP: deleting ReplicationController proxy-service-hkz4p in namespace proxy-9278, will wait for the garbage collector to delete the pods
Jun 22 06:17:22.950: INFO: Deleting ReplicationController proxy-service-hkz4p took: 17.195162ms
Jun 22 06:17:23.352: INFO: Terminating ReplicationController proxy-service-hkz4p pods took: 401.350799ms
[AfterEach] version v1
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:17:33.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9278" for this suite.

• [SLOW TEST:19.399 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":229,"skipped":3715,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:17:33.569: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1489
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 06:17:33.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5597'
Jun 22 06:17:34.008: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 06:17:34.008: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1495
Jun 22 06:17:36.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete deployment e2e-test-httpd-deployment --namespace=kubectl-5597'
Jun 22 06:17:36.157: INFO: stderr: ""
Jun 22 06:17:36.157: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:17:36.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5597" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":230,"skipped":3730,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:17:36.178: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2918
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-02f1eedb-f787-40cb-ac6e-d748a09e481f
STEP: Creating a pod to test consume configMaps
Jun 22 06:17:36.370: INFO: Waiting up to 5m0s for pod "pod-configmaps-47a79496-6b6d-4fd8-a93d-21ad8ca7ce4f" in namespace "configmap-2918" to be "success or failure"
Jun 22 06:17:36.374: INFO: Pod "pod-configmaps-47a79496-6b6d-4fd8-a93d-21ad8ca7ce4f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.789028ms
Jun 22 06:17:38.379: INFO: Pod "pod-configmaps-47a79496-6b6d-4fd8-a93d-21ad8ca7ce4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009026549s
STEP: Saw pod success
Jun 22 06:17:38.379: INFO: Pod "pod-configmaps-47a79496-6b6d-4fd8-a93d-21ad8ca7ce4f" satisfied condition "success or failure"
Jun 22 06:17:38.384: INFO: Trying to get logs from node node4 pod pod-configmaps-47a79496-6b6d-4fd8-a93d-21ad8ca7ce4f container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 06:17:38.430: INFO: Waiting for pod pod-configmaps-47a79496-6b6d-4fd8-a93d-21ad8ca7ce4f to disappear
Jun 22 06:17:38.436: INFO: Pod pod-configmaps-47a79496-6b6d-4fd8-a93d-21ad8ca7ce4f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:17:38.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2918" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":231,"skipped":3733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:17:38.456: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-8670/configmap-test-1fd76d5a-f688-47b0-b8e1-a60a66e4b637
STEP: Creating a pod to test consume configMaps
Jun 22 06:17:38.660: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a586bbd-c045-4d9d-902b-3f7404be30aa" in namespace "configmap-8670" to be "success or failure"
Jun 22 06:17:38.665: INFO: Pod "pod-configmaps-7a586bbd-c045-4d9d-902b-3f7404be30aa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.248828ms
Jun 22 06:17:40.674: INFO: Pod "pod-configmaps-7a586bbd-c045-4d9d-902b-3f7404be30aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014567443s
STEP: Saw pod success
Jun 22 06:17:40.674: INFO: Pod "pod-configmaps-7a586bbd-c045-4d9d-902b-3f7404be30aa" satisfied condition "success or failure"
Jun 22 06:17:40.681: INFO: Trying to get logs from node node4 pod pod-configmaps-7a586bbd-c045-4d9d-902b-3f7404be30aa container env-test: <nil>
STEP: delete the pod
Jun 22 06:17:40.719: INFO: Waiting for pod pod-configmaps-7a586bbd-c045-4d9d-902b-3f7404be30aa to disappear
Jun 22 06:17:40.724: INFO: Pod pod-configmaps-7a586bbd-c045-4d9d-902b-3f7404be30aa no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:17:40.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8670" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":232,"skipped":3756,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:17:40.750: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-45a4ea5b-6fca-4c0f-8c2f-e83568972cdc
STEP: Creating a pod to test consume secrets
Jun 22 06:17:40.942: INFO: Waiting up to 5m0s for pod "pod-secrets-7ed968f9-b3fd-46ab-8f1c-b106db09cf2f" in namespace "secrets-390" to be "success or failure"
Jun 22 06:17:40.948: INFO: Pod "pod-secrets-7ed968f9-b3fd-46ab-8f1c-b106db09cf2f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.835479ms
Jun 22 06:17:42.953: INFO: Pod "pod-secrets-7ed968f9-b3fd-46ab-8f1c-b106db09cf2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01103993s
STEP: Saw pod success
Jun 22 06:17:42.953: INFO: Pod "pod-secrets-7ed968f9-b3fd-46ab-8f1c-b106db09cf2f" satisfied condition "success or failure"
Jun 22 06:17:42.969: INFO: Trying to get logs from node node4 pod pod-secrets-7ed968f9-b3fd-46ab-8f1c-b106db09cf2f container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 06:17:43.030: INFO: Waiting for pod pod-secrets-7ed968f9-b3fd-46ab-8f1c-b106db09cf2f to disappear
Jun 22 06:17:43.042: INFO: Pod pod-secrets-7ed968f9-b3fd-46ab-8f1c-b106db09cf2f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:17:43.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-390" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":233,"skipped":3756,"failed":0}
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:17:43.061: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6801
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 22 06:17:47.325: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:17:47.337: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 06:17:49.337: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:17:49.346: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 06:17:51.337: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:17:51.344: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 06:17:53.337: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:17:53.346: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 06:17:55.337: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:17:55.344: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:17:55.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6801" for this suite.

• [SLOW TEST:12.299 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":234,"skipped":3758,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:17:55.361: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:17:55.545: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95605191-3aff-40e3-9f30-eab7a3c8e7b7" in namespace "projected-8867" to be "success or failure"
Jun 22 06:17:55.554: INFO: Pod "downwardapi-volume-95605191-3aff-40e3-9f30-eab7a3c8e7b7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.654997ms
Jun 22 06:17:57.559: INFO: Pod "downwardapi-volume-95605191-3aff-40e3-9f30-eab7a3c8e7b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014235117s
STEP: Saw pod success
Jun 22 06:17:57.559: INFO: Pod "downwardapi-volume-95605191-3aff-40e3-9f30-eab7a3c8e7b7" satisfied condition "success or failure"
Jun 22 06:17:57.566: INFO: Trying to get logs from node node4 pod downwardapi-volume-95605191-3aff-40e3-9f30-eab7a3c8e7b7 container client-container: <nil>
STEP: delete the pod
Jun 22 06:17:57.603: INFO: Waiting for pod downwardapi-volume-95605191-3aff-40e3-9f30-eab7a3c8e7b7 to disappear
Jun 22 06:17:57.607: INFO: Pod downwardapi-volume-95605191-3aff-40e3-9f30-eab7a3c8e7b7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:17:57.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8867" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":235,"skipped":3771,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:17:57.629: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-704
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-f26508d8-054d-48bf-ad4a-c97a35a53e0f
STEP: Creating a pod to test consume secrets
Jun 22 06:17:57.825: INFO: Waiting up to 5m0s for pod "pod-secrets-2bd9e5c7-7d9d-48af-81d1-080a213a7066" in namespace "secrets-704" to be "success or failure"
Jun 22 06:17:57.831: INFO: Pod "pod-secrets-2bd9e5c7-7d9d-48af-81d1-080a213a7066": Phase="Pending", Reason="", readiness=false. Elapsed: 5.593845ms
Jun 22 06:17:59.839: INFO: Pod "pod-secrets-2bd9e5c7-7d9d-48af-81d1-080a213a7066": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013517012s
STEP: Saw pod success
Jun 22 06:17:59.839: INFO: Pod "pod-secrets-2bd9e5c7-7d9d-48af-81d1-080a213a7066" satisfied condition "success or failure"
Jun 22 06:17:59.846: INFO: Trying to get logs from node node4 pod pod-secrets-2bd9e5c7-7d9d-48af-81d1-080a213a7066 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 06:17:59.883: INFO: Waiting for pod pod-secrets-2bd9e5c7-7d9d-48af-81d1-080a213a7066 to disappear
Jun 22 06:17:59.889: INFO: Pod pod-secrets-2bd9e5c7-7d9d-48af-81d1-080a213a7066 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:17:59.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-704" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":236,"skipped":3784,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:17:59.905: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9922
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1681
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 06:18:00.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9922'
Jun 22 06:18:00.227: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 06:18:00.228: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
Jun 22 06:18:00.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete jobs e2e-test-httpd-job --namespace=kubectl-9922'
Jun 22 06:18:00.393: INFO: stderr: ""
Jun 22 06:18:00.393: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:18:00.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9922" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":280,"completed":237,"skipped":3799,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:18:00.414: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-5541
STEP: creating replication controller nodeport-test in namespace services-5541
I0622 06:18:00.639710      22 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-5541, replica count: 2
Jun 22 06:18:03.691: INFO: Creating new exec pod
I0622 06:18:03.691930      22 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 06:18:08.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=services-5541 execpodrs977 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jun 22 06:18:09.006: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 22 06:18:09.006: INFO: stdout: ""
Jun 22 06:18:09.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=services-5541 execpodrs977 -- /bin/sh -x -c nc -zv -t -w 2 10.97.32.179 80'
Jun 22 06:18:09.312: INFO: stderr: "+ nc -zv -t -w 2 10.97.32.179 80\nConnection to 10.97.32.179 80 port [tcp/http] succeeded!\n"
Jun 22 06:18:09.313: INFO: stdout: ""
Jun 22 06:18:09.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=services-5541 execpodrs977 -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.141 31429'
Jun 22 06:18:09.592: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.141 31429\nConnection to 192.168.1.141 31429 port [tcp/31429] succeeded!\n"
Jun 22 06:18:09.593: INFO: stdout: ""
Jun 22 06:18:09.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=services-5541 execpodrs977 -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.144 31429'
Jun 22 06:18:09.874: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.144 31429\nConnection to 192.168.1.144 31429 port [tcp/31429] succeeded!\n"
Jun 22 06:18:09.875: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:18:09.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5541" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:9.484 seconds]
[sig-network] Services
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":238,"skipped":3804,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:18:09.898: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-ee1cefb7-2d8d-40bb-acb3-683bf8359562
STEP: Creating a pod to test consume configMaps
Jun 22 06:18:10.102: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d23a496-98bb-4e4d-86e0-23a11021576c" in namespace "configmap-5707" to be "success or failure"
Jun 22 06:18:10.108: INFO: Pod "pod-configmaps-9d23a496-98bb-4e4d-86e0-23a11021576c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.489393ms
Jun 22 06:18:12.113: INFO: Pod "pod-configmaps-9d23a496-98bb-4e4d-86e0-23a11021576c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010337617s
STEP: Saw pod success
Jun 22 06:18:12.113: INFO: Pod "pod-configmaps-9d23a496-98bb-4e4d-86e0-23a11021576c" satisfied condition "success or failure"
Jun 22 06:18:12.118: INFO: Trying to get logs from node node1 pod pod-configmaps-9d23a496-98bb-4e4d-86e0-23a11021576c container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 06:18:12.170: INFO: Waiting for pod pod-configmaps-9d23a496-98bb-4e4d-86e0-23a11021576c to disappear
Jun 22 06:18:12.175: INFO: Pod pod-configmaps-9d23a496-98bb-4e4d-86e0-23a11021576c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:18:12.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5707" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":239,"skipped":3812,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:18:12.199: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-583
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jun 22 06:18:12.398: INFO: Found 0 stateful pods, waiting for 3
Jun 22 06:18:22.405: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 06:18:22.405: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 06:18:22.405: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 22 06:18:22.451: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 22 06:18:32.512: INFO: Updating stateful set ss2
Jun 22 06:18:32.525: INFO: Waiting for Pod statefulset-583/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jun 22 06:18:42.645: INFO: Found 1 stateful pods, waiting for 3
Jun 22 06:18:52.654: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 06:18:52.654: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 06:18:52.654: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 22 06:18:52.690: INFO: Updating stateful set ss2
Jun 22 06:18:52.702: INFO: Waiting for Pod statefulset-583/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 06:19:02.719: INFO: Waiting for Pod statefulset-583/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 06:19:12.740: INFO: Updating stateful set ss2
Jun 22 06:19:12.752: INFO: Waiting for StatefulSet statefulset-583/ss2 to complete update
Jun 22 06:19:12.752: INFO: Waiting for Pod statefulset-583/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 06:19:22.765: INFO: Waiting for StatefulSet statefulset-583/ss2 to complete update
Jun 22 06:19:32.768: INFO: Waiting for StatefulSet statefulset-583/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun 22 06:19:42.767: INFO: Deleting all statefulset in ns statefulset-583
Jun 22 06:19:42.773: INFO: Scaling statefulset ss2 to 0
Jun 22 06:20:12.800: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 06:20:12.807: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:20:12.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-583" for this suite.

• [SLOW TEST:120.648 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":240,"skipped":3847,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:20:12.848: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9565
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 22 06:20:13.032: INFO: Waiting up to 5m0s for pod "pod-065bcfae-9fba-434b-aae2-1821971dcda7" in namespace "emptydir-9565" to be "success or failure"
Jun 22 06:20:13.040: INFO: Pod "pod-065bcfae-9fba-434b-aae2-1821971dcda7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.269629ms
Jun 22 06:20:15.047: INFO: Pod "pod-065bcfae-9fba-434b-aae2-1821971dcda7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014114854s
STEP: Saw pod success
Jun 22 06:20:15.047: INFO: Pod "pod-065bcfae-9fba-434b-aae2-1821971dcda7" satisfied condition "success or failure"
Jun 22 06:20:15.052: INFO: Trying to get logs from node node4 pod pod-065bcfae-9fba-434b-aae2-1821971dcda7 container test-container: <nil>
STEP: delete the pod
Jun 22 06:20:15.099: INFO: Waiting for pod pod-065bcfae-9fba-434b-aae2-1821971dcda7 to disappear
Jun 22 06:20:15.105: INFO: Pod pod-065bcfae-9fba-434b-aae2-1821971dcda7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:20:15.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9565" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":241,"skipped":3853,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:20:15.120: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7673
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:20:15.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7673" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":242,"skipped":3875,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:20:15.330: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 22 06:20:19.572: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 06:20:19.577: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 06:20:21.577: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 06:20:21.587: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 06:20:23.577: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 06:20:23.582: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 06:20:25.580: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 06:20:25.586: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 06:20:27.577: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 06:20:27.586: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 06:20:29.578: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 06:20:29.586: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 06:20:31.578: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 06:20:31.584: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 06:20:33.577: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 06:20:33.587: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:20:33.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-236" for this suite.

• [SLOW TEST:18.292 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":243,"skipped":3883,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:20:33.623: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2951
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1525
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 06:20:33.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2951'
Jun 22 06:20:33.921: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 06:20:33.921: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jun 22 06:20:33.935: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-8k69t]
Jun 22 06:20:33.935: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-8k69t" in namespace "kubectl-2951" to be "running and ready"
Jun 22 06:20:33.945: INFO: Pod "e2e-test-httpd-rc-8k69t": Phase="Pending", Reason="", readiness=false. Elapsed: 9.124597ms
Jun 22 06:20:35.949: INFO: Pod "e2e-test-httpd-rc-8k69t": Phase="Running", Reason="", readiness=true. Elapsed: 2.013091453s
Jun 22 06:20:35.949: INFO: Pod "e2e-test-httpd-rc-8k69t" satisfied condition "running and ready"
Jun 22 06:20:35.949: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-8k69t]
Jun 22 06:20:35.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 logs rc/e2e-test-httpd-rc --namespace=kubectl-2951'
Jun 22 06:20:36.109: INFO: stderr: ""
Jun 22 06:20:36.109: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.41.130.139. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.41.130.139. Set the 'ServerName' directive globally to suppress this message\n[Mon Jun 22 06:20:35.119695 2020] [mpm_event:notice] [pid 1:tid 139814896200552] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Jun 22 06:20:35.119739 2020] [core:notice] [pid 1:tid 139814896200552] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1530
Jun 22 06:20:36.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete rc e2e-test-httpd-rc --namespace=kubectl-2951'
Jun 22 06:20:36.247: INFO: stderr: ""
Jun 22 06:20:36.247: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:20:36.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2951" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":280,"completed":244,"skipped":3888,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:20:36.268: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 06:20:37.587: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 06:20:39.607: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403637, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403637, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403637, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403637, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 06:20:42.639: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:20:42.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6622" for this suite.
STEP: Destroying namespace "webhook-6622-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.583 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":245,"skipped":3894,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:20:42.851: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9820
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:20:43.047: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 22 06:20:51.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-9820 create -f -'
Jun 22 06:20:51.841: INFO: stderr: ""
Jun 22 06:20:51.841: INFO: stdout: "e2e-test-crd-publish-openapi-9133-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 22 06:20:51.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-9820 delete e2e-test-crd-publish-openapi-9133-crds test-cr'
Jun 22 06:20:52.011: INFO: stderr: ""
Jun 22 06:20:52.011: INFO: stdout: "e2e-test-crd-publish-openapi-9133-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 22 06:20:52.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-9820 apply -f -'
Jun 22 06:20:52.396: INFO: stderr: ""
Jun 22 06:20:52.396: INFO: stdout: "e2e-test-crd-publish-openapi-9133-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 22 06:20:52.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-9820 delete e2e-test-crd-publish-openapi-9133-crds test-cr'
Jun 22 06:20:52.518: INFO: stderr: ""
Jun 22 06:20:52.518: INFO: stdout: "e2e-test-crd-publish-openapi-9133-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun 22 06:20:52.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 explain e2e-test-crd-publish-openapi-9133-crds'
Jun 22 06:20:52.761: INFO: stderr: ""
Jun 22 06:20:52.761: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9133-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:20:56.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9820" for this suite.

• [SLOW TEST:13.674 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":246,"skipped":3895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:20:56.526: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:20:56.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-5157'
Jun 22 06:20:57.067: INFO: stderr: ""
Jun 22 06:20:57.067: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Jun 22 06:20:57.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-5157'
Jun 22 06:20:57.448: INFO: stderr: ""
Jun 22 06:20:57.448: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jun 22 06:20:58.456: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 06:20:58.456: INFO: Found 0 / 1
Jun 22 06:20:59.458: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 06:20:59.458: INFO: Found 1 / 1
Jun 22 06:20:59.458: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 06:20:59.463: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 06:20:59.463: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 06:20:59.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 describe pod agnhost-master-bszjw --namespace=kubectl-5157'
Jun 22 06:20:59.620: INFO: stderr: ""
Jun 22 06:20:59.620: INFO: stdout: "Name:         agnhost-master-bszjw\nNamespace:    kubectl-5157\nPriority:     0\nNode:         node4/192.168.1.144\nStart Time:   Mon, 22 Jun 2020 06:20:57 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 10.41.130.143/32\n              cni.projectcalico.org/podIPs: 10.41.130.143/32\nStatus:       Running\nIP:           10.41.130.143\nIPs:\n  IP:           10.41.130.143\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://b9105f592e00c5aa3baf6fdcf440821b9200b174d83cbf94c687ba2425a68d01\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 22 Jun 2020 06:20:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-z6mr2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-z6mr2:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-z6mr2\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 30s\n                 node.kubernetes.io/unreachable:NoExecute for 30s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5157/agnhost-master-bszjw to node4\n  Normal  Pulled     1s    kubelet, node4     Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    1s    kubelet, node4     Created container agnhost-master\n  Normal  Started    1s    kubelet, node4     Started container agnhost-master\n"
Jun 22 06:20:59.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 describe rc agnhost-master --namespace=kubectl-5157'
Jun 22 06:20:59.772: INFO: stderr: ""
Jun 22 06:20:59.772: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-5157\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-master-bszjw\n"
Jun 22 06:20:59.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 describe service agnhost-master --namespace=kubectl-5157'
Jun 22 06:20:59.916: INFO: stderr: ""
Jun 22 06:20:59.916: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-5157\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.105.15.29\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.41.130.143:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 22 06:20:59.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 describe node node1'
Jun 22 06:21:00.097: INFO: stderr: ""
Jun 22 06:21:00.097: INFO: stdout: "Name:               node1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cube.acornsoft.io/clusterid=lab01-cluster\n                    cube.acornsoft.io/role=master\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=node1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.1.141/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.37.149.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 22 Jun 2020 04:47:40 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  node1\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 22 Jun 2020 06:20:50 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 22 Jun 2020 04:49:12 +0000   Mon, 22 Jun 2020 04:49:12 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 22 Jun 2020 06:20:56 +0000   Mon, 22 Jun 2020 04:48:53 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 22 Jun 2020 06:20:56 +0000   Mon, 22 Jun 2020 04:48:53 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 22 Jun 2020 06:20:56 +0000   Mon, 22 Jun 2020 04:48:53 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 22 Jun 2020 06:20:56 +0000   Mon, 22 Jun 2020 04:48:53 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.1.141\n  Hostname:    node1\nCapacity:\n  cpu:                6\n  ephemeral-storage:  41921540Ki\n  hugepages-2Mi:      0\n  memory:             5944036Ki\n  pods:               110\nAllocatable:\n  cpu:                6\n  ephemeral-storage:  38634891201\n  hugepages-2Mi:      0\n  memory:             5841636Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 a436c49671e74235a4fb7df130bca9a0\n  System UUID:                A436C496-71E7-4235-A4FB-7DF130BCA9A0\n  Boot ID:                    f815db65-576b-4283-834e-28dfef788301\n  Kernel Version:             3.10.0-1127.10.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.8\n  Kubelet Version:            v1.17.7\n  Kube-Proxy Version:         v1.17.7\nPodCIDR:                      10.32.0.0/24\nPodCIDRs:                     10.32.0.0/24\nNon-terminated Pods:          (15 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  cocktail-system             cocktail-alarm-collector-6cfd6bbb9c-kl9zt                  100m (1%)     200m (3%)   100Mi (1%)       200Mi (3%)     90m\n  cocktail-system             cocktail-api-cmdb-0                                        500m (8%)     1 (16%)     1Gi (17%)        2Gi (35%)      90m\n  cocktail-system             cocktail-batch-server-75c78755c5-c94nd                     100m (1%)     100m (1%)   100Mi (1%)       100Mi (1%)     90m\n  cocktail-system             cocktail-cluster-api-69794f5b78-9htrk                      100m (1%)     1 (16%)     100Mi (1%)       1Gi (17%)      90m\n  cocktail-system             cocktail-monitoring-76bc9fcf6f-r9tw9                       150m (2%)     200m (3%)   150Mi (2%)       200Mi (3%)     90m\n  kube-system                 calico-kube-controllers-77c4b7448-pvlhp                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m\n  kube-system                 calico-node-qkngx                                          250m (4%)     0 (0%)      0 (0%)           0 (0%)         92m\n  kube-system                 coredns-ddbf59985-7mcct                                    100m (1%)     0 (0%)      70Mi (1%)        170Mi (2%)     93m\n  kube-system                 haproxy-node1                                              25m (0%)      0 (0%)      32M (0%)         0 (0%)         92m\n  kube-system                 kube-apiserver-node1                                       250m (4%)     0 (0%)      0 (0%)           0 (0%)         92m\n  kube-system                 kube-controller-manager-node1                              200m (3%)     0 (0%)      0 (0%)           0 (0%)         92m\n  kube-system                 kube-proxy-qbghn                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m\n  kube-system                 kube-scheduler-node1                                       100m (1%)     0 (0%)      0 (0%)           0 (0%)         92m\n  kube-system                 metrics-server-76b7895b66-f4lch                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-kwffd    0 (0%)        0 (0%)      0 (0%)           0 (0%)         71m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1875m (31%)      2500m (41%)\n  memory             1612306Ki (27%)  3742Mi (65%)\n  ephemeral-storage  0 (0%)           0 (0%)\nEvents:              <none>\n"
Jun 22 06:21:00.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 describe namespace kubectl-5157'
Jun 22 06:21:00.242: INFO: stderr: ""
Jun 22 06:21:00.242: INFO: stdout: "Name:         kubectl-5157\nLabels:       e2e-framework=kubectl\n              e2e-run=7996dc72-da4e-477c-96dc-85212d05a2ff\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:21:00.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5157" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":247,"skipped":3924,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:21:00.263: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-5681
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Jun 22 06:21:00.457: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5681" to be "success or failure"
Jun 22 06:21:00.467: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 10.76635ms
Jun 22 06:21:02.474: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017081204s
Jun 22 06:21:04.480: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023238985s
STEP: Saw pod success
Jun 22 06:21:04.480: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun 22 06:21:04.484: INFO: Trying to get logs from node node4 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun 22 06:21:04.524: INFO: Waiting for pod pod-host-path-test to disappear
Jun 22 06:21:04.534: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:21:04.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5681" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":248,"skipped":3936,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:21:04.560: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7248
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1626
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 06:21:04.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-7248'
Jun 22 06:21:04.918: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 06:21:04.918: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1631
Jun 22 06:21:06.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7248'
Jun 22 06:21:07.088: INFO: stderr: ""
Jun 22 06:21:07.088: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:21:07.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7248" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":280,"completed":249,"skipped":3941,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:21:07.105: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3067
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-3067
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jun 22 06:21:07.313: INFO: Found 0 stateful pods, waiting for 3
Jun 22 06:21:17.322: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 06:21:17.322: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 06:21:17.322: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 06:21:17.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-3067 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 06:21:17.634: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 06:21:17.635: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 06:21:17.635: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 22 06:21:27.687: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 22 06:21:37.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-3067 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:21:38.016: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 06:21:38.016: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 06:21:38.016: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 06:21:58.053: INFO: Waiting for StatefulSet statefulset-3067/ss2 to complete update
Jun 22 06:21:58.053: INFO: Waiting for Pod statefulset-3067/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Jun 22 06:22:08.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-3067 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 06:22:08.368: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 06:22:08.368: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 06:22:08.368: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 06:22:18.424: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 22 06:22:28.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 exec --namespace=statefulset-3067 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 06:22:28.752: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 06:22:28.752: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 06:22:28.752: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 06:22:48.789: INFO: Waiting for StatefulSet statefulset-3067/ss2 to complete update
Jun 22 06:22:48.789: INFO: Waiting for Pod statefulset-3067/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun 22 06:22:58.804: INFO: Deleting all statefulset in ns statefulset-3067
Jun 22 06:22:58.809: INFO: Scaling statefulset ss2 to 0
Jun 22 06:23:08.835: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 06:23:08.841: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:23:08.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3067" for this suite.

• [SLOW TEST:121.791 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":250,"skipped":3943,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:23:08.896: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-4632/secret-test-1ed4fe40-fae7-40c2-8cb5-6686c547dcaf
STEP: Creating a pod to test consume secrets
Jun 22 06:23:09.086: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a12775b-9010-400f-8b9f-87115e1ddaee" in namespace "secrets-4632" to be "success or failure"
Jun 22 06:23:09.092: INFO: Pod "pod-configmaps-5a12775b-9010-400f-8b9f-87115e1ddaee": Phase="Pending", Reason="", readiness=false. Elapsed: 5.896281ms
Jun 22 06:23:11.098: INFO: Pod "pod-configmaps-5a12775b-9010-400f-8b9f-87115e1ddaee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011550581s
STEP: Saw pod success
Jun 22 06:23:11.098: INFO: Pod "pod-configmaps-5a12775b-9010-400f-8b9f-87115e1ddaee" satisfied condition "success or failure"
Jun 22 06:23:11.103: INFO: Trying to get logs from node node4 pod pod-configmaps-5a12775b-9010-400f-8b9f-87115e1ddaee container env-test: <nil>
STEP: delete the pod
Jun 22 06:23:11.167: INFO: Waiting for pod pod-configmaps-5a12775b-9010-400f-8b9f-87115e1ddaee to disappear
Jun 22 06:23:11.173: INFO: Pod pod-configmaps-5a12775b-9010-400f-8b9f-87115e1ddaee no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:23:11.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4632" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":251,"skipped":3948,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:23:11.190: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7988
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 22 06:23:11.381: INFO: Waiting up to 5m0s for pod "pod-ef8f8b75-ee5d-4c55-afb7-bf95c3a5c56c" in namespace "emptydir-7988" to be "success or failure"
Jun 22 06:23:11.386: INFO: Pod "pod-ef8f8b75-ee5d-4c55-afb7-bf95c3a5c56c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.431101ms
Jun 22 06:23:13.393: INFO: Pod "pod-ef8f8b75-ee5d-4c55-afb7-bf95c3a5c56c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011086152s
STEP: Saw pod success
Jun 22 06:23:13.393: INFO: Pod "pod-ef8f8b75-ee5d-4c55-afb7-bf95c3a5c56c" satisfied condition "success or failure"
Jun 22 06:23:13.399: INFO: Trying to get logs from node node4 pod pod-ef8f8b75-ee5d-4c55-afb7-bf95c3a5c56c container test-container: <nil>
STEP: delete the pod
Jun 22 06:23:13.431: INFO: Waiting for pod pod-ef8f8b75-ee5d-4c55-afb7-bf95c3a5c56c to disappear
Jun 22 06:23:13.443: INFO: Pod pod-ef8f8b75-ee5d-4c55-afb7-bf95c3a5c56c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:23:13.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7988" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":252,"skipped":3953,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:23:13.467: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 06:23:13.713: INFO: Number of nodes with available pods: 0
Jun 22 06:23:13.714: INFO: Node node1 is running more than one daemon pod
Jun 22 06:23:14.738: INFO: Number of nodes with available pods: 0
Jun 22 06:23:14.738: INFO: Node node1 is running more than one daemon pod
Jun 22 06:23:15.727: INFO: Number of nodes with available pods: 0
Jun 22 06:23:15.727: INFO: Node node1 is running more than one daemon pod
Jun 22 06:23:16.730: INFO: Number of nodes with available pods: 4
Jun 22 06:23:16.730: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 22 06:23:16.776: INFO: Number of nodes with available pods: 3
Jun 22 06:23:16.776: INFO: Node node3 is running more than one daemon pod
Jun 22 06:23:17.799: INFO: Number of nodes with available pods: 3
Jun 22 06:23:17.799: INFO: Node node3 is running more than one daemon pod
Jun 22 06:23:18.790: INFO: Number of nodes with available pods: 3
Jun 22 06:23:18.790: INFO: Node node3 is running more than one daemon pod
Jun 22 06:23:19.792: INFO: Number of nodes with available pods: 4
Jun 22 06:23:19.792: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3024, will wait for the garbage collector to delete the pods
Jun 22 06:23:19.874: INFO: Deleting DaemonSet.extensions daemon-set took: 15.550838ms
Jun 22 06:23:20.275: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.840658ms
Jun 22 06:23:33.583: INFO: Number of nodes with available pods: 0
Jun 22 06:23:33.583: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 06:23:33.590: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3024/daemonsets","resourceVersion":"43070"},"items":null}

Jun 22 06:23:33.596: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3024/pods","resourceVersion":"43070"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:23:33.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3024" for this suite.

• [SLOW TEST:20.180 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":253,"skipped":3959,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:23:33.647: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1294
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun 22 06:23:33.841: INFO: Waiting up to 5m0s for pod "downward-api-28a445c2-a3c7-4b36-b419-8b542080cadb" in namespace "downward-api-1294" to be "success or failure"
Jun 22 06:23:33.849: INFO: Pod "downward-api-28a445c2-a3c7-4b36-b419-8b542080cadb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.967223ms
Jun 22 06:23:35.856: INFO: Pod "downward-api-28a445c2-a3c7-4b36-b419-8b542080cadb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014538223s
STEP: Saw pod success
Jun 22 06:23:35.856: INFO: Pod "downward-api-28a445c2-a3c7-4b36-b419-8b542080cadb" satisfied condition "success or failure"
Jun 22 06:23:35.862: INFO: Trying to get logs from node node4 pod downward-api-28a445c2-a3c7-4b36-b419-8b542080cadb container dapi-container: <nil>
STEP: delete the pod
Jun 22 06:23:35.915: INFO: Waiting for pod downward-api-28a445c2-a3c7-4b36-b419-8b542080cadb to disappear
Jun 22 06:23:35.921: INFO: Pod downward-api-28a445c2-a3c7-4b36-b419-8b542080cadb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:23:35.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1294" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":254,"skipped":3996,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:23:35.939: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6405
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun 22 06:23:36.121: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 06:23:36.144: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 06:23:36.150: INFO: 
Logging pods the kubelet thinks is on node node1 before test
Jun 22 06:23:36.174: INFO: calico-node-qkngx from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 06:23:36.175: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-kwffd from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 06:23:36.175: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 06:23:36.175: INFO: kube-apiserver-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 06:23:36.175: INFO: cocktail-monitoring-76bc9fcf6f-r9tw9 from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container monitoring ready: true, restart count 0
Jun 22 06:23:36.175: INFO: coredns-ddbf59985-7mcct from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container coredns ready: true, restart count 0
Jun 22 06:23:36.175: INFO: kube-scheduler-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 06:23:36.175: INFO: metrics-server-76b7895b66-f4lch from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 06:23:36.175: INFO: cocktail-alarm-collector-6cfd6bbb9c-kl9zt from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container collector ready: true, restart count 0
Jun 22 06:23:36.175: INFO: kube-proxy-qbghn from kube-system started at 2020-06-22 04:48:30 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 06:23:36.175: INFO: kube-controller-manager-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 06:23:36.175: INFO: haproxy-node1 from kube-system started at 2020-06-22 04:48:06 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container haproxy ready: true, restart count 1
Jun 22 06:23:36.175: INFO: calico-kube-controllers-77c4b7448-pvlhp from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 06:23:36.175: INFO: cocktail-batch-server-75c78755c5-c94nd from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container batch ready: true, restart count 0
Jun 22 06:23:36.175: INFO: cocktail-api-cmdb-0 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 06:23:36.175: INFO: cocktail-cluster-api-69794f5b78-9htrk from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.175: INFO: 	Container cluster-api ready: true, restart count 1
Jun 22 06:23:36.175: INFO: 
Logging pods the kubelet thinks is on node node2 before test
Jun 22 06:23:36.200: INFO: haproxy-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.200: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 06:23:36.200: INFO: kube-scheduler-node2 from kube-system started at 2020-06-22 04:48:10 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.200: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 06:23:36.200: INFO: cocktail-dashboard-queue-845dd499b5-zzxrl from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.200: INFO: 	Container dashboard-queue ready: true, restart count 0
Jun 22 06:23:36.200: INFO: addon-manager-5bb9bbcdcc-j2qhp from cocktail-addon started at 2020-06-22 04:49:54 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container addon-manager ready: true, restart count 0
Jun 22 06:23:36.201: INFO: local-storage-nfs-client-provisioner-745d66655-5qmjz from cocktail-addon started at 2020-06-22 04:50:09 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container nfs-client-provisioner ready: true, restart count 0
Jun 22 06:23:36.201: INFO: cocktail-cluster-health-checker-7456f8c49-x62nj from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container checker ready: true, restart count 0
Jun 22 06:23:36.201: INFO: cocktail-dashboard-session-7fb7444f46-xcm2x from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container dashboard-session ready: true, restart count 0
Jun 22 06:23:36.201: INFO: cocktail-api-server-84467f9bf8-8h74n from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container api-server ready: true, restart count 0
Jun 22 06:23:36.201: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-jrq4m from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 06:23:36.201: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 06:23:36.201: INFO: kube-apiserver-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 06:23:36.201: INFO: kube-controller-manager-node2 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 06:23:36.201: INFO: calico-node-xt4w8 from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 06:23:36.201: INFO: kube-proxy-chh9k from kube-system started at 2020-06-22 04:48:19 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 06:23:36.201: INFO: cocktail-metric-collector-5987649879-49kjv from cocktail-system started at 2020-06-22 04:50:15 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container statcollector ready: true, restart count 0
Jun 22 06:23:36.201: INFO: cocktail-dashboard-6bd44cf6c8-mmhlx from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container cocktail-dashboard ready: true, restart count 0
Jun 22 06:23:36.201: INFO: cocktail-build-queue-678588c57d-h5dd4 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container nats-streaming ready: true, restart count 0
Jun 22 06:23:36.201: INFO: cocktail-api-cmdb-2 from cocktail-system started at 2020-06-22 04:52:57 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.201: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 06:23:36.201: INFO: 
Logging pods the kubelet thinks is on node node3 before test
Jun 22 06:23:36.227: INFO: haproxy-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 06:23:36.227: INFO: kube-proxy-sb9hq from kube-system started at 2020-06-22 04:48:28 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 06:23:36.227: INFO: cocktail-package-75488df5f8-7p55f from cocktail-system started at 2020-06-22 04:50:13 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container package ready: true, restart count 1
Jun 22 06:23:36.227: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-88h64 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 06:23:36.227: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 06:23:36.227: INFO: kube-apiserver-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 22 06:23:36.227: INFO: metrics-server-76b7895b66-nz47p from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 06:23:36.227: INFO: cocktail-build-api-7968df5d66-kqcnw from cocktail-system started at 2020-06-22 04:50:14 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container build-api ready: true, restart count 0
Jun 22 06:23:36.227: INFO: kube-controller-manager-node3 from kube-system started at 2020-06-22 04:48:53 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 22 06:23:36.227: INFO: calico-node-lhxv5 from kube-system started at 2020-06-22 04:48:11 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 06:23:36.227: INFO: coredns-ddbf59985-ld84l from kube-system started at 2020-06-22 04:48:56 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container coredns ready: true, restart count 0
Jun 22 06:23:36.227: INFO: cocktail-monitoring-tsdb-799959dd7c-bg2nj from cocktail-system started at 2020-06-22 04:50:15 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container db ready: true, restart count 0
Jun 22 06:23:36.227: INFO: cocktail-dashboard-proxy-6557bc4fbf-fh5f9 from cocktail-system started at 2020-06-22 04:50:18 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container dashboard-proxy-01 ready: true, restart count 0
Jun 22 06:23:36.227: INFO: kube-scheduler-node3 from kube-system started at 2020-06-22 04:48:05 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 22 06:23:36.227: INFO: cocktail-api-cmdb-1 from cocktail-system started at 2020-06-22 04:51:32 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.227: INFO: 	Container api-cmdb-cluster ready: true, restart count 0
Jun 22 06:23:36.227: INFO: 
Logging pods the kubelet thinks is on node node4 before test
Jun 22 06:23:36.237: INFO: kube-proxy-42p24 from kube-system started at 2020-06-22 04:49:39 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.237: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 06:23:36.237: INFO: haproxy-node4 from kube-system started at 2020-06-22 04:50:03 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.237: INFO: 	Container haproxy ready: true, restart count 0
Jun 22 06:23:36.237: INFO: calico-node-q9h8x from kube-system started at 2020-06-22 04:49:39 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.237: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 06:23:36.237: INFO: sonobuoy from sonobuoy started at 2020-06-22 05:09:07 +0000 UTC (1 container statuses recorded)
Jun 22 06:23:36.237: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 06:23:36.237: INFO: sonobuoy-e2e-job-6934f20e32d94a27 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 06:23:36.237: INFO: 	Container e2e ready: true, restart count 0
Jun 22 06:23:36.237: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 06:23:36.237: INFO: sonobuoy-systemd-logs-daemon-set-d109c6d6c4eb4cc4-h48z4 from sonobuoy started at 2020-06-22 05:09:23 +0000 UTC (2 container statuses recorded)
Jun 22 06:23:36.237: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 06:23:36.237: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-7dafe636-68aa-4276-9a6c-619f1eb69717 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-7dafe636-68aa-4276-9a6c-619f1eb69717 off the node node4
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7dafe636-68aa-4276-9a6c-619f1eb69717
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:23:44.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6405" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:8.496 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":255,"skipped":4002,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:23:44.435: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6452
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:23:44.621: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 22 06:23:49.631: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 06:23:49.632: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 22 06:23:51.639: INFO: Creating deployment "test-rollover-deployment"
Jun 22 06:23:51.658: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 22 06:23:53.676: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 22 06:23:53.691: INFO: Ensure that both replica sets have 1 created replica
Jun 22 06:23:53.710: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 22 06:23:53.727: INFO: Updating deployment test-rollover-deployment
Jun 22 06:23:53.727: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 22 06:23:55.741: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 22 06:23:55.756: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 22 06:23:55.770: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 06:23:55.770: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403835, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:23:57.789: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 06:23:57.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403835, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:23:59.786: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 06:23:59.786: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403835, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:24:01.781: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 06:24:01.781: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403835, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:24:03.788: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 06:24:03.788: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403835, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403831, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:24:05.790: INFO: 
Jun 22 06:24:05.790: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun 22 06:24:05.809: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6452 /apis/apps/v1/namespaces/deployment-6452/deployments/test-rollover-deployment 07c6eddd-3c7d-483e-8278-ea918380f0a9 43467 2 2020-06-22 06:23:51 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005c727b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-22 06:23:51 +0000 UTC,LastTransitionTime:2020-06-22 06:23:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-06-22 06:24:05 +0000 UTC,LastTransitionTime:2020-06-22 06:23:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 06:24:05.815: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-6452 /apis/apps/v1/namespaces/deployment-6452/replicasets/test-rollover-deployment-574d6dfbff 1f4003e4-786d-4c16-84ba-037209f3398e 43456 2 2020-06-22 06:23:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 07c6eddd-3c7d-483e-8278-ea918380f0a9 0xc005c72c47 0xc005c72c48}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005c72cb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 06:24:05.815: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 22 06:24:05.815: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6452 /apis/apps/v1/namespaces/deployment-6452/replicasets/test-rollover-controller 1f112ed5-aa86-44a4-b9e2-af7d8dfe846f 43466 2 2020-06-22 06:23:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 07c6eddd-3c7d-483e-8278-ea918380f0a9 0xc005c72b77 0xc005c72b78}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005c72bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 06:24:05.815: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-6452 /apis/apps/v1/namespaces/deployment-6452/replicasets/test-rollover-deployment-f6c94f66c 3120a040-8b63-4cd0-9f41-0667a70f4e47 43381 2 2020-06-22 06:23:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 07c6eddd-3c7d-483e-8278-ea918380f0a9 0xc005c72d20 0xc005c72d21}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005c72d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 06:24:05.822: INFO: Pod "test-rollover-deployment-574d6dfbff-6qqkr" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-6qqkr test-rollover-deployment-574d6dfbff- deployment-6452 /api/v1/namespaces/deployment-6452/pods/test-rollover-deployment-574d6dfbff-6qqkr e5628f0e-f88a-43ed-b0bb-c209ac407175 43403 0 2020-06-22 06:23:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[cni.projectcalico.org/podIP:10.41.130.154/32 cni.projectcalico.org/podIPs:10.41.130.154/32] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff 1f4003e4-786d-4c16-84ba-037209f3398e 0xc005c73307 0xc005c73308}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-46x46,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-46x46,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-46x46,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 06:23:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 06:23:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 06:23:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 06:23:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.144,PodIP:10.41.130.154,StartTime:2020-06-22 06:23:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 06:23:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://95706bd0873e29f5d351e2eab3d3c17da2c7f49578077c2212a8950d9e414322,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.41.130.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:24:05.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6452" for this suite.

• [SLOW TEST:21.414 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":256,"skipped":4015,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:24:05.849: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1008
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-1008
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-1008
Jun 22 06:24:06.089: INFO: Found 0 stateful pods, waiting for 1
Jun 22 06:24:16.102: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun 22 06:24:16.138: INFO: Deleting all statefulset in ns statefulset-1008
Jun 22 06:24:16.145: INFO: Scaling statefulset ss to 0
Jun 22 06:24:46.190: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 06:24:46.196: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:24:46.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1008" for this suite.

• [SLOW TEST:40.396 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":257,"skipped":4016,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:24:46.245: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-38a9ba36-cc9e-4c8d-b6af-fe5458f130d2 in namespace container-probe-2969
Jun 22 06:24:48.443: INFO: Started pod liveness-38a9ba36-cc9e-4c8d-b6af-fe5458f130d2 in namespace container-probe-2969
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 06:24:48.448: INFO: Initial restart count of pod liveness-38a9ba36-cc9e-4c8d-b6af-fe5458f130d2 is 0
Jun 22 06:25:04.511: INFO: Restart count of pod container-probe-2969/liveness-38a9ba36-cc9e-4c8d-b6af-fe5458f130d2 is now 1 (16.063406297s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:25:04.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2969" for this suite.

• [SLOW TEST:18.310 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":258,"skipped":4035,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:25:04.555: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1670
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:25:04.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14f63619-9e8c-48c4-a4da-e6a4181a09c8" in namespace "projected-1670" to be "success or failure"
Jun 22 06:25:04.752: INFO: Pod "downwardapi-volume-14f63619-9e8c-48c4-a4da-e6a4181a09c8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.988113ms
Jun 22 06:25:06.759: INFO: Pod "downwardapi-volume-14f63619-9e8c-48c4-a4da-e6a4181a09c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015057153s
STEP: Saw pod success
Jun 22 06:25:06.759: INFO: Pod "downwardapi-volume-14f63619-9e8c-48c4-a4da-e6a4181a09c8" satisfied condition "success or failure"
Jun 22 06:25:06.764: INFO: Trying to get logs from node node4 pod downwardapi-volume-14f63619-9e8c-48c4-a4da-e6a4181a09c8 container client-container: <nil>
STEP: delete the pod
Jun 22 06:25:06.806: INFO: Waiting for pod downwardapi-volume-14f63619-9e8c-48c4-a4da-e6a4181a09c8 to disappear
Jun 22 06:25:06.811: INFO: Pod downwardapi-volume-14f63619-9e8c-48c4-a4da-e6a4181a09c8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:25:06.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1670" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":259,"skipped":4037,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:25:06.830: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2671
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 06:25:08.031: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 06:25:10.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403908, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403908, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403908, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403908, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 06:25:13.098: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:25:13.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2671" for this suite.
STEP: Destroying namespace "webhook-2671-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.489 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":260,"skipped":4073,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:25:13.320: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4559
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1275
STEP: creating the pod
Jun 22 06:25:13.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 create -f - --namespace=kubectl-4559'
Jun 22 06:25:13.815: INFO: stderr: ""
Jun 22 06:25:13.815: INFO: stdout: "pod/pause created\n"
Jun 22 06:25:13.815: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 22 06:25:13.815: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4559" to be "running and ready"
Jun 22 06:25:13.823: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.142037ms
Jun 22 06:25:15.830: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.014966511s
Jun 22 06:25:15.830: INFO: Pod "pause" satisfied condition "running and ready"
Jun 22 06:25:15.830: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 22 06:25:15.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 label pods pause testing-label=testing-label-value --namespace=kubectl-4559'
Jun 22 06:25:15.950: INFO: stderr: ""
Jun 22 06:25:15.950: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 22 06:25:15.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pod pause -L testing-label --namespace=kubectl-4559'
Jun 22 06:25:16.064: INFO: stderr: ""
Jun 22 06:25:16.064: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 22 06:25:16.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 label pods pause testing-label- --namespace=kubectl-4559'
Jun 22 06:25:16.183: INFO: stderr: ""
Jun 22 06:25:16.183: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 22 06:25:16.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pod pause -L testing-label --namespace=kubectl-4559'
Jun 22 06:25:16.308: INFO: stderr: ""
Jun 22 06:25:16.308: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1282
STEP: using delete to clean up resources
Jun 22 06:25:16.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 delete --grace-period=0 --force -f - --namespace=kubectl-4559'
Jun 22 06:25:16.445: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 06:25:16.445: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 22 06:25:16.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get rc,svc -l name=pause --no-headers --namespace=kubectl-4559'
Jun 22 06:25:16.614: INFO: stderr: "No resources found in kubectl-4559 namespace.\n"
Jun 22 06:25:16.614: INFO: stdout: ""
Jun 22 06:25:16.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 get pods -l name=pause --namespace=kubectl-4559 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 06:25:16.727: INFO: stderr: ""
Jun 22 06:25:16.727: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:25:16.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4559" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":261,"skipped":4077,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:25:16.747: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9828
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 06:25:18.964: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:25:18.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9828" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":262,"skipped":4172,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:25:19.011: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:25:30.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1412" for this suite.

• [SLOW TEST:11.317 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":263,"skipped":4189,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:25:30.328: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1809
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-ce593383-f8df-40a3-ab42-8ea8c7a3ef9e
STEP: Creating a pod to test consume configMaps
Jun 22 06:25:30.528: INFO: Waiting up to 5m0s for pod "pod-configmaps-f64ef600-5eda-4bba-9015-efafe507657a" in namespace "configmap-1809" to be "success or failure"
Jun 22 06:25:30.533: INFO: Pod "pod-configmaps-f64ef600-5eda-4bba-9015-efafe507657a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.743077ms
Jun 22 06:25:32.540: INFO: Pod "pod-configmaps-f64ef600-5eda-4bba-9015-efafe507657a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011440329s
STEP: Saw pod success
Jun 22 06:25:32.540: INFO: Pod "pod-configmaps-f64ef600-5eda-4bba-9015-efafe507657a" satisfied condition "success or failure"
Jun 22 06:25:32.546: INFO: Trying to get logs from node node4 pod pod-configmaps-f64ef600-5eda-4bba-9015-efafe507657a container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 06:25:32.587: INFO: Waiting for pod pod-configmaps-f64ef600-5eda-4bba-9015-efafe507657a to disappear
Jun 22 06:25:32.593: INFO: Pod pod-configmaps-f64ef600-5eda-4bba-9015-efafe507657a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:25:32.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1809" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":264,"skipped":4214,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:25:32.611: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1690
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:25:36.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1690" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":265,"skipped":4221,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:25:36.873: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-168
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:25:48.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-168" for this suite.

• [SLOW TEST:11.252 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":266,"skipped":4223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:25:48.126: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6928
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 22 06:25:48.317: INFO: Waiting up to 5m0s for pod "pod-1888869d-5988-422c-8e10-6b231a68a60c" in namespace "emptydir-6928" to be "success or failure"
Jun 22 06:25:48.326: INFO: Pod "pod-1888869d-5988-422c-8e10-6b231a68a60c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.925203ms
Jun 22 06:25:50.336: INFO: Pod "pod-1888869d-5988-422c-8e10-6b231a68a60c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018723658s
STEP: Saw pod success
Jun 22 06:25:50.336: INFO: Pod "pod-1888869d-5988-422c-8e10-6b231a68a60c" satisfied condition "success or failure"
Jun 22 06:25:50.344: INFO: Trying to get logs from node node4 pod pod-1888869d-5988-422c-8e10-6b231a68a60c container test-container: <nil>
STEP: delete the pod
Jun 22 06:25:50.390: INFO: Waiting for pod pod-1888869d-5988-422c-8e10-6b231a68a60c to disappear
Jun 22 06:25:50.396: INFO: Pod pod-1888869d-5988-422c-8e10-6b231a68a60c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:25:50.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6928" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":267,"skipped":4267,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:25:50.413: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-750
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:25:50.597: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 22 06:25:59.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-750 create -f -'
Jun 22 06:25:59.943: INFO: stderr: ""
Jun 22 06:25:59.943: INFO: stdout: "e2e-test-crd-publish-openapi-611-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 22 06:25:59.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-750 delete e2e-test-crd-publish-openapi-611-crds test-cr'
Jun 22 06:26:00.078: INFO: stderr: ""
Jun 22 06:26:00.078: INFO: stdout: "e2e-test-crd-publish-openapi-611-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 22 06:26:00.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-750 apply -f -'
Jun 22 06:26:00.426: INFO: stderr: ""
Jun 22 06:26:00.426: INFO: stdout: "e2e-test-crd-publish-openapi-611-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 22 06:26:00.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 --namespace=crd-publish-openapi-750 delete e2e-test-crd-publish-openapi-611-crds test-cr'
Jun 22 06:26:00.540: INFO: stderr: ""
Jun 22 06:26:00.540: INFO: stdout: "e2e-test-crd-publish-openapi-611-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 22 06:26:00.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-119706190 explain e2e-test-crd-publish-openapi-611-crds'
Jun 22 06:26:00.768: INFO: stderr: ""
Jun 22 06:26:00.768: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-611-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:26:04.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-750" for this suite.

• [SLOW TEST:14.133 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":268,"skipped":4272,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:26:04.546: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8627
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 06:26:05.239: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 22 06:26:07.260: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403965, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403965, loc:(*time.Location)(0x7925240)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403965, loc:(*time.Location)(0x7925240)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728403965, loc:(*time.Location)(0x7925240)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 06:26:10.298: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:26:10.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8627" for this suite.
STEP: Destroying namespace "webhook-8627-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.193 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":269,"skipped":4282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:26:10.739: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:26:10.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-070ee483-ad94-4546-8229-1c8ae78dcfe4" in namespace "downward-api-6885" to be "success or failure"
Jun 22 06:26:10.962: INFO: Pod "downwardapi-volume-070ee483-ad94-4546-8229-1c8ae78dcfe4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.730229ms
Jun 22 06:26:12.971: INFO: Pod "downwardapi-volume-070ee483-ad94-4546-8229-1c8ae78dcfe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017269189s
Jun 22 06:26:14.977: INFO: Pod "downwardapi-volume-070ee483-ad94-4546-8229-1c8ae78dcfe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023693805s
STEP: Saw pod success
Jun 22 06:26:14.977: INFO: Pod "downwardapi-volume-070ee483-ad94-4546-8229-1c8ae78dcfe4" satisfied condition "success or failure"
Jun 22 06:26:14.985: INFO: Trying to get logs from node node4 pod downwardapi-volume-070ee483-ad94-4546-8229-1c8ae78dcfe4 container client-container: <nil>
STEP: delete the pod
Jun 22 06:26:15.016: INFO: Waiting for pod downwardapi-volume-070ee483-ad94-4546-8229-1c8ae78dcfe4 to disappear
Jun 22 06:26:15.021: INFO: Pod downwardapi-volume-070ee483-ad94-4546-8229-1c8ae78dcfe4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:26:15.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6885" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":270,"skipped":4307,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:26:15.038: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1086
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-eb46e4a8-dd22-4031-a346-51d8eb217e5b
STEP: Creating a pod to test consume secrets
Jun 22 06:26:15.223: INFO: Waiting up to 5m0s for pod "pod-secrets-314961e7-7b42-492a-af5d-d593a8e78b9d" in namespace "secrets-1086" to be "success or failure"
Jun 22 06:26:15.231: INFO: Pod "pod-secrets-314961e7-7b42-492a-af5d-d593a8e78b9d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.671017ms
Jun 22 06:26:17.237: INFO: Pod "pod-secrets-314961e7-7b42-492a-af5d-d593a8e78b9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014153895s
STEP: Saw pod success
Jun 22 06:26:17.237: INFO: Pod "pod-secrets-314961e7-7b42-492a-af5d-d593a8e78b9d" satisfied condition "success or failure"
Jun 22 06:26:17.242: INFO: Trying to get logs from node node4 pod pod-secrets-314961e7-7b42-492a-af5d-d593a8e78b9d container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 06:26:17.282: INFO: Waiting for pod pod-secrets-314961e7-7b42-492a-af5d-d593a8e78b9d to disappear
Jun 22 06:26:17.288: INFO: Pod pod-secrets-314961e7-7b42-492a-af5d-d593a8e78b9d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:26:17.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1086" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":271,"skipped":4323,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:26:17.312: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6397
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Jun 22 06:26:21.518: INFO: Pod pod-hostip-da435448-e215-4e87-a10a-a59438ff0bd6 has hostIP: 192.168.1.144
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:26:21.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6397" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":272,"skipped":4354,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:26:21.539: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-a3047d28-d37f-49cf-82ca-6d530572a108
STEP: Creating a pod to test consume configMaps
Jun 22 06:26:21.726: INFO: Waiting up to 5m0s for pod "pod-configmaps-72020dd4-9de3-4dce-bfbf-e73c18cb15bd" in namespace "configmap-3135" to be "success or failure"
Jun 22 06:26:21.731: INFO: Pod "pod-configmaps-72020dd4-9de3-4dce-bfbf-e73c18cb15bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.503543ms
Jun 22 06:26:23.738: INFO: Pod "pod-configmaps-72020dd4-9de3-4dce-bfbf-e73c18cb15bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012059732s
Jun 22 06:26:25.747: INFO: Pod "pod-configmaps-72020dd4-9de3-4dce-bfbf-e73c18cb15bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02097974s
STEP: Saw pod success
Jun 22 06:26:25.747: INFO: Pod "pod-configmaps-72020dd4-9de3-4dce-bfbf-e73c18cb15bd" satisfied condition "success or failure"
Jun 22 06:26:25.751: INFO: Trying to get logs from node node4 pod pod-configmaps-72020dd4-9de3-4dce-bfbf-e73c18cb15bd container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 06:26:25.785: INFO: Waiting for pod pod-configmaps-72020dd4-9de3-4dce-bfbf-e73c18cb15bd to disappear
Jun 22 06:26:25.792: INFO: Pod pod-configmaps-72020dd4-9de3-4dce-bfbf-e73c18cb15bd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:26:25.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3135" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":273,"skipped":4360,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:26:25.811: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jun 22 06:26:30.034: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-119706190 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun 22 06:26:35.163: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:26:35.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5" for this suite.

• [SLOW TEST:9.374 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":274,"skipped":4375,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:26:35.186: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6781
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6781.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6781.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6781.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6781.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6781.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6781.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 06:26:39.408: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:39.413: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:39.419: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:39.428: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:39.451: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:39.457: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:39.465: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:39.475: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:39.486: INFO: Lookups using dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6781.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6781.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local jessie_udp@dns-test-service-2.dns-6781.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6781.svc.cluster.local]

Jun 22 06:26:44.497: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:44.510: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:44.516: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:44.522: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:44.549: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:44.557: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:44.565: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:44.572: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:44.585: INFO: Lookups using dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6781.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6781.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local jessie_udp@dns-test-service-2.dns-6781.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6781.svc.cluster.local]

Jun 22 06:26:49.502: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:49.508: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:49.514: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:49.523: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:49.544: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:49.553: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:49.559: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:49.566: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:49.578: INFO: Lookups using dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6781.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6781.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local jessie_udp@dns-test-service-2.dns-6781.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6781.svc.cluster.local]

Jun 22 06:26:54.496: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:54.504: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:54.508: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:54.515: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:54.534: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:54.546: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:54.553: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:54.561: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:54.574: INFO: Lookups using dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6781.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6781.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local jessie_udp@dns-test-service-2.dns-6781.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6781.svc.cluster.local]

Jun 22 06:26:59.497: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:59.505: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:59.510: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:59.517: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:59.561: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:59.569: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:59.574: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:59.580: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:26:59.591: INFO: Lookups using dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6781.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6781.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local jessie_udp@dns-test-service-2.dns-6781.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6781.svc.cluster.local]

Jun 22 06:27:04.495: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:27:04.503: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:27:04.509: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:27:04.514: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:27:04.534: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:27:04.541: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:27:04.549: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:27:04.557: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6781.svc.cluster.local from pod dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77: the server could not find the requested resource (get pods dns-test-5110463e-e3eb-481d-85f5-3589a0458c77)
Jun 22 06:27:04.570: INFO: Lookups using dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6781.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6781.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local jessie_udp@dns-test-service-2.dns-6781.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6781.svc.cluster.local]

Jun 22 06:27:09.559: INFO: DNS probes using dns-6781/dns-test-5110463e-e3eb-481d-85f5-3589a0458c77 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:27:09.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6781" for this suite.

• [SLOW TEST:34.525 seconds]
[sig-network] DNS
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":275,"skipped":4390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:27:09.711: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-5416
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5416
STEP: Deleting pre-stop pod
Jun 22 06:27:20.961: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:27:20.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5416" for this suite.

• [SLOW TEST:11.285 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":276,"skipped":4417,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:27:20.996: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:27:21.212: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 22 06:27:21.229: INFO: Number of nodes with available pods: 0
Jun 22 06:27:21.229: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 22 06:27:21.265: INFO: Number of nodes with available pods: 0
Jun 22 06:27:21.265: INFO: Node node4 is running more than one daemon pod
Jun 22 06:27:22.271: INFO: Number of nodes with available pods: 0
Jun 22 06:27:22.272: INFO: Node node4 is running more than one daemon pod
Jun 22 06:27:23.273: INFO: Number of nodes with available pods: 0
Jun 22 06:27:23.273: INFO: Node node4 is running more than one daemon pod
Jun 22 06:27:24.270: INFO: Number of nodes with available pods: 1
Jun 22 06:27:24.270: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 22 06:27:24.297: INFO: Number of nodes with available pods: 1
Jun 22 06:27:24.297: INFO: Number of running nodes: 0, number of available pods: 1
Jun 22 06:27:25.304: INFO: Number of nodes with available pods: 0
Jun 22 06:27:25.304: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 22 06:27:25.322: INFO: Number of nodes with available pods: 0
Jun 22 06:27:25.322: INFO: Node node4 is running more than one daemon pod
Jun 22 06:27:26.330: INFO: Number of nodes with available pods: 0
Jun 22 06:27:26.330: INFO: Node node4 is running more than one daemon pod
Jun 22 06:27:27.330: INFO: Number of nodes with available pods: 0
Jun 22 06:27:27.330: INFO: Node node4 is running more than one daemon pod
Jun 22 06:27:28.331: INFO: Number of nodes with available pods: 0
Jun 22 06:27:28.331: INFO: Node node4 is running more than one daemon pod
Jun 22 06:27:29.331: INFO: Number of nodes with available pods: 0
Jun 22 06:27:29.331: INFO: Node node4 is running more than one daemon pod
Jun 22 06:27:30.330: INFO: Number of nodes with available pods: 1
Jun 22 06:27:30.330: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7035, will wait for the garbage collector to delete the pods
Jun 22 06:27:30.413: INFO: Deleting DaemonSet.extensions daemon-set took: 15.424957ms
Jun 22 06:27:30.813: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.211864ms
Jun 22 06:27:33.820: INFO: Number of nodes with available pods: 0
Jun 22 06:27:33.820: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 06:27:33.825: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7035/daemonsets","resourceVersion":"45314"},"items":null}

Jun 22 06:27:33.830: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7035/pods","resourceVersion":"45314"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:27:33.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7035" for this suite.

• [SLOW TEST:12.889 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":277,"skipped":4424,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:27:33.885: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-008c5095-9609-4578-a983-136d761fa44f in namespace container-probe-3498
Jun 22 06:27:36.075: INFO: Started pod test-webserver-008c5095-9609-4578-a983-136d761fa44f in namespace container-probe-3498
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 06:27:36.080: INFO: Initial restart count of pod test-webserver-008c5095-9609-4578-a983-136d761fa44f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:31:37.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3498" for this suite.

• [SLOW TEST:243.251 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":278,"skipped":4432,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:31:37.137: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5956
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jun 22 06:31:37.330: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:31:42.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5956" for this suite.

• [SLOW TEST:5.045 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":279,"skipped":4535,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 06:31:42.183: INFO: >>> kubeConfig: /tmp/kubeconfig-119706190
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6632
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 22 06:31:42.371: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1f9c3acb-1cca-4427-9158-c2b21288a1bc" in namespace "security-context-test-6632" to be "success or failure"
Jun 22 06:31:42.382: INFO: Pod "busybox-readonly-false-1f9c3acb-1cca-4427-9158-c2b21288a1bc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.819512ms
Jun 22 06:31:44.388: INFO: Pod "busybox-readonly-false-1f9c3acb-1cca-4427-9158-c2b21288a1bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016900231s
Jun 22 06:31:44.388: INFO: Pod "busybox-readonly-false-1f9c3acb-1cca-4427-9158-c2b21288a1bc" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.7-rc.0.50+e4efcc9eb3807d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 06:31:44.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6632" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":280,"skipped":4554,"failed":0}
SSSSSSSSSJun 22 06:31:44.404: INFO: Running AfterSuite actions on all nodes
Jun 22 06:31:44.404: INFO: Running AfterSuite actions on node 1
Jun 22 06:31:44.404: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4563,"failed":0}

Ran 280 of 4843 Specs in 4878.580 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4563 Skipped
PASS

Ginkgo ran 1 suite in 1h21m20.4041955s
Test Suite Passed
