I1214 08:47:55.104845      18 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-014205462
I1214 08:47:55.105239      18 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1214 08:47:55.105880      18 e2e.go:109] Starting e2e run "cceeb2be-c65b-4aa6-a86e-5ad15a0f559d" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1576313273 - Will randomize all specs
Will run 280 of 4814 specs

Dec 14 08:47:55.120: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 08:47:55.123: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E1214 08:47:55.132002      18 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp [::1]:8099: connect: connection refused
Dec 14 08:47:55.139: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 14 08:47:55.166: INFO: 8 / 8 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 14 08:47:55.167: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Dec 14 08:47:55.167: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 14 08:47:55.175: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-amd64' (0 seconds elapsed)
Dec 14 08:47:55.175: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm' (0 seconds elapsed)
Dec 14 08:47:55.175: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm64' (0 seconds elapsed)
Dec 14 08:47:55.176: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-ppc64le' (0 seconds elapsed)
Dec 14 08:47:55.176: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-s390x' (0 seconds elapsed)
Dec 14 08:47:55.176: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'traefik-ingress-controller' (0 seconds elapsed)
Dec 14 08:47:55.176: INFO: e2e test version: v1.17.0
Dec 14 08:47:55.177: INFO: kube-apiserver version: v1.17.0
Dec 14 08:47:55.177: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 08:47:55.182: INFO: Cluster IP family: ipv4
SS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:47:55.183: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
Dec 14 08:47:55.220: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Dec 14 08:47:55.232: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Dec 14 08:47:55.344: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Dec 14 08:47:55.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-9000'
Dec 14 08:47:55.814: INFO: stderr: ""
Dec 14 08:47:55.814: INFO: stdout: "service/agnhost-slave created\n"
Dec 14 08:47:55.814: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Dec 14 08:47:55.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-9000'
Dec 14 08:47:56.093: INFO: stderr: ""
Dec 14 08:47:56.093: INFO: stdout: "service/agnhost-master created\n"
Dec 14 08:47:56.094: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 14 08:47:56.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-9000'
Dec 14 08:47:56.383: INFO: stderr: ""
Dec 14 08:47:56.383: INFO: stdout: "service/frontend created\n"
Dec 14 08:47:56.383: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec 14 08:47:56.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-9000'
Dec 14 08:47:56.671: INFO: stderr: ""
Dec 14 08:47:56.671: INFO: stdout: "deployment.apps/frontend created\n"
Dec 14 08:47:56.671: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 14 08:47:56.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-9000'
Dec 14 08:47:56.989: INFO: stderr: ""
Dec 14 08:47:56.989: INFO: stdout: "deployment.apps/agnhost-master created\n"
Dec 14 08:47:56.989: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 14 08:47:56.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-9000'
Dec 14 08:47:57.291: INFO: stderr: ""
Dec 14 08:47:57.291: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Dec 14 08:47:57.291: INFO: Waiting for all frontend pods to be Running.
Dec 14 08:48:07.341: INFO: Waiting for frontend to serve content.
Dec 14 08:48:07.351: INFO: Trying to add a new entry to the guestbook.
Dec 14 08:48:07.360: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec 14 08:48:07.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete --grace-period=0 --force -f - --namespace=kubectl-9000'
Dec 14 08:48:07.464: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:48:07.464: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 08:48:07.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete --grace-period=0 --force -f - --namespace=kubectl-9000'
Dec 14 08:48:07.608: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:48:07.608: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 08:48:07.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete --grace-period=0 --force -f - --namespace=kubectl-9000'
Dec 14 08:48:07.749: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:48:07.749: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 08:48:07.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete --grace-period=0 --force -f - --namespace=kubectl-9000'
Dec 14 08:48:07.870: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:48:07.870: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 08:48:07.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete --grace-period=0 --force -f - --namespace=kubectl-9000'
Dec 14 08:48:07.976: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:48:07.976: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 08:48:07.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete --grace-period=0 --force -f - --namespace=kubectl-9000'
Dec 14 08:48:08.053: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:48:08.053: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:48:08.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9000" for this suite.

• [SLOW TEST:12.875 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:385
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":1,"skipped":2,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:48:08.059: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 08:48:08.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-52d8849e-7f62-4ac4-ad1b-55ab2a5099ed" in namespace "downward-api-749" to be "success or failure"
Dec 14 08:48:08.259: INFO: Pod "downwardapi-volume-52d8849e-7f62-4ac4-ad1b-55ab2a5099ed": Phase="Pending", Reason="", readiness=false. Elapsed: 5.924838ms
Dec 14 08:48:10.261: INFO: Pod "downwardapi-volume-52d8849e-7f62-4ac4-ad1b-55ab2a5099ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008142299s
Dec 14 08:48:12.263: INFO: Pod "downwardapi-volume-52d8849e-7f62-4ac4-ad1b-55ab2a5099ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010267264s
STEP: Saw pod success
Dec 14 08:48:12.263: INFO: Pod "downwardapi-volume-52d8849e-7f62-4ac4-ad1b-55ab2a5099ed" satisfied condition "success or failure"
Dec 14 08:48:12.265: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-52d8849e-7f62-4ac4-ad1b-55ab2a5099ed container client-container: <nil>
STEP: delete the pod
Dec 14 08:48:12.281: INFO: Waiting for pod downwardapi-volume-52d8849e-7f62-4ac4-ad1b-55ab2a5099ed to disappear
Dec 14 08:48:12.282: INFO: Pod downwardapi-volume-52d8849e-7f62-4ac4-ad1b-55ab2a5099ed no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:48:12.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-749" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":2,"skipped":24,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:48:12.289: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9813
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2154
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:48:18.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5119" for this suite.
STEP: Destroying namespace "nsdeletetest-9813" for this suite.
Dec 14 08:48:18.763: INFO: Namespace nsdeletetest-9813 was already deleted
STEP: Destroying namespace "nsdeletetest-2154" for this suite.

• [SLOW TEST:6.479 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":3,"skipped":60,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:48:18.768: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9906
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-fc0fe3bb-c11e-472b-9c33-2847ebb2d795
STEP: Creating configMap with name cm-test-opt-upd-30aeaaa8-2337-4ed8-9615-664f0d57afe0
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-fc0fe3bb-c11e-472b-9c33-2847ebb2d795
STEP: Updating configmap cm-test-opt-upd-30aeaaa8-2337-4ed8-9615-664f0d57afe0
STEP: Creating configMap with name cm-test-opt-create-835bb111-dfdf-45f0-a0c1-74f6e9f4a274
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:48:27.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9906" for this suite.

• [SLOW TEST:8.249 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":4,"skipped":67,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:48:27.018: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:48:27.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7244" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":5,"skipped":70,"failed":0}

------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:48:27.174: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3415
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 08:48:27.317: INFO: (0) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.960986ms)
Dec 14 08:48:27.319: INFO: (1) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.230743ms)
Dec 14 08:48:27.321: INFO: (2) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.155061ms)
Dec 14 08:48:27.324: INFO: (3) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.436875ms)
Dec 14 08:48:27.326: INFO: (4) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.394701ms)
Dec 14 08:48:27.328: INFO: (5) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.373226ms)
Dec 14 08:48:27.331: INFO: (6) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.367314ms)
Dec 14 08:48:27.333: INFO: (7) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.11214ms)
Dec 14 08:48:27.341: INFO: (8) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 7.801759ms)
Dec 14 08:48:27.345: INFO: (9) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 4.307593ms)
Dec 14 08:48:27.349: INFO: (10) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.540699ms)
Dec 14 08:48:27.353: INFO: (11) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 4.549934ms)
Dec 14 08:48:27.357: INFO: (12) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.428284ms)
Dec 14 08:48:27.362: INFO: (13) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 4.868273ms)
Dec 14 08:48:27.365: INFO: (14) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.857213ms)
Dec 14 08:48:27.368: INFO: (15) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.162884ms)
Dec 14 08:48:27.372: INFO: (16) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.59933ms)
Dec 14 08:48:27.374: INFO: (17) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.396159ms)
Dec 14 08:48:27.377: INFO: (18) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.713937ms)
Dec 14 08:48:27.379: INFO: (19) /api/v1/nodes/k8s-2/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.179225ms)
[AfterEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:48:27.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3415" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":6,"skipped":70,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:48:27.385: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 08:48:27.524: INFO: Waiting up to 5m0s for pod "downwardapi-volume-358b706f-5236-4a01-ad56-cac859e4cbca" in namespace "projected-5340" to be "success or failure"
Dec 14 08:48:27.530: INFO: Pod "downwardapi-volume-358b706f-5236-4a01-ad56-cac859e4cbca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.111395ms
Dec 14 08:48:29.533: INFO: Pod "downwardapi-volume-358b706f-5236-4a01-ad56-cac859e4cbca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009591988s
STEP: Saw pod success
Dec 14 08:48:29.534: INFO: Pod "downwardapi-volume-358b706f-5236-4a01-ad56-cac859e4cbca" satisfied condition "success or failure"
Dec 14 08:48:29.536: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-358b706f-5236-4a01-ad56-cac859e4cbca container client-container: <nil>
STEP: delete the pod
Dec 14 08:48:29.604: INFO: Waiting for pod downwardapi-volume-358b706f-5236-4a01-ad56-cac859e4cbca to disappear
Dec 14 08:48:29.610: INFO: Pod downwardapi-volume-358b706f-5236-4a01-ad56-cac859e4cbca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:48:29.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5340" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":7,"skipped":95,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:48:29.620: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6627
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 08:48:29.811: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 14 08:48:35.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-6627 create -f -'
Dec 14 08:48:35.747: INFO: stderr: ""
Dec 14 08:48:35.747: INFO: stdout: "e2e-test-crd-publish-openapi-3447-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 14 08:48:35.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-6627 delete e2e-test-crd-publish-openapi-3447-crds test-cr'
Dec 14 08:48:35.828: INFO: stderr: ""
Dec 14 08:48:35.828: INFO: stdout: "e2e-test-crd-publish-openapi-3447-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 14 08:48:35.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-6627 apply -f -'
Dec 14 08:48:35.988: INFO: stderr: ""
Dec 14 08:48:35.988: INFO: stdout: "e2e-test-crd-publish-openapi-3447-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 14 08:48:35.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-6627 delete e2e-test-crd-publish-openapi-3447-crds test-cr'
Dec 14 08:48:36.104: INFO: stderr: ""
Dec 14 08:48:36.104: INFO: stdout: "e2e-test-crd-publish-openapi-3447-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 14 08:48:36.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 explain e2e-test-crd-publish-openapi-3447-crds'
Dec 14 08:48:36.385: INFO: stderr: ""
Dec 14 08:48:36.385: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3447-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:48:41.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6627" for this suite.

• [SLOW TEST:11.513 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":8,"skipped":106,"failed":0}
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:48:41.133: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9731
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 14 08:48:45.379: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 08:48:45.383: INFO: Pod pod-with-poststart-http-hook still exists
Dec 14 08:48:47.383: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 08:48:47.386: INFO: Pod pod-with-poststart-http-hook still exists
Dec 14 08:48:49.383: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 08:48:49.386: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:48:49.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9731" for this suite.

• [SLOW TEST:8.258 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":9,"skipped":106,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:48:49.392: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9965
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 14 08:48:49.533: INFO: Waiting up to 5m0s for pod "pod-166ad5ef-d404-4599-adb3-37f190ea260a" in namespace "emptydir-9965" to be "success or failure"
Dec 14 08:48:49.539: INFO: Pod "pod-166ad5ef-d404-4599-adb3-37f190ea260a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346779ms
Dec 14 08:48:51.541: INFO: Pod "pod-166ad5ef-d404-4599-adb3-37f190ea260a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008657025s
Dec 14 08:48:53.544: INFO: Pod "pod-166ad5ef-d404-4599-adb3-37f190ea260a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011152368s
STEP: Saw pod success
Dec 14 08:48:53.544: INFO: Pod "pod-166ad5ef-d404-4599-adb3-37f190ea260a" satisfied condition "success or failure"
Dec 14 08:48:53.546: INFO: Trying to get logs from node k8s-2 pod pod-166ad5ef-d404-4599-adb3-37f190ea260a container test-container: <nil>
STEP: delete the pod
Dec 14 08:48:53.557: INFO: Waiting for pod pod-166ad5ef-d404-4599-adb3-37f190ea260a to disappear
Dec 14 08:48:53.560: INFO: Pod pod-166ad5ef-d404-4599-adb3-37f190ea260a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:48:53.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9965" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":10,"skipped":126,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:48:53.566: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5455
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Dec 14 08:48:53.746: INFO: namespace kubectl-5455
Dec 14 08:48:53.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-5455'
Dec 14 08:48:54.003: INFO: stderr: ""
Dec 14 08:48:54.003: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Dec 14 08:48:55.005: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 08:48:55.005: INFO: Found 0 / 1
Dec 14 08:48:56.005: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 08:48:56.005: INFO: Found 1 / 1
Dec 14 08:48:56.005: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 14 08:48:56.007: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 08:48:56.007: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 08:48:56.007: INFO: wait on agnhost-master startup in kubectl-5455 
Dec 14 08:48:56.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 logs agnhost-master-crrbq agnhost-master --namespace=kubectl-5455'
Dec 14 08:48:56.090: INFO: stderr: ""
Dec 14 08:48:56.090: INFO: stdout: "Paused\n"
STEP: exposing RC
Dec 14 08:48:56.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-5455'
Dec 14 08:48:56.184: INFO: stderr: ""
Dec 14 08:48:56.184: INFO: stdout: "service/rm2 exposed\n"
Dec 14 08:48:56.193: INFO: Service rm2 in namespace kubectl-5455 found.
STEP: exposing service
Dec 14 08:48:58.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-5455'
Dec 14 08:48:58.284: INFO: stderr: ""
Dec 14 08:48:58.284: INFO: stdout: "service/rm3 exposed\n"
Dec 14 08:48:58.299: INFO: Service rm3 in namespace kubectl-5455 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:49:00.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5455" for this suite.

• [SLOW TEST:6.746 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1275
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":11,"skipped":139,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:49:00.313: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3956
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-6f38c8e8-c151-4cd0-8d08-661391814b99
STEP: Creating a pod to test consume configMaps
Dec 14 08:49:00.457: INFO: Waiting up to 5m0s for pod "pod-configmaps-8436a86e-c275-4483-ada0-5e80d4b7fd6c" in namespace "configmap-3956" to be "success or failure"
Dec 14 08:49:00.463: INFO: Pod "pod-configmaps-8436a86e-c275-4483-ada0-5e80d4b7fd6c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.962755ms
Dec 14 08:49:02.465: INFO: Pod "pod-configmaps-8436a86e-c275-4483-ada0-5e80d4b7fd6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008775277s
STEP: Saw pod success
Dec 14 08:49:02.466: INFO: Pod "pod-configmaps-8436a86e-c275-4483-ada0-5e80d4b7fd6c" satisfied condition "success or failure"
Dec 14 08:49:02.467: INFO: Trying to get logs from node k8s-3 pod pod-configmaps-8436a86e-c275-4483-ada0-5e80d4b7fd6c container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 08:49:02.479: INFO: Waiting for pod pod-configmaps-8436a86e-c275-4483-ada0-5e80d4b7fd6c to disappear
Dec 14 08:49:02.483: INFO: Pod pod-configmaps-8436a86e-c275-4483-ada0-5e80d4b7fd6c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:49:02.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3956" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":12,"skipped":143,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:49:02.488: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9227
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 08:49:02.637: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b461e894-5e9c-4600-b2c5-5d01c7f1151a" in namespace "downward-api-9227" to be "success or failure"
Dec 14 08:49:02.643: INFO: Pod "downwardapi-volume-b461e894-5e9c-4600-b2c5-5d01c7f1151a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.200459ms
Dec 14 08:49:04.645: INFO: Pod "downwardapi-volume-b461e894-5e9c-4600-b2c5-5d01c7f1151a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008581527s
STEP: Saw pod success
Dec 14 08:49:04.645: INFO: Pod "downwardapi-volume-b461e894-5e9c-4600-b2c5-5d01c7f1151a" satisfied condition "success or failure"
Dec 14 08:49:04.647: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-b461e894-5e9c-4600-b2c5-5d01c7f1151a container client-container: <nil>
STEP: delete the pod
Dec 14 08:49:04.658: INFO: Waiting for pod downwardapi-volume-b461e894-5e9c-4600-b2c5-5d01c7f1151a to disappear
Dec 14 08:49:04.662: INFO: Pod downwardapi-volume-b461e894-5e9c-4600-b2c5-5d01c7f1151a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:49:04.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9227" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":13,"skipped":145,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:49:04.667: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5760
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 14 08:49:04.805: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5760 /api/v1/namespaces/watch-5760/configmaps/e2e-watch-test-watch-closed 97be5399-4897-4631-a072-3c150cc2b6e2 3072 0 2019-12-14 08:49:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 14 08:49:04.806: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5760 /api/v1/namespaces/watch-5760/configmaps/e2e-watch-test-watch-closed 97be5399-4897-4631-a072-3c150cc2b6e2 3073 0 2019-12-14 08:49:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 14 08:49:04.816: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5760 /api/v1/namespaces/watch-5760/configmaps/e2e-watch-test-watch-closed 97be5399-4897-4631-a072-3c150cc2b6e2 3074 0 2019-12-14 08:49:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 14 08:49:04.817: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5760 /api/v1/namespaces/watch-5760/configmaps/e2e-watch-test-watch-closed 97be5399-4897-4631-a072-3c150cc2b6e2 3075 0 2019-12-14 08:49:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:49:04.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5760" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":14,"skipped":147,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:49:04.828: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 08:49:07.025: INFO: Waiting up to 5m0s for pod "client-envvars-66b42582-ce16-4f94-bf84-c1edba5ab125" in namespace "pods-6952" to be "success or failure"
Dec 14 08:49:07.034: INFO: Pod "client-envvars-66b42582-ce16-4f94-bf84-c1edba5ab125": Phase="Pending", Reason="", readiness=false. Elapsed: 9.240447ms
Dec 14 08:49:09.036: INFO: Pod "client-envvars-66b42582-ce16-4f94-bf84-c1edba5ab125": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011275736s
Dec 14 08:49:11.038: INFO: Pod "client-envvars-66b42582-ce16-4f94-bf84-c1edba5ab125": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013577516s
STEP: Saw pod success
Dec 14 08:49:11.039: INFO: Pod "client-envvars-66b42582-ce16-4f94-bf84-c1edba5ab125" satisfied condition "success or failure"
Dec 14 08:49:11.040: INFO: Trying to get logs from node k8s-3 pod client-envvars-66b42582-ce16-4f94-bf84-c1edba5ab125 container env3cont: <nil>
STEP: delete the pod
Dec 14 08:49:11.055: INFO: Waiting for pod client-envvars-66b42582-ce16-4f94-bf84-c1edba5ab125 to disappear
Dec 14 08:49:11.058: INFO: Pod client-envvars-66b42582-ce16-4f94-bf84-c1edba5ab125 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:49:11.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6952" for this suite.

• [SLOW TEST:6.235 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":15,"skipped":217,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:49:11.063: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Dec 14 08:49:13.721: INFO: Successfully updated pod "annotationupdatee69d0b6b-38b9-46c2-9191-3b6d664278a2"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:49:17.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1723" for this suite.

• [SLOW TEST:6.679 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":16,"skipped":225,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:49:17.744: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4981
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4981.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4981.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 08:49:31.912: INFO: DNS probes using dns-4981/dns-test-cc45593a-152a-4274-99f8-8dfe406c0a9e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:49:31.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4981" for this suite.

• [SLOW TEST:14.187 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":17,"skipped":236,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:49:31.932: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2197
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 08:49:33.105: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 08:49:36.116: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 08:49:36.118: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4398-crds.webhook.example.com via the AdmissionRegistration API
Dec 14 08:49:36.643: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:49:37.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2197" for this suite.
STEP: Destroying namespace "webhook-2197-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.458 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":18,"skipped":254,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:49:37.391: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1181
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Dec 14 08:49:38.084: INFO: created pod pod-service-account-defaultsa
Dec 14 08:49:38.084: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 14 08:49:38.096: INFO: created pod pod-service-account-mountsa
Dec 14 08:49:38.096: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 14 08:49:38.111: INFO: created pod pod-service-account-nomountsa
Dec 14 08:49:38.111: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 14 08:49:38.133: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 14 08:49:38.133: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 14 08:49:38.164: INFO: created pod pod-service-account-mountsa-mountspec
Dec 14 08:49:38.164: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 14 08:49:38.190: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 14 08:49:38.190: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 14 08:49:38.206: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 14 08:49:38.206: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 14 08:49:38.218: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 14 08:49:38.218: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 14 08:49:38.232: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 14 08:49:38.232: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:49:38.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1181" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":19,"skipped":274,"failed":0}

------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:49:38.249: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-322
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Dec 14 08:49:38.440: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 08:50:38.457: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 08:50:38.459: INFO: Starting informer...
STEP: Starting pods...
Dec 14 08:50:38.677: INFO: Pod1 is running on k8s-2. Tainting Node
Dec 14 08:50:40.894: INFO: Pod2 is running on k8s-2. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec 14 08:50:48.545: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 14 08:51:18.182: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:51:18.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-322" for this suite.

• [SLOW TEST:99.959 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":20,"skipped":274,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:51:18.209: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-6667
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 08:51:18.414: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6667
I1214 08:51:18.426328      18 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6667, replica count: 1
I1214 08:51:19.479784      18 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 08:51:20.480028      18 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 08:51:21.480177      18 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 08:51:21.594: INFO: Created: latency-svc-dcrcc
Dec 14 08:51:21.601: INFO: Got endpoints: latency-svc-dcrcc [21.496016ms]
Dec 14 08:51:21.621: INFO: Created: latency-svc-hrlxw
Dec 14 08:51:21.632: INFO: Got endpoints: latency-svc-hrlxw [30.143073ms]
Dec 14 08:51:21.634: INFO: Created: latency-svc-xrblt
Dec 14 08:51:21.643: INFO: Got endpoints: latency-svc-xrblt [40.216732ms]
Dec 14 08:51:21.645: INFO: Created: latency-svc-twjzs
Dec 14 08:51:21.657: INFO: Got endpoints: latency-svc-twjzs [55.033028ms]
Dec 14 08:51:21.658: INFO: Created: latency-svc-s4k96
Dec 14 08:51:21.667: INFO: Got endpoints: latency-svc-s4k96 [65.422286ms]
Dec 14 08:51:21.672: INFO: Created: latency-svc-jr8k6
Dec 14 08:51:21.685: INFO: Created: latency-svc-2ptlc
Dec 14 08:51:21.694: INFO: Got endpoints: latency-svc-jr8k6 [92.376262ms]
Dec 14 08:51:21.713: INFO: Got endpoints: latency-svc-2ptlc [110.9711ms]
Dec 14 08:51:21.732: INFO: Created: latency-svc-2wmf5
Dec 14 08:51:21.736: INFO: Got endpoints: latency-svc-2wmf5 [133.889328ms]
Dec 14 08:51:21.745: INFO: Created: latency-svc-2zxgb
Dec 14 08:51:21.754: INFO: Created: latency-svc-2jv7l
Dec 14 08:51:21.757: INFO: Got endpoints: latency-svc-2zxgb [154.266561ms]
Dec 14 08:51:21.766: INFO: Got endpoints: latency-svc-2jv7l [163.158173ms]
Dec 14 08:51:21.769: INFO: Created: latency-svc-9jjns
Dec 14 08:51:21.778: INFO: Created: latency-svc-shhln
Dec 14 08:51:21.783: INFO: Got endpoints: latency-svc-9jjns [179.893532ms]
Dec 14 08:51:21.789: INFO: Created: latency-svc-kfght
Dec 14 08:51:21.794: INFO: Got endpoints: latency-svc-shhln [191.175358ms]
Dec 14 08:51:21.803: INFO: Created: latency-svc-pjlnf
Dec 14 08:51:21.811: INFO: Got endpoints: latency-svc-kfght [208.492588ms]
Dec 14 08:51:21.817: INFO: Got endpoints: latency-svc-pjlnf [213.814034ms]
Dec 14 08:51:21.821: INFO: Created: latency-svc-jtvzr
Dec 14 08:51:21.825: INFO: Got endpoints: latency-svc-jtvzr [222.258225ms]
Dec 14 08:51:21.831: INFO: Created: latency-svc-8zh25
Dec 14 08:51:21.841: INFO: Created: latency-svc-pmdxr
Dec 14 08:51:21.845: INFO: Got endpoints: latency-svc-8zh25 [241.536693ms]
Dec 14 08:51:21.852: INFO: Got endpoints: latency-svc-pmdxr [220.222396ms]
Dec 14 08:51:21.857: INFO: Created: latency-svc-2z78c
Dec 14 08:51:21.865: INFO: Created: latency-svc-s8b65
Dec 14 08:51:21.870: INFO: Got endpoints: latency-svc-2z78c [226.5939ms]
Dec 14 08:51:21.877: INFO: Created: latency-svc-khqnd
Dec 14 08:51:21.883: INFO: Got endpoints: latency-svc-s8b65 [226.010419ms]
Dec 14 08:51:21.887: INFO: Got endpoints: latency-svc-khqnd [220.017846ms]
Dec 14 08:51:21.893: INFO: Created: latency-svc-n7ggc
Dec 14 08:51:21.901: INFO: Created: latency-svc-q4lgg
Dec 14 08:51:21.905: INFO: Got endpoints: latency-svc-n7ggc [209.99612ms]
Dec 14 08:51:21.913: INFO: Got endpoints: latency-svc-q4lgg [200.055165ms]
Dec 14 08:51:21.917: INFO: Created: latency-svc-wchwq
Dec 14 08:51:21.927: INFO: Got endpoints: latency-svc-wchwq [190.337596ms]
Dec 14 08:51:21.932: INFO: Created: latency-svc-h4q6j
Dec 14 08:51:21.949: INFO: Got endpoints: latency-svc-h4q6j [192.111879ms]
Dec 14 08:51:21.950: INFO: Created: latency-svc-k4k78
Dec 14 08:51:21.960: INFO: Created: latency-svc-jvnr4
Dec 14 08:51:21.967: INFO: Got endpoints: latency-svc-k4k78 [201.473483ms]
Dec 14 08:51:21.972: INFO: Got endpoints: latency-svc-jvnr4 [189.61533ms]
Dec 14 08:51:21.976: INFO: Created: latency-svc-9mdrn
Dec 14 08:51:21.983: INFO: Got endpoints: latency-svc-9mdrn [189.09216ms]
Dec 14 08:51:21.988: INFO: Created: latency-svc-h94bc
Dec 14 08:51:21.997: INFO: Created: latency-svc-mfcqf
Dec 14 08:51:22.005: INFO: Got endpoints: latency-svc-h94bc [37.292246ms]
Dec 14 08:51:22.008: INFO: Created: latency-svc-fl7v8
Dec 14 08:51:22.021: INFO: Got endpoints: latency-svc-mfcqf [209.418101ms]
Dec 14 08:51:22.024: INFO: Got endpoints: latency-svc-fl7v8 [207.522828ms]
Dec 14 08:51:22.032: INFO: Created: latency-svc-bzrtc
Dec 14 08:51:22.042: INFO: Created: latency-svc-gxxxs
Dec 14 08:51:22.049: INFO: Got endpoints: latency-svc-bzrtc [223.902461ms]
Dec 14 08:51:22.061: INFO: Created: latency-svc-jcvf5
Dec 14 08:51:22.066: INFO: Created: latency-svc-sfrwg
Dec 14 08:51:22.068: INFO: Got endpoints: latency-svc-gxxxs [223.162114ms]
Dec 14 08:51:22.082: INFO: Created: latency-svc-8ft67
Dec 14 08:51:22.091: INFO: Got endpoints: latency-svc-sfrwg [221.027562ms]
Dec 14 08:51:22.092: INFO: Got endpoints: latency-svc-jcvf5 [239.485381ms]
Dec 14 08:51:22.099: INFO: Got endpoints: latency-svc-8ft67 [216.039389ms]
Dec 14 08:51:22.102: INFO: Created: latency-svc-4bvwr
Dec 14 08:51:22.111: INFO: Got endpoints: latency-svc-4bvwr [223.969647ms]
Dec 14 08:51:22.112: INFO: Created: latency-svc-d5gsj
Dec 14 08:51:22.123: INFO: Created: latency-svc-6vszl
Dec 14 08:51:22.130: INFO: Got endpoints: latency-svc-d5gsj [225.4175ms]
Dec 14 08:51:22.138: INFO: Got endpoints: latency-svc-6vszl [224.650237ms]
Dec 14 08:51:22.141: INFO: Created: latency-svc-9v4wk
Dec 14 08:51:22.149: INFO: Got endpoints: latency-svc-9v4wk [222.483204ms]
Dec 14 08:51:22.153: INFO: Created: latency-svc-c4swp
Dec 14 08:51:22.163: INFO: Created: latency-svc-bpbnj
Dec 14 08:51:22.170: INFO: Got endpoints: latency-svc-c4swp [221.076881ms]
Dec 14 08:51:22.174: INFO: Created: latency-svc-qcvx4
Dec 14 08:51:22.180: INFO: Got endpoints: latency-svc-bpbnj [207.548499ms]
Dec 14 08:51:22.187: INFO: Created: latency-svc-r5mwg
Dec 14 08:51:22.192: INFO: Created: latency-svc-tpx8r
Dec 14 08:51:22.200: INFO: Created: latency-svc-s7vxw
Dec 14 08:51:22.206: INFO: Got endpoints: latency-svc-qcvx4 [222.825014ms]
Dec 14 08:51:22.211: INFO: Created: latency-svc-z7bzm
Dec 14 08:51:22.217: INFO: Created: latency-svc-lwlnc
Dec 14 08:51:22.224: INFO: Created: latency-svc-gcb7t
Dec 14 08:51:22.231: INFO: Created: latency-svc-xhdzq
Dec 14 08:51:22.241: INFO: Created: latency-svc-wwkm8
Dec 14 08:51:22.243: INFO: Created: latency-svc-4sf9b
Dec 14 08:51:22.254: INFO: Created: latency-svc-wljl8
Dec 14 08:51:22.260: INFO: Got endpoints: latency-svc-r5mwg [255.031546ms]
Dec 14 08:51:22.284: INFO: Created: latency-svc-sx7sv
Dec 14 08:51:22.304: INFO: Got endpoints: latency-svc-tpx8r [283.602807ms]
Dec 14 08:51:22.333: INFO: Created: latency-svc-hntqw
Dec 14 08:51:22.333: INFO: Created: latency-svc-bkmts
Dec 14 08:51:22.340: INFO: Created: latency-svc-7m8wf
Dec 14 08:51:22.343: INFO: Created: latency-svc-hxrtz
Dec 14 08:51:22.352: INFO: Created: latency-svc-6v2tn
Dec 14 08:51:22.357: INFO: Got endpoints: latency-svc-s7vxw [332.159281ms]
Dec 14 08:51:22.362: INFO: Created: latency-svc-snmpt
Dec 14 08:51:22.370: INFO: Created: latency-svc-zfqb9
Dec 14 08:51:22.396: INFO: Got endpoints: latency-svc-z7bzm [346.539487ms]
Dec 14 08:51:22.401: INFO: Created: latency-svc-prr8v
Dec 14 08:51:22.446: INFO: Got endpoints: latency-svc-lwlnc [377.947328ms]
Dec 14 08:51:22.452: INFO: Created: latency-svc-68gg2
Dec 14 08:51:22.495: INFO: Got endpoints: latency-svc-gcb7t [404.073844ms]
Dec 14 08:51:22.502: INFO: Created: latency-svc-kwbsd
Dec 14 08:51:22.547: INFO: Got endpoints: latency-svc-xhdzq [455.840744ms]
Dec 14 08:51:22.554: INFO: Created: latency-svc-rw9vb
Dec 14 08:51:22.596: INFO: Got endpoints: latency-svc-wwkm8 [496.722864ms]
Dec 14 08:51:22.601: INFO: Created: latency-svc-vh6h4
Dec 14 08:51:22.646: INFO: Got endpoints: latency-svc-4sf9b [534.074371ms]
Dec 14 08:51:22.652: INFO: Created: latency-svc-wkpj6
Dec 14 08:51:22.696: INFO: Got endpoints: latency-svc-wljl8 [565.688896ms]
Dec 14 08:51:22.702: INFO: Created: latency-svc-fx8hc
Dec 14 08:51:22.746: INFO: Got endpoints: latency-svc-sx7sv [607.539029ms]
Dec 14 08:51:22.751: INFO: Created: latency-svc-lmsmd
Dec 14 08:51:22.796: INFO: Got endpoints: latency-svc-hntqw [646.760278ms]
Dec 14 08:51:22.802: INFO: Created: latency-svc-2gxpf
Dec 14 08:51:22.847: INFO: Got endpoints: latency-svc-bkmts [676.888361ms]
Dec 14 08:51:22.854: INFO: Created: latency-svc-2fzd4
Dec 14 08:51:22.897: INFO: Got endpoints: latency-svc-7m8wf [717.085056ms]
Dec 14 08:51:22.904: INFO: Created: latency-svc-hmnts
Dec 14 08:51:22.947: INFO: Got endpoints: latency-svc-hxrtz [740.650096ms]
Dec 14 08:51:22.952: INFO: Created: latency-svc-wgm78
Dec 14 08:51:22.996: INFO: Got endpoints: latency-svc-6v2tn [736.151232ms]
Dec 14 08:51:23.002: INFO: Created: latency-svc-h8ftj
Dec 14 08:51:23.047: INFO: Got endpoints: latency-svc-snmpt [742.261906ms]
Dec 14 08:51:23.053: INFO: Created: latency-svc-mlkdr
Dec 14 08:51:23.096: INFO: Got endpoints: latency-svc-zfqb9 [739.187844ms]
Dec 14 08:51:23.104: INFO: Created: latency-svc-45nvt
Dec 14 08:51:23.147: INFO: Got endpoints: latency-svc-prr8v [750.880052ms]
Dec 14 08:51:23.153: INFO: Created: latency-svc-mzcnb
Dec 14 08:51:23.196: INFO: Got endpoints: latency-svc-68gg2 [750.075156ms]
Dec 14 08:51:23.203: INFO: Created: latency-svc-w5gvj
Dec 14 08:51:23.255: INFO: Got endpoints: latency-svc-kwbsd [759.851016ms]
Dec 14 08:51:23.267: INFO: Created: latency-svc-mr4c2
Dec 14 08:51:23.303: INFO: Got endpoints: latency-svc-rw9vb [755.256601ms]
Dec 14 08:51:23.314: INFO: Created: latency-svc-g9b7f
Dec 14 08:51:23.354: INFO: Got endpoints: latency-svc-vh6h4 [758.189371ms]
Dec 14 08:51:23.368: INFO: Created: latency-svc-6gnds
Dec 14 08:51:23.399: INFO: Got endpoints: latency-svc-wkpj6 [753.157799ms]
Dec 14 08:51:23.412: INFO: Created: latency-svc-t7cbn
Dec 14 08:51:23.447: INFO: Got endpoints: latency-svc-fx8hc [750.666556ms]
Dec 14 08:51:23.471: INFO: Created: latency-svc-87zws
Dec 14 08:51:23.506: INFO: Got endpoints: latency-svc-lmsmd [760.502071ms]
Dec 14 08:51:23.529: INFO: Created: latency-svc-77n7w
Dec 14 08:51:23.549: INFO: Got endpoints: latency-svc-2gxpf [752.691817ms]
Dec 14 08:51:23.560: INFO: Created: latency-svc-c62vq
Dec 14 08:51:23.596: INFO: Got endpoints: latency-svc-2fzd4 [749.378066ms]
Dec 14 08:51:23.608: INFO: Created: latency-svc-4r2c8
Dec 14 08:51:23.647: INFO: Got endpoints: latency-svc-hmnts [749.404053ms]
Dec 14 08:51:23.651: INFO: Created: latency-svc-4zdsh
Dec 14 08:51:23.696: INFO: Got endpoints: latency-svc-wgm78 [749.2765ms]
Dec 14 08:51:23.702: INFO: Created: latency-svc-99tmv
Dec 14 08:51:23.746: INFO: Got endpoints: latency-svc-h8ftj [749.927666ms]
Dec 14 08:51:23.753: INFO: Created: latency-svc-hxpmn
Dec 14 08:51:23.796: INFO: Got endpoints: latency-svc-mlkdr [748.846527ms]
Dec 14 08:51:23.801: INFO: Created: latency-svc-2cgxk
Dec 14 08:51:23.845: INFO: Got endpoints: latency-svc-45nvt [749.433079ms]
Dec 14 08:51:23.852: INFO: Created: latency-svc-jrbml
Dec 14 08:51:23.897: INFO: Got endpoints: latency-svc-mzcnb [750.022032ms]
Dec 14 08:51:23.905: INFO: Created: latency-svc-x9lq9
Dec 14 08:51:23.946: INFO: Got endpoints: latency-svc-w5gvj [749.695276ms]
Dec 14 08:51:23.952: INFO: Created: latency-svc-zfbwr
Dec 14 08:51:23.996: INFO: Got endpoints: latency-svc-mr4c2 [740.805039ms]
Dec 14 08:51:24.002: INFO: Created: latency-svc-2kqpx
Dec 14 08:51:24.045: INFO: Got endpoints: latency-svc-g9b7f [742.654218ms]
Dec 14 08:51:24.056: INFO: Created: latency-svc-87dpp
Dec 14 08:51:24.096: INFO: Got endpoints: latency-svc-6gnds [742.183388ms]
Dec 14 08:51:24.103: INFO: Created: latency-svc-kqw5d
Dec 14 08:51:24.146: INFO: Got endpoints: latency-svc-t7cbn [747.412853ms]
Dec 14 08:51:24.153: INFO: Created: latency-svc-vhl56
Dec 14 08:51:24.201: INFO: Got endpoints: latency-svc-87zws [754.629686ms]
Dec 14 08:51:24.208: INFO: Created: latency-svc-4859p
Dec 14 08:51:24.247: INFO: Got endpoints: latency-svc-77n7w [740.55601ms]
Dec 14 08:51:24.254: INFO: Created: latency-svc-rj5js
Dec 14 08:51:24.298: INFO: Got endpoints: latency-svc-c62vq [749.461196ms]
Dec 14 08:51:24.305: INFO: Created: latency-svc-srpx9
Dec 14 08:51:24.351: INFO: Got endpoints: latency-svc-4r2c8 [754.536324ms]
Dec 14 08:51:24.360: INFO: Created: latency-svc-l49zs
Dec 14 08:51:24.398: INFO: Got endpoints: latency-svc-4zdsh [750.927094ms]
Dec 14 08:51:24.404: INFO: Created: latency-svc-fnvhn
Dec 14 08:51:24.447: INFO: Got endpoints: latency-svc-99tmv [751.103151ms]
Dec 14 08:51:24.456: INFO: Created: latency-svc-vz65v
Dec 14 08:51:24.497: INFO: Got endpoints: latency-svc-hxpmn [751.281247ms]
Dec 14 08:51:24.507: INFO: Created: latency-svc-h7v8v
Dec 14 08:51:24.546: INFO: Got endpoints: latency-svc-2cgxk [750.594061ms]
Dec 14 08:51:24.555: INFO: Created: latency-svc-69x78
Dec 14 08:51:24.597: INFO: Got endpoints: latency-svc-jrbml [751.700903ms]
Dec 14 08:51:24.606: INFO: Created: latency-svc-5czdv
Dec 14 08:51:24.647: INFO: Got endpoints: latency-svc-x9lq9 [749.794961ms]
Dec 14 08:51:24.657: INFO: Created: latency-svc-nmmdg
Dec 14 08:51:24.697: INFO: Got endpoints: latency-svc-zfbwr [751.287972ms]
Dec 14 08:51:24.705: INFO: Created: latency-svc-mlxmq
Dec 14 08:51:24.747: INFO: Got endpoints: latency-svc-2kqpx [750.74951ms]
Dec 14 08:51:24.754: INFO: Created: latency-svc-m7pg9
Dec 14 08:51:24.797: INFO: Got endpoints: latency-svc-87dpp [751.237837ms]
Dec 14 08:51:24.803: INFO: Created: latency-svc-jl8j2
Dec 14 08:51:24.847: INFO: Got endpoints: latency-svc-kqw5d [750.551262ms]
Dec 14 08:51:24.856: INFO: Created: latency-svc-8qsb8
Dec 14 08:51:24.897: INFO: Got endpoints: latency-svc-vhl56 [750.989918ms]
Dec 14 08:51:24.905: INFO: Created: latency-svc-grcb9
Dec 14 08:51:24.948: INFO: Got endpoints: latency-svc-4859p [746.473101ms]
Dec 14 08:51:24.958: INFO: Created: latency-svc-fq8xp
Dec 14 08:51:24.998: INFO: Got endpoints: latency-svc-rj5js [750.729017ms]
Dec 14 08:51:25.011: INFO: Created: latency-svc-jpzb6
Dec 14 08:51:25.047: INFO: Got endpoints: latency-svc-srpx9 [748.550518ms]
Dec 14 08:51:25.053: INFO: Created: latency-svc-x8lk4
Dec 14 08:51:25.097: INFO: Got endpoints: latency-svc-l49zs [745.934917ms]
Dec 14 08:51:25.104: INFO: Created: latency-svc-wmtzl
Dec 14 08:51:25.147: INFO: Got endpoints: latency-svc-fnvhn [749.478206ms]
Dec 14 08:51:25.172: INFO: Created: latency-svc-smmsx
Dec 14 08:51:25.197: INFO: Got endpoints: latency-svc-vz65v [749.787671ms]
Dec 14 08:51:25.205: INFO: Created: latency-svc-pt95k
Dec 14 08:51:25.247: INFO: Got endpoints: latency-svc-h7v8v [749.92164ms]
Dec 14 08:51:25.254: INFO: Created: latency-svc-r8sf7
Dec 14 08:51:25.299: INFO: Got endpoints: latency-svc-69x78 [752.372012ms]
Dec 14 08:51:25.306: INFO: Created: latency-svc-5k2sw
Dec 14 08:51:25.347: INFO: Got endpoints: latency-svc-5czdv [749.778461ms]
Dec 14 08:51:25.353: INFO: Created: latency-svc-ks6wg
Dec 14 08:51:25.397: INFO: Got endpoints: latency-svc-nmmdg [749.757215ms]
Dec 14 08:51:25.406: INFO: Created: latency-svc-tvtq6
Dec 14 08:51:25.446: INFO: Got endpoints: latency-svc-mlxmq [748.71158ms]
Dec 14 08:51:25.453: INFO: Created: latency-svc-ggtpx
Dec 14 08:51:25.497: INFO: Got endpoints: latency-svc-m7pg9 [750.156236ms]
Dec 14 08:51:25.505: INFO: Created: latency-svc-cmqms
Dec 14 08:51:25.547: INFO: Got endpoints: latency-svc-jl8j2 [750.209057ms]
Dec 14 08:51:25.553: INFO: Created: latency-svc-dkj62
Dec 14 08:51:25.596: INFO: Got endpoints: latency-svc-8qsb8 [749.379205ms]
Dec 14 08:51:25.604: INFO: Created: latency-svc-csgqn
Dec 14 08:51:25.647: INFO: Got endpoints: latency-svc-grcb9 [749.161683ms]
Dec 14 08:51:25.653: INFO: Created: latency-svc-8tp7m
Dec 14 08:51:25.696: INFO: Got endpoints: latency-svc-fq8xp [748.4059ms]
Dec 14 08:51:25.705: INFO: Created: latency-svc-b2b8t
Dec 14 08:51:25.746: INFO: Got endpoints: latency-svc-jpzb6 [748.553446ms]
Dec 14 08:51:25.753: INFO: Created: latency-svc-5kj9x
Dec 14 08:51:25.797: INFO: Got endpoints: latency-svc-x8lk4 [749.553548ms]
Dec 14 08:51:25.803: INFO: Created: latency-svc-8mjqx
Dec 14 08:51:25.846: INFO: Got endpoints: latency-svc-wmtzl [749.301781ms]
Dec 14 08:51:25.855: INFO: Created: latency-svc-vvzwl
Dec 14 08:51:25.896: INFO: Got endpoints: latency-svc-smmsx [748.62458ms]
Dec 14 08:51:25.904: INFO: Created: latency-svc-sl6r5
Dec 14 08:51:25.946: INFO: Got endpoints: latency-svc-pt95k [749.181838ms]
Dec 14 08:51:25.953: INFO: Created: latency-svc-kpz9w
Dec 14 08:51:25.996: INFO: Got endpoints: latency-svc-r8sf7 [748.96755ms]
Dec 14 08:51:26.003: INFO: Created: latency-svc-bjwrz
Dec 14 08:51:26.048: INFO: Got endpoints: latency-svc-5k2sw [749.196524ms]
Dec 14 08:51:26.055: INFO: Created: latency-svc-xt8c6
Dec 14 08:51:26.096: INFO: Got endpoints: latency-svc-ks6wg [748.915988ms]
Dec 14 08:51:26.103: INFO: Created: latency-svc-86p8b
Dec 14 08:51:26.146: INFO: Got endpoints: latency-svc-tvtq6 [749.323258ms]
Dec 14 08:51:26.153: INFO: Created: latency-svc-z5pgv
Dec 14 08:51:26.195: INFO: Got endpoints: latency-svc-ggtpx [749.475044ms]
Dec 14 08:51:26.201: INFO: Created: latency-svc-59ll9
Dec 14 08:51:26.246: INFO: Got endpoints: latency-svc-cmqms [748.764076ms]
Dec 14 08:51:26.253: INFO: Created: latency-svc-jqss7
Dec 14 08:51:26.296: INFO: Got endpoints: latency-svc-dkj62 [749.036644ms]
Dec 14 08:51:26.303: INFO: Created: latency-svc-vhcfv
Dec 14 08:51:26.346: INFO: Got endpoints: latency-svc-csgqn [749.499357ms]
Dec 14 08:51:26.357: INFO: Created: latency-svc-zprkg
Dec 14 08:51:26.396: INFO: Got endpoints: latency-svc-8tp7m [749.632734ms]
Dec 14 08:51:26.403: INFO: Created: latency-svc-45c9x
Dec 14 08:51:26.446: INFO: Got endpoints: latency-svc-b2b8t [749.770047ms]
Dec 14 08:51:26.455: INFO: Created: latency-svc-46262
Dec 14 08:51:26.496: INFO: Got endpoints: latency-svc-5kj9x [749.417763ms]
Dec 14 08:51:26.505: INFO: Created: latency-svc-glnxb
Dec 14 08:51:26.547: INFO: Got endpoints: latency-svc-8mjqx [749.958018ms]
Dec 14 08:51:26.552: INFO: Created: latency-svc-6v85r
Dec 14 08:51:26.597: INFO: Got endpoints: latency-svc-vvzwl [750.730021ms]
Dec 14 08:51:26.604: INFO: Created: latency-svc-wdf5r
Dec 14 08:51:26.647: INFO: Got endpoints: latency-svc-sl6r5 [751.524378ms]
Dec 14 08:51:26.657: INFO: Created: latency-svc-qm2xj
Dec 14 08:51:26.697: INFO: Got endpoints: latency-svc-kpz9w [750.296037ms]
Dec 14 08:51:26.703: INFO: Created: latency-svc-hzw9v
Dec 14 08:51:26.746: INFO: Got endpoints: latency-svc-bjwrz [749.522032ms]
Dec 14 08:51:26.756: INFO: Created: latency-svc-k7hbb
Dec 14 08:51:26.797: INFO: Got endpoints: latency-svc-xt8c6 [748.716651ms]
Dec 14 08:51:26.804: INFO: Created: latency-svc-njslg
Dec 14 08:51:26.847: INFO: Got endpoints: latency-svc-86p8b [750.591378ms]
Dec 14 08:51:26.855: INFO: Created: latency-svc-98nsl
Dec 14 08:51:26.897: INFO: Got endpoints: latency-svc-z5pgv [750.9282ms]
Dec 14 08:51:26.905: INFO: Created: latency-svc-zt22z
Dec 14 08:51:26.947: INFO: Got endpoints: latency-svc-59ll9 [751.163887ms]
Dec 14 08:51:26.954: INFO: Created: latency-svc-rhmj4
Dec 14 08:51:26.999: INFO: Got endpoints: latency-svc-jqss7 [752.693753ms]
Dec 14 08:51:27.007: INFO: Created: latency-svc-xhx4n
Dec 14 08:51:27.046: INFO: Got endpoints: latency-svc-vhcfv [749.530168ms]
Dec 14 08:51:27.055: INFO: Created: latency-svc-h9rfr
Dec 14 08:51:27.097: INFO: Got endpoints: latency-svc-zprkg [750.569351ms]
Dec 14 08:51:27.102: INFO: Created: latency-svc-cx7qw
Dec 14 08:51:27.150: INFO: Got endpoints: latency-svc-45c9x [753.445532ms]
Dec 14 08:51:27.177: INFO: Created: latency-svc-8m8dq
Dec 14 08:51:27.196: INFO: Got endpoints: latency-svc-46262 [750.252353ms]
Dec 14 08:51:27.205: INFO: Created: latency-svc-b7dd7
Dec 14 08:51:27.246: INFO: Got endpoints: latency-svc-glnxb [750.254969ms]
Dec 14 08:51:27.253: INFO: Created: latency-svc-xbgvz
Dec 14 08:51:27.298: INFO: Got endpoints: latency-svc-6v85r [750.917864ms]
Dec 14 08:51:27.305: INFO: Created: latency-svc-xnrsd
Dec 14 08:51:27.346: INFO: Got endpoints: latency-svc-wdf5r [748.9616ms]
Dec 14 08:51:27.354: INFO: Created: latency-svc-6rrk4
Dec 14 08:51:27.397: INFO: Got endpoints: latency-svc-qm2xj [749.360012ms]
Dec 14 08:51:27.403: INFO: Created: latency-svc-94dg4
Dec 14 08:51:27.447: INFO: Got endpoints: latency-svc-hzw9v [750.059718ms]
Dec 14 08:51:27.454: INFO: Created: latency-svc-gx986
Dec 14 08:51:27.497: INFO: Got endpoints: latency-svc-k7hbb [750.783659ms]
Dec 14 08:51:27.504: INFO: Created: latency-svc-tjs6t
Dec 14 08:51:27.548: INFO: Got endpoints: latency-svc-njslg [750.683145ms]
Dec 14 08:51:27.554: INFO: Created: latency-svc-25fzn
Dec 14 08:51:27.595: INFO: Got endpoints: latency-svc-98nsl [748.777429ms]
Dec 14 08:51:27.605: INFO: Created: latency-svc-7thdf
Dec 14 08:51:27.646: INFO: Got endpoints: latency-svc-zt22z [749.199461ms]
Dec 14 08:51:27.653: INFO: Created: latency-svc-p6m27
Dec 14 08:51:27.696: INFO: Got endpoints: latency-svc-rhmj4 [749.346946ms]
Dec 14 08:51:27.701: INFO: Created: latency-svc-v4c4c
Dec 14 08:51:27.746: INFO: Got endpoints: latency-svc-xhx4n [746.734427ms]
Dec 14 08:51:27.754: INFO: Created: latency-svc-n6mdg
Dec 14 08:51:27.797: INFO: Got endpoints: latency-svc-h9rfr [751.128519ms]
Dec 14 08:51:27.802: INFO: Created: latency-svc-plx79
Dec 14 08:51:27.846: INFO: Got endpoints: latency-svc-cx7qw [749.546061ms]
Dec 14 08:51:27.853: INFO: Created: latency-svc-rcb4c
Dec 14 08:51:27.897: INFO: Got endpoints: latency-svc-8m8dq [746.578308ms]
Dec 14 08:51:27.905: INFO: Created: latency-svc-r8fr6
Dec 14 08:51:27.946: INFO: Got endpoints: latency-svc-b7dd7 [749.235161ms]
Dec 14 08:51:27.952: INFO: Created: latency-svc-ffmst
Dec 14 08:51:27.996: INFO: Got endpoints: latency-svc-xbgvz [749.791127ms]
Dec 14 08:51:28.007: INFO: Created: latency-svc-fmv52
Dec 14 08:51:28.047: INFO: Got endpoints: latency-svc-xnrsd [748.925882ms]
Dec 14 08:51:28.053: INFO: Created: latency-svc-nzbpx
Dec 14 08:51:28.097: INFO: Got endpoints: latency-svc-6rrk4 [750.252737ms]
Dec 14 08:51:28.103: INFO: Created: latency-svc-vf4vk
Dec 14 08:51:28.147: INFO: Got endpoints: latency-svc-94dg4 [750.470691ms]
Dec 14 08:51:28.154: INFO: Created: latency-svc-qqgwl
Dec 14 08:51:28.197: INFO: Got endpoints: latency-svc-gx986 [749.602826ms]
Dec 14 08:51:28.204: INFO: Created: latency-svc-c5mwd
Dec 14 08:51:28.248: INFO: Got endpoints: latency-svc-tjs6t [750.911027ms]
Dec 14 08:51:28.257: INFO: Created: latency-svc-q49x6
Dec 14 08:51:28.297: INFO: Got endpoints: latency-svc-25fzn [749.182886ms]
Dec 14 08:51:28.307: INFO: Created: latency-svc-rdtkj
Dec 14 08:51:28.347: INFO: Got endpoints: latency-svc-7thdf [751.325457ms]
Dec 14 08:51:28.354: INFO: Created: latency-svc-n7rzv
Dec 14 08:51:28.398: INFO: Got endpoints: latency-svc-p6m27 [751.452999ms]
Dec 14 08:51:28.407: INFO: Created: latency-svc-q8k77
Dec 14 08:51:28.447: INFO: Got endpoints: latency-svc-v4c4c [750.887973ms]
Dec 14 08:51:28.454: INFO: Created: latency-svc-qkfkb
Dec 14 08:51:28.497: INFO: Got endpoints: latency-svc-n6mdg [751.305711ms]
Dec 14 08:51:28.505: INFO: Created: latency-svc-ffzc2
Dec 14 08:51:28.547: INFO: Got endpoints: latency-svc-plx79 [750.028353ms]
Dec 14 08:51:28.553: INFO: Created: latency-svc-ld6kl
Dec 14 08:51:28.597: INFO: Got endpoints: latency-svc-rcb4c [750.650653ms]
Dec 14 08:51:28.605: INFO: Created: latency-svc-x4sq5
Dec 14 08:51:28.646: INFO: Got endpoints: latency-svc-r8fr6 [749.640894ms]
Dec 14 08:51:28.655: INFO: Created: latency-svc-2rgh2
Dec 14 08:51:28.696: INFO: Got endpoints: latency-svc-ffmst [749.815549ms]
Dec 14 08:51:28.706: INFO: Created: latency-svc-h5qv4
Dec 14 08:51:28.747: INFO: Got endpoints: latency-svc-fmv52 [750.837665ms]
Dec 14 08:51:28.753: INFO: Created: latency-svc-8kn47
Dec 14 08:51:28.795: INFO: Got endpoints: latency-svc-nzbpx [748.723727ms]
Dec 14 08:51:28.801: INFO: Created: latency-svc-qz7bc
Dec 14 08:51:28.848: INFO: Got endpoints: latency-svc-vf4vk [751.272209ms]
Dec 14 08:51:28.854: INFO: Created: latency-svc-vgb4k
Dec 14 08:51:28.897: INFO: Got endpoints: latency-svc-qqgwl [749.231449ms]
Dec 14 08:51:28.902: INFO: Created: latency-svc-vsn5p
Dec 14 08:51:28.947: INFO: Got endpoints: latency-svc-c5mwd [750.015309ms]
Dec 14 08:51:28.952: INFO: Created: latency-svc-j8sh7
Dec 14 08:51:28.996: INFO: Got endpoints: latency-svc-q49x6 [748.209488ms]
Dec 14 08:51:29.003: INFO: Created: latency-svc-zq6qk
Dec 14 08:51:29.046: INFO: Got endpoints: latency-svc-rdtkj [749.10598ms]
Dec 14 08:51:29.052: INFO: Created: latency-svc-5tb7t
Dec 14 08:51:29.096: INFO: Got endpoints: latency-svc-n7rzv [749.037029ms]
Dec 14 08:51:29.105: INFO: Created: latency-svc-qsw7j
Dec 14 08:51:29.146: INFO: Got endpoints: latency-svc-q8k77 [748.047417ms]
Dec 14 08:51:29.152: INFO: Created: latency-svc-7q568
Dec 14 08:51:29.196: INFO: Got endpoints: latency-svc-qkfkb [749.251779ms]
Dec 14 08:51:29.211: INFO: Created: latency-svc-f47dm
Dec 14 08:51:29.246: INFO: Got endpoints: latency-svc-ffzc2 [749.248285ms]
Dec 14 08:51:29.254: INFO: Created: latency-svc-nrjkc
Dec 14 08:51:29.305: INFO: Got endpoints: latency-svc-ld6kl [757.957207ms]
Dec 14 08:51:29.316: INFO: Created: latency-svc-xwv2t
Dec 14 08:51:29.347: INFO: Got endpoints: latency-svc-x4sq5 [750.25836ms]
Dec 14 08:51:29.354: INFO: Created: latency-svc-bmcml
Dec 14 08:51:29.396: INFO: Got endpoints: latency-svc-2rgh2 [749.923149ms]
Dec 14 08:51:29.403: INFO: Created: latency-svc-dtthf
Dec 14 08:51:29.447: INFO: Got endpoints: latency-svc-h5qv4 [751.242982ms]
Dec 14 08:51:29.497: INFO: Got endpoints: latency-svc-8kn47 [749.564012ms]
Dec 14 08:51:29.546: INFO: Got endpoints: latency-svc-qz7bc [751.030193ms]
Dec 14 08:51:29.597: INFO: Got endpoints: latency-svc-vgb4k [748.792571ms]
Dec 14 08:51:29.646: INFO: Got endpoints: latency-svc-vsn5p [749.659618ms]
Dec 14 08:51:29.697: INFO: Got endpoints: latency-svc-j8sh7 [750.046019ms]
Dec 14 08:51:29.747: INFO: Got endpoints: latency-svc-zq6qk [750.749492ms]
Dec 14 08:51:29.796: INFO: Got endpoints: latency-svc-5tb7t [749.878574ms]
Dec 14 08:51:29.848: INFO: Got endpoints: latency-svc-qsw7j [752.369766ms]
Dec 14 08:51:29.897: INFO: Got endpoints: latency-svc-7q568 [751.012602ms]
Dec 14 08:51:29.947: INFO: Got endpoints: latency-svc-f47dm [750.556902ms]
Dec 14 08:51:29.996: INFO: Got endpoints: latency-svc-nrjkc [749.810282ms]
Dec 14 08:51:30.046: INFO: Got endpoints: latency-svc-xwv2t [741.040099ms]
Dec 14 08:51:30.096: INFO: Got endpoints: latency-svc-bmcml [748.428943ms]
Dec 14 08:51:30.147: INFO: Got endpoints: latency-svc-dtthf [750.204498ms]
Dec 14 08:51:30.147: INFO: Latencies: [30.143073ms 37.292246ms 40.216732ms 55.033028ms 65.422286ms 92.376262ms 110.9711ms 133.889328ms 154.266561ms 163.158173ms 179.893532ms 189.09216ms 189.61533ms 190.337596ms 191.175358ms 192.111879ms 200.055165ms 201.473483ms 207.522828ms 207.548499ms 208.492588ms 209.418101ms 209.99612ms 213.814034ms 216.039389ms 220.017846ms 220.222396ms 221.027562ms 221.076881ms 222.258225ms 222.483204ms 222.825014ms 223.162114ms 223.902461ms 223.969647ms 224.650237ms 225.4175ms 226.010419ms 226.5939ms 239.485381ms 241.536693ms 255.031546ms 283.602807ms 332.159281ms 346.539487ms 377.947328ms 404.073844ms 455.840744ms 496.722864ms 534.074371ms 565.688896ms 607.539029ms 646.760278ms 676.888361ms 717.085056ms 736.151232ms 739.187844ms 740.55601ms 740.650096ms 740.805039ms 741.040099ms 742.183388ms 742.261906ms 742.654218ms 745.934917ms 746.473101ms 746.578308ms 746.734427ms 747.412853ms 748.047417ms 748.209488ms 748.4059ms 748.428943ms 748.550518ms 748.553446ms 748.62458ms 748.71158ms 748.716651ms 748.723727ms 748.764076ms 748.777429ms 748.792571ms 748.846527ms 748.915988ms 748.925882ms 748.9616ms 748.96755ms 749.036644ms 749.037029ms 749.10598ms 749.161683ms 749.181838ms 749.182886ms 749.196524ms 749.199461ms 749.231449ms 749.235161ms 749.248285ms 749.251779ms 749.2765ms 749.301781ms 749.323258ms 749.346946ms 749.360012ms 749.378066ms 749.379205ms 749.404053ms 749.417763ms 749.433079ms 749.461196ms 749.475044ms 749.478206ms 749.499357ms 749.522032ms 749.530168ms 749.546061ms 749.553548ms 749.564012ms 749.602826ms 749.632734ms 749.640894ms 749.659618ms 749.695276ms 749.757215ms 749.770047ms 749.778461ms 749.787671ms 749.791127ms 749.794961ms 749.810282ms 749.815549ms 749.878574ms 749.92164ms 749.923149ms 749.927666ms 749.958018ms 750.015309ms 750.022032ms 750.028353ms 750.046019ms 750.059718ms 750.075156ms 750.156236ms 750.204498ms 750.209057ms 750.252353ms 750.252737ms 750.254969ms 750.25836ms 750.296037ms 750.470691ms 750.551262ms 750.556902ms 750.569351ms 750.591378ms 750.594061ms 750.650653ms 750.666556ms 750.683145ms 750.729017ms 750.730021ms 750.749492ms 750.74951ms 750.783659ms 750.837665ms 750.880052ms 750.887973ms 750.911027ms 750.917864ms 750.927094ms 750.9282ms 750.989918ms 751.012602ms 751.030193ms 751.103151ms 751.128519ms 751.163887ms 751.237837ms 751.242982ms 751.272209ms 751.281247ms 751.287972ms 751.305711ms 751.325457ms 751.452999ms 751.524378ms 751.700903ms 752.369766ms 752.372012ms 752.691817ms 752.693753ms 753.157799ms 753.445532ms 754.536324ms 754.629686ms 755.256601ms 757.957207ms 758.189371ms 759.851016ms 760.502071ms]
Dec 14 08:51:30.147: INFO: 50 %ile: 749.301781ms
Dec 14 08:51:30.147: INFO: 90 %ile: 751.281247ms
Dec 14 08:51:30.147: INFO: 99 %ile: 759.851016ms
Dec 14 08:51:30.147: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:51:30.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6667" for this suite.

• [SLOW TEST:11.947 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":21,"skipped":294,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:51:30.157: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 08:51:30.314: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7db7cb87-a899-4c4d-9c65-42e6ceb0eba6" in namespace "projected-5108" to be "success or failure"
Dec 14 08:51:30.317: INFO: Pod "downwardapi-volume-7db7cb87-a899-4c4d-9c65-42e6ceb0eba6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.00693ms
Dec 14 08:51:32.319: INFO: Pod "downwardapi-volume-7db7cb87-a899-4c4d-9c65-42e6ceb0eba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00506983s
STEP: Saw pod success
Dec 14 08:51:32.319: INFO: Pod "downwardapi-volume-7db7cb87-a899-4c4d-9c65-42e6ceb0eba6" satisfied condition "success or failure"
Dec 14 08:51:32.321: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-7db7cb87-a899-4c4d-9c65-42e6ceb0eba6 container client-container: <nil>
STEP: delete the pod
Dec 14 08:51:32.333: INFO: Waiting for pod downwardapi-volume-7db7cb87-a899-4c4d-9c65-42e6ceb0eba6 to disappear
Dec 14 08:51:32.336: INFO: Pod downwardapi-volume-7db7cb87-a899-4c4d-9c65-42e6ceb0eba6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:51:32.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5108" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":22,"skipped":312,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:51:32.342: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-814
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-814
STEP: Deleting pre-stop pod
Dec 14 08:51:43.538: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:51:43.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-814" for this suite.

• [SLOW TEST:11.215 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":23,"skipped":317,"failed":0}
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:51:43.558: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 08:51:43.704: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f81bdd27-50e6-48a4-aee0-b44eb9b0fbcd" in namespace "downward-api-5158" to be "success or failure"
Dec 14 08:51:43.710: INFO: Pod "downwardapi-volume-f81bdd27-50e6-48a4-aee0-b44eb9b0fbcd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.925346ms
Dec 14 08:51:45.713: INFO: Pod "downwardapi-volume-f81bdd27-50e6-48a4-aee0-b44eb9b0fbcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00824004s
STEP: Saw pod success
Dec 14 08:51:45.713: INFO: Pod "downwardapi-volume-f81bdd27-50e6-48a4-aee0-b44eb9b0fbcd" satisfied condition "success or failure"
Dec 14 08:51:45.714: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-f81bdd27-50e6-48a4-aee0-b44eb9b0fbcd container client-container: <nil>
STEP: delete the pod
Dec 14 08:51:45.744: INFO: Waiting for pod downwardapi-volume-f81bdd27-50e6-48a4-aee0-b44eb9b0fbcd to disappear
Dec 14 08:51:45.747: INFO: Pod downwardapi-volume-f81bdd27-50e6-48a4-aee0-b44eb9b0fbcd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:51:45.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5158" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":24,"skipped":317,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:51:45.759: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4864
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 08:51:45.889: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec 14 08:51:50.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-4864 create -f -'
Dec 14 08:51:51.056: INFO: stderr: ""
Dec 14 08:51:51.056: INFO: stdout: "e2e-test-crd-publish-openapi-2620-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 14 08:51:51.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-4864 delete e2e-test-crd-publish-openapi-2620-crds test-foo'
Dec 14 08:51:51.210: INFO: stderr: ""
Dec 14 08:51:51.210: INFO: stdout: "e2e-test-crd-publish-openapi-2620-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 14 08:51:51.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-4864 apply -f -'
Dec 14 08:51:51.403: INFO: stderr: ""
Dec 14 08:51:51.403: INFO: stdout: "e2e-test-crd-publish-openapi-2620-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 14 08:51:51.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-4864 delete e2e-test-crd-publish-openapi-2620-crds test-foo'
Dec 14 08:51:51.481: INFO: stderr: ""
Dec 14 08:51:51.481: INFO: stdout: "e2e-test-crd-publish-openapi-2620-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 14 08:51:51.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-4864 create -f -'
Dec 14 08:51:51.667: INFO: rc: 1
Dec 14 08:51:51.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-4864 apply -f -'
Dec 14 08:51:51.813: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec 14 08:51:51.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-4864 create -f -'
Dec 14 08:51:51.953: INFO: rc: 1
Dec 14 08:51:51.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-4864 apply -f -'
Dec 14 08:51:52.107: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 14 08:51:52.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 explain e2e-test-crd-publish-openapi-2620-crds'
Dec 14 08:51:52.262: INFO: stderr: ""
Dec 14 08:51:52.262: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2620-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 14 08:51:52.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 explain e2e-test-crd-publish-openapi-2620-crds.metadata'
Dec 14 08:51:52.416: INFO: stderr: ""
Dec 14 08:51:52.416: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2620-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 14 08:51:52.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 explain e2e-test-crd-publish-openapi-2620-crds.spec'
Dec 14 08:51:52.575: INFO: stderr: ""
Dec 14 08:51:52.575: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2620-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 14 08:51:52.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 explain e2e-test-crd-publish-openapi-2620-crds.spec.bars'
Dec 14 08:51:52.728: INFO: stderr: ""
Dec 14 08:51:52.728: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2620-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 14 08:51:52.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 explain e2e-test-crd-publish-openapi-2620-crds.spec.bars2'
Dec 14 08:51:52.888: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:51:57.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4864" for this suite.

• [SLOW TEST:11.360 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":25,"skipped":324,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:51:57.120: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1529
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 08:51:57.867: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 08:52:00.877: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:52:01.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1529" for this suite.
STEP: Destroying namespace "webhook-1529-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":26,"skipped":343,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:52:01.089: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4388
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 08:52:01.245: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 14 08:52:03.280: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:52:03.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4388" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":27,"skipped":352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:52:03.301: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3335
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Dec 14 08:52:03.435: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:52:25.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3335" for this suite.

• [SLOW TEST:22.536 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":28,"skipped":377,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:52:25.838: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2982
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 14 08:52:25.976: INFO: Waiting up to 5m0s for pod "pod-9ad68277-a2b4-4a6d-937c-83f65527def9" in namespace "emptydir-2982" to be "success or failure"
Dec 14 08:52:25.982: INFO: Pod "pod-9ad68277-a2b4-4a6d-937c-83f65527def9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038524ms
Dec 14 08:52:27.984: INFO: Pod "pod-9ad68277-a2b4-4a6d-937c-83f65527def9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008085274s
STEP: Saw pod success
Dec 14 08:52:27.984: INFO: Pod "pod-9ad68277-a2b4-4a6d-937c-83f65527def9" satisfied condition "success or failure"
Dec 14 08:52:27.986: INFO: Trying to get logs from node k8s-2 pod pod-9ad68277-a2b4-4a6d-937c-83f65527def9 container test-container: <nil>
STEP: delete the pod
Dec 14 08:52:27.999: INFO: Waiting for pod pod-9ad68277-a2b4-4a6d-937c-83f65527def9 to disappear
Dec 14 08:52:28.001: INFO: Pod pod-9ad68277-a2b4-4a6d-937c-83f65527def9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:52:28.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2982" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":29,"skipped":384,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:52:28.006: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-7620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 14 08:52:28.704: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 08:52:31.714: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 08:52:31.716: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:52:32.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7620" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":30,"skipped":400,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:52:33.002: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Dec 14 08:52:33.266: INFO: Waiting up to 5m0s for pod "var-expansion-832b32f6-1b87-4fd7-bfaf-5629fd1993d3" in namespace "var-expansion-1141" to be "success or failure"
Dec 14 08:52:33.277: INFO: Pod "var-expansion-832b32f6-1b87-4fd7-bfaf-5629fd1993d3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.300162ms
Dec 14 08:52:35.279: INFO: Pod "var-expansion-832b32f6-1b87-4fd7-bfaf-5629fd1993d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013456621s
STEP: Saw pod success
Dec 14 08:52:35.279: INFO: Pod "var-expansion-832b32f6-1b87-4fd7-bfaf-5629fd1993d3" satisfied condition "success or failure"
Dec 14 08:52:35.281: INFO: Trying to get logs from node k8s-2 pod var-expansion-832b32f6-1b87-4fd7-bfaf-5629fd1993d3 container dapi-container: <nil>
STEP: delete the pod
Dec 14 08:52:35.293: INFO: Waiting for pod var-expansion-832b32f6-1b87-4fd7-bfaf-5629fd1993d3 to disappear
Dec 14 08:52:35.303: INFO: Pod var-expansion-832b32f6-1b87-4fd7-bfaf-5629fd1993d3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:52:35.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1141" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":31,"skipped":408,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:52:35.310: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5991
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5991
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Dec 14 08:52:35.456: INFO: Found 0 stateful pods, waiting for 3
Dec 14 08:52:45.459: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:52:45.459: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:52:45.459: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:52:45.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-5991 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:52:45.703: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:52:45.703: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:52:45.704: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 14 08:52:55.731: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 14 08:53:05.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-5991 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:53:05.983: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 08:53:05.983: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 08:53:05.983: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 08:53:06.007: INFO: Waiting for StatefulSet statefulset-5991/ss2 to complete update
Dec 14 08:53:06.007: INFO: Waiting for Pod statefulset-5991/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 08:53:06.007: INFO: Waiting for Pod statefulset-5991/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 08:53:06.007: INFO: Waiting for Pod statefulset-5991/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 08:53:16.011: INFO: Waiting for StatefulSet statefulset-5991/ss2 to complete update
Dec 14 08:53:16.011: INFO: Waiting for Pod statefulset-5991/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 08:53:16.011: INFO: Waiting for Pod statefulset-5991/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 08:53:16.011: INFO: Waiting for Pod statefulset-5991/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 08:53:26.011: INFO: Waiting for StatefulSet statefulset-5991/ss2 to complete update
Dec 14 08:53:26.011: INFO: Waiting for Pod statefulset-5991/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 08:53:26.011: INFO: Waiting for Pod statefulset-5991/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 08:53:36.011: INFO: Waiting for StatefulSet statefulset-5991/ss2 to complete update
Dec 14 08:53:36.012: INFO: Waiting for Pod statefulset-5991/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Dec 14 08:53:46.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-5991 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:53:46.230: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:53:46.231: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:53:46.231: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:53:56.254: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 14 08:54:06.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-5991 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:54:06.476: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 08:54:06.476: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 08:54:06.476: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 08:54:26.489: INFO: Waiting for StatefulSet statefulset-5991/ss2 to complete update
Dec 14 08:54:26.489: INFO: Waiting for Pod statefulset-5991/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Dec 14 08:54:36.493: INFO: Deleting all statefulset in ns statefulset-5991
Dec 14 08:54:36.495: INFO: Scaling statefulset ss2 to 0
Dec 14 08:54:46.508: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:54:46.510: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:54:46.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5991" for this suite.

• [SLOW TEST:131.221 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":32,"skipped":411,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:54:46.531: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3643
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 14 08:54:49.231: INFO: Successfully updated pod "adopt-release-cjr9d"
STEP: Checking that the Job readopts the Pod
Dec 14 08:54:49.232: INFO: Waiting up to 15m0s for pod "adopt-release-cjr9d" in namespace "job-3643" to be "adopted"
Dec 14 08:54:49.235: INFO: Pod "adopt-release-cjr9d": Phase="Running", Reason="", readiness=true. Elapsed: 3.943223ms
Dec 14 08:54:51.238: INFO: Pod "adopt-release-cjr9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006453459s
Dec 14 08:54:51.238: INFO: Pod "adopt-release-cjr9d" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 14 08:54:51.752: INFO: Successfully updated pod "adopt-release-cjr9d"
STEP: Checking that the Job releases the Pod
Dec 14 08:54:51.752: INFO: Waiting up to 15m0s for pod "adopt-release-cjr9d" in namespace "job-3643" to be "released"
Dec 14 08:54:51.761: INFO: Pod "adopt-release-cjr9d": Phase="Running", Reason="", readiness=true. Elapsed: 9.489095ms
Dec 14 08:54:51.762: INFO: Pod "adopt-release-cjr9d" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:54:51.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3643" for this suite.

• [SLOW TEST:5.258 seconds]
[sig-apps] Job
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":33,"skipped":424,"failed":0}
SSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:54:51.790: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-7646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec 14 08:54:51.949: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Dec 14 08:54:52.608: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 14 08:54:54.657: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:54:56.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:54:58.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:55:00.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:55:02.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:55:04.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711910492, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:55:07.617: INFO: Waited 953.148822ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:55:08.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7646" for this suite.

• [SLOW TEST:16.931 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":34,"skipped":427,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:55:08.722: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-665
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:329
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Dec 14 08:55:08.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-665'
Dec 14 08:55:09.145: INFO: stderr: ""
Dec 14 08:55:09.145: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 08:55:09.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-665'
Dec 14 08:55:09.301: INFO: stderr: ""
Dec 14 08:55:09.301: INFO: stdout: "update-demo-nautilus-c66jc update-demo-nautilus-xsfd5 "
Dec 14 08:55:09.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-c66jc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-665'
Dec 14 08:55:09.378: INFO: stderr: ""
Dec 14 08:55:09.378: INFO: stdout: ""
Dec 14 08:55:09.378: INFO: update-demo-nautilus-c66jc is created but not running
Dec 14 08:55:14.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-665'
Dec 14 08:55:14.457: INFO: stderr: ""
Dec 14 08:55:14.457: INFO: stdout: "update-demo-nautilus-c66jc update-demo-nautilus-xsfd5 "
Dec 14 08:55:14.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-c66jc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-665'
Dec 14 08:55:14.533: INFO: stderr: ""
Dec 14 08:55:14.533: INFO: stdout: "true"
Dec 14 08:55:14.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-c66jc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-665'
Dec 14 08:55:14.611: INFO: stderr: ""
Dec 14 08:55:14.611: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 08:55:14.611: INFO: validating pod update-demo-nautilus-c66jc
Dec 14 08:55:14.615: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 08:55:14.615: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 08:55:14.615: INFO: update-demo-nautilus-c66jc is verified up and running
Dec 14 08:55:14.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-xsfd5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-665'
Dec 14 08:55:14.691: INFO: stderr: ""
Dec 14 08:55:14.691: INFO: stdout: "true"
Dec 14 08:55:14.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-xsfd5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-665'
Dec 14 08:55:14.785: INFO: stderr: ""
Dec 14 08:55:14.785: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 08:55:14.785: INFO: validating pod update-demo-nautilus-xsfd5
Dec 14 08:55:14.788: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 08:55:14.788: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 08:55:14.788: INFO: update-demo-nautilus-xsfd5 is verified up and running
STEP: using delete to clean up resources
Dec 14 08:55:14.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete --grace-period=0 --force -f - --namespace=kubectl-665'
Dec 14 08:55:14.867: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:55:14.868: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 14 08:55:14.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-665'
Dec 14 08:55:14.973: INFO: stderr: "No resources found in kubectl-665 namespace.\n"
Dec 14 08:55:14.973: INFO: stdout: ""
Dec 14 08:55:14.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -l name=update-demo --namespace=kubectl-665 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 08:55:15.054: INFO: stderr: ""
Dec 14 08:55:15.054: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:55:15.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-665" for this suite.

• [SLOW TEST:6.339 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:327
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":35,"skipped":429,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:55:15.061: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-86
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-44f3b74a-97b9-4f7f-a811-d3db8e078d84
STEP: Creating a pod to test consume configMaps
Dec 14 08:55:15.256: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e667d3a4-e6bb-453b-87fd-36feac23e25f" in namespace "projected-86" to be "success or failure"
Dec 14 08:55:15.262: INFO: Pod "pod-projected-configmaps-e667d3a4-e6bb-453b-87fd-36feac23e25f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.913714ms
Dec 14 08:55:17.264: INFO: Pod "pod-projected-configmaps-e667d3a4-e6bb-453b-87fd-36feac23e25f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008224747s
STEP: Saw pod success
Dec 14 08:55:17.264: INFO: Pod "pod-projected-configmaps-e667d3a4-e6bb-453b-87fd-36feac23e25f" satisfied condition "success or failure"
Dec 14 08:55:17.266: INFO: Trying to get logs from node k8s-2 pod pod-projected-configmaps-e667d3a4-e6bb-453b-87fd-36feac23e25f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 08:55:17.279: INFO: Waiting for pod pod-projected-configmaps-e667d3a4-e6bb-453b-87fd-36feac23e25f to disappear
Dec 14 08:55:17.282: INFO: Pod pod-projected-configmaps-e667d3a4-e6bb-453b-87fd-36feac23e25f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:55:17.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-86" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":36,"skipped":468,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:55:17.288: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 08:55:17.887: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 08:55:20.898: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:55:31.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-244" for this suite.
STEP: Destroying namespace "webhook-244-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.865 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":37,"skipped":468,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:55:31.154: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-ct8l
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 08:55:31.344: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ct8l" in namespace "subpath-2634" to be "success or failure"
Dec 14 08:55:31.350: INFO: Pod "pod-subpath-test-configmap-ct8l": Phase="Pending", Reason="", readiness=false. Elapsed: 5.919077ms
Dec 14 08:55:33.353: INFO: Pod "pod-subpath-test-configmap-ct8l": Phase="Running", Reason="", readiness=true. Elapsed: 2.008243046s
Dec 14 08:55:35.355: INFO: Pod "pod-subpath-test-configmap-ct8l": Phase="Running", Reason="", readiness=true. Elapsed: 4.010164704s
Dec 14 08:55:37.357: INFO: Pod "pod-subpath-test-configmap-ct8l": Phase="Running", Reason="", readiness=true. Elapsed: 6.012344183s
Dec 14 08:55:39.359: INFO: Pod "pod-subpath-test-configmap-ct8l": Phase="Running", Reason="", readiness=true. Elapsed: 8.014605587s
Dec 14 08:55:41.362: INFO: Pod "pod-subpath-test-configmap-ct8l": Phase="Running", Reason="", readiness=true. Elapsed: 10.017065834s
Dec 14 08:55:43.364: INFO: Pod "pod-subpath-test-configmap-ct8l": Phase="Running", Reason="", readiness=true. Elapsed: 12.019484581s
Dec 14 08:55:45.367: INFO: Pod "pod-subpath-test-configmap-ct8l": Phase="Running", Reason="", readiness=true. Elapsed: 14.022365958s
Dec 14 08:55:47.369: INFO: Pod "pod-subpath-test-configmap-ct8l": Phase="Running", Reason="", readiness=true. Elapsed: 16.024145738s
Dec 14 08:55:49.371: INFO: Pod "pod-subpath-test-configmap-ct8l": Phase="Running", Reason="", readiness=true. Elapsed: 18.026468414s
Dec 14 08:55:51.373: INFO: Pod "pod-subpath-test-configmap-ct8l": Phase="Running", Reason="", readiness=true. Elapsed: 20.02883367s
Dec 14 08:55:53.377: INFO: Pod "pod-subpath-test-configmap-ct8l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.03212426s
STEP: Saw pod success
Dec 14 08:55:53.377: INFO: Pod "pod-subpath-test-configmap-ct8l" satisfied condition "success or failure"
Dec 14 08:55:53.379: INFO: Trying to get logs from node k8s-2 pod pod-subpath-test-configmap-ct8l container test-container-subpath-configmap-ct8l: <nil>
STEP: delete the pod
Dec 14 08:55:53.397: INFO: Waiting for pod pod-subpath-test-configmap-ct8l to disappear
Dec 14 08:55:53.399: INFO: Pod pod-subpath-test-configmap-ct8l no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ct8l
Dec 14 08:55:53.399: INFO: Deleting pod "pod-subpath-test-configmap-ct8l" in namespace "subpath-2634"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:55:53.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2634" for this suite.

• [SLOW TEST:22.255 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":38,"skipped":491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:55:53.410: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-1098f671-7439-4009-aaa4-c2819edf1d3f
STEP: Creating a pod to test consume configMaps
Dec 14 08:55:53.568: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f9321b6d-552a-4e6d-9f70-84c8d31aaf7c" in namespace "projected-4931" to be "success or failure"
Dec 14 08:55:53.576: INFO: Pod "pod-projected-configmaps-f9321b6d-552a-4e6d-9f70-84c8d31aaf7c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.269626ms
Dec 14 08:55:55.579: INFO: Pod "pod-projected-configmaps-f9321b6d-552a-4e6d-9f70-84c8d31aaf7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010572617s
STEP: Saw pod success
Dec 14 08:55:55.579: INFO: Pod "pod-projected-configmaps-f9321b6d-552a-4e6d-9f70-84c8d31aaf7c" satisfied condition "success or failure"
Dec 14 08:55:55.581: INFO: Trying to get logs from node k8s-2 pod pod-projected-configmaps-f9321b6d-552a-4e6d-9f70-84c8d31aaf7c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 08:55:55.594: INFO: Waiting for pod pod-projected-configmaps-f9321b6d-552a-4e6d-9f70-84c8d31aaf7c to disappear
Dec 14 08:55:55.596: INFO: Pod pod-projected-configmaps-f9321b6d-552a-4e6d-9f70-84c8d31aaf7c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:55:55.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4931" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":39,"skipped":543,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:55:55.602: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-64273f2c-4b10-4018-9cbf-a21c212479fa
STEP: Creating a pod to test consume secrets
Dec 14 08:55:55.801: INFO: Waiting up to 5m0s for pod "pod-secrets-94aa8558-ea5a-40ca-9b17-e23b1e959494" in namespace "secrets-7868" to be "success or failure"
Dec 14 08:55:55.807: INFO: Pod "pod-secrets-94aa8558-ea5a-40ca-9b17-e23b1e959494": Phase="Pending", Reason="", readiness=false. Elapsed: 5.667805ms
Dec 14 08:55:57.809: INFO: Pod "pod-secrets-94aa8558-ea5a-40ca-9b17-e23b1e959494": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007949905s
STEP: Saw pod success
Dec 14 08:55:57.809: INFO: Pod "pod-secrets-94aa8558-ea5a-40ca-9b17-e23b1e959494" satisfied condition "success or failure"
Dec 14 08:55:57.811: INFO: Trying to get logs from node k8s-2 pod pod-secrets-94aa8558-ea5a-40ca-9b17-e23b1e959494 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 08:55:57.823: INFO: Waiting for pod pod-secrets-94aa8558-ea5a-40ca-9b17-e23b1e959494 to disappear
Dec 14 08:55:57.826: INFO: Pod pod-secrets-94aa8558-ea5a-40ca-9b17-e23b1e959494 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:55:57.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7868" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":40,"skipped":546,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:55:57.831: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 08:55:57.969: INFO: Waiting up to 5m0s for pod "busybox-user-65534-24edca1a-def1-400b-995c-55de0955ef3c" in namespace "security-context-test-9814" to be "success or failure"
Dec 14 08:55:57.975: INFO: Pod "busybox-user-65534-24edca1a-def1-400b-995c-55de0955ef3c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.867281ms
Dec 14 08:55:59.977: INFO: Pod "busybox-user-65534-24edca1a-def1-400b-995c-55de0955ef3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007926516s
Dec 14 08:55:59.977: INFO: Pod "busybox-user-65534-24edca1a-def1-400b-995c-55de0955ef3c" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:55:59.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9814" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":41,"skipped":553,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:55:59.983: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 14 08:56:04.160: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 08:56:04.165: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 08:56:06.165: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 08:56:06.167: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 08:56:08.165: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 08:56:08.167: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 08:56:10.165: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 08:56:10.167: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 08:56:12.165: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 08:56:12.167: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 08:56:14.165: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 08:56:14.167: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 08:56:16.165: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 08:56:16.167: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 08:56:18.165: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 08:56:18.169: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 08:56:20.165: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 08:56:20.167: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:56:20.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6033" for this suite.

• [SLOW TEST:20.189 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":42,"skipped":565,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:56:20.173: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1672
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 14 08:56:20.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-3145'
Dec 14 08:56:20.397: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 14 08:56:20.397: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Dec 14 08:56:20.403: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Dec 14 08:56:20.418: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Dec 14 08:56:20.434: INFO: scanned /root for discovery docs: <nil>
Dec 14 08:56:20.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-3145'
Dec 14 08:56:36.204: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 14 08:56:36.204: INFO: stdout: "Created e2e-test-httpd-rc-3a36a3032bc4ebb233fb9e2cfae519e8\nScaling up e2e-test-httpd-rc-3a36a3032bc4ebb233fb9e2cfae519e8 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-3a36a3032bc4ebb233fb9e2cfae519e8 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-3a36a3032bc4ebb233fb9e2cfae519e8 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Dec 14 08:56:36.204: INFO: stdout: "Created e2e-test-httpd-rc-3a36a3032bc4ebb233fb9e2cfae519e8\nScaling up e2e-test-httpd-rc-3a36a3032bc4ebb233fb9e2cfae519e8 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-3a36a3032bc4ebb233fb9e2cfae519e8 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-3a36a3032bc4ebb233fb9e2cfae519e8 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Dec 14 08:56:36.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-3145'
Dec 14 08:56:36.309: INFO: stderr: ""
Dec 14 08:56:36.309: INFO: stdout: "e2e-test-httpd-rc-3a36a3032bc4ebb233fb9e2cfae519e8-swjzg e2e-test-httpd-rc-rdc6l "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Dec 14 08:56:41.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-3145'
Dec 14 08:56:41.409: INFO: stderr: ""
Dec 14 08:56:41.409: INFO: stdout: "e2e-test-httpd-rc-3a36a3032bc4ebb233fb9e2cfae519e8-swjzg "
Dec 14 08:56:41.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods e2e-test-httpd-rc-3a36a3032bc4ebb233fb9e2cfae519e8-swjzg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3145'
Dec 14 08:56:41.486: INFO: stderr: ""
Dec 14 08:56:41.486: INFO: stdout: "true"
Dec 14 08:56:41.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods e2e-test-httpd-rc-3a36a3032bc4ebb233fb9e2cfae519e8-swjzg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3145'
Dec 14 08:56:41.564: INFO: stderr: ""
Dec 14 08:56:41.564: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Dec 14 08:56:41.564: INFO: e2e-test-httpd-rc-3a36a3032bc4ebb233fb9e2cfae519e8-swjzg is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1678
Dec 14 08:56:41.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete rc e2e-test-httpd-rc --namespace=kubectl-3145'
Dec 14 08:56:41.679: INFO: stderr: ""
Dec 14 08:56:41.679: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:56:41.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3145" for this suite.

• [SLOW TEST:21.517 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1667
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":280,"completed":43,"skipped":583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:56:41.691: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-446
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 08:56:41.822: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 14 08:57:01.895: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.33.1.67:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-446 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 08:57:01.895: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 08:57:02.022: INFO: Found all expected endpoints: [netserver-0]
Dec 14 08:57:02.025: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.33.2.32:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-446 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 08:57:02.026: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 08:57:02.160: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:57:02.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-446" for this suite.

• [SLOW TEST:20.476 seconds]
[sig-network] Networking
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":44,"skipped":609,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:57:02.167: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8667
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-c14c0e54-ded1-45b0-8b2d-329a9d35abde
STEP: Creating a pod to test consume configMaps
Dec 14 08:57:02.308: INFO: Waiting up to 5m0s for pod "pod-configmaps-f7bacbd9-9425-4bbc-a440-38ff404cffc7" in namespace "configmap-8667" to be "success or failure"
Dec 14 08:57:02.318: INFO: Pod "pod-configmaps-f7bacbd9-9425-4bbc-a440-38ff404cffc7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.992224ms
Dec 14 08:57:04.320: INFO: Pod "pod-configmaps-f7bacbd9-9425-4bbc-a440-38ff404cffc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011971529s
STEP: Saw pod success
Dec 14 08:57:04.320: INFO: Pod "pod-configmaps-f7bacbd9-9425-4bbc-a440-38ff404cffc7" satisfied condition "success or failure"
Dec 14 08:57:04.322: INFO: Trying to get logs from node k8s-2 pod pod-configmaps-f7bacbd9-9425-4bbc-a440-38ff404cffc7 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 08:57:04.334: INFO: Waiting for pod pod-configmaps-f7bacbd9-9425-4bbc-a440-38ff404cffc7 to disappear
Dec 14 08:57:04.336: INFO: Pod pod-configmaps-f7bacbd9-9425-4bbc-a440-38ff404cffc7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:57:04.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8667" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":45,"skipped":610,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:57:04.343: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9009
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 08:57:04.484: INFO: Create a RollingUpdate DaemonSet
Dec 14 08:57:04.487: INFO: Check that daemon pods launch on every node of the cluster
Dec 14 08:57:04.492: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 08:57:04.496: INFO: Number of nodes with available pods: 0
Dec 14 08:57:04.496: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 08:57:05.499: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 08:57:05.502: INFO: Number of nodes with available pods: 1
Dec 14 08:57:05.502: INFO: Node k8s-3 is running more than one daemon pod
Dec 14 08:57:06.499: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 08:57:06.501: INFO: Number of nodes with available pods: 1
Dec 14 08:57:06.501: INFO: Node k8s-3 is running more than one daemon pod
Dec 14 08:57:07.499: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 08:57:07.501: INFO: Number of nodes with available pods: 1
Dec 14 08:57:07.501: INFO: Node k8s-3 is running more than one daemon pod
Dec 14 08:57:08.499: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 08:57:08.500: INFO: Number of nodes with available pods: 1
Dec 14 08:57:08.501: INFO: Node k8s-3 is running more than one daemon pod
Dec 14 08:57:09.502: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 08:57:09.507: INFO: Number of nodes with available pods: 1
Dec 14 08:57:09.507: INFO: Node k8s-3 is running more than one daemon pod
Dec 14 08:57:10.498: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 08:57:10.501: INFO: Number of nodes with available pods: 1
Dec 14 08:57:10.501: INFO: Node k8s-3 is running more than one daemon pod
Dec 14 08:57:11.498: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 08:57:11.500: INFO: Number of nodes with available pods: 2
Dec 14 08:57:11.500: INFO: Number of running nodes: 2, number of available pods: 2
Dec 14 08:57:11.500: INFO: Update the DaemonSet to trigger a rollout
Dec 14 08:57:11.506: INFO: Updating DaemonSet daemon-set
Dec 14 08:57:18.517: INFO: Roll back the DaemonSet before rollout is complete
Dec 14 08:57:18.522: INFO: Updating DaemonSet daemon-set
Dec 14 08:57:18.522: INFO: Make sure DaemonSet rollback is complete
Dec 14 08:57:18.526: INFO: Wrong image for pod: daemon-set-86rjs. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 14 08:57:18.526: INFO: Pod daemon-set-86rjs is not available
Dec 14 08:57:18.530: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 08:57:19.533: INFO: Wrong image for pod: daemon-set-86rjs. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 14 08:57:19.533: INFO: Pod daemon-set-86rjs is not available
Dec 14 08:57:19.535: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 08:57:20.533: INFO: Pod daemon-set-m652h is not available
Dec 14 08:57:20.535: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9009, will wait for the garbage collector to delete the pods
Dec 14 08:57:20.595: INFO: Deleting DaemonSet.extensions daemon-set took: 4.406466ms
Dec 14 08:57:21.596: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000167294s
Dec 14 08:57:28.399: INFO: Number of nodes with available pods: 0
Dec 14 08:57:28.399: INFO: Number of running nodes: 0, number of available pods: 0
Dec 14 08:57:28.401: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9009/daemonsets","resourceVersion":"7550"},"items":null}

Dec 14 08:57:28.402: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9009/pods","resourceVersion":"7550"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:57:28.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9009" for this suite.

• [SLOW TEST:24.071 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":46,"skipped":616,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:57:28.414: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-cc047f6e-285b-4680-8011-6f19b7ad304d
STEP: Creating a pod to test consume configMaps
Dec 14 08:57:28.554: INFO: Waiting up to 5m0s for pod "pod-configmaps-c525345d-855a-43f9-bda0-b817fae55700" in namespace "configmap-7980" to be "success or failure"
Dec 14 08:57:28.560: INFO: Pod "pod-configmaps-c525345d-855a-43f9-bda0-b817fae55700": Phase="Pending", Reason="", readiness=false. Elapsed: 5.926462ms
Dec 14 08:57:30.562: INFO: Pod "pod-configmaps-c525345d-855a-43f9-bda0-b817fae55700": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007830399s
STEP: Saw pod success
Dec 14 08:57:30.562: INFO: Pod "pod-configmaps-c525345d-855a-43f9-bda0-b817fae55700" satisfied condition "success or failure"
Dec 14 08:57:30.563: INFO: Trying to get logs from node k8s-2 pod pod-configmaps-c525345d-855a-43f9-bda0-b817fae55700 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 08:57:30.575: INFO: Waiting for pod pod-configmaps-c525345d-855a-43f9-bda0-b817fae55700 to disappear
Dec 14 08:57:30.577: INFO: Pod pod-configmaps-c525345d-855a-43f9-bda0-b817fae55700 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:57:30.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7980" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":47,"skipped":627,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:57:30.583: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1465
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 14 08:57:30.721: INFO: Waiting up to 5m0s for pod "pod-612612b2-8729-4231-aa69-81b257a29846" in namespace "emptydir-1465" to be "success or failure"
Dec 14 08:57:30.727: INFO: Pod "pod-612612b2-8729-4231-aa69-81b257a29846": Phase="Pending", Reason="", readiness=false. Elapsed: 5.775954ms
Dec 14 08:57:32.729: INFO: Pod "pod-612612b2-8729-4231-aa69-81b257a29846": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007914706s
STEP: Saw pod success
Dec 14 08:57:32.729: INFO: Pod "pod-612612b2-8729-4231-aa69-81b257a29846" satisfied condition "success or failure"
Dec 14 08:57:32.731: INFO: Trying to get logs from node k8s-2 pod pod-612612b2-8729-4231-aa69-81b257a29846 container test-container: <nil>
STEP: delete the pod
Dec 14 08:57:32.749: INFO: Waiting for pod pod-612612b2-8729-4231-aa69-81b257a29846 to disappear
Dec 14 08:57:32.755: INFO: Pod pod-612612b2-8729-4231-aa69-81b257a29846 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:57:32.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1465" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":48,"skipped":635,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:57:32.764: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-9350
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-9350
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9350
Dec 14 08:57:32.929: INFO: Found 0 stateful pods, waiting for 1
Dec 14 08:57:42.932: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 14 08:57:42.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-9350 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:57:43.141: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:57:43.141: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:57:43.141: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:57:43.144: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 14 08:57:53.147: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 08:57:53.147: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:57:53.159: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Dec 14 08:57:53.159: INFO: ss-0  k8s-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:32 +0000 UTC  }]
Dec 14 08:57:53.159: INFO: 
Dec 14 08:57:53.159: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 14 08:57:54.162: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994983389s
Dec 14 08:57:55.165: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991976836s
Dec 14 08:57:56.167: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989066883s
Dec 14 08:57:57.170: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986483566s
Dec 14 08:57:58.173: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983779729s
Dec 14 08:57:59.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.98094317s
Dec 14 08:58:00.178: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.978296986s
Dec 14 08:58:01.181: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.975465981s
Dec 14 08:58:02.183: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.19351ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9350
Dec 14 08:58:03.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-9350 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:58:03.399: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 08:58:03.399: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 08:58:03.399: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 08:58:03.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-9350 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:58:03.628: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 14 08:58:03.628: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 08:58:03.628: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 08:58:03.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-9350 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:58:03.833: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 14 08:58:03.833: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 08:58:03.833: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 08:58:03.836: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:58:03.836: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:58:03.836: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 14 08:58:03.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-9350 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:58:04.064: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:58:04.064: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:58:04.064: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:58:04.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-9350 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:58:04.270: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:58:04.270: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:58:04.270: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:58:04.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-9350 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:58:04.504: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:58:04.504: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:58:04.504: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:58:04.504: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:58:04.507: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 14 08:58:14.511: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 08:58:14.511: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 08:58:14.511: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 08:58:14.522: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Dec 14 08:58:14.522: INFO: ss-0  k8s-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:32 +0000 UTC  }]
Dec 14 08:58:14.523: INFO: ss-1  k8s-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:53 +0000 UTC  }]
Dec 14 08:58:14.523: INFO: ss-2  k8s-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:53 +0000 UTC  }]
Dec 14 08:58:14.523: INFO: 
Dec 14 08:58:14.523: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 08:58:15.525: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Dec 14 08:58:15.526: INFO: ss-0  k8s-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:32 +0000 UTC  }]
Dec 14 08:58:15.526: INFO: ss-1  k8s-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:53 +0000 UTC  }]
Dec 14 08:58:15.526: INFO: ss-2  k8s-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:53 +0000 UTC  }]
Dec 14 08:58:15.526: INFO: 
Dec 14 08:58:15.526: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 08:58:16.528: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Dec 14 08:58:16.528: INFO: ss-0  k8s-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:32 +0000 UTC  }]
Dec 14 08:58:16.528: INFO: ss-1  k8s-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:53 +0000 UTC  }]
Dec 14 08:58:16.528: INFO: 
Dec 14 08:58:16.528: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 14 08:58:17.531: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Dec 14 08:58:17.531: INFO: ss-0  k8s-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:32 +0000 UTC  }]
Dec 14 08:58:17.531: INFO: ss-1  k8s-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:58:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-14 08:57:53 +0000 UTC  }]
Dec 14 08:58:17.531: INFO: 
Dec 14 08:58:17.531: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 14 08:58:18.533: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.985824978s
Dec 14 08:58:19.535: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.983602345s
Dec 14 08:58:20.537: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.981464132s
Dec 14 08:58:21.540: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.979301968s
Dec 14 08:58:22.542: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.976988786s
Dec 14 08:58:23.544: INFO: Verifying statefulset ss doesn't scale past 0 for another 974.606941ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9350
Dec 14 08:58:24.547: INFO: Scaling statefulset ss to 0
Dec 14 08:58:24.553: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Dec 14 08:58:24.554: INFO: Deleting all statefulset in ns statefulset-9350
Dec 14 08:58:24.556: INFO: Scaling statefulset ss to 0
Dec 14 08:58:24.561: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:58:24.563: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:58:24.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9350" for this suite.

• [SLOW TEST:51.818 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":49,"skipped":647,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:58:24.582: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:58:35.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2687" for this suite.

• [SLOW TEST:11.191 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":50,"skipped":656,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:58:35.773: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2901
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:58:38.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2901" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":51,"skipped":660,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:58:38.940: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5906
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-7d35a1b8-871f-4a94-97db-61c7010292d9
STEP: Creating a pod to test consume secrets
Dec 14 08:58:39.080: INFO: Waiting up to 5m0s for pod "pod-secrets-f69cc32b-1405-43b0-b641-4d7762d910ec" in namespace "secrets-5906" to be "success or failure"
Dec 14 08:58:39.086: INFO: Pod "pod-secrets-f69cc32b-1405-43b0-b641-4d7762d910ec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.573661ms
Dec 14 08:58:41.088: INFO: Pod "pod-secrets-f69cc32b-1405-43b0-b641-4d7762d910ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007886727s
STEP: Saw pod success
Dec 14 08:58:41.089: INFO: Pod "pod-secrets-f69cc32b-1405-43b0-b641-4d7762d910ec" satisfied condition "success or failure"
Dec 14 08:58:41.090: INFO: Trying to get logs from node k8s-2 pod pod-secrets-f69cc32b-1405-43b0-b641-4d7762d910ec container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 08:58:41.103: INFO: Waiting for pod pod-secrets-f69cc32b-1405-43b0-b641-4d7762d910ec to disappear
Dec 14 08:58:41.105: INFO: Pod pod-secrets-f69cc32b-1405-43b0-b641-4d7762d910ec no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:58:41.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5906" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":52,"skipped":717,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:58:41.111: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-890
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:58:57.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-890" for this suite.

• [SLOW TEST:16.277 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":53,"skipped":731,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:58:57.388: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9038
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:59:13.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9038" for this suite.

• [SLOW TEST:16.214 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":54,"skipped":741,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:59:13.603: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1303
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-5fa47bce-fa60-4da4-9b09-d5c70e791779
STEP: Creating secret with name s-test-opt-upd-8cfdd65d-1989-4611-9ea0-6ee7012820c6
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-5fa47bce-fa60-4da4-9b09-d5c70e791779
STEP: Updating secret s-test-opt-upd-8cfdd65d-1989-4611-9ea0-6ee7012820c6
STEP: Creating secret with name s-test-opt-create-53d8f3c9-6400-485c-8bbb-536f7ab5ec95
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:59:19.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1303" for this suite.

• [SLOW TEST:6.206 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":55,"skipped":752,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:59:19.809: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5411
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Dec 14 08:59:19.999: INFO: Waiting up to 5m0s for pod "downward-api-bf9a8a67-d42e-496d-a1cc-1340f2688b6b" in namespace "downward-api-5411" to be "success or failure"
Dec 14 08:59:20.005: INFO: Pod "downward-api-bf9a8a67-d42e-496d-a1cc-1340f2688b6b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.912963ms
Dec 14 08:59:22.007: INFO: Pod "downward-api-bf9a8a67-d42e-496d-a1cc-1340f2688b6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008295722s
STEP: Saw pod success
Dec 14 08:59:22.007: INFO: Pod "downward-api-bf9a8a67-d42e-496d-a1cc-1340f2688b6b" satisfied condition "success or failure"
Dec 14 08:59:22.009: INFO: Trying to get logs from node k8s-2 pod downward-api-bf9a8a67-d42e-496d-a1cc-1340f2688b6b container dapi-container: <nil>
STEP: delete the pod
Dec 14 08:59:22.025: INFO: Waiting for pod downward-api-bf9a8a67-d42e-496d-a1cc-1340f2688b6b to disappear
Dec 14 08:59:22.027: INFO: Pod downward-api-bf9a8a67-d42e-496d-a1cc-1340f2688b6b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:59:22.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5411" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":56,"skipped":754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:59:22.034: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1056
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 08:59:22.555: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 08:59:25.566: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
Dec 14 08:59:25.581: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:59:25.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1056" for this suite.
STEP: Destroying namespace "webhook-1056-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":57,"skipped":794,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:59:25.810: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-943
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 14 08:59:25.984: INFO: Waiting up to 5m0s for pod "pod-fa5a72d7-dc3b-4ff6-afef-8c86fb9b51c0" in namespace "emptydir-943" to be "success or failure"
Dec 14 08:59:25.990: INFO: Pod "pod-fa5a72d7-dc3b-4ff6-afef-8c86fb9b51c0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.283501ms
Dec 14 08:59:27.993: INFO: Pod "pod-fa5a72d7-dc3b-4ff6-afef-8c86fb9b51c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008747949s
STEP: Saw pod success
Dec 14 08:59:27.993: INFO: Pod "pod-fa5a72d7-dc3b-4ff6-afef-8c86fb9b51c0" satisfied condition "success or failure"
Dec 14 08:59:27.995: INFO: Trying to get logs from node k8s-2 pod pod-fa5a72d7-dc3b-4ff6-afef-8c86fb9b51c0 container test-container: <nil>
STEP: delete the pod
Dec 14 08:59:28.006: INFO: Waiting for pod pod-fa5a72d7-dc3b-4ff6-afef-8c86fb9b51c0 to disappear
Dec 14 08:59:28.009: INFO: Pod pod-fa5a72d7-dc3b-4ff6-afef-8c86fb9b51c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:59:28.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-943" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":58,"skipped":797,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:59:28.015: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Dec 14 08:59:30.680: INFO: Successfully updated pod "annotationupdatede06d855-6d49-4476-ba79-2f3b1cdf008d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:59:34.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8997" for this suite.

• [SLOW TEST:6.688 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":59,"skipped":801,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:59:34.703: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6894
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-1838
STEP: Creating secret with name secret-test-8cc7ef4e-dd2d-43f7-9b28-086685e704bd
STEP: Creating a pod to test consume secrets
Dec 14 08:59:34.975: INFO: Waiting up to 5m0s for pod "pod-secrets-01fe7d51-cd4a-41b5-9739-9592141d7714" in namespace "secrets-6894" to be "success or failure"
Dec 14 08:59:34.981: INFO: Pod "pod-secrets-01fe7d51-cd4a-41b5-9739-9592141d7714": Phase="Pending", Reason="", readiness=false. Elapsed: 5.788058ms
Dec 14 08:59:36.984: INFO: Pod "pod-secrets-01fe7d51-cd4a-41b5-9739-9592141d7714": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008011288s
STEP: Saw pod success
Dec 14 08:59:36.984: INFO: Pod "pod-secrets-01fe7d51-cd4a-41b5-9739-9592141d7714" satisfied condition "success or failure"
Dec 14 08:59:36.985: INFO: Trying to get logs from node k8s-2 pod pod-secrets-01fe7d51-cd4a-41b5-9739-9592141d7714 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 08:59:36.999: INFO: Waiting for pod pod-secrets-01fe7d51-cd4a-41b5-9739-9592141d7714 to disappear
Dec 14 08:59:37.001: INFO: Pod pod-secrets-01fe7d51-cd4a-41b5-9739-9592141d7714 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:59:37.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6894" for this suite.
STEP: Destroying namespace "secret-namespace-1838" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":60,"skipped":832,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:59:37.011: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2827
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 14 08:59:37.148: INFO: Waiting up to 5m0s for pod "pod-63c5177f-920b-4da5-8594-8a26c0a439c0" in namespace "emptydir-2827" to be "success or failure"
Dec 14 08:59:37.154: INFO: Pod "pod-63c5177f-920b-4da5-8594-8a26c0a439c0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.716834ms
Dec 14 08:59:39.157: INFO: Pod "pod-63c5177f-920b-4da5-8594-8a26c0a439c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008915082s
STEP: Saw pod success
Dec 14 08:59:39.157: INFO: Pod "pod-63c5177f-920b-4da5-8594-8a26c0a439c0" satisfied condition "success or failure"
Dec 14 08:59:39.159: INFO: Trying to get logs from node k8s-2 pod pod-63c5177f-920b-4da5-8594-8a26c0a439c0 container test-container: <nil>
STEP: delete the pod
Dec 14 08:59:39.173: INFO: Waiting for pod pod-63c5177f-920b-4da5-8594-8a26c0a439c0 to disappear
Dec 14 08:59:39.178: INFO: Pod pod-63c5177f-920b-4da5-8594-8a26c0a439c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 08:59:39.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2827" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":61,"skipped":832,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 08:59:39.187: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2069
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 14 08:59:39.339: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 08:59:44.181: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:00.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2069" for this suite.

• [SLOW TEST:21.740 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":62,"skipped":839,"failed":0}
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:00.927: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1403
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-1403
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1403 to expose endpoints map[]
Dec 14 09:00:01.081: INFO: successfully validated that service endpoint-test2 in namespace services-1403 exposes endpoints map[] (16.839988ms elapsed)
STEP: Creating pod pod1 in namespace services-1403
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1403 to expose endpoints map[pod1:[80]]
Dec 14 09:00:03.148: INFO: successfully validated that service endpoint-test2 in namespace services-1403 exposes endpoints map[pod1:[80]] (2.057546279s elapsed)
STEP: Creating pod pod2 in namespace services-1403
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1403 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 14 09:00:05.180: INFO: successfully validated that service endpoint-test2 in namespace services-1403 exposes endpoints map[pod1:[80] pod2:[80]] (2.0255219s elapsed)
STEP: Deleting pod pod1 in namespace services-1403
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1403 to expose endpoints map[pod2:[80]]
Dec 14 09:00:05.201: INFO: successfully validated that service endpoint-test2 in namespace services-1403 exposes endpoints map[pod2:[80]] (17.598596ms elapsed)
STEP: Deleting pod pod2 in namespace services-1403
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1403 to expose endpoints map[]
Dec 14 09:00:06.212: INFO: successfully validated that service endpoint-test2 in namespace services-1403 exposes endpoints map[] (1.006555344s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:06.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1403" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:5.316 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":63,"skipped":839,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:06.244: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 09:00:06.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83b07b34-4157-4c47-8ef7-765d407ee651" in namespace "projected-5537" to be "success or failure"
Dec 14 09:00:06.404: INFO: Pod "downwardapi-volume-83b07b34-4157-4c47-8ef7-765d407ee651": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055054ms
Dec 14 09:00:08.408: INFO: Pod "downwardapi-volume-83b07b34-4157-4c47-8ef7-765d407ee651": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010093554s
STEP: Saw pod success
Dec 14 09:00:08.408: INFO: Pod "downwardapi-volume-83b07b34-4157-4c47-8ef7-765d407ee651" satisfied condition "success or failure"
Dec 14 09:00:08.410: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-83b07b34-4157-4c47-8ef7-765d407ee651 container client-container: <nil>
STEP: delete the pod
Dec 14 09:00:08.429: INFO: Waiting for pod downwardapi-volume-83b07b34-4157-4c47-8ef7-765d407ee651 to disappear
Dec 14 09:00:08.435: INFO: Pod downwardapi-volume-83b07b34-4157-4c47-8ef7-765d407ee651 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:08.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5537" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":64,"skipped":852,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:08.447: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 14 09:00:08.593: INFO: Waiting up to 5m0s for pod "pod-8373f42d-5d4d-4bb0-af1b-d1fa42751745" in namespace "emptydir-9680" to be "success or failure"
Dec 14 09:00:08.598: INFO: Pod "pod-8373f42d-5d4d-4bb0-af1b-d1fa42751745": Phase="Pending", Reason="", readiness=false. Elapsed: 5.368469ms
Dec 14 09:00:10.601: INFO: Pod "pod-8373f42d-5d4d-4bb0-af1b-d1fa42751745": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00775602s
STEP: Saw pod success
Dec 14 09:00:10.601: INFO: Pod "pod-8373f42d-5d4d-4bb0-af1b-d1fa42751745" satisfied condition "success or failure"
Dec 14 09:00:10.603: INFO: Trying to get logs from node k8s-2 pod pod-8373f42d-5d4d-4bb0-af1b-d1fa42751745 container test-container: <nil>
STEP: delete the pod
Dec 14 09:00:10.616: INFO: Waiting for pod pod-8373f42d-5d4d-4bb0-af1b-d1fa42751745 to disappear
Dec 14 09:00:10.618: INFO: Pod pod-8373f42d-5d4d-4bb0-af1b-d1fa42751745 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:10.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9680" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":65,"skipped":854,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:10.625: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4389
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:00:11.210: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:00:14.228: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:14.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4389" for this suite.
STEP: Destroying namespace "webhook-4389-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":66,"skipped":890,"failed":0}

------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:14.429: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:00:14.603: INFO: Creating deployment "test-recreate-deployment"
Dec 14 09:00:14.609: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 14 09:00:14.626: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 14 09:00:16.630: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 14 09:00:16.632: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 14 09:00:16.637: INFO: Updating deployment test-recreate-deployment
Dec 14 09:00:16.637: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Dec 14 09:00:16.742: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7972 /apis/apps/v1/namespaces/deployment-7972/deployments/test-recreate-deployment e19d9ae0-d867-4e49-b635-90a392f7510f 8852 2 2019-12-14 09:00:14 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0042cbe28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-14 09:00:16 +0000 UTC,LastTransitionTime:2019-12-14 09:00:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2019-12-14 09:00:16 +0000 UTC,LastTransitionTime:2019-12-14 09:00:14 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 14 09:00:16.745: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-7972 /apis/apps/v1/namespaces/deployment-7972/replicasets/test-recreate-deployment-5f94c574ff 8b88aca4-6337-4b4c-b909-50903deabbf9 8849 1 2019-12-14 09:00:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment e19d9ae0-d867-4e49-b635-90a392f7510f 0xc0042fc417 0xc0042fc418}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0042fc4a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 09:00:16.745: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 14 09:00:16.745: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-7972 /apis/apps/v1/namespaces/deployment-7972/replicasets/test-recreate-deployment-799c574856 16272324-a101-4747-b81b-444e98fd31f1 8839 2 2019-12-14 09:00:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment e19d9ae0-d867-4e49-b635-90a392f7510f 0xc0042fc537 0xc0042fc538}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0042fc5b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 09:00:16.747: INFO: Pod "test-recreate-deployment-5f94c574ff-lkv9m" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-lkv9m test-recreate-deployment-5f94c574ff- deployment-7972 /api/v1/namespaces/deployment-7972/pods/test-recreate-deployment-5f94c574ff-lkv9m 56056e41-e2fc-445e-a6ee-df2ec07c7a5c 8853 0 2019-12-14 09:00:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 8b88aca4-6337-4b4c-b909-50903deabbf9 0xc0042fccc7 0xc0042fccc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-phcpj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-phcpj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-phcpj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:00:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:00:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:00:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:00:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.5,PodIP:,StartTime:2019-12-14 09:00:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:16.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7972" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":67,"skipped":890,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:16.756: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Dec 14 09:00:16.917: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:19.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-552" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":68,"skipped":891,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:19.736: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Dec 14 09:00:19.892: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 09:00:19.921: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 09:00:19.928: INFO: 
Logging pods the kubelet thinks is on node k8s-2 before test
Dec 14 09:00:19.935: INFO: kube-flannel-ds-amd64-282d7 from kube-system started at 2019-12-14 08:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 09:00:19.935: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 09:00:19.935: INFO: sonobuoy-systemd-logs-daemon-set-59631ba8c0bd4cad-xc9m6 from sonobuoy started at 2019-12-14 08:47:19 +0000 UTC (2 container statuses recorded)
Dec 14 09:00:19.935: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 09:00:19.935: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 09:00:19.935: INFO: traefik-ingress-controller-5bcw7 from kube-system started at 2019-12-14 08:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 09:00:19.935: INFO: 	Container traefik-ingress-lb ready: true, restart count 0
Dec 14 09:00:19.935: INFO: test-recreate-deployment-5f94c574ff-lkv9m from deployment-7972 started at 2019-12-14 09:00:16 +0000 UTC (1 container statuses recorded)
Dec 14 09:00:19.935: INFO: 	Container httpd ready: true, restart count 0
Dec 14 09:00:19.935: INFO: pod-init-07760c3d-0a51-46c6-b710-d0c4c5617dfa from init-container-552 started at 2019-12-14 09:00:16 +0000 UTC (1 container statuses recorded)
Dec 14 09:00:19.935: INFO: 	Container run1 ready: false, restart count 0
Dec 14 09:00:19.935: INFO: 
Logging pods the kubelet thinks is on node k8s-3 before test
Dec 14 09:00:19.951: INFO: linkerd-prometheus-68779bb867-5d2ff from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:00:19.951: INFO: 	Container prometheus ready: true, restart count 0
Dec 14 09:00:19.951: INFO: linkerd-tap-85775cdf7f-qh42m from linkerd started at 2019-12-14 08:38:31 +0000 UTC (2 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:00:19.951: INFO: 	Container tap ready: true, restart count 0
Dec 14 09:00:19.951: INFO: traefik-ingress-controller-5sddf from kube-system started at 2019-12-14 08:38:22 +0000 UTC (1 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container traefik-ingress-lb ready: true, restart count 0
Dec 14 09:00:19.951: INFO: kubernetes-metrics-scraper-6b97c6d857-x7vmq from kubernetes-dashboard started at 2019-12-14 08:38:23 +0000 UTC (1 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container kubernetes-metrics-scraper ready: true, restart count 0
Dec 14 09:00:19.951: INFO: linkerd-destination-77564695ff-fv2kq from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container destination ready: true, restart count 0
Dec 14 09:00:19.951: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:00:19.951: INFO: coredns-b7f8c8654-f8h4p from kube-system started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:00:19.951: INFO: coredns-b7f8c8654-cw42m from kube-system started at 2019-12-14 08:38:21 +0000 UTC (1 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:00:19.951: INFO: linkerd-proxy-injector-7cbf76d445-xnj8j from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:00:19.951: INFO: 	Container proxy-injector ready: true, restart count 0
Dec 14 09:00:19.951: INFO: linkerd-web-864c75894b-946dh from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:00:19.951: INFO: 	Container web ready: true, restart count 0
Dec 14 09:00:19.951: INFO: linkerd-sp-validator-689c4bcc4d-8j98j from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:00:19.951: INFO: 	Container sp-validator ready: true, restart count 0
Dec 14 09:00:19.951: INFO: linkerd-grafana-79c6dd8fc-v6gln from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container grafana ready: true, restart count 0
Dec 14 09:00:19.951: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:00:19.951: INFO: tiller-deploy-5798768fb-f49sm from kube-system started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container tiller ready: true, restart count 0
Dec 14 09:00:19.951: INFO: linkerd-identity-d55b64556-7kjgh from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container identity ready: true, restart count 0
Dec 14 09:00:19.951: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:00:19.951: INFO: sonobuoy from sonobuoy started at 2019-12-14 08:47:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:00:19.951: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 09:00:19.951: INFO: sonobuoy-systemd-logs-daemon-set-59631ba8c0bd4cad-k2bx9 from sonobuoy started at 2019-12-14 08:47:19 +0000 UTC (2 container statuses recorded)
Dec 14 09:00:19.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 09:00:19.952: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 09:00:19.952: INFO: kube-flannel-ds-amd64-5d54w from kube-system started at 2019-12-14 08:38:20 +0000 UTC (1 container statuses recorded)
Dec 14 09:00:19.952: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 09:00:19.952: INFO: linkerd-controller-57d9d8f5dd-sm2ls from linkerd started at 2019-12-14 08:50:41 +0000 UTC (3 container statuses recorded)
Dec 14 09:00:19.952: INFO: 	Container destination ready: true, restart count 0
Dec 14 09:00:19.952: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:00:19.952: INFO: 	Container public-api ready: true, restart count 0
Dec 14 09:00:19.952: INFO: kubernetes-dashboard-bf855c94d-npnfm from kubernetes-dashboard started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:00:19.952: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c441c88f-04d9-4dd3-9d06-514e00804f67 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c441c88f-04d9-4dd3-9d06-514e00804f67 off the node k8s-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c441c88f-04d9-4dd3-9d06-514e00804f67
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:24.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2477" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":69,"skipped":904,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:24.032: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:00:24.829: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:00:27.846: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:27.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2501" for this suite.
STEP: Destroying namespace "webhook-2501-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":70,"skipped":926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:27.946: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7812
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:34.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7812" for this suite.

• [SLOW TEST:6.202 seconds]
[sig-apps] Job
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":71,"skipped":955,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:34.147: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Dec 14 09:00:34.290: INFO: Waiting up to 5m0s for pod "var-expansion-09091024-964e-4562-9115-4f467fc98f9e" in namespace "var-expansion-9029" to be "success or failure"
Dec 14 09:00:34.300: INFO: Pod "var-expansion-09091024-964e-4562-9115-4f467fc98f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.573164ms
Dec 14 09:00:36.303: INFO: Pod "var-expansion-09091024-964e-4562-9115-4f467fc98f9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013147288s
STEP: Saw pod success
Dec 14 09:00:36.303: INFO: Pod "var-expansion-09091024-964e-4562-9115-4f467fc98f9e" satisfied condition "success or failure"
Dec 14 09:00:36.305: INFO: Trying to get logs from node k8s-2 pod var-expansion-09091024-964e-4562-9115-4f467fc98f9e container dapi-container: <nil>
STEP: delete the pod
Dec 14 09:00:36.318: INFO: Waiting for pod var-expansion-09091024-964e-4562-9115-4f467fc98f9e to disappear
Dec 14 09:00:36.320: INFO: Pod var-expansion-09091024-964e-4562-9115-4f467fc98f9e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:36.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9029" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":72,"skipped":959,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:36.328: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6969
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-e6f168d2-c5b4-4c74-89c2-6dbe95b353d3
STEP: Creating a pod to test consume configMaps
Dec 14 09:00:36.469: INFO: Waiting up to 5m0s for pod "pod-configmaps-2bd6727b-a2b9-4c8a-a99c-d184a1626278" in namespace "configmap-6969" to be "success or failure"
Dec 14 09:00:36.474: INFO: Pod "pod-configmaps-2bd6727b-a2b9-4c8a-a99c-d184a1626278": Phase="Pending", Reason="", readiness=false. Elapsed: 5.597845ms
Dec 14 09:00:38.477: INFO: Pod "pod-configmaps-2bd6727b-a2b9-4c8a-a99c-d184a1626278": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008002357s
STEP: Saw pod success
Dec 14 09:00:38.477: INFO: Pod "pod-configmaps-2bd6727b-a2b9-4c8a-a99c-d184a1626278" satisfied condition "success or failure"
Dec 14 09:00:38.478: INFO: Trying to get logs from node k8s-2 pod pod-configmaps-2bd6727b-a2b9-4c8a-a99c-d184a1626278 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 09:00:38.491: INFO: Waiting for pod pod-configmaps-2bd6727b-a2b9-4c8a-a99c-d184a1626278 to disappear
Dec 14 09:00:38.494: INFO: Pod pod-configmaps-2bd6727b-a2b9-4c8a-a99c-d184a1626278 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:38.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6969" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":73,"skipped":972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:38.500: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2916
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 09:00:38.638: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c518e4b5-bd12-41e5-8133-73e0396ac18a" in namespace "projected-2916" to be "success or failure"
Dec 14 09:00:38.645: INFO: Pod "downwardapi-volume-c518e4b5-bd12-41e5-8133-73e0396ac18a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.777758ms
Dec 14 09:00:40.647: INFO: Pod "downwardapi-volume-c518e4b5-bd12-41e5-8133-73e0396ac18a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009126205s
STEP: Saw pod success
Dec 14 09:00:40.647: INFO: Pod "downwardapi-volume-c518e4b5-bd12-41e5-8133-73e0396ac18a" satisfied condition "success or failure"
Dec 14 09:00:40.648: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-c518e4b5-bd12-41e5-8133-73e0396ac18a container client-container: <nil>
STEP: delete the pod
Dec 14 09:00:40.662: INFO: Waiting for pod downwardapi-volume-c518e4b5-bd12-41e5-8133-73e0396ac18a to disappear
Dec 14 09:00:40.664: INFO: Pod downwardapi-volume-c518e4b5-bd12-41e5-8133-73e0396ac18a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:40.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2916" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":74,"skipped":1017,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:40.669: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1925
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 14 09:00:45.322: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ca034ebf-ad3d-49ee-a115-4bf9ff6ae84f"
Dec 14 09:00:45.322: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ca034ebf-ad3d-49ee-a115-4bf9ff6ae84f" in namespace "pods-1925" to be "terminated due to deadline exceeded"
Dec 14 09:00:45.324: INFO: Pod "pod-update-activedeadlineseconds-ca034ebf-ad3d-49ee-a115-4bf9ff6ae84f": Phase="Running", Reason="", readiness=true. Elapsed: 1.922296ms
Dec 14 09:00:47.326: INFO: Pod "pod-update-activedeadlineseconds-ca034ebf-ad3d-49ee-a115-4bf9ff6ae84f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.004054372s
Dec 14 09:00:47.326: INFO: Pod "pod-update-activedeadlineseconds-ca034ebf-ad3d-49ee-a115-4bf9ff6ae84f" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:00:47.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1925" for this suite.

• [SLOW TEST:6.662 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":75,"skipped":1047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:00:47.332: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:01:03.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9259" for this suite.

• [SLOW TEST:16.177 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":76,"skipped":1090,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:01:03.510: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-40
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1214 09:01:04.725162      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 14 09:01:04.725: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:01:04.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-40" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":77,"skipped":1107,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:01:04.731: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8681
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8681.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8681.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8681.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8681.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8681.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8681.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 09:01:06.950: INFO: DNS probes using dns-8681/dns-test-5b6400d0-7408-4eec-b890-dab3dcc821af succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:01:06.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8681" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":78,"skipped":1113,"failed":0}

------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:01:06.969: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7666
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Dec 14 09:01:07.112: INFO: Waiting up to 5m0s for pod "client-containers-99da988b-8ed8-4548-9912-d00ae63f5c36" in namespace "containers-7666" to be "success or failure"
Dec 14 09:01:07.126: INFO: Pod "client-containers-99da988b-8ed8-4548-9912-d00ae63f5c36": Phase="Pending", Reason="", readiness=false. Elapsed: 14.050811ms
Dec 14 09:01:09.128: INFO: Pod "client-containers-99da988b-8ed8-4548-9912-d00ae63f5c36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015974292s
STEP: Saw pod success
Dec 14 09:01:09.128: INFO: Pod "client-containers-99da988b-8ed8-4548-9912-d00ae63f5c36" satisfied condition "success or failure"
Dec 14 09:01:09.130: INFO: Trying to get logs from node k8s-2 pod client-containers-99da988b-8ed8-4548-9912-d00ae63f5c36 container test-container: <nil>
STEP: delete the pod
Dec 14 09:01:09.145: INFO: Waiting for pod client-containers-99da988b-8ed8-4548-9912-d00ae63f5c36 to disappear
Dec 14 09:01:09.148: INFO: Pod client-containers-99da988b-8ed8-4548-9912-d00ae63f5c36 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:01:09.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7666" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":79,"skipped":1113,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:01:09.154: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5087
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 14 09:01:09.338: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:01:18.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5087" for this suite.

• [SLOW TEST:9.036 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":80,"skipped":1115,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:01:18.190: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4419
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Dec 14 09:01:18.330: INFO: Waiting up to 5m0s for pod "downward-api-0fd1956e-20d2-4c65-bdbf-f792a959cf71" in namespace "downward-api-4419" to be "success or failure"
Dec 14 09:01:18.336: INFO: Pod "downward-api-0fd1956e-20d2-4c65-bdbf-f792a959cf71": Phase="Pending", Reason="", readiness=false. Elapsed: 5.845032ms
Dec 14 09:01:20.338: INFO: Pod "downward-api-0fd1956e-20d2-4c65-bdbf-f792a959cf71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008327995s
STEP: Saw pod success
Dec 14 09:01:20.338: INFO: Pod "downward-api-0fd1956e-20d2-4c65-bdbf-f792a959cf71" satisfied condition "success or failure"
Dec 14 09:01:20.340: INFO: Trying to get logs from node k8s-2 pod downward-api-0fd1956e-20d2-4c65-bdbf-f792a959cf71 container dapi-container: <nil>
STEP: delete the pod
Dec 14 09:01:20.353: INFO: Waiting for pod downward-api-0fd1956e-20d2-4c65-bdbf-f792a959cf71 to disappear
Dec 14 09:01:20.356: INFO: Pod downward-api-0fd1956e-20d2-4c65-bdbf-f792a959cf71 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:01:20.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4419" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":81,"skipped":1126,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:01:20.362: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1653
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:01:20.491: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:01:22.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1653" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":82,"skipped":1142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:01:22.638: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 14 09:01:22.780: INFO: Waiting up to 5m0s for pod "pod-a33bbcdf-3e50-423b-9cfb-dff4832b0d15" in namespace "emptydir-5967" to be "success or failure"
Dec 14 09:01:22.786: INFO: Pod "pod-a33bbcdf-3e50-423b-9cfb-dff4832b0d15": Phase="Pending", Reason="", readiness=false. Elapsed: 5.431211ms
Dec 14 09:01:24.788: INFO: Pod "pod-a33bbcdf-3e50-423b-9cfb-dff4832b0d15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007800989s
STEP: Saw pod success
Dec 14 09:01:24.788: INFO: Pod "pod-a33bbcdf-3e50-423b-9cfb-dff4832b0d15" satisfied condition "success or failure"
Dec 14 09:01:24.790: INFO: Trying to get logs from node k8s-2 pod pod-a33bbcdf-3e50-423b-9cfb-dff4832b0d15 container test-container: <nil>
STEP: delete the pod
Dec 14 09:01:24.804: INFO: Waiting for pod pod-a33bbcdf-3e50-423b-9cfb-dff4832b0d15 to disappear
Dec 14 09:01:24.806: INFO: Pod pod-a33bbcdf-3e50-423b-9cfb-dff4832b0d15 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:01:24.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5967" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":83,"skipped":1181,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:01:24.811: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9985
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Dec 14 09:01:24.948: INFO: Waiting up to 5m0s for pod "client-containers-556623ef-36b1-409b-a9ee-1dcf9718d752" in namespace "containers-9985" to be "success or failure"
Dec 14 09:01:24.954: INFO: Pod "client-containers-556623ef-36b1-409b-a9ee-1dcf9718d752": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093456ms
Dec 14 09:01:26.956: INFO: Pod "client-containers-556623ef-36b1-409b-a9ee-1dcf9718d752": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007992063s
STEP: Saw pod success
Dec 14 09:01:26.956: INFO: Pod "client-containers-556623ef-36b1-409b-a9ee-1dcf9718d752" satisfied condition "success or failure"
Dec 14 09:01:26.958: INFO: Trying to get logs from node k8s-2 pod client-containers-556623ef-36b1-409b-a9ee-1dcf9718d752 container test-container: <nil>
STEP: delete the pod
Dec 14 09:01:26.969: INFO: Waiting for pod client-containers-556623ef-36b1-409b-a9ee-1dcf9718d752 to disappear
Dec 14 09:01:26.971: INFO: Pod client-containers-556623ef-36b1-409b-a9ee-1dcf9718d752 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:01:26.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9985" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":84,"skipped":1184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:01:26.979: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6279
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-6279
STEP: creating replication controller nodeport-test in namespace services-6279
I1214 09:01:27.182468      18 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-6279, replica count: 2
Dec 14 09:01:30.232: INFO: Creating new exec pod
I1214 09:01:30.232918      18 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 09:01:33.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=services-6279 execpodw6lq9 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Dec 14 09:01:33.474: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 14 09:01:33.474: INFO: stdout: ""
Dec 14 09:01:33.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=services-6279 execpodw6lq9 -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.205 80'
Dec 14 09:01:33.748: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.205 80\nConnection to 10.32.0.205 80 port [tcp/http] succeeded!\n"
Dec 14 09:01:33.748: INFO: stdout: ""
Dec 14 09:01:33.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=services-6279 execpodw6lq9 -- /bin/sh -x -c nc -zv -t -w 2 10.20.20.5 30001'
Dec 14 09:01:33.981: INFO: stderr: "+ nc -zv -t -w 2 10.20.20.5 30001\nConnection to 10.20.20.5 30001 port [tcp/30001] succeeded!\n"
Dec 14 09:01:33.981: INFO: stdout: ""
Dec 14 09:01:33.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=services-6279 execpodw6lq9 -- /bin/sh -x -c nc -zv -t -w 2 10.20.20.6 30001'
Dec 14 09:01:34.224: INFO: stderr: "+ nc -zv -t -w 2 10.20.20.6 30001\nConnection to 10.20.20.6 30001 port [tcp/30001] succeeded!\n"
Dec 14 09:01:34.224: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:01:34.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6279" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.252 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":85,"skipped":1234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:01:34.231: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3367
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-32a405f5-3c33-4f70-9c11-43d791ebd540 in namespace container-probe-3367
Dec 14 09:01:36.380: INFO: Started pod busybox-32a405f5-3c33-4f70-9c11-43d791ebd540 in namespace container-probe-3367
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 09:01:36.382: INFO: Initial restart count of pod busybox-32a405f5-3c33-4f70-9c11-43d791ebd540 is 0
Dec 14 09:02:30.468: INFO: Restart count of pod container-probe-3367/busybox-32a405f5-3c33-4f70-9c11-43d791ebd540 is now 1 (54.085355772s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:02:30.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3367" for this suite.

• [SLOW TEST:56.254 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":86,"skipped":1292,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:02:30.486: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-3fb27fb5-106a-4bc4-ae4a-4acbe9f9349a
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:02:30.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":87,"skipped":1294,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:02:30.621: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-4401/configmap-test-fe08b3bf-9de5-4496-9b5d-e8b27c6f87cc
STEP: Creating a pod to test consume configMaps
Dec 14 09:02:30.761: INFO: Waiting up to 5m0s for pod "pod-configmaps-f7a80db4-aa9d-43d3-ad26-fe852ac0ee46" in namespace "configmap-4401" to be "success or failure"
Dec 14 09:02:30.766: INFO: Pod "pod-configmaps-f7a80db4-aa9d-43d3-ad26-fe852ac0ee46": Phase="Pending", Reason="", readiness=false. Elapsed: 5.073233ms
Dec 14 09:02:32.769: INFO: Pod "pod-configmaps-f7a80db4-aa9d-43d3-ad26-fe852ac0ee46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007228639s
STEP: Saw pod success
Dec 14 09:02:32.769: INFO: Pod "pod-configmaps-f7a80db4-aa9d-43d3-ad26-fe852ac0ee46" satisfied condition "success or failure"
Dec 14 09:02:32.770: INFO: Trying to get logs from node k8s-2 pod pod-configmaps-f7a80db4-aa9d-43d3-ad26-fe852ac0ee46 container env-test: <nil>
STEP: delete the pod
Dec 14 09:02:32.784: INFO: Waiting for pod pod-configmaps-f7a80db4-aa9d-43d3-ad26-fe852ac0ee46 to disappear
Dec 14 09:02:32.787: INFO: Pod pod-configmaps-f7a80db4-aa9d-43d3-ad26-fe852ac0ee46 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:02:32.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4401" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":88,"skipped":1302,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:02:32.793: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:02:32.936: INFO: Creating ReplicaSet my-hostname-basic-95a882de-50df-451b-8236-81c94fafe832
Dec 14 09:02:32.945: INFO: Pod name my-hostname-basic-95a882de-50df-451b-8236-81c94fafe832: Found 0 pods out of 1
Dec 14 09:02:37.948: INFO: Pod name my-hostname-basic-95a882de-50df-451b-8236-81c94fafe832: Found 1 pods out of 1
Dec 14 09:02:37.948: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-95a882de-50df-451b-8236-81c94fafe832" is running
Dec 14 09:02:37.951: INFO: Pod "my-hostname-basic-95a882de-50df-451b-8236-81c94fafe832-2n4hj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-14 09:02:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-14 09:02:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-14 09:02:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-14 09:02:32 +0000 UTC Reason: Message:}])
Dec 14 09:02:37.951: INFO: Trying to dial the pod
Dec 14 09:02:42.958: INFO: Controller my-hostname-basic-95a882de-50df-451b-8236-81c94fafe832: Got expected result from replica 1 [my-hostname-basic-95a882de-50df-451b-8236-81c94fafe832-2n4hj]: "my-hostname-basic-95a882de-50df-451b-8236-81c94fafe832-2n4hj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:02:42.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9407" for this suite.

• [SLOW TEST:10.171 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":89,"skipped":1305,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:02:42.964: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9759
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-a653b928-5702-4e07-a4a0-3092690b9738 in namespace container-probe-9759
Dec 14 09:02:45.110: INFO: Started pod busybox-a653b928-5702-4e07-a4a0-3092690b9738 in namespace container-probe-9759
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 09:02:45.111: INFO: Initial restart count of pod busybox-a653b928-5702-4e07-a4a0-3092690b9738 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:06:45.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9759" for this suite.

• [SLOW TEST:242.634 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":90,"skipped":1325,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:06:45.599: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2990
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:06:45.730: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:06:46.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2990" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":91,"skipped":1343,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:06:46.758: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9859
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Dec 14 09:06:46.902: INFO: Waiting up to 5m0s for pod "downward-api-824ee3ac-4db4-42f4-a91e-7eff3060de06" in namespace "downward-api-9859" to be "success or failure"
Dec 14 09:06:46.909: INFO: Pod "downward-api-824ee3ac-4db4-42f4-a91e-7eff3060de06": Phase="Pending", Reason="", readiness=false. Elapsed: 6.390353ms
Dec 14 09:06:48.911: INFO: Pod "downward-api-824ee3ac-4db4-42f4-a91e-7eff3060de06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008702461s
STEP: Saw pod success
Dec 14 09:06:48.911: INFO: Pod "downward-api-824ee3ac-4db4-42f4-a91e-7eff3060de06" satisfied condition "success or failure"
Dec 14 09:06:48.913: INFO: Trying to get logs from node k8s-2 pod downward-api-824ee3ac-4db4-42f4-a91e-7eff3060de06 container dapi-container: <nil>
STEP: delete the pod
Dec 14 09:06:48.927: INFO: Waiting for pod downward-api-824ee3ac-4db4-42f4-a91e-7eff3060de06 to disappear
Dec 14 09:06:48.929: INFO: Pod downward-api-824ee3ac-4db4-42f4-a91e-7eff3060de06 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:06:48.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9859" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":92,"skipped":1356,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:06:48.935: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9641
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:06:49.067: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:06:57.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9641" for this suite.

• [SLOW TEST:8.834 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":93,"skipped":1373,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:06:57.768: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9274
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 14 09:07:01.981: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9274 PodName:pod-sharedvolume-c1b9ca44-f7ed-4b1c-a2a8-0c8cc618cfc5 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:07:01.981: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:07:02.115: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:07:02.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9274" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":94,"skipped":1376,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:07:02.123: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 09:07:02.272: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d704037a-af94-4dcb-9853-06aa20ca9c73" in namespace "downward-api-5577" to be "success or failure"
Dec 14 09:07:02.278: INFO: Pod "downwardapi-volume-d704037a-af94-4dcb-9853-06aa20ca9c73": Phase="Pending", Reason="", readiness=false. Elapsed: 6.183932ms
Dec 14 09:07:04.280: INFO: Pod "downwardapi-volume-d704037a-af94-4dcb-9853-06aa20ca9c73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008532407s
STEP: Saw pod success
Dec 14 09:07:04.281: INFO: Pod "downwardapi-volume-d704037a-af94-4dcb-9853-06aa20ca9c73" satisfied condition "success or failure"
Dec 14 09:07:04.283: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-d704037a-af94-4dcb-9853-06aa20ca9c73 container client-container: <nil>
STEP: delete the pod
Dec 14 09:07:04.297: INFO: Waiting for pod downwardapi-volume-d704037a-af94-4dcb-9853-06aa20ca9c73 to disappear
Dec 14 09:07:04.303: INFO: Pod downwardapi-volume-d704037a-af94-4dcb-9853-06aa20ca9c73 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:07:04.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5577" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":95,"skipped":1386,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:07:04.309: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8791
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-20420eae-d050-4870-a871-c62a2bcaf572
STEP: Creating secret with name s-test-opt-upd-80c1fdb8-7c00-4230-a80c-ab7df64c0d27
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-20420eae-d050-4870-a871-c62a2bcaf572
STEP: Updating secret s-test-opt-upd-80c1fdb8-7c00-4230-a80c-ab7df64c0d27
STEP: Creating secret with name s-test-opt-create-dc38dcac-7154-4d29-a7b3-dfb9bc4f9d31
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:08:32.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8791" for this suite.

• [SLOW TEST:88.533 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":96,"skipped":1397,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:08:32.842: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-201
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:08:33.632: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:08:36.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:08:36.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-201" for this suite.
STEP: Destroying namespace "webhook-201-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":97,"skipped":1397,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:08:36.752: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9441
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9441
STEP: creating replication controller externalsvc in namespace services-9441
I1214 09:08:36.951066      18 runners.go:189] Created replication controller with name: externalsvc, namespace: services-9441, replica count: 2
I1214 09:08:40.001522      18 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 14 09:08:40.010: INFO: Creating new exec pod
Dec 14 09:08:42.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=services-9441 execpodctssj -- /bin/sh -x -c nslookup clusterip-service'
Dec 14 09:08:42.362: INFO: stderr: "+ nslookup clusterip-service\n"
Dec 14 09:08:42.362: INFO: stdout: "Server:\t\t10.32.0.10\nAddress:\t10.32.0.10#53\n\nclusterip-service.services-9441.svc.cluster.local\tcanonical name = externalsvc.services-9441.svc.cluster.local.\nName:\texternalsvc.services-9441.svc.cluster.local\nAddress: 10.32.0.168\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9441, will wait for the garbage collector to delete the pods
Dec 14 09:08:42.421: INFO: Deleting ReplicationController externalsvc took: 4.29104ms
Dec 14 09:08:42.521: INFO: Terminating ReplicationController externalsvc pods took: 100.222951ms
Dec 14 09:08:48.444: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:08:48.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9441" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:11.716 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":98,"skipped":1397,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:08:48.468: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:08:49.049: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:08:52.059: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:08:52.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3646" for this suite.
STEP: Destroying namespace "webhook-3646-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":99,"skipped":1402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:08:52.156: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8950
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:08:54.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8950" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":100,"skipped":1452,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:08:54.335: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-1051
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:08:54.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1051" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":101,"skipped":1482,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:08:54.500: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1739
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Dec 14 09:08:54.651: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 09:08:54.658: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 09:08:54.660: INFO: 
Logging pods the kubelet thinks is on node k8s-2 before test
Dec 14 09:08:54.664: INFO: kube-flannel-ds-amd64-282d7 from kube-system started at 2019-12-14 08:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 09:08:54.665: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 09:08:54.665: INFO: sonobuoy-systemd-logs-daemon-set-59631ba8c0bd4cad-xc9m6 from sonobuoy started at 2019-12-14 08:47:19 +0000 UTC (2 container statuses recorded)
Dec 14 09:08:54.665: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 09:08:54.665: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 09:08:54.665: INFO: traefik-ingress-controller-5bcw7 from kube-system started at 2019-12-14 08:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 09:08:54.665: INFO: 	Container traefik-ingress-lb ready: true, restart count 0
Dec 14 09:08:54.665: INFO: busybox-host-aliases7faf661f-26ed-43f3-964e-d3b7ced1c3b9 from kubelet-test-8950 started at 2019-12-14 09:08:52 +0000 UTC (1 container statuses recorded)
Dec 14 09:08:54.665: INFO: 	Container busybox-host-aliases7faf661f-26ed-43f3-964e-d3b7ced1c3b9 ready: true, restart count 0
Dec 14 09:08:54.665: INFO: 
Logging pods the kubelet thinks is on node k8s-3 before test
Dec 14 09:08:54.678: INFO: tiller-deploy-5798768fb-f49sm from kube-system started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:08:54.678: INFO: 	Container tiller ready: true, restart count 0
Dec 14 09:08:54.678: INFO: linkerd-identity-d55b64556-7kjgh from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:08:54.678: INFO: 	Container identity ready: true, restart count 0
Dec 14 09:08:54.678: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:08:54.678: INFO: sonobuoy from sonobuoy started at 2019-12-14 08:47:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:08:54.678: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 09:08:54.678: INFO: sonobuoy-systemd-logs-daemon-set-59631ba8c0bd4cad-k2bx9 from sonobuoy started at 2019-12-14 08:47:19 +0000 UTC (2 container statuses recorded)
Dec 14 09:08:54.678: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 09:08:54.678: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 09:08:54.678: INFO: linkerd-sp-validator-689c4bcc4d-8j98j from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:08:54.678: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:08:54.678: INFO: 	Container sp-validator ready: true, restart count 0
Dec 14 09:08:54.678: INFO: linkerd-grafana-79c6dd8fc-v6gln from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:08:54.678: INFO: 	Container grafana ready: true, restart count 0
Dec 14 09:08:54.678: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:08:54.678: INFO: kube-flannel-ds-amd64-5d54w from kube-system started at 2019-12-14 08:38:20 +0000 UTC (1 container statuses recorded)
Dec 14 09:08:54.678: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 09:08:54.678: INFO: linkerd-controller-57d9d8f5dd-sm2ls from linkerd started at 2019-12-14 08:50:41 +0000 UTC (3 container statuses recorded)
Dec 14 09:08:54.678: INFO: 	Container destination ready: true, restart count 0
Dec 14 09:08:54.678: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:08:54.678: INFO: 	Container public-api ready: true, restart count 0
Dec 14 09:08:54.678: INFO: kubernetes-dashboard-bf855c94d-npnfm from kubernetes-dashboard started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:08:54.678: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 14 09:08:54.678: INFO: traefik-ingress-controller-5sddf from kube-system started at 2019-12-14 08:38:22 +0000 UTC (1 container statuses recorded)
Dec 14 09:08:54.678: INFO: 	Container traefik-ingress-lb ready: true, restart count 0
Dec 14 09:08:54.678: INFO: kubernetes-metrics-scraper-6b97c6d857-x7vmq from kubernetes-dashboard started at 2019-12-14 08:38:23 +0000 UTC (1 container statuses recorded)
Dec 14 09:08:54.678: INFO: 	Container kubernetes-metrics-scraper ready: true, restart count 0
Dec 14 09:08:54.678: INFO: linkerd-destination-77564695ff-fv2kq from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:08:54.678: INFO: 	Container destination ready: true, restart count 0
Dec 14 09:08:54.678: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:08:54.678: INFO: linkerd-prometheus-68779bb867-5d2ff from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:08:54.678: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:08:54.678: INFO: 	Container prometheus ready: true, restart count 0
Dec 14 09:08:54.679: INFO: linkerd-tap-85775cdf7f-qh42m from linkerd started at 2019-12-14 08:38:31 +0000 UTC (2 container statuses recorded)
Dec 14 09:08:54.679: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:08:54.679: INFO: 	Container tap ready: true, restart count 0
Dec 14 09:08:54.679: INFO: coredns-b7f8c8654-cw42m from kube-system started at 2019-12-14 08:38:21 +0000 UTC (1 container statuses recorded)
Dec 14 09:08:54.679: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:08:54.679: INFO: linkerd-proxy-injector-7cbf76d445-xnj8j from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:08:54.679: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:08:54.679: INFO: 	Container proxy-injector ready: true, restart count 0
Dec 14 09:08:54.679: INFO: linkerd-web-864c75894b-946dh from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:08:54.679: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:08:54.679: INFO: 	Container web ready: true, restart count 0
Dec 14 09:08:54.679: INFO: coredns-b7f8c8654-f8h4p from kube-system started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:08:54.679: INFO: 	Container coredns ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node k8s-2
STEP: verifying the node has the label node k8s-3
Dec 14 09:08:54.717: INFO: Pod coredns-b7f8c8654-cw42m requesting resource cpu=100m on Node k8s-3
Dec 14 09:08:54.717: INFO: Pod coredns-b7f8c8654-f8h4p requesting resource cpu=100m on Node k8s-3
Dec 14 09:08:54.717: INFO: Pod kube-flannel-ds-amd64-282d7 requesting resource cpu=100m on Node k8s-2
Dec 14 09:08:54.717: INFO: Pod kube-flannel-ds-amd64-5d54w requesting resource cpu=100m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod tiller-deploy-5798768fb-f49sm requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod traefik-ingress-controller-5bcw7 requesting resource cpu=0m on Node k8s-2
Dec 14 09:08:54.718: INFO: Pod traefik-ingress-controller-5sddf requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod busybox-host-aliases7faf661f-26ed-43f3-964e-d3b7ced1c3b9 requesting resource cpu=0m on Node k8s-2
Dec 14 09:08:54.718: INFO: Pod kubernetes-dashboard-bf855c94d-npnfm requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod kubernetes-metrics-scraper-6b97c6d857-x7vmq requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod linkerd-controller-57d9d8f5dd-sm2ls requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod linkerd-destination-77564695ff-fv2kq requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod linkerd-grafana-79c6dd8fc-v6gln requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod linkerd-identity-d55b64556-7kjgh requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod linkerd-prometheus-68779bb867-5d2ff requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod linkerd-proxy-injector-7cbf76d445-xnj8j requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod linkerd-sp-validator-689c4bcc4d-8j98j requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod linkerd-tap-85775cdf7f-qh42m requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod linkerd-web-864c75894b-946dh requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod sonobuoy-systemd-logs-daemon-set-59631ba8c0bd4cad-k2bx9 requesting resource cpu=0m on Node k8s-3
Dec 14 09:08:54.718: INFO: Pod sonobuoy-systemd-logs-daemon-set-59631ba8c0bd4cad-xc9m6 requesting resource cpu=0m on Node k8s-2
STEP: Starting Pods to consume most of the cluster CPU.
Dec 14 09:08:54.718: INFO: Creating a pod which consumes cpu=630m on Node k8s-2
Dec 14 09:08:54.726: INFO: Creating a pod which consumes cpu=490m on Node k8s-3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-12309288-7f2d-4998-9644-e4777c55907c.15e031b7ba158a8f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1739/filler-pod-12309288-7f2d-4998-9644-e4777c55907c to k8s-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-12309288-7f2d-4998-9644-e4777c55907c.15e031b7d8ee8697], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-12309288-7f2d-4998-9644-e4777c55907c.15e031b7db41906c], Reason = [Created], Message = [Created container filler-pod-12309288-7f2d-4998-9644-e4777c55907c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-12309288-7f2d-4998-9644-e4777c55907c.15e031b7e1793a06], Reason = [Started], Message = [Started container filler-pod-12309288-7f2d-4998-9644-e4777c55907c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b4ed771-a76f-403f-aad9-643dbcf391c1.15e031b7b9124bbc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1739/filler-pod-4b4ed771-a76f-403f-aad9-643dbcf391c1 to k8s-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b4ed771-a76f-403f-aad9-643dbcf391c1.15e031b7d6b73b91], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b4ed771-a76f-403f-aad9-643dbcf391c1.15e031b7d9204d7b], Reason = [Created], Message = [Created container filler-pod-4b4ed771-a76f-403f-aad9-643dbcf391c1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b4ed771-a76f-403f-aad9-643dbcf391c1.15e031b7ddde06ca], Reason = [Started], Message = [Started container filler-pod-4b4ed771-a76f-403f-aad9-643dbcf391c1]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15e031b8323caef0], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node k8s-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:08:57.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1739" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":102,"skipped":1493,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:08:57.823: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-592
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Dec 14 09:08:58.013: INFO: Waiting up to 5m0s for pod "var-expansion-30004a10-1b68-4bac-ae6e-957f60ae9973" in namespace "var-expansion-592" to be "success or failure"
Dec 14 09:08:58.021: INFO: Pod "var-expansion-30004a10-1b68-4bac-ae6e-957f60ae9973": Phase="Pending", Reason="", readiness=false. Elapsed: 7.647634ms
Dec 14 09:09:00.024: INFO: Pod "var-expansion-30004a10-1b68-4bac-ae6e-957f60ae9973": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010074143s
STEP: Saw pod success
Dec 14 09:09:00.024: INFO: Pod "var-expansion-30004a10-1b68-4bac-ae6e-957f60ae9973" satisfied condition "success or failure"
Dec 14 09:09:00.025: INFO: Trying to get logs from node k8s-2 pod var-expansion-30004a10-1b68-4bac-ae6e-957f60ae9973 container dapi-container: <nil>
STEP: delete the pod
Dec 14 09:09:00.040: INFO: Waiting for pod var-expansion-30004a10-1b68-4bac-ae6e-957f60ae9973 to disappear
Dec 14 09:09:00.042: INFO: Pod var-expansion-30004a10-1b68-4bac-ae6e-957f60ae9973 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:09:00.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-592" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":103,"skipped":1509,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:09:00.047: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5147
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 09:09:02.255: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:09:02.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5147" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":104,"skipped":1516,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:09:02.293: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1924
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Dec 14 09:09:02.471: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-014205462 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:09:02.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1924" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":105,"skipped":1523,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:09:02.541: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3916
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:09:02.670: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:09:04.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3916" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":106,"skipped":1539,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:09:04.713: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-6259/configmap-test-dbd6cdb2-a571-4621-b8b7-9eef0f323d65
STEP: Creating a pod to test consume configMaps
Dec 14 09:09:04.867: INFO: Waiting up to 5m0s for pod "pod-configmaps-d5133172-297d-415c-a91f-0d6f57aaf218" in namespace "configmap-6259" to be "success or failure"
Dec 14 09:09:04.873: INFO: Pod "pod-configmaps-d5133172-297d-415c-a91f-0d6f57aaf218": Phase="Pending", Reason="", readiness=false. Elapsed: 5.712809ms
Dec 14 09:09:06.876: INFO: Pod "pod-configmaps-d5133172-297d-415c-a91f-0d6f57aaf218": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008148411s
STEP: Saw pod success
Dec 14 09:09:06.876: INFO: Pod "pod-configmaps-d5133172-297d-415c-a91f-0d6f57aaf218" satisfied condition "success or failure"
Dec 14 09:09:06.878: INFO: Trying to get logs from node k8s-2 pod pod-configmaps-d5133172-297d-415c-a91f-0d6f57aaf218 container env-test: <nil>
STEP: delete the pod
Dec 14 09:09:06.890: INFO: Waiting for pod pod-configmaps-d5133172-297d-415c-a91f-0d6f57aaf218 to disappear
Dec 14 09:09:06.893: INFO: Pod pod-configmaps-d5133172-297d-415c-a91f-0d6f57aaf218 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:09:06.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6259" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":107,"skipped":1550,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:09:06.899: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2496
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-g784c in namespace proxy-2496
I1214 09:09:07.048608      18 runners.go:189] Created replication controller with name: proxy-service-g784c, namespace: proxy-2496, replica count: 1
I1214 09:09:08.099055      18 runners.go:189] proxy-service-g784c Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 09:09:09.099281      18 runners.go:189] proxy-service-g784c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1214 09:09:10.099505      18 runners.go:189] proxy-service-g784c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1214 09:09:11.099715      18 runners.go:189] proxy-service-g784c Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 09:09:11.102: INFO: setup took 4.071981694s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 14 09:09:11.117: INFO: (0) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 14.316078ms)
Dec 14 09:09:11.121: INFO: (0) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 17.978839ms)
Dec 14 09:09:11.121: INFO: (0) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 19.261947ms)
Dec 14 09:09:11.121: INFO: (0) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 19.422746ms)
Dec 14 09:09:11.122: INFO: (0) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 20.037582ms)
Dec 14 09:09:11.122: INFO: (0) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 19.232731ms)
Dec 14 09:09:11.125: INFO: (0) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 22.136573ms)
Dec 14 09:09:11.125: INFO: (0) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 23.636792ms)
Dec 14 09:09:11.126: INFO: (0) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 23.389264ms)
Dec 14 09:09:11.126: INFO: (0) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 23.866748ms)
Dec 14 09:09:11.131: INFO: (0) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 28.546838ms)
Dec 14 09:09:11.132: INFO: (0) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 29.715327ms)
Dec 14 09:09:11.132: INFO: (0) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 29.722539ms)
Dec 14 09:09:11.132: INFO: (0) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 30.598555ms)
Dec 14 09:09:11.133: INFO: (0) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 30.156735ms)
Dec 14 09:09:11.133: INFO: (0) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 30.748929ms)
Dec 14 09:09:11.145: INFO: (1) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 11.071555ms)
Dec 14 09:09:11.146: INFO: (1) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 12.108156ms)
Dec 14 09:09:11.147: INFO: (1) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 13.043887ms)
Dec 14 09:09:11.149: INFO: (1) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 14.590055ms)
Dec 14 09:09:11.149: INFO: (1) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 14.923231ms)
Dec 14 09:09:11.149: INFO: (1) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 15.579487ms)
Dec 14 09:09:11.150: INFO: (1) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 15.892703ms)
Dec 14 09:09:11.151: INFO: (1) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 17.437919ms)
Dec 14 09:09:11.152: INFO: (1) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 17.800431ms)
Dec 14 09:09:11.152: INFO: (1) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 18.621003ms)
Dec 14 09:09:11.152: INFO: (1) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 18.443193ms)
Dec 14 09:09:11.153: INFO: (1) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 18.673004ms)
Dec 14 09:09:11.153: INFO: (1) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 18.823534ms)
Dec 14 09:09:11.154: INFO: (1) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 20.301583ms)
Dec 14 09:09:11.154: INFO: (1) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 21.124597ms)
Dec 14 09:09:11.155: INFO: (1) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 21.199001ms)
Dec 14 09:09:11.167: INFO: (2) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 11.965464ms)
Dec 14 09:09:11.169: INFO: (2) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 13.928189ms)
Dec 14 09:09:11.170: INFO: (2) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 13.978585ms)
Dec 14 09:09:11.170: INFO: (2) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 14.140271ms)
Dec 14 09:09:11.170: INFO: (2) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 14.310533ms)
Dec 14 09:09:11.170: INFO: (2) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 15.330503ms)
Dec 14 09:09:11.170: INFO: (2) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 15.498741ms)
Dec 14 09:09:11.171: INFO: (2) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 15.872721ms)
Dec 14 09:09:11.171: INFO: (2) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 15.160209ms)
Dec 14 09:09:11.171: INFO: (2) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 15.889179ms)
Dec 14 09:09:11.173: INFO: (2) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 17.585684ms)
Dec 14 09:09:11.173: INFO: (2) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 17.54308ms)
Dec 14 09:09:11.174: INFO: (2) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 19.115205ms)
Dec 14 09:09:11.174: INFO: (2) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 19.019599ms)
Dec 14 09:09:11.175: INFO: (2) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 19.356835ms)
Dec 14 09:09:11.175: INFO: (2) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 19.226254ms)
Dec 14 09:09:11.189: INFO: (3) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 12.782702ms)
Dec 14 09:09:11.189: INFO: (3) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 13.134716ms)
Dec 14 09:09:11.189: INFO: (3) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 12.930351ms)
Dec 14 09:09:11.189: INFO: (3) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 13.462738ms)
Dec 14 09:09:11.190: INFO: (3) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 14.408915ms)
Dec 14 09:09:11.190: INFO: (3) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 14.793947ms)
Dec 14 09:09:11.190: INFO: (3) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 13.962473ms)
Dec 14 09:09:11.190: INFO: (3) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 15.387462ms)
Dec 14 09:09:11.191: INFO: (3) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 15.311001ms)
Dec 14 09:09:11.194: INFO: (3) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 18.424821ms)
Dec 14 09:09:11.195: INFO: (3) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 19.193473ms)
Dec 14 09:09:11.196: INFO: (3) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 20.030235ms)
Dec 14 09:09:11.196: INFO: (3) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 20.329359ms)
Dec 14 09:09:11.196: INFO: (3) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 20.363054ms)
Dec 14 09:09:11.196: INFO: (3) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 20.434123ms)
Dec 14 09:09:11.197: INFO: (3) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 21.072776ms)
Dec 14 09:09:11.210: INFO: (4) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 12.230879ms)
Dec 14 09:09:11.210: INFO: (4) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 12.984154ms)
Dec 14 09:09:11.210: INFO: (4) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 13.309758ms)
Dec 14 09:09:11.211: INFO: (4) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 12.782294ms)
Dec 14 09:09:11.211: INFO: (4) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 12.939017ms)
Dec 14 09:09:11.211: INFO: (4) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 14.295572ms)
Dec 14 09:09:11.211: INFO: (4) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 14.183186ms)
Dec 14 09:09:11.215: INFO: (4) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 17.554111ms)
Dec 14 09:09:11.215: INFO: (4) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 17.849323ms)
Dec 14 09:09:11.216: INFO: (4) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 18.104367ms)
Dec 14 09:09:11.216: INFO: (4) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 18.526741ms)
Dec 14 09:09:11.217: INFO: (4) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 20.406765ms)
Dec 14 09:09:11.218: INFO: (4) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 19.829654ms)
Dec 14 09:09:11.218: INFO: (4) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 19.98403ms)
Dec 14 09:09:11.218: INFO: (4) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 20.35492ms)
Dec 14 09:09:11.218: INFO: (4) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 21.298812ms)
Dec 14 09:09:11.231: INFO: (5) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 12.094257ms)
Dec 14 09:09:11.233: INFO: (5) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 14.224915ms)
Dec 14 09:09:11.234: INFO: (5) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 15.68581ms)
Dec 14 09:09:11.235: INFO: (5) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 15.348879ms)
Dec 14 09:09:11.235: INFO: (5) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 16.487393ms)
Dec 14 09:09:11.235: INFO: (5) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 16.573848ms)
Dec 14 09:09:11.235: INFO: (5) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 17.106812ms)
Dec 14 09:09:11.236: INFO: (5) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 17.148724ms)
Dec 14 09:09:11.236: INFO: (5) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 17.069003ms)
Dec 14 09:09:11.236: INFO: (5) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 16.975421ms)
Dec 14 09:09:11.238: INFO: (5) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 18.674014ms)
Dec 14 09:09:11.238: INFO: (5) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 18.566494ms)
Dec 14 09:09:11.238: INFO: (5) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 19.649139ms)
Dec 14 09:09:11.239: INFO: (5) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 19.63663ms)
Dec 14 09:09:11.239: INFO: (5) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 20.230776ms)
Dec 14 09:09:11.239: INFO: (5) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 19.765329ms)
Dec 14 09:09:11.252: INFO: (6) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 11.422514ms)
Dec 14 09:09:11.252: INFO: (6) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 11.78279ms)
Dec 14 09:09:11.252: INFO: (6) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 12.850416ms)
Dec 14 09:09:11.253: INFO: (6) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 11.961208ms)
Dec 14 09:09:11.253: INFO: (6) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 13.475201ms)
Dec 14 09:09:11.253: INFO: (6) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 13.505053ms)
Dec 14 09:09:11.253: INFO: (6) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 12.657335ms)
Dec 14 09:09:11.255: INFO: (6) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 15.595397ms)
Dec 14 09:09:11.257: INFO: (6) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 17.313567ms)
Dec 14 09:09:11.257: INFO: (6) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 17.253381ms)
Dec 14 09:09:11.258: INFO: (6) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 17.454748ms)
Dec 14 09:09:11.259: INFO: (6) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 19.29748ms)
Dec 14 09:09:11.260: INFO: (6) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 19.70075ms)
Dec 14 09:09:11.260: INFO: (6) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 19.745061ms)
Dec 14 09:09:11.260: INFO: (6) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 20.041646ms)
Dec 14 09:09:11.260: INFO: (6) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 19.925268ms)
Dec 14 09:09:11.274: INFO: (7) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 13.114873ms)
Dec 14 09:09:11.276: INFO: (7) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 14.919706ms)
Dec 14 09:09:11.276: INFO: (7) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 15.120914ms)
Dec 14 09:09:11.276: INFO: (7) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 15.32097ms)
Dec 14 09:09:11.277: INFO: (7) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 15.516199ms)
Dec 14 09:09:11.277: INFO: (7) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 15.479056ms)
Dec 14 09:09:11.277: INFO: (7) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 15.615775ms)
Dec 14 09:09:11.277: INFO: (7) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 16.698483ms)
Dec 14 09:09:11.279: INFO: (7) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 18.39168ms)
Dec 14 09:09:11.280: INFO: (7) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 18.374062ms)
Dec 14 09:09:11.280: INFO: (7) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 18.275174ms)
Dec 14 09:09:11.281: INFO: (7) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 19.62852ms)
Dec 14 09:09:11.281: INFO: (7) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 20.800575ms)
Dec 14 09:09:11.281: INFO: (7) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 20.97711ms)
Dec 14 09:09:11.282: INFO: (7) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 20.704411ms)
Dec 14 09:09:11.282: INFO: (7) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 21.38338ms)
Dec 14 09:09:11.293: INFO: (8) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 9.787907ms)
Dec 14 09:09:11.302: INFO: (8) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 18.981005ms)
Dec 14 09:09:11.303: INFO: (8) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 19.364705ms)
Dec 14 09:09:11.303: INFO: (8) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 19.862798ms)
Dec 14 09:09:11.303: INFO: (8) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 20.524336ms)
Dec 14 09:09:11.303: INFO: (8) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 20.861755ms)
Dec 14 09:09:11.304: INFO: (8) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 20.875849ms)
Dec 14 09:09:11.304: INFO: (8) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 21.43636ms)
Dec 14 09:09:11.304: INFO: (8) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 21.353222ms)
Dec 14 09:09:11.306: INFO: (8) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 22.736532ms)
Dec 14 09:09:11.307: INFO: (8) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 24.024288ms)
Dec 14 09:09:11.307: INFO: (8) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 24.275422ms)
Dec 14 09:09:11.308: INFO: (8) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 26.010621ms)
Dec 14 09:09:11.309: INFO: (8) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 25.480073ms)
Dec 14 09:09:11.309: INFO: (8) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 26.587847ms)
Dec 14 09:09:11.309: INFO: (8) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 26.759876ms)
Dec 14 09:09:11.323: INFO: (9) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 13.465016ms)
Dec 14 09:09:11.324: INFO: (9) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 13.87678ms)
Dec 14 09:09:11.324: INFO: (9) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 14.178026ms)
Dec 14 09:09:11.325: INFO: (9) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 14.083556ms)
Dec 14 09:09:11.325: INFO: (9) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 14.53371ms)
Dec 14 09:09:11.325: INFO: (9) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 15.088315ms)
Dec 14 09:09:11.325: INFO: (9) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 15.618462ms)
Dec 14 09:09:11.326: INFO: (9) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 14.889971ms)
Dec 14 09:09:11.326: INFO: (9) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 16.195243ms)
Dec 14 09:09:11.326: INFO: (9) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 16.06017ms)
Dec 14 09:09:11.332: INFO: (9) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 22.26326ms)
Dec 14 09:09:11.332: INFO: (9) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 21.833923ms)
Dec 14 09:09:11.333: INFO: (9) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 22.890395ms)
Dec 14 09:09:11.333: INFO: (9) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 22.507317ms)
Dec 14 09:09:11.333: INFO: (9) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 22.543313ms)
Dec 14 09:09:11.333: INFO: (9) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 22.983383ms)
Dec 14 09:09:11.351: INFO: (10) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 17.517283ms)
Dec 14 09:09:11.353: INFO: (10) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 19.323411ms)
Dec 14 09:09:11.354: INFO: (10) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 19.530356ms)
Dec 14 09:09:11.354: INFO: (10) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 19.749901ms)
Dec 14 09:09:11.354: INFO: (10) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 19.964497ms)
Dec 14 09:09:11.354: INFO: (10) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 20.893762ms)
Dec 14 09:09:11.355: INFO: (10) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 20.905977ms)
Dec 14 09:09:11.355: INFO: (10) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 21.61929ms)
Dec 14 09:09:11.355: INFO: (10) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 21.286855ms)
Dec 14 09:09:11.355: INFO: (10) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 21.138484ms)
Dec 14 09:09:11.359: INFO: (10) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 25.240046ms)
Dec 14 09:09:11.359: INFO: (10) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 25.858291ms)
Dec 14 09:09:11.360: INFO: (10) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 25.857953ms)
Dec 14 09:09:11.360: INFO: (10) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 25.759313ms)
Dec 14 09:09:11.360: INFO: (10) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 26.501ms)
Dec 14 09:09:11.361: INFO: (10) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 27.220556ms)
Dec 14 09:09:11.374: INFO: (11) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 12.165995ms)
Dec 14 09:09:11.375: INFO: (11) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 12.873285ms)
Dec 14 09:09:11.375: INFO: (11) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 13.470269ms)
Dec 14 09:09:11.380: INFO: (11) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 17.745598ms)
Dec 14 09:09:11.381: INFO: (11) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 19.640265ms)
Dec 14 09:09:11.382: INFO: (11) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 19.899517ms)
Dec 14 09:09:11.383: INFO: (11) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 20.942347ms)
Dec 14 09:09:11.384: INFO: (11) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 21.791714ms)
Dec 14 09:09:11.385: INFO: (11) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 22.539319ms)
Dec 14 09:09:11.386: INFO: (11) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 23.593944ms)
Dec 14 09:09:11.396: INFO: (11) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 33.48154ms)
Dec 14 09:09:11.397: INFO: (11) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 34.897646ms)
Dec 14 09:09:11.397: INFO: (11) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 35.995022ms)
Dec 14 09:09:11.401: INFO: (11) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 38.6593ms)
Dec 14 09:09:11.401: INFO: (11) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 39.847813ms)
Dec 14 09:09:11.402: INFO: (11) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 40.375258ms)
Dec 14 09:09:11.418: INFO: (12) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 15.201925ms)
Dec 14 09:09:11.420: INFO: (12) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 16.808168ms)
Dec 14 09:09:11.421: INFO: (12) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 17.018269ms)
Dec 14 09:09:11.421: INFO: (12) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 18.003866ms)
Dec 14 09:09:11.421: INFO: (12) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 18.228709ms)
Dec 14 09:09:11.421: INFO: (12) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 18.623105ms)
Dec 14 09:09:11.423: INFO: (12) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 19.16152ms)
Dec 14 09:09:11.423: INFO: (12) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 20.002256ms)
Dec 14 09:09:11.424: INFO: (12) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 20.795451ms)
Dec 14 09:09:11.424: INFO: (12) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 20.964084ms)
Dec 14 09:09:11.426: INFO: (12) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 22.968118ms)
Dec 14 09:09:11.427: INFO: (12) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 24.548124ms)
Dec 14 09:09:11.428: INFO: (12) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 24.574427ms)
Dec 14 09:09:11.429: INFO: (12) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 25.188973ms)
Dec 14 09:09:11.429: INFO: (12) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 26.081711ms)
Dec 14 09:09:11.429: INFO: (12) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 25.740687ms)
Dec 14 09:09:11.442: INFO: (13) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 11.888797ms)
Dec 14 09:09:11.443: INFO: (13) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 13.147565ms)
Dec 14 09:09:11.444: INFO: (13) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 14.383754ms)
Dec 14 09:09:11.444: INFO: (13) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 14.548573ms)
Dec 14 09:09:11.444: INFO: (13) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 14.925332ms)
Dec 14 09:09:11.444: INFO: (13) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 14.010997ms)
Dec 14 09:09:11.445: INFO: (13) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 15.139244ms)
Dec 14 09:09:11.446: INFO: (13) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 15.987849ms)
Dec 14 09:09:11.446: INFO: (13) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 16.569183ms)
Dec 14 09:09:11.447: INFO: (13) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 16.675063ms)
Dec 14 09:09:11.451: INFO: (13) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 21.08859ms)
Dec 14 09:09:11.452: INFO: (13) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 22.414432ms)
Dec 14 09:09:11.453: INFO: (13) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 22.316574ms)
Dec 14 09:09:11.453: INFO: (13) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 22.958565ms)
Dec 14 09:09:11.453: INFO: (13) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 23.331437ms)
Dec 14 09:09:11.454: INFO: (13) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 23.340108ms)
Dec 14 09:09:11.465: INFO: (14) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 10.936448ms)
Dec 14 09:09:11.467: INFO: (14) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 13.012532ms)
Dec 14 09:09:11.468: INFO: (14) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 13.146309ms)
Dec 14 09:09:11.468: INFO: (14) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 14.155564ms)
Dec 14 09:09:11.468: INFO: (14) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 13.720591ms)
Dec 14 09:09:11.469: INFO: (14) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 15.0422ms)
Dec 14 09:09:11.469: INFO: (14) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 14.80198ms)
Dec 14 09:09:11.469: INFO: (14) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 15.058451ms)
Dec 14 09:09:11.475: INFO: (14) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 20.253186ms)
Dec 14 09:09:11.475: INFO: (14) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 20.544844ms)
Dec 14 09:09:11.476: INFO: (14) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 21.819365ms)
Dec 14 09:09:11.477: INFO: (14) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 23.00324ms)
Dec 14 09:09:11.478: INFO: (14) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 23.8614ms)
Dec 14 09:09:11.478: INFO: (14) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 23.617503ms)
Dec 14 09:09:11.478: INFO: (14) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 24.092121ms)
Dec 14 09:09:11.479: INFO: (14) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 24.887434ms)
Dec 14 09:09:11.491: INFO: (15) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 11.433345ms)
Dec 14 09:09:11.492: INFO: (15) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 12.575906ms)
Dec 14 09:09:11.492: INFO: (15) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 11.96004ms)
Dec 14 09:09:11.492: INFO: (15) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 13.293529ms)
Dec 14 09:09:11.492: INFO: (15) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 13.263591ms)
Dec 14 09:09:11.497: INFO: (15) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 17.675588ms)
Dec 14 09:09:11.497: INFO: (15) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 17.421107ms)
Dec 14 09:09:11.497: INFO: (15) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 17.750578ms)
Dec 14 09:09:11.498: INFO: (15) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 18.356529ms)
Dec 14 09:09:11.498: INFO: (15) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 18.147164ms)
Dec 14 09:09:11.498: INFO: (15) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 18.417189ms)
Dec 14 09:09:11.499: INFO: (15) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 20.063656ms)
Dec 14 09:09:11.500: INFO: (15) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 20.29932ms)
Dec 14 09:09:11.500: INFO: (15) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 20.232068ms)
Dec 14 09:09:11.500: INFO: (15) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 20.966444ms)
Dec 14 09:09:11.500: INFO: (15) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 21.139146ms)
Dec 14 09:09:11.512: INFO: (16) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 11.76451ms)
Dec 14 09:09:11.513: INFO: (16) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 12.187483ms)
Dec 14 09:09:11.513: INFO: (16) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 11.525138ms)
Dec 14 09:09:11.513: INFO: (16) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 12.790173ms)
Dec 14 09:09:11.514: INFO: (16) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 12.804596ms)
Dec 14 09:09:11.517: INFO: (16) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 16.151285ms)
Dec 14 09:09:11.518: INFO: (16) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 17.583863ms)
Dec 14 09:09:11.519: INFO: (16) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 17.536186ms)
Dec 14 09:09:11.519: INFO: (16) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 17.862901ms)
Dec 14 09:09:11.519: INFO: (16) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 18.300789ms)
Dec 14 09:09:11.520: INFO: (16) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 18.342564ms)
Dec 14 09:09:11.520: INFO: (16) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 18.659558ms)
Dec 14 09:09:11.521: INFO: (16) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 20.135994ms)
Dec 14 09:09:11.522: INFO: (16) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 20.56577ms)
Dec 14 09:09:11.522: INFO: (16) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 20.853959ms)
Dec 14 09:09:11.522: INFO: (16) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 20.603622ms)
Dec 14 09:09:11.533: INFO: (17) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 10.686539ms)
Dec 14 09:09:11.536: INFO: (17) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 13.444349ms)
Dec 14 09:09:11.536: INFO: (17) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 13.773509ms)
Dec 14 09:09:11.536: INFO: (17) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 13.237948ms)
Dec 14 09:09:11.537: INFO: (17) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 13.427398ms)
Dec 14 09:09:11.537: INFO: (17) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 14.728731ms)
Dec 14 09:09:11.537: INFO: (17) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 14.717942ms)
Dec 14 09:09:11.538: INFO: (17) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 14.816883ms)
Dec 14 09:09:11.538: INFO: (17) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 15.340939ms)
Dec 14 09:09:11.538: INFO: (17) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 15.153827ms)
Dec 14 09:09:11.542: INFO: (17) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 19.001701ms)
Dec 14 09:09:11.542: INFO: (17) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 19.48802ms)
Dec 14 09:09:11.543: INFO: (17) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 20.628471ms)
Dec 14 09:09:11.543: INFO: (17) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 20.99441ms)
Dec 14 09:09:11.544: INFO: (17) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 20.735309ms)
Dec 14 09:09:11.544: INFO: (17) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 21.020006ms)
Dec 14 09:09:11.555: INFO: (18) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 10.316179ms)
Dec 14 09:09:11.557: INFO: (18) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 12.326062ms)
Dec 14 09:09:11.558: INFO: (18) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 12.661701ms)
Dec 14 09:09:11.558: INFO: (18) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 12.774715ms)
Dec 14 09:09:11.558: INFO: (18) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 14.141063ms)
Dec 14 09:09:11.558: INFO: (18) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 13.914973ms)
Dec 14 09:09:11.559: INFO: (18) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 14.044096ms)
Dec 14 09:09:11.559: INFO: (18) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 14.285498ms)
Dec 14 09:09:11.559: INFO: (18) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 14.397955ms)
Dec 14 09:09:11.560: INFO: (18) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 14.745853ms)
Dec 14 09:09:11.562: INFO: (18) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 18.169906ms)
Dec 14 09:09:11.563: INFO: (18) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 18.658215ms)
Dec 14 09:09:11.563: INFO: (18) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 19.275029ms)
Dec 14 09:09:11.565: INFO: (18) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 20.26775ms)
Dec 14 09:09:11.565: INFO: (18) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 21.007548ms)
Dec 14 09:09:11.566: INFO: (18) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 20.893523ms)
Dec 14 09:09:11.588: INFO: (19) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:462/proxy/: tls qux (200; 22.550602ms)
Dec 14 09:09:11.598: INFO: (19) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:160/proxy/: foo (200; 32.065415ms)
Dec 14 09:09:11.599: INFO: (19) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:162/proxy/: bar (200; 32.216717ms)
Dec 14 09:09:11.599: INFO: (19) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:160/proxy/: foo (200; 32.589395ms)
Dec 14 09:09:11.599: INFO: (19) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:443/proxy/tlsrewritem... (200; 33.136581ms)
Dec 14 09:09:11.600: INFO: (19) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx:1080/proxy/rewriteme">test<... (200; 32.741816ms)
Dec 14 09:09:11.600: INFO: (19) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname1/proxy/: tls baz (200; 33.190216ms)
Dec 14 09:09:11.600: INFO: (19) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:1080/proxy/rewriteme">... (200; 34.16104ms)
Dec 14 09:09:11.600: INFO: (19) /api/v1/namespaces/proxy-2496/pods/http:proxy-service-g784c-h5qpx:162/proxy/: bar (200; 33.61628ms)
Dec 14 09:09:11.601: INFO: (19) /api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/: <a href="/api/v1/namespaces/proxy-2496/pods/proxy-service-g784c-h5qpx/proxy/rewriteme">test</a> (200; 34.433316ms)
Dec 14 09:09:11.601: INFO: (19) /api/v1/namespaces/proxy-2496/pods/https:proxy-service-g784c-h5qpx:460/proxy/: tls baz (200; 34.789299ms)
Dec 14 09:09:11.602: INFO: (19) /api/v1/namespaces/proxy-2496/services/https:proxy-service-g784c:tlsportname2/proxy/: tls qux (200; 34.961184ms)
Dec 14 09:09:11.602: INFO: (19) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname2/proxy/: bar (200; 36.190084ms)
Dec 14 09:09:11.603: INFO: (19) /api/v1/namespaces/proxy-2496/services/proxy-service-g784c:portname1/proxy/: foo (200; 37.20854ms)
Dec 14 09:09:11.603: INFO: (19) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname2/proxy/: bar (200; 36.543348ms)
Dec 14 09:09:11.603: INFO: (19) /api/v1/namespaces/proxy-2496/services/http:proxy-service-g784c:portname1/proxy/: foo (200; 36.908406ms)
STEP: deleting ReplicationController proxy-service-g784c in namespace proxy-2496, will wait for the garbage collector to delete the pods
Dec 14 09:09:11.659: INFO: Deleting ReplicationController proxy-service-g784c took: 4.233397ms
Dec 14 09:09:12.660: INFO: Terminating ReplicationController proxy-service-g784c pods took: 1.000154535s
[AfterEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:09:14.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2496" for this suite.

• [SLOW TEST:7.567 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":108,"skipped":1578,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:09:14.466: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3435
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:09:14.595: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 14 09:09:18.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-3435 create -f -'
Dec 14 09:09:18.791: INFO: stderr: ""
Dec 14 09:09:18.791: INFO: stdout: "e2e-test-crd-publish-openapi-1611-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 14 09:09:18.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-3435 delete e2e-test-crd-publish-openapi-1611-crds test-cr'
Dec 14 09:09:18.871: INFO: stderr: ""
Dec 14 09:09:18.871: INFO: stdout: "e2e-test-crd-publish-openapi-1611-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 14 09:09:18.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-3435 apply -f -'
Dec 14 09:09:19.032: INFO: stderr: ""
Dec 14 09:09:19.032: INFO: stdout: "e2e-test-crd-publish-openapi-1611-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 14 09:09:19.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-3435 delete e2e-test-crd-publish-openapi-1611-crds test-cr'
Dec 14 09:09:19.118: INFO: stderr: ""
Dec 14 09:09:19.118: INFO: stdout: "e2e-test-crd-publish-openapi-1611-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 14 09:09:19.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 explain e2e-test-crd-publish-openapi-1611-crds'
Dec 14 09:09:19.281: INFO: stderr: ""
Dec 14 09:09:19.281: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1611-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:09:23.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3435" for this suite.

• [SLOW TEST:9.498 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":109,"skipped":1579,"failed":0}
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:09:23.964: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6784
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Dec 14 09:09:24.093: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:09:27.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6784" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":110,"skipped":1586,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:09:27.469: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8807
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8807
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Dec 14 09:09:27.616: INFO: Found 0 stateful pods, waiting for 3
Dec 14 09:09:37.618: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 09:09:37.618: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 09:09:37.618: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 14 09:09:37.639: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 14 09:09:47.670: INFO: Updating stateful set ss2
Dec 14 09:09:47.679: INFO: Waiting for Pod statefulset-8807/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 09:09:57.683: INFO: Waiting for Pod statefulset-8807/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Dec 14 09:10:07.741: INFO: Found 2 stateful pods, waiting for 3
Dec 14 09:10:17.744: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 09:10:17.744: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 09:10:17.744: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 14 09:10:17.765: INFO: Updating stateful set ss2
Dec 14 09:10:17.777: INFO: Waiting for Pod statefulset-8807/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 09:10:27.799: INFO: Updating stateful set ss2
Dec 14 09:10:27.810: INFO: Waiting for StatefulSet statefulset-8807/ss2 to complete update
Dec 14 09:10:27.810: INFO: Waiting for Pod statefulset-8807/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 09:10:37.814: INFO: Waiting for StatefulSet statefulset-8807/ss2 to complete update
Dec 14 09:10:37.814: INFO: Waiting for Pod statefulset-8807/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Dec 14 09:10:47.814: INFO: Deleting all statefulset in ns statefulset-8807
Dec 14 09:10:47.816: INFO: Scaling statefulset ss2 to 0
Dec 14 09:11:27.829: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 09:11:27.831: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:11:27.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8807" for this suite.

• [SLOW TEST:120.385 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":111,"skipped":1603,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:11:27.857: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7032
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 14 09:11:28.004: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:11:32.286: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:11:48.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7032" for this suite.

• [SLOW TEST:20.816 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":112,"skipped":1666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:11:48.674: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6011
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Dec 14 09:11:48.805: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-014205462 proxy --unix-socket=/tmp/kubectl-proxy-unix949346021/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:11:48.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6011" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":113,"skipped":1699,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:11:48.870: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-76803c2c-bdcc-406a-9ed3-ec16385c3a2e
STEP: Creating a pod to test consume configMaps
Dec 14 09:11:49.020: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-399a4e5b-a75f-42ed-b16b-f0272973efd5" in namespace "projected-4438" to be "success or failure"
Dec 14 09:11:49.027: INFO: Pod "pod-projected-configmaps-399a4e5b-a75f-42ed-b16b-f0272973efd5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.423291ms
Dec 14 09:11:51.029: INFO: Pod "pod-projected-configmaps-399a4e5b-a75f-42ed-b16b-f0272973efd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008626094s
STEP: Saw pod success
Dec 14 09:11:51.029: INFO: Pod "pod-projected-configmaps-399a4e5b-a75f-42ed-b16b-f0272973efd5" satisfied condition "success or failure"
Dec 14 09:11:51.031: INFO: Trying to get logs from node k8s-2 pod pod-projected-configmaps-399a4e5b-a75f-42ed-b16b-f0272973efd5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 09:11:51.044: INFO: Waiting for pod pod-projected-configmaps-399a4e5b-a75f-42ed-b16b-f0272973efd5 to disappear
Dec 14 09:11:51.047: INFO: Pod pod-projected-configmaps-399a4e5b-a75f-42ed-b16b-f0272973efd5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:11:51.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4438" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":114,"skipped":1699,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:11:51.052: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4321
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4321
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4321
STEP: Creating statefulset with conflicting port in namespace statefulset-4321
STEP: Waiting until pod test-pod will start running in namespace statefulset-4321
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4321
Dec 14 09:11:55.211: INFO: Observed stateful pod in namespace: statefulset-4321, name: ss-0, uid: b9cb8fea-89ff-401a-b862-7e24f1c58f20, status phase: Pending. Waiting for statefulset controller to delete.
Dec 14 09:11:55.237: INFO: Observed stateful pod in namespace: statefulset-4321, name: ss-0, uid: b9cb8fea-89ff-401a-b862-7e24f1c58f20, status phase: Failed. Waiting for statefulset controller to delete.
Dec 14 09:11:55.243: INFO: Observed stateful pod in namespace: statefulset-4321, name: ss-0, uid: b9cb8fea-89ff-401a-b862-7e24f1c58f20, status phase: Failed. Waiting for statefulset controller to delete.
Dec 14 09:11:55.249: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4321
STEP: Removing pod with conflicting port in namespace statefulset-4321
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4321 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Dec 14 09:11:59.279: INFO: Deleting all statefulset in ns statefulset-4321
Dec 14 09:11:59.281: INFO: Scaling statefulset ss to 0
Dec 14 09:12:09.294: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 09:12:09.299: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:12:09.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4321" for this suite.

• [SLOW TEST:18.304 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":115,"skipped":1700,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:12:09.357: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6798
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:12:11.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6798" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":116,"skipped":1706,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:12:11.514: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2767
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:12:11.651: INFO: (0) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 4.314936ms)
Dec 14 09:12:11.653: INFO: (1) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.252913ms)
Dec 14 09:12:11.656: INFO: (2) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.283379ms)
Dec 14 09:12:11.658: INFO: (3) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.409023ms)
Dec 14 09:12:11.661: INFO: (4) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.8153ms)
Dec 14 09:12:11.663: INFO: (5) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.197402ms)
Dec 14 09:12:11.666: INFO: (6) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.325017ms)
Dec 14 09:12:11.668: INFO: (7) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.5126ms)
Dec 14 09:12:11.671: INFO: (8) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.402743ms)
Dec 14 09:12:11.673: INFO: (9) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.153909ms)
Dec 14 09:12:11.675: INFO: (10) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.156014ms)
Dec 14 09:12:11.677: INFO: (11) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.079999ms)
Dec 14 09:12:11.680: INFO: (12) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.287816ms)
Dec 14 09:12:11.682: INFO: (13) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.13663ms)
Dec 14 09:12:11.684: INFO: (14) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.125073ms)
Dec 14 09:12:11.686: INFO: (15) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.224264ms)
Dec 14 09:12:11.689: INFO: (16) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.340933ms)
Dec 14 09:12:11.691: INFO: (17) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.498995ms)
Dec 14 09:12:11.693: INFO: (18) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.148116ms)
Dec 14 09:12:11.696: INFO: (19) /api/v1/nodes/k8s-2:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.294302ms)
[AfterEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:12:11.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2767" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":117,"skipped":1720,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:12:11.701: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6114
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:12:11.876: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"4ed38255-edb9-4f1d-82f7-de396f480064", Controller:(*bool)(0xc005432faa), BlockOwnerDeletion:(*bool)(0xc005432fab)}}
Dec 14 09:12:11.881: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9aaf2e75-c2ef-4ba2-8077-ff5f811a809f", Controller:(*bool)(0xc005433206), BlockOwnerDeletion:(*bool)(0xc005433207)}}
Dec 14 09:12:11.887: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"75856d12-e9e7-4dc1-99bc-2b2549383ea0", Controller:(*bool)(0xc005433446), BlockOwnerDeletion:(*bool)(0xc005433447)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:12:16.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6114" for this suite.

• [SLOW TEST:5.211 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":118,"skipped":1721,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:12:16.913: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9731
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-1f38f394-69f1-4ed1-b4c3-112ee2927e9f
STEP: Creating a pod to test consume secrets
Dec 14 09:12:17.148: INFO: Waiting up to 5m0s for pod "pod-secrets-224df539-1923-4a2e-8123-96d7826b2801" in namespace "secrets-9731" to be "success or failure"
Dec 14 09:12:17.156: INFO: Pod "pod-secrets-224df539-1923-4a2e-8123-96d7826b2801": Phase="Pending", Reason="", readiness=false. Elapsed: 7.496078ms
Dec 14 09:12:19.158: INFO: Pod "pod-secrets-224df539-1923-4a2e-8123-96d7826b2801": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009846638s
STEP: Saw pod success
Dec 14 09:12:19.158: INFO: Pod "pod-secrets-224df539-1923-4a2e-8123-96d7826b2801" satisfied condition "success or failure"
Dec 14 09:12:19.160: INFO: Trying to get logs from node k8s-2 pod pod-secrets-224df539-1923-4a2e-8123-96d7826b2801 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 09:12:19.176: INFO: Waiting for pod pod-secrets-224df539-1923-4a2e-8123-96d7826b2801 to disappear
Dec 14 09:12:19.183: INFO: Pod pod-secrets-224df539-1923-4a2e-8123-96d7826b2801 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:12:19.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9731" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":119,"skipped":1722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:12:19.197: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:12:19.355: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-65c9473f-6274-4ba1-a8bc-f1aa94900e16" in namespace "security-context-test-5173" to be "success or failure"
Dec 14 09:12:19.361: INFO: Pod "busybox-readonly-false-65c9473f-6274-4ba1-a8bc-f1aa94900e16": Phase="Pending", Reason="", readiness=false. Elapsed: 6.669926ms
Dec 14 09:12:21.363: INFO: Pod "busybox-readonly-false-65c9473f-6274-4ba1-a8bc-f1aa94900e16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008637439s
Dec 14 09:12:21.363: INFO: Pod "busybox-readonly-false-65c9473f-6274-4ba1-a8bc-f1aa94900e16" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:12:21.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5173" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":120,"skipped":1759,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:12:21.370: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9480
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1841
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 14 09:12:21.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9480'
Dec 14 09:12:21.601: INFO: stderr: ""
Dec 14 09:12:21.601: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1846
Dec 14 09:12:21.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete pods e2e-test-httpd-pod --namespace=kubectl-9480'
Dec 14 09:12:38.178: INFO: stderr: ""
Dec 14 09:12:38.178: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:12:38.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9480" for this suite.

• [SLOW TEST:16.816 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1837
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":121,"skipped":1787,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:12:38.187: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6853
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:12:49.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6853" for this suite.

• [SLOW TEST:11.161 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":122,"skipped":1787,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:12:49.349: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7378
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 09:12:49.498: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70e62607-23f7-42e7-9861-11ab8e7d87db" in namespace "downward-api-7378" to be "success or failure"
Dec 14 09:12:49.503: INFO: Pod "downwardapi-volume-70e62607-23f7-42e7-9861-11ab8e7d87db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.387022ms
Dec 14 09:12:51.505: INFO: Pod "downwardapi-volume-70e62607-23f7-42e7-9861-11ab8e7d87db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00680281s
STEP: Saw pod success
Dec 14 09:12:51.505: INFO: Pod "downwardapi-volume-70e62607-23f7-42e7-9861-11ab8e7d87db" satisfied condition "success or failure"
Dec 14 09:12:51.507: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-70e62607-23f7-42e7-9861-11ab8e7d87db container client-container: <nil>
STEP: delete the pod
Dec 14 09:12:51.520: INFO: Waiting for pod downwardapi-volume-70e62607-23f7-42e7-9861-11ab8e7d87db to disappear
Dec 14 09:12:51.523: INFO: Pod downwardapi-volume-70e62607-23f7-42e7-9861-11ab8e7d87db no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:12:51.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7378" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":123,"skipped":1802,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:12:51.532: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 09:12:51.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31c1a32a-8600-44b2-b6c3-098052949bb5" in namespace "downward-api-3818" to be "success or failure"
Dec 14 09:12:51.732: INFO: Pod "downwardapi-volume-31c1a32a-8600-44b2-b6c3-098052949bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73735ms
Dec 14 09:12:53.734: INFO: Pod "downwardapi-volume-31c1a32a-8600-44b2-b6c3-098052949bb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008301393s
STEP: Saw pod success
Dec 14 09:12:53.735: INFO: Pod "downwardapi-volume-31c1a32a-8600-44b2-b6c3-098052949bb5" satisfied condition "success or failure"
Dec 14 09:12:53.736: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-31c1a32a-8600-44b2-b6c3-098052949bb5 container client-container: <nil>
STEP: delete the pod
Dec 14 09:12:53.749: INFO: Waiting for pod downwardapi-volume-31c1a32a-8600-44b2-b6c3-098052949bb5 to disappear
Dec 14 09:12:53.752: INFO: Pod downwardapi-volume-31c1a32a-8600-44b2-b6c3-098052949bb5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:12:53.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3818" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":124,"skipped":1819,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:12:53.758: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-801
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-801
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[]
Dec 14 09:12:53.906: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[] (13.820474ms elapsed)
STEP: Creating pod pod1 in namespace services-801
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[pod1:[100]]
Dec 14 09:12:55.943: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[pod1:[100]] (2.02563225s elapsed)
STEP: Creating pod pod2 in namespace services-801
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 14 09:12:56.969: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[pod1:[100] pod2:[101]] (1.019369333s elapsed)
STEP: Deleting pod pod1 in namespace services-801
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[pod2:[101]]
Dec 14 09:12:56.987: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[pod2:[101]] (14.453986ms elapsed)
STEP: Deleting pod pod2 in namespace services-801
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[]
Dec 14 09:12:57.002: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[] (10.260323ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:12:57.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-801" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":125,"skipped":1822,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:12:57.028: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3469
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:12:59.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3469" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":126,"skipped":1853,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:12:59.193: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4492
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-83bb1fa5-85fc-41f6-b2d7-dec848c8e001
STEP: Creating a pod to test consume configMaps
Dec 14 09:12:59.338: INFO: Waiting up to 5m0s for pod "pod-configmaps-5aaf7b8f-2cca-4702-b648-7584ca90d6ea" in namespace "configmap-4492" to be "success or failure"
Dec 14 09:12:59.346: INFO: Pod "pod-configmaps-5aaf7b8f-2cca-4702-b648-7584ca90d6ea": Phase="Pending", Reason="", readiness=false. Elapsed: 7.553494ms
Dec 14 09:13:01.348: INFO: Pod "pod-configmaps-5aaf7b8f-2cca-4702-b648-7584ca90d6ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010041078s
STEP: Saw pod success
Dec 14 09:13:01.349: INFO: Pod "pod-configmaps-5aaf7b8f-2cca-4702-b648-7584ca90d6ea" satisfied condition "success or failure"
Dec 14 09:13:01.350: INFO: Trying to get logs from node k8s-2 pod pod-configmaps-5aaf7b8f-2cca-4702-b648-7584ca90d6ea container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 09:13:01.367: INFO: Waiting for pod pod-configmaps-5aaf7b8f-2cca-4702-b648-7584ca90d6ea to disappear
Dec 14 09:13:01.371: INFO: Pod pod-configmaps-5aaf7b8f-2cca-4702-b648-7584ca90d6ea no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:13:01.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4492" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":127,"skipped":1860,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:13:01.378: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Dec 14 09:13:01.521: INFO: Waiting up to 5m0s for pod "downward-api-1a0293d5-b8f2-4b72-9105-aa08f325f106" in namespace "downward-api-2834" to be "success or failure"
Dec 14 09:13:01.527: INFO: Pod "downward-api-1a0293d5-b8f2-4b72-9105-aa08f325f106": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05651ms
Dec 14 09:13:03.529: INFO: Pod "downward-api-1a0293d5-b8f2-4b72-9105-aa08f325f106": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008602588s
STEP: Saw pod success
Dec 14 09:13:03.530: INFO: Pod "downward-api-1a0293d5-b8f2-4b72-9105-aa08f325f106" satisfied condition "success or failure"
Dec 14 09:13:03.531: INFO: Trying to get logs from node k8s-2 pod downward-api-1a0293d5-b8f2-4b72-9105-aa08f325f106 container dapi-container: <nil>
STEP: delete the pod
Dec 14 09:13:03.544: INFO: Waiting for pod downward-api-1a0293d5-b8f2-4b72-9105-aa08f325f106 to disappear
Dec 14 09:13:03.548: INFO: Pod downward-api-1a0293d5-b8f2-4b72-9105-aa08f325f106 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:13:03.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2834" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":128,"skipped":1867,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:13:03.555: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-126
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1214 09:13:13.749320      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 14 09:13:13.749: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:13:13.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-126" for this suite.

• [SLOW TEST:10.200 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":129,"skipped":1867,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:13:13.755: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2726
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-a5dc8f93-0958-4c25-893d-60175e4ae26f
STEP: Creating a pod to test consume secrets
Dec 14 09:13:13.909: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-34ea2c45-ac38-4c19-9787-89e26b5bbb86" in namespace "projected-2726" to be "success or failure"
Dec 14 09:13:13.918: INFO: Pod "pod-projected-secrets-34ea2c45-ac38-4c19-9787-89e26b5bbb86": Phase="Pending", Reason="", readiness=false. Elapsed: 9.572794ms
Dec 14 09:13:15.922: INFO: Pod "pod-projected-secrets-34ea2c45-ac38-4c19-9787-89e26b5bbb86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01364006s
STEP: Saw pod success
Dec 14 09:13:15.922: INFO: Pod "pod-projected-secrets-34ea2c45-ac38-4c19-9787-89e26b5bbb86" satisfied condition "success or failure"
Dec 14 09:13:15.924: INFO: Trying to get logs from node k8s-2 pod pod-projected-secrets-34ea2c45-ac38-4c19-9787-89e26b5bbb86 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 09:13:15.939: INFO: Waiting for pod pod-projected-secrets-34ea2c45-ac38-4c19-9787-89e26b5bbb86 to disappear
Dec 14 09:13:15.941: INFO: Pod pod-projected-secrets-34ea2c45-ac38-4c19-9787-89e26b5bbb86 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:13:15.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2726" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":130,"skipped":1883,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:13:15.946: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8606
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:13:16.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 version'
Dec 14 09:13:16.158: INFO: stderr: ""
Dec 14 09:13:16.158: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.0\", GitCommit:\"70132b0f130acc0bed193d9ba59dd186f0e634cf\", GitTreeState:\"clean\", BuildDate:\"2019-12-07T21:20:10Z\", GoVersion:\"go1.13.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.0\", GitCommit:\"70132b0f130acc0bed193d9ba59dd186f0e634cf\", GitTreeState:\"clean\", BuildDate:\"2019-12-07T21:12:17Z\", GoVersion:\"go1.13.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:13:16.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8606" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":131,"skipped":1890,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:13:16.166: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:13:16.835: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 09:13:18.848: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911596, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911596, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911596, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911596, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:13:21.862: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:13:21.864: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9695-crds.webhook.example.com via the AdmissionRegistration API
Dec 14 09:13:22.388: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:13:24.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5030" for this suite.
STEP: Destroying namespace "webhook-5030-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.021 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":132,"skipped":1893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:13:24.189: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:13:24.410: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 14 09:13:29.414: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 14 09:13:29.414: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 14 09:13:31.416: INFO: Creating deployment "test-rollover-deployment"
Dec 14 09:13:31.423: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 14 09:13:33.432: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 14 09:13:33.436: INFO: Ensure that both replica sets have 1 created replica
Dec 14 09:13:33.439: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 14 09:13:33.444: INFO: Updating deployment test-rollover-deployment
Dec 14 09:13:33.444: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 14 09:13:35.450: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 14 09:13:35.454: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 14 09:13:35.458: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 09:13:35.458: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911614, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 09:13:37.462: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 09:13:37.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911614, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 09:13:39.462: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 09:13:39.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911614, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 09:13:41.462: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 09:13:41.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911614, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 09:13:43.462: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 09:13:43.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911614, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911611, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 09:13:45.462: INFO: 
Dec 14 09:13:45.462: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Dec 14 09:13:45.466: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5192 /apis/apps/v1/namespaces/deployment-5192/deployments/test-rollover-deployment 34a8cef7-7031-43da-bcaf-ed568b0f6916 13408 2 2019-12-14 09:13:31 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0076db788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-14 09:13:31 +0000 UTC,LastTransitionTime:2019-12-14 09:13:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2019-12-14 09:13:44 +0000 UTC,LastTransitionTime:2019-12-14 09:13:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 09:13:45.469: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-5192 /apis/apps/v1/namespaces/deployment-5192/replicasets/test-rollover-deployment-574d6dfbff 3c65ac30-4b48-4dcd-9572-f1edf0b04667 13397 2 2019-12-14 09:13:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 34a8cef7-7031-43da-bcaf-ed568b0f6916 0xc0076dbce7 0xc0076dbce8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0076dbd58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 09:13:45.469: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 14 09:13:45.469: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5192 /apis/apps/v1/namespaces/deployment-5192/replicasets/test-rollover-controller f1aad90e-7c5d-4e81-ab29-92d2bc04f382 13407 2 2019-12-14 09:13:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 34a8cef7-7031-43da-bcaf-ed568b0f6916 0xc0076dbc07 0xc0076dbc08}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0076dbc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 09:13:45.469: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-5192 /apis/apps/v1/namespaces/deployment-5192/replicasets/test-rollover-deployment-f6c94f66c 6090725f-0901-4e96-872d-79e4cad0f89e 13354 2 2019-12-14 09:13:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 34a8cef7-7031-43da-bcaf-ed568b0f6916 0xc0076dbdc0 0xc0076dbdc1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0076dbe38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 09:13:45.471: INFO: Pod "test-rollover-deployment-574d6dfbff-vrxmh" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-vrxmh test-rollover-deployment-574d6dfbff- deployment-5192 /api/v1/namespaces/deployment-5192/pods/test-rollover-deployment-574d6dfbff-vrxmh aed8f11e-1511-44ba-955e-4ff1dc67c675 13370 0 2019-12-14 09:13:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff 3c65ac30-4b48-4dcd-9572-f1edf0b04667 0xc00770c3b7 0xc00770c3b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qqq9v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qqq9v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qqq9v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.5,PodIP:10.33.1.172,StartTime:2019-12-14 09:13:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-14 09:13:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://d18860aa59f317f1f7c18409addb828d6674c51ddcce94558f0789c325206f48,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.33.1.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:13:45.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5192" for this suite.

• [SLOW TEST:21.288 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":133,"skipped":1940,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:13:45.477: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5079
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Dec 14 09:13:47.628: INFO: Pod pod-hostip-be16c38e-0302-42e2-b3d4-2efd2cab4af4 has hostIP: 10.20.20.5
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:13:47.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5079" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":134,"skipped":1942,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:13:47.634: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7980
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7980
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7980
I1214 09:13:47.801291      18 runners.go:189] Created replication controller with name: externalname-service, namespace: services-7980, replica count: 2
Dec 14 09:13:50.851: INFO: Creating new exec pod
I1214 09:13:50.851684      18 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 09:13:53.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=services-7980 execpodq7xm2 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 14 09:13:54.103: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 09:13:54.103: INFO: stdout: ""
Dec 14 09:13:54.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=services-7980 execpodq7xm2 -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.161 80'
Dec 14 09:13:54.351: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.161 80\nConnection to 10.32.0.161 80 port [tcp/http] succeeded!\n"
Dec 14 09:13:54.351: INFO: stdout: ""
Dec 14 09:13:54.351: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:13:54.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7980" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:6.745 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":135,"skipped":1948,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:13:54.379: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 09:13:54.540: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75178b3a-31dd-4805-a804-e24c3f8ab260" in namespace "downward-api-1584" to be "success or failure"
Dec 14 09:13:54.548: INFO: Pod "downwardapi-volume-75178b3a-31dd-4805-a804-e24c3f8ab260": Phase="Pending", Reason="", readiness=false. Elapsed: 7.844562ms
Dec 14 09:13:56.551: INFO: Pod "downwardapi-volume-75178b3a-31dd-4805-a804-e24c3f8ab260": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010135027s
STEP: Saw pod success
Dec 14 09:13:56.551: INFO: Pod "downwardapi-volume-75178b3a-31dd-4805-a804-e24c3f8ab260" satisfied condition "success or failure"
Dec 14 09:13:56.552: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-75178b3a-31dd-4805-a804-e24c3f8ab260 container client-container: <nil>
STEP: delete the pod
Dec 14 09:13:56.567: INFO: Waiting for pod downwardapi-volume-75178b3a-31dd-4805-a804-e24c3f8ab260 to disappear
Dec 14 09:13:56.569: INFO: Pod downwardapi-volume-75178b3a-31dd-4805-a804-e24c3f8ab260 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:13:56.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1584" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":136,"skipped":1952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:13:56.576: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2058
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-ab6a39cb-93da-4353-b23f-518ef9366dd8
STEP: Creating a pod to test consume secrets
Dec 14 09:13:56.719: INFO: Waiting up to 5m0s for pod "pod-secrets-49496365-a1ba-4aaf-bfcb-abb405fae408" in namespace "secrets-2058" to be "success or failure"
Dec 14 09:13:56.729: INFO: Pod "pod-secrets-49496365-a1ba-4aaf-bfcb-abb405fae408": Phase="Pending", Reason="", readiness=false. Elapsed: 9.407351ms
Dec 14 09:13:58.731: INFO: Pod "pod-secrets-49496365-a1ba-4aaf-bfcb-abb405fae408": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011880472s
STEP: Saw pod success
Dec 14 09:13:58.731: INFO: Pod "pod-secrets-49496365-a1ba-4aaf-bfcb-abb405fae408" satisfied condition "success or failure"
Dec 14 09:13:58.733: INFO: Trying to get logs from node k8s-2 pod pod-secrets-49496365-a1ba-4aaf-bfcb-abb405fae408 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 09:13:58.745: INFO: Waiting for pod pod-secrets-49496365-a1ba-4aaf-bfcb-abb405fae408 to disappear
Dec 14 09:13:58.748: INFO: Pod pod-secrets-49496365-a1ba-4aaf-bfcb-abb405fae408 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:13:58.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2058" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":137,"skipped":2011,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:13:58.754: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9403
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Dec 14 09:13:58.895: INFO: Waiting up to 5m0s for pod "client-containers-44d50d14-df04-4c53-8611-6aaed152dc9a" in namespace "containers-9403" to be "success or failure"
Dec 14 09:13:58.901: INFO: Pod "client-containers-44d50d14-df04-4c53-8611-6aaed152dc9a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.864262ms
Dec 14 09:14:00.903: INFO: Pod "client-containers-44d50d14-df04-4c53-8611-6aaed152dc9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008356593s
STEP: Saw pod success
Dec 14 09:14:00.903: INFO: Pod "client-containers-44d50d14-df04-4c53-8611-6aaed152dc9a" satisfied condition "success or failure"
Dec 14 09:14:00.905: INFO: Trying to get logs from node k8s-2 pod client-containers-44d50d14-df04-4c53-8611-6aaed152dc9a container test-container: <nil>
STEP: delete the pod
Dec 14 09:14:00.918: INFO: Waiting for pod client-containers-44d50d14-df04-4c53-8611-6aaed152dc9a to disappear
Dec 14 09:14:00.920: INFO: Pod client-containers-44d50d14-df04-4c53-8611-6aaed152dc9a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:14:00.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9403" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":138,"skipped":2018,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:14:00.927: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9455
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-j2qj
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 09:14:01.123: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j2qj" in namespace "subpath-9455" to be "success or failure"
Dec 14 09:14:01.128: INFO: Pod "pod-subpath-test-configmap-j2qj": Phase="Pending", Reason="", readiness=false. Elapsed: 5.801916ms
Dec 14 09:14:03.131: INFO: Pod "pod-subpath-test-configmap-j2qj": Phase="Running", Reason="", readiness=true. Elapsed: 2.007967105s
Dec 14 09:14:05.133: INFO: Pod "pod-subpath-test-configmap-j2qj": Phase="Running", Reason="", readiness=true. Elapsed: 4.0104637s
Dec 14 09:14:07.136: INFO: Pod "pod-subpath-test-configmap-j2qj": Phase="Running", Reason="", readiness=true. Elapsed: 6.012942104s
Dec 14 09:14:09.138: INFO: Pod "pod-subpath-test-configmap-j2qj": Phase="Running", Reason="", readiness=true. Elapsed: 8.014915625s
Dec 14 09:14:11.140: INFO: Pod "pod-subpath-test-configmap-j2qj": Phase="Running", Reason="", readiness=true. Elapsed: 10.01703185s
Dec 14 09:14:13.142: INFO: Pod "pod-subpath-test-configmap-j2qj": Phase="Running", Reason="", readiness=true. Elapsed: 12.019355535s
Dec 14 09:14:15.145: INFO: Pod "pod-subpath-test-configmap-j2qj": Phase="Running", Reason="", readiness=true. Elapsed: 14.021828304s
Dec 14 09:14:17.147: INFO: Pod "pod-subpath-test-configmap-j2qj": Phase="Running", Reason="", readiness=true. Elapsed: 16.024066356s
Dec 14 09:14:19.149: INFO: Pod "pod-subpath-test-configmap-j2qj": Phase="Running", Reason="", readiness=true. Elapsed: 18.026296233s
Dec 14 09:14:21.151: INFO: Pod "pod-subpath-test-configmap-j2qj": Phase="Running", Reason="", readiness=true. Elapsed: 20.028575401s
Dec 14 09:14:23.154: INFO: Pod "pod-subpath-test-configmap-j2qj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.031082605s
STEP: Saw pod success
Dec 14 09:14:23.154: INFO: Pod "pod-subpath-test-configmap-j2qj" satisfied condition "success or failure"
Dec 14 09:14:23.156: INFO: Trying to get logs from node k8s-2 pod pod-subpath-test-configmap-j2qj container test-container-subpath-configmap-j2qj: <nil>
STEP: delete the pod
Dec 14 09:14:23.169: INFO: Waiting for pod pod-subpath-test-configmap-j2qj to disappear
Dec 14 09:14:23.172: INFO: Pod pod-subpath-test-configmap-j2qj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-j2qj
Dec 14 09:14:23.172: INFO: Deleting pod "pod-subpath-test-configmap-j2qj" in namespace "subpath-9455"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:14:23.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9455" for this suite.

• [SLOW TEST:22.253 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":139,"skipped":2029,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:14:23.180: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-938
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 14 09:14:23.317: INFO: Waiting up to 5m0s for pod "pod-adf7e613-8733-4c90-a1f9-4771afa57706" in namespace "emptydir-938" to be "success or failure"
Dec 14 09:14:23.323: INFO: Pod "pod-adf7e613-8733-4c90-a1f9-4771afa57706": Phase="Pending", Reason="", readiness=false. Elapsed: 5.918365ms
Dec 14 09:14:25.325: INFO: Pod "pod-adf7e613-8733-4c90-a1f9-4771afa57706": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008024338s
STEP: Saw pod success
Dec 14 09:14:25.325: INFO: Pod "pod-adf7e613-8733-4c90-a1f9-4771afa57706" satisfied condition "success or failure"
Dec 14 09:14:25.327: INFO: Trying to get logs from node k8s-2 pod pod-adf7e613-8733-4c90-a1f9-4771afa57706 container test-container: <nil>
STEP: delete the pod
Dec 14 09:14:25.339: INFO: Waiting for pod pod-adf7e613-8733-4c90-a1f9-4771afa57706 to disappear
Dec 14 09:14:25.342: INFO: Pod pod-adf7e613-8733-4c90-a1f9-4771afa57706 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:14:25.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-938" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":140,"skipped":2035,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:14:25.347: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9927
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-db8aa814-ff39-4477-9983-d8cb2f14e19b
Dec 14 09:14:25.485: INFO: Pod name my-hostname-basic-db8aa814-ff39-4477-9983-d8cb2f14e19b: Found 0 pods out of 1
Dec 14 09:14:30.488: INFO: Pod name my-hostname-basic-db8aa814-ff39-4477-9983-d8cb2f14e19b: Found 1 pods out of 1
Dec 14 09:14:30.489: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-db8aa814-ff39-4477-9983-d8cb2f14e19b" are running
Dec 14 09:14:30.492: INFO: Pod "my-hostname-basic-db8aa814-ff39-4477-9983-d8cb2f14e19b-lzp9d" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-14 09:14:25 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-14 09:14:26 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-14 09:14:26 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-14 09:14:25 +0000 UTC Reason: Message:}])
Dec 14 09:14:30.492: INFO: Trying to dial the pod
Dec 14 09:14:35.499: INFO: Controller my-hostname-basic-db8aa814-ff39-4477-9983-d8cb2f14e19b: Got expected result from replica 1 [my-hostname-basic-db8aa814-ff39-4477-9983-d8cb2f14e19b-lzp9d]: "my-hostname-basic-db8aa814-ff39-4477-9983-d8cb2f14e19b-lzp9d", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:14:35.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9927" for this suite.

• [SLOW TEST:10.158 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":141,"skipped":2036,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:14:35.506: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-57
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:14:35.687: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Creating first CR 
Dec 14 09:14:36.242: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-14T09:14:36Z generation:1 name:name1 resourceVersion:13821 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5345968d-7146-4c99-8448-143376cd23d1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 14 09:14:46.246: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-14T09:14:46Z generation:1 name:name2 resourceVersion:13858 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:373ec6c1-038f-4f00-9381-8402db50c2f8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 14 09:14:56.251: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-14T09:14:36Z generation:2 name:name1 resourceVersion:13878 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5345968d-7146-4c99-8448-143376cd23d1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 14 09:15:06.256: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-14T09:14:46Z generation:2 name:name2 resourceVersion:13898 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:373ec6c1-038f-4f00-9381-8402db50c2f8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 14 09:15:16.262: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-14T09:14:36Z generation:2 name:name1 resourceVersion:13918 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5345968d-7146-4c99-8448-143376cd23d1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 14 09:15:26.268: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-14T09:14:46Z generation:2 name:name2 resourceVersion:13938 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:373ec6c1-038f-4f00-9381-8402db50c2f8] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:15:36.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-57" for this suite.

• [SLOW TEST:61.278 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":142,"skipped":2049,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:15:36.784: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4782
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-2b1ec634-b0a9-44c3-9e4d-1d3df4162ee3
STEP: Creating a pod to test consume configMaps
Dec 14 09:15:36.937: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-843bfb3c-c084-4739-9ccb-a609a0bbdb55" in namespace "projected-4782" to be "success or failure"
Dec 14 09:15:36.939: INFO: Pod "pod-projected-configmaps-843bfb3c-c084-4739-9ccb-a609a0bbdb55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.245048ms
Dec 14 09:15:38.942: INFO: Pod "pod-projected-configmaps-843bfb3c-c084-4739-9ccb-a609a0bbdb55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004831835s
STEP: Saw pod success
Dec 14 09:15:38.942: INFO: Pod "pod-projected-configmaps-843bfb3c-c084-4739-9ccb-a609a0bbdb55" satisfied condition "success or failure"
Dec 14 09:15:38.944: INFO: Trying to get logs from node k8s-2 pod pod-projected-configmaps-843bfb3c-c084-4739-9ccb-a609a0bbdb55 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 09:15:38.959: INFO: Waiting for pod pod-projected-configmaps-843bfb3c-c084-4739-9ccb-a609a0bbdb55 to disappear
Dec 14 09:15:38.961: INFO: Pod pod-projected-configmaps-843bfb3c-c084-4739-9ccb-a609a0bbdb55 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:15:38.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4782" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":143,"skipped":2073,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:15:38.967: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1572
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Dec 14 09:15:39.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 cluster-info'
Dec 14 09:15:39.179: INFO: stderr: ""
Dec 14 09:15:39.179: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.32.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.32.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:15:39.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1572" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":144,"skipped":2078,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:15:39.185: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2034
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 14 09:15:39.324: INFO: Waiting up to 5m0s for pod "pod-4bc30cee-2b45-4aa1-98a0-4e8d784b7d80" in namespace "emptydir-2034" to be "success or failure"
Dec 14 09:15:39.330: INFO: Pod "pod-4bc30cee-2b45-4aa1-98a0-4e8d784b7d80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.249637ms
Dec 14 09:15:41.332: INFO: Pod "pod-4bc30cee-2b45-4aa1-98a0-4e8d784b7d80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008516059s
STEP: Saw pod success
Dec 14 09:15:41.333: INFO: Pod "pod-4bc30cee-2b45-4aa1-98a0-4e8d784b7d80" satisfied condition "success or failure"
Dec 14 09:15:41.334: INFO: Trying to get logs from node k8s-2 pod pod-4bc30cee-2b45-4aa1-98a0-4e8d784b7d80 container test-container: <nil>
STEP: delete the pod
Dec 14 09:15:41.347: INFO: Waiting for pod pod-4bc30cee-2b45-4aa1-98a0-4e8d784b7d80 to disappear
Dec 14 09:15:41.349: INFO: Pod pod-4bc30cee-2b45-4aa1-98a0-4e8d784b7d80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:15:41.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2034" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":145,"skipped":2083,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:15:41.354: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:15:41.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2113" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":146,"skipped":2089,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:15:41.494: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1001
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:15:43.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1001" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":147,"skipped":2104,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:15:43.653: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-942
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec 14 09:15:53.818: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:15:53.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1214 09:15:53.818926      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-942" for this suite.

• [SLOW TEST:10.171 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":148,"skipped":2105,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:15:53.825: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-225
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 14 09:15:58.011: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 09:15:58.015: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 09:16:00.015: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 09:16:00.018: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 09:16:02.015: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 09:16:02.018: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 09:16:04.015: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 09:16:04.018: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 09:16:06.015: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 09:16:06.018: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 09:16:08.015: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 09:16:08.018: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 09:16:10.015: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 09:16:10.017: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:16:10.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-225" for this suite.

• [SLOW TEST:16.225 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":149,"skipped":2110,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:16:10.051: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 14 09:16:10.416: INFO: Pod name wrapped-volume-race-bc946740-cbf3-4a03-ab4c-4f4c2ccaa6fe: Found 1 pods out of 5
Dec 14 09:16:15.420: INFO: Pod name wrapped-volume-race-bc946740-cbf3-4a03-ab4c-4f4c2ccaa6fe: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-bc946740-cbf3-4a03-ab4c-4f4c2ccaa6fe in namespace emptydir-wrapper-4365, will wait for the garbage collector to delete the pods
Dec 14 09:16:25.500: INFO: Deleting ReplicationController wrapped-volume-race-bc946740-cbf3-4a03-ab4c-4f4c2ccaa6fe took: 10.408874ms
Dec 14 09:16:26.501: INFO: Terminating ReplicationController wrapped-volume-race-bc946740-cbf3-4a03-ab4c-4f4c2ccaa6fe pods took: 1.000314194s
STEP: Creating RC which spawns configmap-volume pods
Dec 14 09:16:38.614: INFO: Pod name wrapped-volume-race-1d2d3761-159d-478c-bf94-11c4cdeffd50: Found 0 pods out of 5
Dec 14 09:16:43.619: INFO: Pod name wrapped-volume-race-1d2d3761-159d-478c-bf94-11c4cdeffd50: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1d2d3761-159d-478c-bf94-11c4cdeffd50 in namespace emptydir-wrapper-4365, will wait for the garbage collector to delete the pods
Dec 14 09:16:53.698: INFO: Deleting ReplicationController wrapped-volume-race-1d2d3761-159d-478c-bf94-11c4cdeffd50 took: 10.489825ms
Dec 14 09:16:53.798: INFO: Terminating ReplicationController wrapped-volume-race-1d2d3761-159d-478c-bf94-11c4cdeffd50 pods took: 100.26169ms
STEP: Creating RC which spawns configmap-volume pods
Dec 14 09:17:08.413: INFO: Pod name wrapped-volume-race-b4ef02bd-36a9-410f-a09d-a31270a579f1: Found 0 pods out of 5
Dec 14 09:17:13.417: INFO: Pod name wrapped-volume-race-b4ef02bd-36a9-410f-a09d-a31270a579f1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b4ef02bd-36a9-410f-a09d-a31270a579f1 in namespace emptydir-wrapper-4365, will wait for the garbage collector to delete the pods
Dec 14 09:17:23.491: INFO: Deleting ReplicationController wrapped-volume-race-b4ef02bd-36a9-410f-a09d-a31270a579f1 took: 4.866472ms
Dec 14 09:17:24.491: INFO: Terminating ReplicationController wrapped-volume-race-b4ef02bd-36a9-410f-a09d-a31270a579f1 pods took: 1.000215994s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:17:38.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4365" for this suite.

• [SLOW TEST:88.574 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":150,"skipped":2124,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:17:38.626: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 09:17:38.815: INFO: Waiting up to 5m0s for pod "downwardapi-volume-62ee501d-475a-4c60-adf6-65ac7c982f45" in namespace "projected-732" to be "success or failure"
Dec 14 09:17:38.825: INFO: Pod "downwardapi-volume-62ee501d-475a-4c60-adf6-65ac7c982f45": Phase="Pending", Reason="", readiness=false. Elapsed: 10.202966ms
Dec 14 09:17:40.827: INFO: Pod "downwardapi-volume-62ee501d-475a-4c60-adf6-65ac7c982f45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012644881s
STEP: Saw pod success
Dec 14 09:17:40.828: INFO: Pod "downwardapi-volume-62ee501d-475a-4c60-adf6-65ac7c982f45" satisfied condition "success or failure"
Dec 14 09:17:40.829: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-62ee501d-475a-4c60-adf6-65ac7c982f45 container client-container: <nil>
STEP: delete the pod
Dec 14 09:17:40.844: INFO: Waiting for pod downwardapi-volume-62ee501d-475a-4c60-adf6-65ac7c982f45 to disappear
Dec 14 09:17:40.846: INFO: Pod downwardapi-volume-62ee501d-475a-4c60-adf6-65ac7c982f45 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:17:40.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-732" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":151,"skipped":2142,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:17:40.852: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2644
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 14 09:17:40.989: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2644 /api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a c67ecf17-2bcc-422a-a598-621ecc684e2c 15117 0 2019-12-14 09:17:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 14 09:17:40.989: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2644 /api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a c67ecf17-2bcc-422a-a598-621ecc684e2c 15117 0 2019-12-14 09:17:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 14 09:17:50.995: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2644 /api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a c67ecf17-2bcc-422a-a598-621ecc684e2c 15336 0 2019-12-14 09:17:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 14 09:17:50.995: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2644 /api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a c67ecf17-2bcc-422a-a598-621ecc684e2c 15336 0 2019-12-14 09:17:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 14 09:18:01.000: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2644 /api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a c67ecf17-2bcc-422a-a598-621ecc684e2c 15356 0 2019-12-14 09:17:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 14 09:18:01.000: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2644 /api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a c67ecf17-2bcc-422a-a598-621ecc684e2c 15356 0 2019-12-14 09:17:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 14 09:18:11.005: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2644 /api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a c67ecf17-2bcc-422a-a598-621ecc684e2c 15377 0 2019-12-14 09:17:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 14 09:18:11.005: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2644 /api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a c67ecf17-2bcc-422a-a598-621ecc684e2c 15377 0 2019-12-14 09:17:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 14 09:18:21.010: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2644 /api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-b 819f3b5e-8189-4cad-a82e-223a6dc4dc8a 15397 0 2019-12-14 09:18:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 14 09:18:21.011: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2644 /api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-b 819f3b5e-8189-4cad-a82e-223a6dc4dc8a 15397 0 2019-12-14 09:18:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 14 09:18:31.015: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2644 /api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-b 819f3b5e-8189-4cad-a82e-223a6dc4dc8a 15417 0 2019-12-14 09:18:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 14 09:18:31.016: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2644 /api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-b 819f3b5e-8189-4cad-a82e-223a6dc4dc8a 15417 0 2019-12-14 09:18:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:18:41.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2644" for this suite.

• [SLOW TEST:60.170 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":152,"skipped":2147,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:18:41.023: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-286
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1768
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 14 09:18:41.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-286'
Dec 14 09:18:41.248: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 14 09:18:41.248: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1773
Dec 14 09:18:41.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete jobs e2e-test-httpd-job --namespace=kubectl-286'
Dec 14 09:18:41.361: INFO: stderr: ""
Dec 14 09:18:41.361: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:18:41.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-286" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":280,"completed":153,"skipped":2178,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:18:41.372: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2657
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 09:18:42.533: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:18:42.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2657" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":154,"skipped":2180,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:18:42.559: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2494
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:18:43.152: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 14 09:18:45.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911923, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911923, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911923, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711911923, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:18:48.166: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:18:48.169: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:18:50.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2494" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:7.510 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":155,"skipped":2234,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:18:50.070: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-296
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-705c264d-24a8-44aa-8e40-34edbe969924 in namespace container-probe-296
Dec 14 09:18:54.263: INFO: Started pod test-webserver-705c264d-24a8-44aa-8e40-34edbe969924 in namespace container-probe-296
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 09:18:54.265: INFO: Initial restart count of pod test-webserver-705c264d-24a8-44aa-8e40-34edbe969924 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:22:54.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-296" for this suite.

• [SLOW TEST:244.522 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":156,"skipped":2241,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:22:54.592: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5888
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 09:22:54.741: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ba4951c-16ab-4f31-9a99-b9643405b55e" in namespace "projected-5888" to be "success or failure"
Dec 14 09:22:54.747: INFO: Pod "downwardapi-volume-1ba4951c-16ab-4f31-9a99-b9643405b55e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.948576ms
Dec 14 09:22:56.749: INFO: Pod "downwardapi-volume-1ba4951c-16ab-4f31-9a99-b9643405b55e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008413413s
STEP: Saw pod success
Dec 14 09:22:56.749: INFO: Pod "downwardapi-volume-1ba4951c-16ab-4f31-9a99-b9643405b55e" satisfied condition "success or failure"
Dec 14 09:22:56.751: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-1ba4951c-16ab-4f31-9a99-b9643405b55e container client-container: <nil>
STEP: delete the pod
Dec 14 09:22:56.767: INFO: Waiting for pod downwardapi-volume-1ba4951c-16ab-4f31-9a99-b9643405b55e to disappear
Dec 14 09:22:56.769: INFO: Pod downwardapi-volume-1ba4951c-16ab-4f31-9a99-b9643405b55e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:22:56.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5888" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":157,"skipped":2270,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:22:56.775: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-1a1f1c18-dcf8-4418-bbba-047fa6e1fe00
STEP: Creating a pod to test consume configMaps
Dec 14 09:22:56.982: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-05b10e12-ec71-473d-b98d-ef56bb618b2f" in namespace "projected-4903" to be "success or failure"
Dec 14 09:22:56.991: INFO: Pod "pod-projected-configmaps-05b10e12-ec71-473d-b98d-ef56bb618b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.840595ms
Dec 14 09:22:58.994: INFO: Pod "pod-projected-configmaps-05b10e12-ec71-473d-b98d-ef56bb618b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01140708s
STEP: Saw pod success
Dec 14 09:22:58.994: INFO: Pod "pod-projected-configmaps-05b10e12-ec71-473d-b98d-ef56bb618b2f" satisfied condition "success or failure"
Dec 14 09:22:58.996: INFO: Trying to get logs from node k8s-2 pod pod-projected-configmaps-05b10e12-ec71-473d-b98d-ef56bb618b2f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 09:22:59.008: INFO: Waiting for pod pod-projected-configmaps-05b10e12-ec71-473d-b98d-ef56bb618b2f to disappear
Dec 14 09:22:59.011: INFO: Pod pod-projected-configmaps-05b10e12-ec71-473d-b98d-ef56bb618b2f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:22:59.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4903" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":158,"skipped":2298,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:22:59.016: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 14 09:22:59.205: INFO: Waiting up to 5m0s for pod "pod-1e160dc7-3f42-4ebd-b321-22f28b0eb151" in namespace "emptydir-4741" to be "success or failure"
Dec 14 09:22:59.212: INFO: Pod "pod-1e160dc7-3f42-4ebd-b321-22f28b0eb151": Phase="Pending", Reason="", readiness=false. Elapsed: 6.162077ms
Dec 14 09:23:01.214: INFO: Pod "pod-1e160dc7-3f42-4ebd-b321-22f28b0eb151": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008427644s
STEP: Saw pod success
Dec 14 09:23:01.214: INFO: Pod "pod-1e160dc7-3f42-4ebd-b321-22f28b0eb151" satisfied condition "success or failure"
Dec 14 09:23:01.216: INFO: Trying to get logs from node k8s-2 pod pod-1e160dc7-3f42-4ebd-b321-22f28b0eb151 container test-container: <nil>
STEP: delete the pod
Dec 14 09:23:01.229: INFO: Waiting for pod pod-1e160dc7-3f42-4ebd-b321-22f28b0eb151 to disappear
Dec 14 09:23:01.230: INFO: Pod pod-1e160dc7-3f42-4ebd-b321-22f28b0eb151 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:23:01.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4741" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":159,"skipped":2299,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:23:01.236: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8151
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-40eb17f4-6c67-4514-a614-af8bc863281a
STEP: Creating a pod to test consume configMaps
Dec 14 09:23:01.373: INFO: Waiting up to 5m0s for pod "pod-configmaps-1a05a63b-5250-4da8-a29f-abd28500a631" in namespace "configmap-8151" to be "success or failure"
Dec 14 09:23:01.378: INFO: Pod "pod-configmaps-1a05a63b-5250-4da8-a29f-abd28500a631": Phase="Pending", Reason="", readiness=false. Elapsed: 5.688306ms
Dec 14 09:23:03.380: INFO: Pod "pod-configmaps-1a05a63b-5250-4da8-a29f-abd28500a631": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00776682s
Dec 14 09:23:05.383: INFO: Pod "pod-configmaps-1a05a63b-5250-4da8-a29f-abd28500a631": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009928714s
STEP: Saw pod success
Dec 14 09:23:05.383: INFO: Pod "pod-configmaps-1a05a63b-5250-4da8-a29f-abd28500a631" satisfied condition "success or failure"
Dec 14 09:23:05.384: INFO: Trying to get logs from node k8s-2 pod pod-configmaps-1a05a63b-5250-4da8-a29f-abd28500a631 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 09:23:05.397: INFO: Waiting for pod pod-configmaps-1a05a63b-5250-4da8-a29f-abd28500a631 to disappear
Dec 14 09:23:05.400: INFO: Pod pod-configmaps-1a05a63b-5250-4da8-a29f-abd28500a631 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:23:05.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8151" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":160,"skipped":2302,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:23:05.405: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1713
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 14 09:23:05.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-6580'
Dec 14 09:23:05.728: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 14 09:23:05.728: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1718
Dec 14 09:23:09.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete deployment e2e-test-httpd-deployment --namespace=kubectl-6580'
Dec 14 09:23:09.832: INFO: stderr: ""
Dec 14 09:23:09.832: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:23:09.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6580" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":280,"completed":161,"skipped":2302,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:23:09.841: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3899
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 14 09:23:10.044: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3899 /api/v1/namespaces/watch-3899/configmaps/e2e-watch-test-resource-version b8ecdc9f-c159-4f3e-8a33-5a165ab81263 16241 0 2019-12-14 09:23:10 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 14 09:23:10.045: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3899 /api/v1/namespaces/watch-3899/configmaps/e2e-watch-test-resource-version b8ecdc9f-c159-4f3e-8a33-5a165ab81263 16242 0 2019-12-14 09:23:10 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:23:10.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3899" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":162,"skipped":2366,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:23:10.053: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Dec 14 09:23:10.242: INFO: Created pod &Pod{ObjectMeta:{dns-7736  dns-7736 /api/v1/namespaces/dns-7736/pods/dns-7736 397cee4d-0b44-4bc0-942e-08416a51c383 16249 0 2019-12-14 09:23:10 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-l68t8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-l68t8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-l68t8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Dec 14 09:23:12.251: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7736 PodName:dns-7736 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:23:12.251: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Verifying customized DNS server is configured on pod...
Dec 14 09:23:12.408: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7736 PodName:dns-7736 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:23:12.408: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:23:12.543: INFO: Deleting pod dns-7736...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:23:12.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7736" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":163,"skipped":2385,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:23:12.561: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5418
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:23:13.236: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:23:16.246: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:23:16.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5418" for this suite.
STEP: Destroying namespace "webhook-5418-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":164,"skipped":2389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:23:16.430: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1240
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:23:17.522: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:23:20.534: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:23:20.536: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-32-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:23:22.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1240" for this suite.
STEP: Destroying namespace "webhook-1240-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.846 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":165,"skipped":2436,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:23:22.278: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6420
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:23:29.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6420" for this suite.

• [SLOW TEST:7.188 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":166,"skipped":2478,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:23:29.467: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7290
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 14 09:23:29.611: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7290 /api/v1/namespaces/watch-7290/configmaps/e2e-watch-test-label-changed 2ca3606e-3e19-43ed-9209-0973ed369a52 16520 0 2019-12-14 09:23:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 14 09:23:29.611: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7290 /api/v1/namespaces/watch-7290/configmaps/e2e-watch-test-label-changed 2ca3606e-3e19-43ed-9209-0973ed369a52 16521 0 2019-12-14 09:23:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 14 09:23:29.611: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7290 /api/v1/namespaces/watch-7290/configmaps/e2e-watch-test-label-changed 2ca3606e-3e19-43ed-9209-0973ed369a52 16522 0 2019-12-14 09:23:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 14 09:23:39.628: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7290 /api/v1/namespaces/watch-7290/configmaps/e2e-watch-test-label-changed 2ca3606e-3e19-43ed-9209-0973ed369a52 16552 0 2019-12-14 09:23:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 14 09:23:39.628: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7290 /api/v1/namespaces/watch-7290/configmaps/e2e-watch-test-label-changed 2ca3606e-3e19-43ed-9209-0973ed369a52 16553 0 2019-12-14 09:23:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec 14 09:23:39.629: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7290 /api/v1/namespaces/watch-7290/configmaps/e2e-watch-test-label-changed 2ca3606e-3e19-43ed-9209-0973ed369a52 16554 0 2019-12-14 09:23:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:23:39.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7290" for this suite.

• [SLOW TEST:10.167 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":167,"skipped":2503,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:23:39.635: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5959 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5959;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5959 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5959;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5959.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5959.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5959.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5959.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5959.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5959.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5959.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5959.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5959.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5959.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5959.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5959.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5959.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 250.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.250_udp@PTR;check="$$(dig +tcp +noall +answer +search 250.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.250_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5959 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5959;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5959 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5959;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5959.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5959.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5959.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5959.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5959.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5959.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5959.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5959.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5959.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5959.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5959.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5959.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5959.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 250.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.250_udp@PTR;check="$$(dig +tcp +noall +answer +search 250.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.250_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 09:23:41.822: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.824: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.827: INFO: Unable to read wheezy_udp@dns-test-service.dns-5959 from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.829: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5959 from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.831: INFO: Unable to read wheezy_udp@dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.834: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.836: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.842: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.873: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.877: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.879: INFO: Unable to read jessie_udp@dns-test-service.dns-5959 from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.882: INFO: Unable to read jessie_tcp@dns-test-service.dns-5959 from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.885: INFO: Unable to read jessie_udp@dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.887: INFO: Unable to read jessie_tcp@dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.889: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.891: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:41.903: INFO: Lookups using dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5959 wheezy_tcp@dns-test-service.dns-5959 wheezy_udp@dns-test-service.dns-5959.svc wheezy_tcp@dns-test-service.dns-5959.svc wheezy_udp@_http._tcp.dns-test-service.dns-5959.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5959.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5959 jessie_tcp@dns-test-service.dns-5959 jessie_udp@dns-test-service.dns-5959.svc jessie_tcp@dns-test-service.dns-5959.svc jessie_udp@_http._tcp.dns-test-service.dns-5959.svc jessie_tcp@_http._tcp.dns-test-service.dns-5959.svc]

Dec 14 09:23:46.907: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.910: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.912: INFO: Unable to read wheezy_udp@dns-test-service.dns-5959 from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.914: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5959 from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.916: INFO: Unable to read wheezy_udp@dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.918: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.920: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.922: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.936: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.938: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.941: INFO: Unable to read jessie_udp@dns-test-service.dns-5959 from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.943: INFO: Unable to read jessie_tcp@dns-test-service.dns-5959 from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.945: INFO: Unable to read jessie_udp@dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.947: INFO: Unable to read jessie_tcp@dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.950: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.952: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5959.svc from pod dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5: the server could not find the requested resource (get pods dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5)
Dec 14 09:23:46.964: INFO: Lookups using dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5959 wheezy_tcp@dns-test-service.dns-5959 wheezy_udp@dns-test-service.dns-5959.svc wheezy_tcp@dns-test-service.dns-5959.svc wheezy_udp@_http._tcp.dns-test-service.dns-5959.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5959.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5959 jessie_tcp@dns-test-service.dns-5959 jessie_udp@dns-test-service.dns-5959.svc jessie_tcp@dns-test-service.dns-5959.svc jessie_udp@_http._tcp.dns-test-service.dns-5959.svc jessie_tcp@_http._tcp.dns-test-service.dns-5959.svc]

Dec 14 09:23:51.959: INFO: DNS probes using dns-5959/dns-test-1cac4043-bd1e-4543-8c3d-b1c40d755da5 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:23:52.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5959" for this suite.

• [SLOW TEST:12.410 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":168,"skipped":2529,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:23:52.046: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3373
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1214 09:24:22.223089      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 14 09:24:22.223: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:24:22.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3373" for this suite.

• [SLOW TEST:30.182 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":169,"skipped":2531,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:24:22.229: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6048
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:24:26.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6048" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":170,"skipped":2540,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:24:26.389: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6490
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 09:24:28.538: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:24:28.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6490" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":171,"skipped":2545,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:24:28.554: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
STEP: creating an pod
Dec 14 09:24:28.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-3476 -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 14 09:24:28.777: INFO: stderr: ""
Dec 14 09:24:28.777: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Dec 14 09:24:28.777: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 14 09:24:28.777: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3476" to be "running and ready, or succeeded"
Dec 14 09:24:28.780: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.390278ms
Dec 14 09:24:30.782: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.005512128s
Dec 14 09:24:30.782: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 14 09:24:30.782: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 14 09:24:30.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 logs logs-generator logs-generator --namespace=kubectl-3476'
Dec 14 09:24:30.874: INFO: stderr: ""
Dec 14 09:24:30.874: INFO: stdout: "I1214 09:24:29.329159       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/v6n 294\nI1214 09:24:29.529265       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/qp4 334\nI1214 09:24:29.729384       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/m72j 400\nI1214 09:24:29.929387       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/nxv 247\nI1214 09:24:30.129339       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/dh6 483\nI1214 09:24:30.329266       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/v6rf 268\nI1214 09:24:30.529250       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/mzv 216\nI1214 09:24:30.729264       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/xl9 442\n"
STEP: limiting log lines
Dec 14 09:24:30.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 logs logs-generator logs-generator --namespace=kubectl-3476 --tail=1'
Dec 14 09:24:30.954: INFO: stderr: ""
Dec 14 09:24:30.954: INFO: stdout: "I1214 09:24:30.929268       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/qzdw 510\n"
Dec 14 09:24:30.954: INFO: got output "I1214 09:24:30.929268       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/qzdw 510\n"
STEP: limiting log bytes
Dec 14 09:24:30.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 logs logs-generator logs-generator --namespace=kubectl-3476 --limit-bytes=1'
Dec 14 09:24:31.040: INFO: stderr: ""
Dec 14 09:24:31.040: INFO: stdout: "I"
Dec 14 09:24:31.040: INFO: got output "I"
STEP: exposing timestamps
Dec 14 09:24:31.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 logs logs-generator logs-generator --namespace=kubectl-3476 --tail=1 --timestamps'
Dec 14 09:24:31.123: INFO: stderr: ""
Dec 14 09:24:31.123: INFO: stdout: "2019-12-14T09:24:30.929346856Z I1214 09:24:30.929268       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/qzdw 510\n"
Dec 14 09:24:31.123: INFO: got output "2019-12-14T09:24:30.929346856Z I1214 09:24:30.929268       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/qzdw 510\n"
STEP: restricting to a time range
Dec 14 09:24:33.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 logs logs-generator logs-generator --namespace=kubectl-3476 --since=1s'
Dec 14 09:24:33.787: INFO: stderr: ""
Dec 14 09:24:33.787: INFO: stdout: "I1214 09:24:32.929330       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/f9t 343\nI1214 09:24:33.129373       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/nzf 376\nI1214 09:24:33.329378       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/khxg 351\nI1214 09:24:33.529353       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/tlc 236\nI1214 09:24:33.729377       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/xzv6 477\n"
Dec 14 09:24:33.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 logs logs-generator logs-generator --namespace=kubectl-3476 --since=24h'
Dec 14 09:24:33.908: INFO: stderr: ""
Dec 14 09:24:33.908: INFO: stdout: "I1214 09:24:29.329159       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/v6n 294\nI1214 09:24:29.529265       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/qp4 334\nI1214 09:24:29.729384       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/m72j 400\nI1214 09:24:29.929387       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/nxv 247\nI1214 09:24:30.129339       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/dh6 483\nI1214 09:24:30.329266       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/v6rf 268\nI1214 09:24:30.529250       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/mzv 216\nI1214 09:24:30.729264       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/xl9 442\nI1214 09:24:30.929268       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/qzdw 510\nI1214 09:24:31.129294       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/zv5 422\nI1214 09:24:31.329411       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/7cg 591\nI1214 09:24:31.529354       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/brj 212\nI1214 09:24:31.729333       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/9d2 285\nI1214 09:24:31.929377       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/2wcs 558\nI1214 09:24:32.129375       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/gfh 565\nI1214 09:24:32.329306       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/xwhm 272\nI1214 09:24:32.529301       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/qd7l 368\nI1214 09:24:32.729309       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/lhb 447\nI1214 09:24:32.929330       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/f9t 343\nI1214 09:24:33.129373       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/nzf 376\nI1214 09:24:33.329378       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/khxg 351\nI1214 09:24:33.529353       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/tlc 236\nI1214 09:24:33.729377       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/xzv6 477\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
Dec 14 09:24:33.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete pod logs-generator --namespace=kubectl-3476'
Dec 14 09:24:38.182: INFO: stderr: ""
Dec 14 09:24:38.182: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:24:38.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3476" for this suite.

• [SLOW TEST:9.636 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1440
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":172,"skipped":2547,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:24:38.190: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3160
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 09:24:38.333: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bb8d047-20ee-4415-8229-3c00ae18cb96" in namespace "projected-3160" to be "success or failure"
Dec 14 09:24:38.340: INFO: Pod "downwardapi-volume-8bb8d047-20ee-4415-8229-3c00ae18cb96": Phase="Pending", Reason="", readiness=false. Elapsed: 6.304117ms
Dec 14 09:24:40.342: INFO: Pod "downwardapi-volume-8bb8d047-20ee-4415-8229-3c00ae18cb96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008362652s
Dec 14 09:24:42.344: INFO: Pod "downwardapi-volume-8bb8d047-20ee-4415-8229-3c00ae18cb96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01060837s
STEP: Saw pod success
Dec 14 09:24:42.344: INFO: Pod "downwardapi-volume-8bb8d047-20ee-4415-8229-3c00ae18cb96" satisfied condition "success or failure"
Dec 14 09:24:42.346: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-8bb8d047-20ee-4415-8229-3c00ae18cb96 container client-container: <nil>
STEP: delete the pod
Dec 14 09:24:42.359: INFO: Waiting for pod downwardapi-volume-8bb8d047-20ee-4415-8229-3c00ae18cb96 to disappear
Dec 14 09:24:42.362: INFO: Pod downwardapi-volume-8bb8d047-20ee-4415-8229-3c00ae18cb96 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:24:42.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3160" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":173,"skipped":2553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:24:42.369: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1413
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1214 09:24:48.518825      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 14 09:24:48.518: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:24:48.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1413" for this suite.

• [SLOW TEST:6.155 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":174,"skipped":2580,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:24:48.525: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9056
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-8sgq
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 09:24:48.676: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8sgq" in namespace "subpath-9056" to be "success or failure"
Dec 14 09:24:48.683: INFO: Pod "pod-subpath-test-secret-8sgq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.978719ms
Dec 14 09:24:50.685: INFO: Pod "pod-subpath-test-secret-8sgq": Phase="Running", Reason="", readiness=true. Elapsed: 2.009084387s
Dec 14 09:24:52.688: INFO: Pod "pod-subpath-test-secret-8sgq": Phase="Running", Reason="", readiness=true. Elapsed: 4.011316156s
Dec 14 09:24:54.690: INFO: Pod "pod-subpath-test-secret-8sgq": Phase="Running", Reason="", readiness=true. Elapsed: 6.014081056s
Dec 14 09:24:56.693: INFO: Pod "pod-subpath-test-secret-8sgq": Phase="Running", Reason="", readiness=true. Elapsed: 8.016617191s
Dec 14 09:24:58.695: INFO: Pod "pod-subpath-test-secret-8sgq": Phase="Running", Reason="", readiness=true. Elapsed: 10.018611325s
Dec 14 09:25:00.697: INFO: Pod "pod-subpath-test-secret-8sgq": Phase="Running", Reason="", readiness=true. Elapsed: 12.021101926s
Dec 14 09:25:02.699: INFO: Pod "pod-subpath-test-secret-8sgq": Phase="Running", Reason="", readiness=true. Elapsed: 14.023127966s
Dec 14 09:25:04.701: INFO: Pod "pod-subpath-test-secret-8sgq": Phase="Running", Reason="", readiness=true. Elapsed: 16.025222063s
Dec 14 09:25:06.704: INFO: Pod "pod-subpath-test-secret-8sgq": Phase="Running", Reason="", readiness=true. Elapsed: 18.027635233s
Dec 14 09:25:08.706: INFO: Pod "pod-subpath-test-secret-8sgq": Phase="Running", Reason="", readiness=true. Elapsed: 20.030134079s
Dec 14 09:25:10.709: INFO: Pod "pod-subpath-test-secret-8sgq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.032805085s
STEP: Saw pod success
Dec 14 09:25:10.709: INFO: Pod "pod-subpath-test-secret-8sgq" satisfied condition "success or failure"
Dec 14 09:25:10.711: INFO: Trying to get logs from node k8s-2 pod pod-subpath-test-secret-8sgq container test-container-subpath-secret-8sgq: <nil>
STEP: delete the pod
Dec 14 09:25:10.726: INFO: Waiting for pod pod-subpath-test-secret-8sgq to disappear
Dec 14 09:25:10.729: INFO: Pod pod-subpath-test-secret-8sgq no longer exists
STEP: Deleting pod pod-subpath-test-secret-8sgq
Dec 14 09:25:10.729: INFO: Deleting pod "pod-subpath-test-secret-8sgq" in namespace "subpath-9056"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:25:10.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9056" for this suite.

• [SLOW TEST:22.211 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":175,"skipped":2601,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:25:10.736: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3529
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:25:10.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3529" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":176,"skipped":2611,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:25:10.902: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5243
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:25:11.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5243" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":177,"skipped":2619,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:25:11.057: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3080
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-91ff6388-8e82-463d-bdc2-35df44298198
STEP: Creating a pod to test consume configMaps
Dec 14 09:25:11.198: INFO: Waiting up to 5m0s for pod "pod-configmaps-351f5943-6b58-4e47-b7c9-2d20cbd7072f" in namespace "configmap-3080" to be "success or failure"
Dec 14 09:25:11.204: INFO: Pod "pod-configmaps-351f5943-6b58-4e47-b7c9-2d20cbd7072f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.442295ms
Dec 14 09:25:13.206: INFO: Pod "pod-configmaps-351f5943-6b58-4e47-b7c9-2d20cbd7072f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008435058s
STEP: Saw pod success
Dec 14 09:25:13.206: INFO: Pod "pod-configmaps-351f5943-6b58-4e47-b7c9-2d20cbd7072f" satisfied condition "success or failure"
Dec 14 09:25:13.208: INFO: Trying to get logs from node k8s-2 pod pod-configmaps-351f5943-6b58-4e47-b7c9-2d20cbd7072f container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 09:25:13.222: INFO: Waiting for pod pod-configmaps-351f5943-6b58-4e47-b7c9-2d20cbd7072f to disappear
Dec 14 09:25:13.224: INFO: Pod pod-configmaps-351f5943-6b58-4e47-b7c9-2d20cbd7072f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:25:13.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3080" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":178,"skipped":2639,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:25:13.230: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2788
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-99f5102e-266c-49a8-8dbd-495f16e17b68
STEP: Creating a pod to test consume secrets
Dec 14 09:25:13.397: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0a7108c9-b431-4337-a66d-426f46bee6ec" in namespace "projected-2788" to be "success or failure"
Dec 14 09:25:13.407: INFO: Pod "pod-projected-secrets-0a7108c9-b431-4337-a66d-426f46bee6ec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.269499ms
Dec 14 09:25:15.409: INFO: Pod "pod-projected-secrets-0a7108c9-b431-4337-a66d-426f46bee6ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011507467s
Dec 14 09:25:17.411: INFO: Pod "pod-projected-secrets-0a7108c9-b431-4337-a66d-426f46bee6ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013867699s
STEP: Saw pod success
Dec 14 09:25:17.411: INFO: Pod "pod-projected-secrets-0a7108c9-b431-4337-a66d-426f46bee6ec" satisfied condition "success or failure"
Dec 14 09:25:17.413: INFO: Trying to get logs from node k8s-2 pod pod-projected-secrets-0a7108c9-b431-4337-a66d-426f46bee6ec container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 09:25:17.427: INFO: Waiting for pod pod-projected-secrets-0a7108c9-b431-4337-a66d-426f46bee6ec to disappear
Dec 14 09:25:17.428: INFO: Pod pod-projected-secrets-0a7108c9-b431-4337-a66d-426f46bee6ec no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:25:17.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2788" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":179,"skipped":2644,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:25:17.435: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:25:17.577: INFO: Creating deployment "webserver-deployment"
Dec 14 09:25:17.581: INFO: Waiting for observed generation 1
Dec 14 09:25:19.595: INFO: Waiting for all required pods to come up
Dec 14 09:25:19.604: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 14 09:25:21.631: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 14 09:25:21.634: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 14 09:25:21.639: INFO: Updating deployment webserver-deployment
Dec 14 09:25:21.639: INFO: Waiting for observed generation 2
Dec 14 09:25:23.645: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 14 09:25:23.647: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 14 09:25:23.649: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 14 09:25:23.653: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 14 09:25:23.653: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 14 09:25:23.655: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 14 09:25:23.658: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 14 09:25:23.658: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 14 09:25:23.663: INFO: Updating deployment webserver-deployment
Dec 14 09:25:23.663: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 14 09:25:23.674: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 14 09:25:23.681: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Dec 14 09:25:23.702: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9698 /apis/apps/v1/namespaces/deployment-9698/deployments/webserver-deployment 68cb4a82-ba40-4612-a6d9-3aa29a61f4b3 17561 3 2019-12-14 09:25:17 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000f71d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2019-12-14 09:25:21 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-14 09:25:23 +0000 UTC,LastTransitionTime:2019-12-14 09:25:23 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 14 09:25:23.729: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-9698 /apis/apps/v1/namespaces/deployment-9698/replicasets/webserver-deployment-c7997dcc8 26665b32-df56-4e04-92ca-16a29512be10 17558 3 2019-12-14 09:25:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 68cb4a82-ba40-4612-a6d9-3aa29a61f4b3 0xc00432c357 0xc00432c358}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00432c508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 09:25:23.729: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 14 09:25:23.730: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-9698 /apis/apps/v1/namespaces/deployment-9698/replicasets/webserver-deployment-595b5b9587 79739f66-3e1d-415b-8ca7-7a9a977d8b4d 17556 3 2019-12-14 09:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 68cb4a82-ba40-4612-a6d9-3aa29a61f4b3 0xc00432c1f7 0xc00432c1f8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00432c278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 14 09:25:23.748: INFO: Pod "webserver-deployment-595b5b9587-989gp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-989gp webserver-deployment-595b5b9587- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-595b5b9587-989gp 5599491f-d0c0-4ccf-ba6a-604d5ca3a700 17459 0 2019-12-14 09:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 79739f66-3e1d-415b-8ca7-7a9a977d8b4d 0xc00432cce7 0xc00432cce8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.6,PodIP:10.33.2.60,StartTime:2019-12-14 09:25:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-14 09:25:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://429b9809f7ef66543578a9b891178629666856a93bd89f5e1dc2840a0885207e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.33.2.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.749: INFO: Pod "webserver-deployment-595b5b9587-99272" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-99272 webserver-deployment-595b5b9587- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-595b5b9587-99272 79883b3a-62f2-467d-b50e-4ed534c11cf0 17447 0 2019-12-14 09:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 79739f66-3e1d-415b-8ca7-7a9a977d8b4d 0xc00432d040 0xc00432d041}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.5,PodIP:10.33.1.227,StartTime:2019-12-14 09:25:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-14 09:25:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://3735107682fb4e3ec929db5c46180b2e04bdd4e17b7427cba6453aae65b557c4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.33.1.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.749: INFO: Pod "webserver-deployment-595b5b9587-fb2z2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fb2z2 webserver-deployment-595b5b9587- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-595b5b9587-fb2z2 2818ea04-0208-4fee-9f77-82c34cb25f4b 17564 0 2019-12-14 09:25:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 79739f66-3e1d-415b-8ca7-7a9a977d8b4d 0xc00432d2c0 0xc00432d2c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.749: INFO: Pod "webserver-deployment-595b5b9587-fn6qd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fn6qd webserver-deployment-595b5b9587- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-595b5b9587-fn6qd 7d3b7750-04ed-486d-87f1-a1546e3edd9a 17572 0 2019-12-14 09:25:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 79739f66-3e1d-415b-8ca7-7a9a977d8b4d 0xc00432d3f0 0xc00432d3f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.749: INFO: Pod "webserver-deployment-595b5b9587-fzgr8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fzgr8 webserver-deployment-595b5b9587- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-595b5b9587-fzgr8 840d4531-0e74-4614-8ec5-1aa946a1b1aa 17465 0 2019-12-14 09:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 79739f66-3e1d-415b-8ca7-7a9a977d8b4d 0xc00432d510 0xc00432d511}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.6,PodIP:10.33.2.59,StartTime:2019-12-14 09:25:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-14 09:25:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://2f78cd21700bfa56b57fc29fbb977b992f5987a1a3b89aaf8916ff94fb9d24d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.33.2.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.749: INFO: Pod "webserver-deployment-595b5b9587-qt9nr" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qt9nr webserver-deployment-595b5b9587- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-595b5b9587-qt9nr 96726fa0-7491-4863-a05d-122949733d8d 17427 0 2019-12-14 09:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 79739f66-3e1d-415b-8ca7-7a9a977d8b4d 0xc00432d680 0xc00432d681}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.5,PodIP:10.33.1.222,StartTime:2019-12-14 09:25:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-14 09:25:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://cb6c6e2cafbcba85f52591826de7421f40278c8a67b7fc0ed3e75e0fcd864907,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.33.1.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.750: INFO: Pod "webserver-deployment-595b5b9587-r879q" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-r879q webserver-deployment-595b5b9587- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-595b5b9587-r879q a1443c9f-56c1-4b36-9397-e9b9bfc0cb11 17450 0 2019-12-14 09:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 79739f66-3e1d-415b-8ca7-7a9a977d8b4d 0xc00432d7f0 0xc00432d7f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.5,PodIP:10.33.1.223,StartTime:2019-12-14 09:25:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-14 09:25:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://577f8bcd3ce6473515a159b084717534985cc002c996041e887559531646ddae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.33.1.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.750: INFO: Pod "webserver-deployment-595b5b9587-rmx7p" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rmx7p webserver-deployment-595b5b9587- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-595b5b9587-rmx7p 89840fd6-9cc4-414b-868a-83e18e2bff32 17574 0 2019-12-14 09:25:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 79739f66-3e1d-415b-8ca7-7a9a977d8b4d 0xc00432d960 0xc00432d961}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.750: INFO: Pod "webserver-deployment-595b5b9587-rvh8v" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rvh8v webserver-deployment-595b5b9587- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-595b5b9587-rvh8v 2dbe5d46-ff94-498e-b7ba-092d3f5ef832 17462 0 2019-12-14 09:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 79739f66-3e1d-415b-8ca7-7a9a977d8b4d 0xc00432da70 0xc00432da71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.6,PodIP:10.33.2.57,StartTime:2019-12-14 09:25:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-14 09:25:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://3c7706ce700a8e23d494e45fca203502665502df991d9133cec25baf364efb63,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.33.2.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.750: INFO: Pod "webserver-deployment-595b5b9587-sglhc" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-sglhc webserver-deployment-595b5b9587- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-595b5b9587-sglhc 3480a946-ce9b-44d3-aec9-d3e3815905fc 17468 0 2019-12-14 09:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 79739f66-3e1d-415b-8ca7-7a9a977d8b4d 0xc00432dbe0 0xc00432dbe1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.6,PodIP:10.33.2.58,StartTime:2019-12-14 09:25:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-14 09:25:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://c8fff9838077011e00700b23b5eed3551163982a329324add62e58858b340b73,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.33.2.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.750: INFO: Pod "webserver-deployment-595b5b9587-w998f" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-w998f webserver-deployment-595b5b9587- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-595b5b9587-w998f ac69af33-8f23-4452-a236-584dae23b71b 17444 0 2019-12-14 09:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 79739f66-3e1d-415b-8ca7-7a9a977d8b4d 0xc00432dd50 0xc00432dd51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.5,PodIP:10.33.1.225,StartTime:2019-12-14 09:25:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-14 09:25:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://84340632f266663edc925b7c31900c122cf7a77d26dab095a89f787c7597d8e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.33.1.225,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.751: INFO: Pod "webserver-deployment-c7997dcc8-fljw9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-fljw9 webserver-deployment-c7997dcc8- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-c7997dcc8-fljw9 e2d0f764-96b9-4145-a587-c6947fdecbac 17551 0 2019-12-14 09:25:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 26665b32-df56-4e04-92ca-16a29512be10 0xc00432ded0 0xc00432ded1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.6,PodIP:10.33.2.61,StartTime:2019-12-14 09:25:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.33.2.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.751: INFO: Pod "webserver-deployment-c7997dcc8-nrdtx" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nrdtx webserver-deployment-c7997dcc8- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-c7997dcc8-nrdtx ca12ea5b-dbc6-4291-8808-8712f0926c23 17495 0 2019-12-14 09:25:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 26665b32-df56-4e04-92ca-16a29512be10 0xc003728070 0xc003728071}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.5,PodIP:,StartTime:2019-12-14 09:25:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.751: INFO: Pod "webserver-deployment-c7997dcc8-sgfs8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-sgfs8 webserver-deployment-c7997dcc8- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-c7997dcc8-sgfs8 c08a8c90-9969-4962-9d92-280264d35f88 17520 0 2019-12-14 09:25:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 26665b32-df56-4e04-92ca-16a29512be10 0xc0037281e0 0xc0037281e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.6,PodIP:,StartTime:2019-12-14 09:25:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.751: INFO: Pod "webserver-deployment-c7997dcc8-wsp6b" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wsp6b webserver-deployment-c7997dcc8- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-c7997dcc8-wsp6b 53a6941d-e09e-4392-8af3-f7a5a99bb028 17525 0 2019-12-14 09:25:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 26665b32-df56-4e04-92ca-16a29512be10 0xc003728350 0xc003728351}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.5,PodIP:,StartTime:2019-12-14 09:25:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.751: INFO: Pod "webserver-deployment-c7997dcc8-zpx7w" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zpx7w webserver-deployment-c7997dcc8- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-c7997dcc8-zpx7w df51553b-a70c-43a3-ac94-89c2e92a2746 17529 0 2019-12-14 09:25:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 26665b32-df56-4e04-92ca-16a29512be10 0xc0037284c0 0xc0037284c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.5,PodIP:,StartTime:2019-12-14 09:25:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:25:23.751: INFO: Pod "webserver-deployment-c7997dcc8-zskz5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zskz5 webserver-deployment-c7997dcc8- deployment-9698 /api/v1/namespaces/deployment-9698/pods/webserver-deployment-c7997dcc8-zskz5 96f436ec-a8f9-4545-81f6-f02814b1bba9 17569 0 2019-12-14 09:25:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 26665b32-df56-4e04-92ca-16a29512be10 0xc003728630 0xc003728631}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ng9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ng9sc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ng9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:25:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:25:23.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9698" for this suite.

• [SLOW TEST:6.345 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":180,"skipped":2678,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:25:23.780: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8841
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:25:48.059: INFO: Container started at 2019-12-14 09:25:25 +0000 UTC, pod became ready at 2019-12-14 09:25:46 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:25:48.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8841" for this suite.

• [SLOW TEST:24.284 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":181,"skipped":2684,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:25:48.067: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8306
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-da8164d2-6cd3-4e96-a771-5b74eeed8669
STEP: Creating a pod to test consume secrets
Dec 14 09:25:48.262: INFO: Waiting up to 5m0s for pod "pod-secrets-5db3ea39-7fb9-4c58-9481-33225fb3c059" in namespace "secrets-8306" to be "success or failure"
Dec 14 09:25:48.274: INFO: Pod "pod-secrets-5db3ea39-7fb9-4c58-9481-33225fb3c059": Phase="Pending", Reason="", readiness=false. Elapsed: 11.812914ms
Dec 14 09:25:50.276: INFO: Pod "pod-secrets-5db3ea39-7fb9-4c58-9481-33225fb3c059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014502996s
STEP: Saw pod success
Dec 14 09:25:50.277: INFO: Pod "pod-secrets-5db3ea39-7fb9-4c58-9481-33225fb3c059" satisfied condition "success or failure"
Dec 14 09:25:50.278: INFO: Trying to get logs from node k8s-2 pod pod-secrets-5db3ea39-7fb9-4c58-9481-33225fb3c059 container secret-env-test: <nil>
STEP: delete the pod
Dec 14 09:25:50.291: INFO: Waiting for pod pod-secrets-5db3ea39-7fb9-4c58-9481-33225fb3c059 to disappear
Dec 14 09:25:50.293: INFO: Pod pod-secrets-5db3ea39-7fb9-4c58-9481-33225fb3c059 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:25:50.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8306" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":182,"skipped":2744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:25:50.300: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8618
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 09:25:52.448: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:25:52.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8618" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":183,"skipped":2771,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:25:52.466: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Dec 14 09:25:52.597: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 09:25:52.603: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 09:25:52.605: INFO: 
Logging pods the kubelet thinks is on node k8s-2 before test
Dec 14 09:25:52.609: INFO: traefik-ingress-controller-5bcw7 from kube-system started at 2019-12-14 08:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 09:25:52.609: INFO: 	Container traefik-ingress-lb ready: true, restart count 0
Dec 14 09:25:52.609: INFO: test-webserver-4474a70c-7c97-4329-99c8-53f513ef8ff0 from container-probe-8841 started at 2019-12-14 09:25:24 +0000 UTC (1 container statuses recorded)
Dec 14 09:25:52.609: INFO: 	Container test-webserver ready: true, restart count 0
Dec 14 09:25:52.610: INFO: kube-flannel-ds-amd64-282d7 from kube-system started at 2019-12-14 08:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 09:25:52.610: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 09:25:52.610: INFO: sonobuoy-systemd-logs-daemon-set-59631ba8c0bd4cad-xc9m6 from sonobuoy started at 2019-12-14 08:47:19 +0000 UTC (2 container statuses recorded)
Dec 14 09:25:52.610: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 09:25:52.610: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 09:25:52.610: INFO: 
Logging pods the kubelet thinks is on node k8s-3 before test
Dec 14 09:25:52.628: INFO: linkerd-controller-57d9d8f5dd-sm2ls from linkerd started at 2019-12-14 08:50:41 +0000 UTC (3 container statuses recorded)
Dec 14 09:25:52.628: INFO: 	Container destination ready: true, restart count 0
Dec 14 09:25:52.628: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:25:52.628: INFO: 	Container public-api ready: true, restart count 0
Dec 14 09:25:52.628: INFO: kubernetes-dashboard-bf855c94d-npnfm from kubernetes-dashboard started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:25:52.628: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 14 09:25:52.628: INFO: kube-flannel-ds-amd64-5d54w from kube-system started at 2019-12-14 08:38:20 +0000 UTC (1 container statuses recorded)
Dec 14 09:25:52.628: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 09:25:52.628: INFO: kubernetes-metrics-scraper-6b97c6d857-x7vmq from kubernetes-dashboard started at 2019-12-14 08:38:23 +0000 UTC (1 container statuses recorded)
Dec 14 09:25:52.628: INFO: 	Container kubernetes-metrics-scraper ready: true, restart count 0
Dec 14 09:25:52.628: INFO: linkerd-destination-77564695ff-fv2kq from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:25:52.628: INFO: 	Container destination ready: true, restart count 0
Dec 14 09:25:52.628: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:25:52.628: INFO: linkerd-prometheus-68779bb867-5d2ff from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:25:52.628: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:25:52.628: INFO: 	Container prometheus ready: true, restart count 0
Dec 14 09:25:52.629: INFO: linkerd-tap-85775cdf7f-qh42m from linkerd started at 2019-12-14 08:38:31 +0000 UTC (2 container statuses recorded)
Dec 14 09:25:52.629: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:25:52.629: INFO: 	Container tap ready: true, restart count 0
Dec 14 09:25:52.629: INFO: traefik-ingress-controller-5sddf from kube-system started at 2019-12-14 08:38:22 +0000 UTC (1 container statuses recorded)
Dec 14 09:25:52.629: INFO: 	Container traefik-ingress-lb ready: true, restart count 0
Dec 14 09:25:52.629: INFO: linkerd-proxy-injector-7cbf76d445-xnj8j from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:25:52.629: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:25:52.629: INFO: 	Container proxy-injector ready: true, restart count 0
Dec 14 09:25:52.629: INFO: linkerd-web-864c75894b-946dh from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:25:52.629: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:25:52.629: INFO: 	Container web ready: true, restart count 0
Dec 14 09:25:52.629: INFO: coredns-b7f8c8654-f8h4p from kube-system started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:25:52.629: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:25:52.629: INFO: coredns-b7f8c8654-cw42m from kube-system started at 2019-12-14 08:38:21 +0000 UTC (1 container statuses recorded)
Dec 14 09:25:52.629: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:25:52.629: INFO: sonobuoy from sonobuoy started at 2019-12-14 08:47:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:25:52.629: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 09:25:52.629: INFO: sonobuoy-systemd-logs-daemon-set-59631ba8c0bd4cad-k2bx9 from sonobuoy started at 2019-12-14 08:47:19 +0000 UTC (2 container statuses recorded)
Dec 14 09:25:52.629: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 09:25:52.629: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 09:25:52.629: INFO: linkerd-sp-validator-689c4bcc4d-8j98j from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:25:52.629: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:25:52.629: INFO: 	Container sp-validator ready: true, restart count 0
Dec 14 09:25:52.629: INFO: linkerd-grafana-79c6dd8fc-v6gln from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:25:52.629: INFO: 	Container grafana ready: true, restart count 0
Dec 14 09:25:52.629: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:25:52.629: INFO: tiller-deploy-5798768fb-f49sm from kube-system started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:25:52.629: INFO: 	Container tiller ready: true, restart count 0
Dec 14 09:25:52.629: INFO: linkerd-identity-d55b64556-7kjgh from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:25:52.629: INFO: 	Container identity ready: true, restart count 0
Dec 14 09:25:52.629: INFO: 	Container linkerd-proxy ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c11650fe-b8a6-43a8-b09a-9595c2b6fd86 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-c11650fe-b8a6-43a8-b09a-9595c2b6fd86 off the node k8s-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c11650fe-b8a6-43a8-b09a-9595c2b6fd86
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:26:02.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2774" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:10.270 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":184,"skipped":2830,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:26:02.736: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9326
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-54235114-c906-4c34-be88-1f456632e3ae
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:26:04.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9326" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":185,"skipped":2831,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:26:04.905: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3003
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Dec 14 09:26:05.050: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 09:26:05.057: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 09:26:05.059: INFO: 
Logging pods the kubelet thinks is on node k8s-2 before test
Dec 14 09:26:05.064: INFO: pod-configmaps-704c3729-1298-4cd0-9119-787817a1dca9 from configmap-9326 started at 2019-12-14 09:26:02 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:05.064: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
Dec 14 09:26:05.064: INFO: 	Container configmap-volume-data-test ready: true, restart count 0
Dec 14 09:26:05.064: INFO: pod3 from sched-pred-2774 started at 2019-12-14 09:26:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:05.064: INFO: 	Container pod3 ready: true, restart count 0
Dec 14 09:26:05.064: INFO: sonobuoy-systemd-logs-daemon-set-59631ba8c0bd4cad-xc9m6 from sonobuoy started at 2019-12-14 08:47:19 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:05.064: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 09:26:05.064: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 09:26:05.064: INFO: kube-flannel-ds-amd64-282d7 from kube-system started at 2019-12-14 08:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:05.064: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 09:26:05.064: INFO: pod1 from sched-pred-2774 started at 2019-12-14 09:25:56 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:05.064: INFO: 	Container pod1 ready: true, restart count 0
Dec 14 09:26:05.064: INFO: traefik-ingress-controller-5bcw7 from kube-system started at 2019-12-14 08:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:05.064: INFO: 	Container traefik-ingress-lb ready: true, restart count 0
Dec 14 09:26:05.064: INFO: pod2 from sched-pred-2774 started at 2019-12-14 09:25:58 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:05.064: INFO: 	Container pod2 ready: true, restart count 0
Dec 14 09:26:05.064: INFO: 
Logging pods the kubelet thinks is on node k8s-3 before test
Dec 14 09:26:05.076: INFO: linkerd-identity-d55b64556-7kjgh from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:05.076: INFO: 	Container identity ready: true, restart count 0
Dec 14 09:26:05.076: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:05.076: INFO: sonobuoy from sonobuoy started at 2019-12-14 08:47:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:05.076: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 09:26:05.076: INFO: sonobuoy-systemd-logs-daemon-set-59631ba8c0bd4cad-k2bx9 from sonobuoy started at 2019-12-14 08:47:19 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:05.076: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 09:26:05.076: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 09:26:05.076: INFO: linkerd-sp-validator-689c4bcc4d-8j98j from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:05.076: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:05.076: INFO: 	Container sp-validator ready: true, restart count 0
Dec 14 09:26:05.076: INFO: linkerd-grafana-79c6dd8fc-v6gln from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:05.076: INFO: 	Container grafana ready: true, restart count 0
Dec 14 09:26:05.076: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:05.076: INFO: tiller-deploy-5798768fb-f49sm from kube-system started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:05.076: INFO: 	Container tiller ready: true, restart count 0
Dec 14 09:26:05.076: INFO: kube-flannel-ds-amd64-5d54w from kube-system started at 2019-12-14 08:38:20 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:05.076: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 09:26:05.076: INFO: linkerd-controller-57d9d8f5dd-sm2ls from linkerd started at 2019-12-14 08:50:41 +0000 UTC (3 container statuses recorded)
Dec 14 09:26:05.076: INFO: 	Container destination ready: true, restart count 0
Dec 14 09:26:05.076: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:05.076: INFO: 	Container public-api ready: true, restart count 0
Dec 14 09:26:05.076: INFO: kubernetes-dashboard-bf855c94d-npnfm from kubernetes-dashboard started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:05.076: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 14 09:26:05.076: INFO: traefik-ingress-controller-5sddf from kube-system started at 2019-12-14 08:38:22 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:05.076: INFO: 	Container traefik-ingress-lb ready: true, restart count 0
Dec 14 09:26:05.076: INFO: kubernetes-metrics-scraper-6b97c6d857-x7vmq from kubernetes-dashboard started at 2019-12-14 08:38:23 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:05.076: INFO: 	Container kubernetes-metrics-scraper ready: true, restart count 0
Dec 14 09:26:05.076: INFO: linkerd-destination-77564695ff-fv2kq from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:05.076: INFO: 	Container destination ready: true, restart count 0
Dec 14 09:26:05.076: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:05.076: INFO: linkerd-prometheus-68779bb867-5d2ff from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:05.076: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:05.077: INFO: 	Container prometheus ready: true, restart count 0
Dec 14 09:26:05.077: INFO: linkerd-tap-85775cdf7f-qh42m from linkerd started at 2019-12-14 08:38:31 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:05.077: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:05.077: INFO: 	Container tap ready: true, restart count 0
Dec 14 09:26:05.077: INFO: coredns-b7f8c8654-cw42m from kube-system started at 2019-12-14 08:38:21 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:05.077: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:26:05.077: INFO: linkerd-proxy-injector-7cbf76d445-xnj8j from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:05.077: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:05.077: INFO: 	Container proxy-injector ready: true, restart count 0
Dec 14 09:26:05.077: INFO: linkerd-web-864c75894b-946dh from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:05.077: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:05.077: INFO: 	Container web ready: true, restart count 0
Dec 14 09:26:05.077: INFO: coredns-b7f8c8654-f8h4p from kube-system started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:05.077: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15e032a79f7a21b7], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:26:06.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3003" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":186,"skipped":2832,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:26:06.105: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6956
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-ee056d40-f614-441d-8b47-50aea1d62bf1
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:26:06.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6956" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":187,"skipped":2865,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:26:06.243: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7772
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7772.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7772.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7772.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7772.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 09:26:08.415: INFO: DNS probes using dns-test-4b9b7215-7f94-48ee-8607-6d487dbf3a10 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7772.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7772.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7772.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7772.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 09:26:10.489: INFO: File wheezy_udp@dns-test-service-3.dns-7772.svc.cluster.local from pod  dns-7772/dns-test-2d60bfb1-5ba0-4f94-8b5f-18480dcb1646 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 09:26:10.492: INFO: File jessie_udp@dns-test-service-3.dns-7772.svc.cluster.local from pod  dns-7772/dns-test-2d60bfb1-5ba0-4f94-8b5f-18480dcb1646 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 09:26:10.492: INFO: Lookups using dns-7772/dns-test-2d60bfb1-5ba0-4f94-8b5f-18480dcb1646 failed for: [wheezy_udp@dns-test-service-3.dns-7772.svc.cluster.local jessie_udp@dns-test-service-3.dns-7772.svc.cluster.local]

Dec 14 09:26:15.502: INFO: DNS probes using dns-test-2d60bfb1-5ba0-4f94-8b5f-18480dcb1646 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7772.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7772.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7772.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7772.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 09:26:17.643: INFO: DNS probes using dns-test-f3e1b079-b213-434d-94c0-21cf78a4d141 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:26:17.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7772" for this suite.

• [SLOW TEST:11.454 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":188,"skipped":2870,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:26:17.699: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8984
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8984.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8984.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8984.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8984.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8984.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8984.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 09:26:21.926: INFO: DNS probes using dns-8984/dns-test-632429d6-5d59-4b8d-9f73-466da20f7501 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:26:21.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8984" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":189,"skipped":2890,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:26:21.983: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4565
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Dec 14 09:26:22.134: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 09:26:22.141: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 09:26:22.143: INFO: 
Logging pods the kubelet thinks is on node k8s-2 before test
Dec 14 09:26:22.147: INFO: kube-flannel-ds-amd64-282d7 from kube-system started at 2019-12-14 08:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:22.147: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 09:26:22.147: INFO: sonobuoy-systemd-logs-daemon-set-59631ba8c0bd4cad-xc9m6 from sonobuoy started at 2019-12-14 08:47:19 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:22.147: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 09:26:22.147: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 09:26:22.147: INFO: traefik-ingress-controller-5bcw7 from kube-system started at 2019-12-14 08:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:22.147: INFO: 	Container traefik-ingress-lb ready: true, restart count 0
Dec 14 09:26:22.147: INFO: 
Logging pods the kubelet thinks is on node k8s-3 before test
Dec 14 09:26:22.159: INFO: coredns-b7f8c8654-cw42m from kube-system started at 2019-12-14 08:38:21 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:22.159: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:26:22.159: INFO: linkerd-proxy-injector-7cbf76d445-xnj8j from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:22.159: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:22.159: INFO: 	Container proxy-injector ready: true, restart count 0
Dec 14 09:26:22.159: INFO: linkerd-web-864c75894b-946dh from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:22.159: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:22.159: INFO: 	Container web ready: true, restart count 0
Dec 14 09:26:22.159: INFO: coredns-b7f8c8654-f8h4p from kube-system started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:22.159: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:26:22.159: INFO: linkerd-grafana-79c6dd8fc-v6gln from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:22.159: INFO: 	Container grafana ready: true, restart count 0
Dec 14 09:26:22.159: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:22.159: INFO: tiller-deploy-5798768fb-f49sm from kube-system started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:22.159: INFO: 	Container tiller ready: true, restart count 0
Dec 14 09:26:22.159: INFO: linkerd-identity-d55b64556-7kjgh from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:22.159: INFO: 	Container identity ready: true, restart count 0
Dec 14 09:26:22.159: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:22.159: INFO: sonobuoy from sonobuoy started at 2019-12-14 08:47:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:22.159: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 09:26:22.159: INFO: sonobuoy-systemd-logs-daemon-set-59631ba8c0bd4cad-k2bx9 from sonobuoy started at 2019-12-14 08:47:19 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:22.159: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 09:26:22.159: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 09:26:22.159: INFO: linkerd-sp-validator-689c4bcc4d-8j98j from linkerd started at 2019-12-14 08:50:41 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:22.159: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:22.159: INFO: 	Container sp-validator ready: true, restart count 0
Dec 14 09:26:22.159: INFO: kube-flannel-ds-amd64-5d54w from kube-system started at 2019-12-14 08:38:20 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:22.159: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 09:26:22.159: INFO: linkerd-controller-57d9d8f5dd-sm2ls from linkerd started at 2019-12-14 08:50:41 +0000 UTC (3 container statuses recorded)
Dec 14 09:26:22.159: INFO: 	Container destination ready: true, restart count 0
Dec 14 09:26:22.159: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:22.159: INFO: 	Container public-api ready: true, restart count 0
Dec 14 09:26:22.159: INFO: kubernetes-dashboard-bf855c94d-npnfm from kubernetes-dashboard started at 2019-12-14 08:50:41 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:22.160: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 14 09:26:22.160: INFO: linkerd-tap-85775cdf7f-qh42m from linkerd started at 2019-12-14 08:38:31 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:22.160: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:22.160: INFO: 	Container tap ready: true, restart count 0
Dec 14 09:26:22.160: INFO: traefik-ingress-controller-5sddf from kube-system started at 2019-12-14 08:38:22 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:22.160: INFO: 	Container traefik-ingress-lb ready: true, restart count 0
Dec 14 09:26:22.160: INFO: kubernetes-metrics-scraper-6b97c6d857-x7vmq from kubernetes-dashboard started at 2019-12-14 08:38:23 +0000 UTC (1 container statuses recorded)
Dec 14 09:26:22.160: INFO: 	Container kubernetes-metrics-scraper ready: true, restart count 0
Dec 14 09:26:22.160: INFO: linkerd-destination-77564695ff-fv2kq from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:22.160: INFO: 	Container destination ready: true, restart count 0
Dec 14 09:26:22.160: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:22.160: INFO: linkerd-prometheus-68779bb867-5d2ff from linkerd started at 2019-12-14 08:38:30 +0000 UTC (2 container statuses recorded)
Dec 14 09:26:22.160: INFO: 	Container linkerd-proxy ready: true, restart count 0
Dec 14 09:26:22.160: INFO: 	Container prometheus ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3e52701b-1a3d-4f32-8e74-6656c64f01be 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-3e52701b-1a3d-4f32-8e74-6656c64f01be off the node k8s-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3e52701b-1a3d-4f32-8e74-6656c64f01be
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:31:26.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4565" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:304.300 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":190,"skipped":2913,"failed":0}
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:31:26.283: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:31:26.424: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 14 09:31:26.432: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:26.435: INFO: Number of nodes with available pods: 0
Dec 14 09:31:26.435: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:31:27.438: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:27.440: INFO: Number of nodes with available pods: 1
Dec 14 09:31:27.440: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:31:28.439: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:28.441: INFO: Number of nodes with available pods: 2
Dec 14 09:31:28.442: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 14 09:31:28.466: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:28.466: INFO: Wrong image for pod: daemon-set-hb8wz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:28.474: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:29.477: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:29.477: INFO: Wrong image for pod: daemon-set-hb8wz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:29.480: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:30.480: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:30.480: INFO: Wrong image for pod: daemon-set-hb8wz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:30.493: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:31.482: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:31.482: INFO: Wrong image for pod: daemon-set-hb8wz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:31.482: INFO: Pod daemon-set-hb8wz is not available
Dec 14 09:31:31.486: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:32.477: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:32.477: INFO: Wrong image for pod: daemon-set-hb8wz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:32.477: INFO: Pod daemon-set-hb8wz is not available
Dec 14 09:31:32.484: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:33.476: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:33.476: INFO: Wrong image for pod: daemon-set-hb8wz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:33.476: INFO: Pod daemon-set-hb8wz is not available
Dec 14 09:31:33.482: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:34.476: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:34.476: INFO: Wrong image for pod: daemon-set-hb8wz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:34.476: INFO: Pod daemon-set-hb8wz is not available
Dec 14 09:31:34.479: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:35.476: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:35.477: INFO: Wrong image for pod: daemon-set-hb8wz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:35.477: INFO: Pod daemon-set-hb8wz is not available
Dec 14 09:31:35.479: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:36.476: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:36.476: INFO: Wrong image for pod: daemon-set-hb8wz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:36.477: INFO: Pod daemon-set-hb8wz is not available
Dec 14 09:31:36.479: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:37.477: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:37.477: INFO: Wrong image for pod: daemon-set-hb8wz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:37.477: INFO: Pod daemon-set-hb8wz is not available
Dec 14 09:31:37.479: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:38.476: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:38.476: INFO: Pod daemon-set-p4zvh is not available
Dec 14 09:31:38.478: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:39.476: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:39.476: INFO: Pod daemon-set-p4zvh is not available
Dec 14 09:31:39.479: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:40.476: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:40.477: INFO: Pod daemon-set-p4zvh is not available
Dec 14 09:31:40.479: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:41.476: INFO: Wrong image for pod: daemon-set-ggrxm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 14 09:31:41.476: INFO: Pod daemon-set-ggrxm is not available
Dec 14 09:31:41.479: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:42.476: INFO: Pod daemon-set-sgccq is not available
Dec 14 09:31:42.478: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 14 09:31:42.481: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:42.483: INFO: Number of nodes with available pods: 1
Dec 14 09:31:42.483: INFO: Node k8s-3 is running more than one daemon pod
Dec 14 09:31:43.486: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:31:43.488: INFO: Number of nodes with available pods: 2
Dec 14 09:31:43.488: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7766, will wait for the garbage collector to delete the pods
Dec 14 09:31:43.554: INFO: Deleting DaemonSet.extensions daemon-set took: 5.983738ms
Dec 14 09:31:44.554: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000238475s
Dec 14 09:31:58.356: INFO: Number of nodes with available pods: 0
Dec 14 09:31:58.356: INFO: Number of running nodes: 0, number of available pods: 0
Dec 14 09:31:58.357: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7766/daemonsets","resourceVersion":"19088"},"items":null}

Dec 14 09:31:58.359: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7766/pods","resourceVersion":"19088"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:31:58.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7766" for this suite.

• [SLOW TEST:32.086 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":191,"skipped":2913,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:31:58.370: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:31:59.139: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 09:32:01.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711912719, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711912719, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711912719, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711912719, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:32:04.152: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
Dec 14 09:32:06.338: INFO: Waiting for webhook configuration to be ready...
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:32:16.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6527" for this suite.
STEP: Destroying namespace "webhook-6527-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.173 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":192,"skipped":2926,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:32:16.543: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-546
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 14 09:32:19.733: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:32:19.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-546" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":193,"skipped":2927,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:32:19.768: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4193
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-f8360010-adb3-4766-aaac-faf812ac3692
STEP: Creating configMap with name cm-test-opt-upd-5292aa6e-e862-48d3-8703-2d5a72adca5a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f8360010-adb3-4766-aaac-faf812ac3692
STEP: Updating configmap cm-test-opt-upd-5292aa6e-e862-48d3-8703-2d5a72adca5a
STEP: Creating configMap with name cm-test-opt-create-25946599-c8a0-4fcc-8a20-4f285c0f0f14
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:32:23.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4193" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":194,"skipped":2928,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:32:23.976: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3902
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 09:32:24.115: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bfb1ce0a-0a0b-4a41-b4c8-ef497492bd47" in namespace "downward-api-3902" to be "success or failure"
Dec 14 09:32:24.121: INFO: Pod "downwardapi-volume-bfb1ce0a-0a0b-4a41-b4c8-ef497492bd47": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02643ms
Dec 14 09:32:26.123: INFO: Pod "downwardapi-volume-bfb1ce0a-0a0b-4a41-b4c8-ef497492bd47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008300351s
STEP: Saw pod success
Dec 14 09:32:26.124: INFO: Pod "downwardapi-volume-bfb1ce0a-0a0b-4a41-b4c8-ef497492bd47" satisfied condition "success or failure"
Dec 14 09:32:26.125: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-bfb1ce0a-0a0b-4a41-b4c8-ef497492bd47 container client-container: <nil>
STEP: delete the pod
Dec 14 09:32:26.137: INFO: Waiting for pod downwardapi-volume-bfb1ce0a-0a0b-4a41-b4c8-ef497492bd47 to disappear
Dec 14 09:32:26.141: INFO: Pod downwardapi-volume-bfb1ce0a-0a0b-4a41-b4c8-ef497492bd47 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:32:26.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3902" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":195,"skipped":2942,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:32:26.147: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:32:26.827: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:32:29.844: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
Dec 14 09:32:29.857: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 14 09:32:29.991: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:32:30.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8885" for this suite.
STEP: Destroying namespace "webhook-8885-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":196,"skipped":2956,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:32:30.102: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7145
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7145
I1214 09:32:30.352315      18 runners.go:189] Created replication controller with name: externalname-service, namespace: services-7145, replica count: 2
I1214 09:32:33.402639      18 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 09:32:33.402: INFO: Creating new exec pod
Dec 14 09:32:36.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=services-7145 execpod54jjb -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 14 09:32:36.642: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 09:32:36.642: INFO: stdout: ""
Dec 14 09:32:36.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=services-7145 execpod54jjb -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.198 80'
Dec 14 09:32:36.853: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.198 80\nConnection to 10.32.0.198 80 port [tcp/http] succeeded!\n"
Dec 14 09:32:36.853: INFO: stdout: ""
Dec 14 09:32:36.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=services-7145 execpod54jjb -- /bin/sh -x -c nc -zv -t -w 2 10.20.20.5 31055'
Dec 14 09:32:37.058: INFO: stderr: "+ nc -zv -t -w 2 10.20.20.5 31055\nConnection to 10.20.20.5 31055 port [tcp/31055] succeeded!\n"
Dec 14 09:32:37.058: INFO: stdout: ""
Dec 14 09:32:37.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=services-7145 execpod54jjb -- /bin/sh -x -c nc -zv -t -w 2 10.20.20.6 31055'
Dec 14 09:32:37.262: INFO: stderr: "+ nc -zv -t -w 2 10.20.20.6 31055\nConnection to 10.20.20.6 31055 port [tcp/31055] succeeded!\n"
Dec 14 09:32:37.262: INFO: stdout: ""
Dec 14 09:32:37.262: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:32:37.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7145" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.191 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":197,"skipped":2962,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:32:37.293: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-876
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Dec 14 09:32:37.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=kubectl-876 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec 14 09:32:38.657: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec 14 09:32:38.658: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:32:40.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-876" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":280,"completed":198,"skipped":2981,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:32:40.667: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4942
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:329
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Dec 14 09:32:40.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-4942'
Dec 14 09:32:41.060: INFO: stderr: ""
Dec 14 09:32:41.060: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 09:32:41.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4942'
Dec 14 09:32:41.200: INFO: stderr: ""
Dec 14 09:32:41.200: INFO: stdout: "update-demo-nautilus-2gl2j update-demo-nautilus-vmqw7 "
Dec 14 09:32:41.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-2gl2j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4942'
Dec 14 09:32:41.278: INFO: stderr: ""
Dec 14 09:32:41.278: INFO: stdout: ""
Dec 14 09:32:41.278: INFO: update-demo-nautilus-2gl2j is created but not running
Dec 14 09:32:46.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4942'
Dec 14 09:32:46.359: INFO: stderr: ""
Dec 14 09:32:46.359: INFO: stdout: "update-demo-nautilus-2gl2j update-demo-nautilus-vmqw7 "
Dec 14 09:32:46.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-2gl2j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4942'
Dec 14 09:32:46.437: INFO: stderr: ""
Dec 14 09:32:46.437: INFO: stdout: "true"
Dec 14 09:32:46.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-2gl2j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4942'
Dec 14 09:32:46.513: INFO: stderr: ""
Dec 14 09:32:46.514: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 09:32:46.514: INFO: validating pod update-demo-nautilus-2gl2j
Dec 14 09:32:46.518: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 09:32:46.518: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 09:32:46.518: INFO: update-demo-nautilus-2gl2j is verified up and running
Dec 14 09:32:46.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-vmqw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4942'
Dec 14 09:32:46.603: INFO: stderr: ""
Dec 14 09:32:46.603: INFO: stdout: "true"
Dec 14 09:32:46.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-vmqw7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4942'
Dec 14 09:32:46.686: INFO: stderr: ""
Dec 14 09:32:46.686: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 09:32:46.686: INFO: validating pod update-demo-nautilus-vmqw7
Dec 14 09:32:46.690: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 09:32:46.690: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 09:32:46.690: INFO: update-demo-nautilus-vmqw7 is verified up and running
STEP: scaling down the replication controller
Dec 14 09:32:46.692: INFO: scanned /root for discovery docs: <nil>
Dec 14 09:32:46.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4942'
Dec 14 09:32:47.808: INFO: stderr: ""
Dec 14 09:32:47.808: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 09:32:47.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4942'
Dec 14 09:32:47.898: INFO: stderr: ""
Dec 14 09:32:47.898: INFO: stdout: "update-demo-nautilus-2gl2j update-demo-nautilus-vmqw7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 14 09:32:52.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4942'
Dec 14 09:32:52.976: INFO: stderr: ""
Dec 14 09:32:52.976: INFO: stdout: "update-demo-nautilus-vmqw7 "
Dec 14 09:32:52.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-vmqw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4942'
Dec 14 09:32:53.056: INFO: stderr: ""
Dec 14 09:32:53.056: INFO: stdout: "true"
Dec 14 09:32:53.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-vmqw7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4942'
Dec 14 09:32:53.130: INFO: stderr: ""
Dec 14 09:32:53.130: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 09:32:53.130: INFO: validating pod update-demo-nautilus-vmqw7
Dec 14 09:32:53.133: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 09:32:53.133: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 09:32:53.133: INFO: update-demo-nautilus-vmqw7 is verified up and running
STEP: scaling up the replication controller
Dec 14 09:32:53.135: INFO: scanned /root for discovery docs: <nil>
Dec 14 09:32:53.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4942'
Dec 14 09:32:54.240: INFO: stderr: ""
Dec 14 09:32:54.240: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 09:32:54.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4942'
Dec 14 09:32:54.316: INFO: stderr: ""
Dec 14 09:32:54.316: INFO: stdout: "update-demo-nautilus-g5hsh update-demo-nautilus-vmqw7 "
Dec 14 09:32:54.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-g5hsh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4942'
Dec 14 09:32:54.389: INFO: stderr: ""
Dec 14 09:32:54.389: INFO: stdout: ""
Dec 14 09:32:54.389: INFO: update-demo-nautilus-g5hsh is created but not running
Dec 14 09:32:59.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4942'
Dec 14 09:32:59.471: INFO: stderr: ""
Dec 14 09:32:59.471: INFO: stdout: "update-demo-nautilus-g5hsh update-demo-nautilus-vmqw7 "
Dec 14 09:32:59.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-g5hsh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4942'
Dec 14 09:32:59.546: INFO: stderr: ""
Dec 14 09:32:59.546: INFO: stdout: "true"
Dec 14 09:32:59.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-g5hsh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4942'
Dec 14 09:32:59.623: INFO: stderr: ""
Dec 14 09:32:59.623: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 09:32:59.623: INFO: validating pod update-demo-nautilus-g5hsh
Dec 14 09:32:59.627: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 09:32:59.627: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 09:32:59.627: INFO: update-demo-nautilus-g5hsh is verified up and running
Dec 14 09:32:59.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-vmqw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4942'
Dec 14 09:32:59.702: INFO: stderr: ""
Dec 14 09:32:59.702: INFO: stdout: "true"
Dec 14 09:32:59.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-vmqw7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4942'
Dec 14 09:32:59.778: INFO: stderr: ""
Dec 14 09:32:59.778: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 09:32:59.778: INFO: validating pod update-demo-nautilus-vmqw7
Dec 14 09:32:59.781: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 09:32:59.781: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 09:32:59.781: INFO: update-demo-nautilus-vmqw7 is verified up and running
STEP: using delete to clean up resources
Dec 14 09:32:59.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete --grace-period=0 --force -f - --namespace=kubectl-4942'
Dec 14 09:32:59.858: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 09:32:59.858: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 14 09:32:59.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4942'
Dec 14 09:32:59.964: INFO: stderr: "No resources found in kubectl-4942 namespace.\n"
Dec 14 09:32:59.964: INFO: stdout: ""
Dec 14 09:32:59.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -l name=update-demo --namespace=kubectl-4942 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 09:33:00.046: INFO: stderr: ""
Dec 14 09:33:00.046: INFO: stdout: "update-demo-nautilus-g5hsh\nupdate-demo-nautilus-vmqw7\n"
Dec 14 09:33:00.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4942'
Dec 14 09:33:00.642: INFO: stderr: "No resources found in kubectl-4942 namespace.\n"
Dec 14 09:33:00.642: INFO: stdout: ""
Dec 14 09:33:00.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -l name=update-demo --namespace=kubectl-4942 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 09:33:00.722: INFO: stderr: ""
Dec 14 09:33:00.722: INFO: stdout: "update-demo-nautilus-g5hsh\nupdate-demo-nautilus-vmqw7\n"
Dec 14 09:33:01.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4942'
Dec 14 09:33:01.134: INFO: stderr: "No resources found in kubectl-4942 namespace.\n"
Dec 14 09:33:01.134: INFO: stdout: ""
Dec 14 09:33:01.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -l name=update-demo --namespace=kubectl-4942 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 09:33:01.216: INFO: stderr: ""
Dec 14 09:33:01.216: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:33:01.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4942" for this suite.

• [SLOW TEST:20.555 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:327
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":199,"skipped":2993,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:33:01.223: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6778
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 14 09:33:01.372: INFO: Waiting up to 5m0s for pod "pod-7494afa1-2941-4e13-9366-a8712b8aaba2" in namespace "emptydir-6778" to be "success or failure"
Dec 14 09:33:01.378: INFO: Pod "pod-7494afa1-2941-4e13-9366-a8712b8aaba2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.556074ms
Dec 14 09:33:03.380: INFO: Pod "pod-7494afa1-2941-4e13-9366-a8712b8aaba2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007931269s
STEP: Saw pod success
Dec 14 09:33:03.380: INFO: Pod "pod-7494afa1-2941-4e13-9366-a8712b8aaba2" satisfied condition "success or failure"
Dec 14 09:33:03.382: INFO: Trying to get logs from node k8s-2 pod pod-7494afa1-2941-4e13-9366-a8712b8aaba2 container test-container: <nil>
STEP: delete the pod
Dec 14 09:33:03.397: INFO: Waiting for pod pod-7494afa1-2941-4e13-9366-a8712b8aaba2 to disappear
Dec 14 09:33:03.399: INFO: Pod pod-7494afa1-2941-4e13-9366-a8712b8aaba2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:33:03.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6778" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":200,"skipped":2996,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:33:03.406: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-835
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:33:07.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-835" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":201,"skipped":3049,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:33:07.592: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4043
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:33:08.165: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 09:33:10.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711912788, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711912788, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711912788, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711912788, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:33:13.181: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
Dec 14 09:33:13.192: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 14 09:33:15.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 attach --namespace=webhook-4043 to-be-attached-pod -i -c=container1'
Dec 14 09:33:15.511: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:33:15.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4043" for this suite.
STEP: Destroying namespace "webhook-4043-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.986 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":202,"skipped":3060,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:33:15.579: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9611
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1214 09:33:55.768102      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 14 09:33:55.768: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:33:55.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9611" for this suite.

• [SLOW TEST:40.202 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":203,"skipped":3068,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:33:55.784: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5151
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-760e28ae-b480-4db6-bee6-991493f64438
STEP: Creating secret with name secret-projected-all-test-volume-c3f43156-9bdc-4ae1-99a8-4dbe214484ba
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 14 09:33:55.946: INFO: Waiting up to 5m0s for pod "projected-volume-1664f18d-8265-4342-be0a-87b9f477fe13" in namespace "projected-5151" to be "success or failure"
Dec 14 09:33:55.954: INFO: Pod "projected-volume-1664f18d-8265-4342-be0a-87b9f477fe13": Phase="Pending", Reason="", readiness=false. Elapsed: 7.341029ms
Dec 14 09:33:57.956: INFO: Pod "projected-volume-1664f18d-8265-4342-be0a-87b9f477fe13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009854078s
STEP: Saw pod success
Dec 14 09:33:57.956: INFO: Pod "projected-volume-1664f18d-8265-4342-be0a-87b9f477fe13" satisfied condition "success or failure"
Dec 14 09:33:57.958: INFO: Trying to get logs from node k8s-2 pod projected-volume-1664f18d-8265-4342-be0a-87b9f477fe13 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 14 09:33:57.970: INFO: Waiting for pod projected-volume-1664f18d-8265-4342-be0a-87b9f477fe13 to disappear
Dec 14 09:33:57.973: INFO: Pod projected-volume-1664f18d-8265-4342-be0a-87b9f477fe13 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:33:57.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5151" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":204,"skipped":3083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:33:57.980: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:329
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Dec 14 09:33:58.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-7799'
Dec 14 09:33:58.310: INFO: stderr: ""
Dec 14 09:33:58.310: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 09:33:58.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7799'
Dec 14 09:33:58.444: INFO: stderr: ""
Dec 14 09:33:58.444: INFO: stdout: "update-demo-nautilus-7dcpn update-demo-nautilus-krltd "
Dec 14 09:33:58.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-7dcpn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7799'
Dec 14 09:33:58.521: INFO: stderr: ""
Dec 14 09:33:58.521: INFO: stdout: ""
Dec 14 09:33:58.521: INFO: update-demo-nautilus-7dcpn is created but not running
Dec 14 09:34:03.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7799'
Dec 14 09:34:03.609: INFO: stderr: ""
Dec 14 09:34:03.609: INFO: stdout: "update-demo-nautilus-7dcpn update-demo-nautilus-krltd "
Dec 14 09:34:03.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-7dcpn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7799'
Dec 14 09:34:03.686: INFO: stderr: ""
Dec 14 09:34:03.686: INFO: stdout: "true"
Dec 14 09:34:03.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-7dcpn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7799'
Dec 14 09:34:03.762: INFO: stderr: ""
Dec 14 09:34:03.762: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 09:34:03.762: INFO: validating pod update-demo-nautilus-7dcpn
Dec 14 09:34:03.766: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 09:34:03.766: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 09:34:03.766: INFO: update-demo-nautilus-7dcpn is verified up and running
Dec 14 09:34:03.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-krltd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7799'
Dec 14 09:34:03.849: INFO: stderr: ""
Dec 14 09:34:03.849: INFO: stdout: "true"
Dec 14 09:34:03.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-nautilus-krltd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7799'
Dec 14 09:34:03.923: INFO: stderr: ""
Dec 14 09:34:03.923: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 09:34:03.923: INFO: validating pod update-demo-nautilus-krltd
Dec 14 09:34:03.927: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 09:34:03.927: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 09:34:03.927: INFO: update-demo-nautilus-krltd is verified up and running
STEP: rolling-update to new replication controller
Dec 14 09:34:03.929: INFO: scanned /root for discovery docs: <nil>
Dec 14 09:34:03.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7799'
Dec 14 09:34:26.301: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 14 09:34:26.301: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 09:34:26.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7799'
Dec 14 09:34:26.405: INFO: stderr: ""
Dec 14 09:34:26.405: INFO: stdout: "update-demo-kitten-9fr2x update-demo-kitten-g99kz "
Dec 14 09:34:26.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-kitten-9fr2x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7799'
Dec 14 09:34:26.485: INFO: stderr: ""
Dec 14 09:34:26.485: INFO: stdout: "true"
Dec 14 09:34:26.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-kitten-9fr2x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7799'
Dec 14 09:34:26.564: INFO: stderr: ""
Dec 14 09:34:26.564: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 14 09:34:26.564: INFO: validating pod update-demo-kitten-9fr2x
Dec 14 09:34:26.571: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 14 09:34:26.571: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 14 09:34:26.571: INFO: update-demo-kitten-9fr2x is verified up and running
Dec 14 09:34:26.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-kitten-g99kz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7799'
Dec 14 09:34:26.648: INFO: stderr: ""
Dec 14 09:34:26.648: INFO: stdout: "true"
Dec 14 09:34:26.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods update-demo-kitten-g99kz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7799'
Dec 14 09:34:26.734: INFO: stderr: ""
Dec 14 09:34:26.734: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 14 09:34:26.734: INFO: validating pod update-demo-kitten-g99kz
Dec 14 09:34:26.737: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 14 09:34:26.737: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 14 09:34:26.737: INFO: update-demo-kitten-g99kz is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:34:26.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7799" for this suite.

• [SLOW TEST:28.762 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:327
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":205,"skipped":3161,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:34:26.743: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:34:37.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4077" for this suite.

• [SLOW TEST:11.169 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":206,"skipped":3161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:34:37.913: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-cec6fe7b-27cf-40b6-be3f-063a612f4d2c in namespace container-probe-8169
Dec 14 09:34:40.062: INFO: Started pod liveness-cec6fe7b-27cf-40b6-be3f-063a612f4d2c in namespace container-probe-8169
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 09:34:40.065: INFO: Initial restart count of pod liveness-cec6fe7b-27cf-40b6-be3f-063a612f4d2c is 0
Dec 14 09:34:58.088: INFO: Restart count of pod container-probe-8169/liveness-cec6fe7b-27cf-40b6-be3f-063a612f4d2c is now 1 (18.023284628s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:34:58.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8169" for this suite.

• [SLOW TEST:20.193 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":207,"skipped":3187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:34:58.107: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2018
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:35:18.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2018" for this suite.

• [SLOW TEST:20.368 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":208,"skipped":3209,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:35:18.476: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec 14 09:35:20.637: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-014205462 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 14 09:35:30.710: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:35:30.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9755" for this suite.

• [SLOW TEST:12.242 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":209,"skipped":3213,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:35:30.719: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 14 09:35:33.377: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7789 pod-service-account-7008dbc5-e048-420c-8d79-0504f75553dc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 14 09:35:33.596: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7789 pod-service-account-7008dbc5-e048-420c-8d79-0504f75553dc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 14 09:35:33.823: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7789 pod-service-account-7008dbc5-e048-420c-8d79-0504f75553dc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:35:34.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7789" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":210,"skipped":3220,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:35:34.049: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9589
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-94843d78-a0c1-4924-82bd-4b804b40c7fb
STEP: Creating a pod to test consume secrets
Dec 14 09:35:34.201: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5ba105ab-a248-4ab4-b879-5038843f5459" in namespace "projected-9589" to be "success or failure"
Dec 14 09:35:34.207: INFO: Pod "pod-projected-secrets-5ba105ab-a248-4ab4-b879-5038843f5459": Phase="Pending", Reason="", readiness=false. Elapsed: 6.545629ms
Dec 14 09:35:36.210: INFO: Pod "pod-projected-secrets-5ba105ab-a248-4ab4-b879-5038843f5459": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009163548s
STEP: Saw pod success
Dec 14 09:35:36.210: INFO: Pod "pod-projected-secrets-5ba105ab-a248-4ab4-b879-5038843f5459" satisfied condition "success or failure"
Dec 14 09:35:36.212: INFO: Trying to get logs from node k8s-2 pod pod-projected-secrets-5ba105ab-a248-4ab4-b879-5038843f5459 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 09:35:36.225: INFO: Waiting for pod pod-projected-secrets-5ba105ab-a248-4ab4-b879-5038843f5459 to disappear
Dec 14 09:35:36.228: INFO: Pod pod-projected-secrets-5ba105ab-a248-4ab4-b879-5038843f5459 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:35:36.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9589" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":211,"skipped":3220,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:35:36.236: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-1192
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1192
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1192
Dec 14 09:35:36.384: INFO: Found 0 stateful pods, waiting for 1
Dec 14 09:35:46.387: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 14 09:35:46.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-1192 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 09:35:46.630: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 09:35:46.630: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 09:35:46.630: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 09:35:46.633: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 14 09:35:56.645: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 09:35:56.646: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 09:35:56.690: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998951s
Dec 14 09:35:57.692: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995428226s
Dec 14 09:35:58.694: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993289062s
Dec 14 09:35:59.697: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990805402s
Dec 14 09:36:00.699: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988289631s
Dec 14 09:36:01.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.985960165s
Dec 14 09:36:02.706: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.982117761s
Dec 14 09:36:03.708: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.979337139s
Dec 14 09:36:04.711: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.976895125s
Dec 14 09:36:05.713: INFO: Verifying statefulset ss doesn't scale past 1 for another 974.43716ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1192
Dec 14 09:36:06.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-1192 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 09:36:06.921: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 09:36:06.921: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 09:36:06.921: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 09:36:06.925: INFO: Found 1 stateful pods, waiting for 3
Dec 14 09:36:16.927: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 09:36:16.927: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 09:36:16.927: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 14 09:36:16.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-1192 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 09:36:17.144: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 09:36:17.144: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 09:36:17.144: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 09:36:17.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-1192 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 09:36:17.357: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 09:36:17.357: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 09:36:17.357: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 09:36:17.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-1192 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 09:36:17.561: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 09:36:17.561: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 09:36:17.561: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 09:36:17.561: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 09:36:17.565: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 14 09:36:27.569: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 09:36:27.569: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 09:36:27.569: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 09:36:27.581: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999166s
Dec 14 09:36:28.583: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995375708s
Dec 14 09:36:29.586: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992509814s
Dec 14 09:36:30.589: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989895631s
Dec 14 09:36:31.591: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987110847s
Dec 14 09:36:32.595: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984527578s
Dec 14 09:36:33.598: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980525695s
Dec 14 09:36:34.601: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.977573115s
Dec 14 09:36:35.604: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.974548988s
Dec 14 09:36:36.608: INFO: Verifying statefulset ss doesn't scale past 3 for another 971.711256ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1192
Dec 14 09:36:37.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-1192 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 09:36:37.858: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 09:36:37.858: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 09:36:37.858: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 09:36:37.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-1192 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 09:36:38.085: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 09:36:38.085: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 09:36:38.085: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 09:36:38.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=statefulset-1192 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 09:36:38.298: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 09:36:38.298: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 09:36:38.298: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 09:36:38.298: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Dec 14 09:37:08.331: INFO: Deleting all statefulset in ns statefulset-1192
Dec 14 09:37:08.333: INFO: Scaling statefulset ss to 0
Dec 14 09:37:08.338: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 09:37:08.340: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:37:08.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1192" for this suite.

• [SLOW TEST:92.122 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":212,"skipped":3248,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:37:08.360: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7919
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:37:08.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7919" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":213,"skipped":3284,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:37:08.512: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6824
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:37:08.654: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 14 09:37:13.658: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 14 09:37:13.658: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Dec 14 09:37:15.695: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6824 /apis/apps/v1/namespaces/deployment-6824/deployments/test-cleanup-deployment 51c89fb4-ee72-4980-a84e-f65bb41c2bf0 21230 1 2019-12-14 09:37:13 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001bb7478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-14 09:37:13 +0000 UTC,LastTransitionTime:2019-12-14 09:37:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-55ffc6b7b6" has successfully progressed.,LastUpdateTime:2019-12-14 09:37:15 +0000 UTC,LastTransitionTime:2019-12-14 09:37:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 09:37:15.697: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-6824 /apis/apps/v1/namespaces/deployment-6824/replicasets/test-cleanup-deployment-55ffc6b7b6 f7651c23-118b-4d06-9d9f-477a2e3b9042 21219 1 2019-12-14 09:37:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 51c89fb4-ee72-4980-a84e-f65bb41c2bf0 0xc001bb7867 0xc001bb7868}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001bb78d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 09:37:15.699: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-vjqwt" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-vjqwt test-cleanup-deployment-55ffc6b7b6- deployment-6824 /api/v1/namespaces/deployment-6824/pods/test-cleanup-deployment-55ffc6b7b6-vjqwt ab01f49a-b755-44ce-8c25-8799b154a03e 21218 0 2019-12-14 09:37:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 f7651c23-118b-4d06-9d9f-477a2e3b9042 0xc001bb7c87 0xc001bb7c88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pz848,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pz848,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pz848,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:37:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:37:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.5,PodIP:10.33.1.36,StartTime:2019-12-14 09:37:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-14 09:37:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://e27b1f9f82063e49672d5eed906a3563a6ae359e7bdc47e0ae92460e3abaad80,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.33.1.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:37:15.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6824" for this suite.

• [SLOW TEST:7.192 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":214,"skipped":3287,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:37:15.705: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:37:15.840: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-7c542dca-10b0-47bd-ba98-263f90f4a226" in namespace "security-context-test-705" to be "success or failure"
Dec 14 09:37:15.846: INFO: Pod "busybox-privileged-false-7c542dca-10b0-47bd-ba98-263f90f4a226": Phase="Pending", Reason="", readiness=false. Elapsed: 5.771455ms
Dec 14 09:37:17.848: INFO: Pod "busybox-privileged-false-7c542dca-10b0-47bd-ba98-263f90f4a226": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007666102s
Dec 14 09:37:17.848: INFO: Pod "busybox-privileged-false-7c542dca-10b0-47bd-ba98-263f90f4a226" satisfied condition "success or failure"
Dec 14 09:37:17.852: INFO: Got logs for pod "busybox-privileged-false-7c542dca-10b0-47bd-ba98-263f90f4a226": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:37:17.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-705" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":215,"skipped":3312,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:37:17.858: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6227
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-b25a25ef-7fb8-4701-809e-324724eb76a5
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-b25a25ef-7fb8-4701-809e-324724eb76a5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:37:22.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6227" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":216,"skipped":3318,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:37:22.046: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-311
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:37:26.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-311" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":217,"skipped":3335,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:37:26.378: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1249
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:37:27.579: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 09:37:29.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711913047, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711913047, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711913047, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711913047, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:37:32.604: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
Dec 14 09:37:32.623: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:37:32.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1249" for this suite.
STEP: Destroying namespace "webhook-1249-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.536 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":218,"skipped":3352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:37:32.915: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4660
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:37:33.854: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:37:36.865: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:37:36.878: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Dec 14 09:37:37.402: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:37:39.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4660" for this suite.
STEP: Destroying namespace "webhook-4660-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.345 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":219,"skipped":3375,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:37:39.260: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1319
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-d0226066-69cf-4aa9-8ffd-e55b68e8eb2c
STEP: Creating a pod to test consume secrets
Dec 14 09:37:39.429: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e39ce356-702d-4517-a055-39f43544a0c3" in namespace "projected-1319" to be "success or failure"
Dec 14 09:37:39.435: INFO: Pod "pod-projected-secrets-e39ce356-702d-4517-a055-39f43544a0c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.937666ms
Dec 14 09:37:41.438: INFO: Pod "pod-projected-secrets-e39ce356-702d-4517-a055-39f43544a0c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008307059s
STEP: Saw pod success
Dec 14 09:37:41.438: INFO: Pod "pod-projected-secrets-e39ce356-702d-4517-a055-39f43544a0c3" satisfied condition "success or failure"
Dec 14 09:37:41.439: INFO: Trying to get logs from node k8s-2 pod pod-projected-secrets-e39ce356-702d-4517-a055-39f43544a0c3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 09:37:41.459: INFO: Waiting for pod pod-projected-secrets-e39ce356-702d-4517-a055-39f43544a0c3 to disappear
Dec 14 09:37:41.463: INFO: Pod pod-projected-secrets-e39ce356-702d-4517-a055-39f43544a0c3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:37:41.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1319" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":220,"skipped":3387,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:37:41.469: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4995
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4995
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4995
STEP: creating replication controller externalsvc in namespace services-4995
I1214 09:37:41.637978      18 runners.go:189] Created replication controller with name: externalsvc, namespace: services-4995, replica count: 2
I1214 09:37:44.688453      18 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 14 09:37:44.706: INFO: Creating new exec pod
Dec 14 09:37:46.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 exec --namespace=services-4995 execpodng5zf -- /bin/sh -x -c nslookup nodeport-service'
Dec 14 09:37:46.945: INFO: stderr: "+ nslookup nodeport-service\n"
Dec 14 09:37:46.946: INFO: stdout: "Server:\t\t10.32.0.10\nAddress:\t10.32.0.10#53\n\nnodeport-service.services-4995.svc.cluster.local\tcanonical name = externalsvc.services-4995.svc.cluster.local.\nName:\texternalsvc.services-4995.svc.cluster.local\nAddress: 10.32.0.62\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4995, will wait for the garbage collector to delete the pods
Dec 14 09:37:47.010: INFO: Deleting ReplicationController externalsvc took: 4.433723ms
Dec 14 09:37:47.110: INFO: Terminating ReplicationController externalsvc pods took: 100.234158ms
Dec 14 09:37:58.230: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:37:58.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4995" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:16.783 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":221,"skipped":3402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:37:58.253: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5643
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-c3fe1040-7c59-4470-9b5c-31ec286332db
STEP: Creating a pod to test consume secrets
Dec 14 09:37:58.404: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0eb0b0d9-784d-4f45-b1cf-f14a48b03cd2" in namespace "projected-5643" to be "success or failure"
Dec 14 09:37:58.410: INFO: Pod "pod-projected-secrets-0eb0b0d9-784d-4f45-b1cf-f14a48b03cd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.723407ms
Dec 14 09:38:00.412: INFO: Pod "pod-projected-secrets-0eb0b0d9-784d-4f45-b1cf-f14a48b03cd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007922396s
STEP: Saw pod success
Dec 14 09:38:00.412: INFO: Pod "pod-projected-secrets-0eb0b0d9-784d-4f45-b1cf-f14a48b03cd2" satisfied condition "success or failure"
Dec 14 09:38:00.414: INFO: Trying to get logs from node k8s-2 pod pod-projected-secrets-0eb0b0d9-784d-4f45-b1cf-f14a48b03cd2 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 09:38:00.427: INFO: Waiting for pod pod-projected-secrets-0eb0b0d9-784d-4f45-b1cf-f14a48b03cd2 to disappear
Dec 14 09:38:00.428: INFO: Pod pod-projected-secrets-0eb0b0d9-784d-4f45-b1cf-f14a48b03cd2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:38:00.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5643" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":222,"skipped":3440,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:38:00.436: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-5486
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:38:00.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5486" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":223,"skipped":3447,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:38:00.577: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4049
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1576
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 14 09:38:00.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-4049'
Dec 14 09:38:00.794: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 14 09:38:00.794: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1582
Dec 14 09:38:02.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete deployment e2e-test-httpd-deployment --namespace=kubectl-4049'
Dec 14 09:38:02.901: INFO: stderr: ""
Dec 14 09:38:02.901: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:38:02.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4049" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":224,"skipped":3463,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:38:02.908: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8321
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 14 09:38:03.105: INFO: Waiting up to 5m0s for pod "pod-d7cdf696-aa65-4101-a78a-0bad679fa7a3" in namespace "emptydir-8321" to be "success or failure"
Dec 14 09:38:03.114: INFO: Pod "pod-d7cdf696-aa65-4101-a78a-0bad679fa7a3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.986196ms
Dec 14 09:38:05.116: INFO: Pod "pod-d7cdf696-aa65-4101-a78a-0bad679fa7a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011111542s
STEP: Saw pod success
Dec 14 09:38:05.116: INFO: Pod "pod-d7cdf696-aa65-4101-a78a-0bad679fa7a3" satisfied condition "success or failure"
Dec 14 09:38:05.118: INFO: Trying to get logs from node k8s-2 pod pod-d7cdf696-aa65-4101-a78a-0bad679fa7a3 container test-container: <nil>
STEP: delete the pod
Dec 14 09:38:05.131: INFO: Waiting for pod pod-d7cdf696-aa65-4101-a78a-0bad679fa7a3 to disappear
Dec 14 09:38:05.133: INFO: Pod pod-d7cdf696-aa65-4101-a78a-0bad679fa7a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:38:05.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8321" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":225,"skipped":3491,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:38:05.139: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8576, will wait for the garbage collector to delete the pods
Dec 14 09:38:07.341: INFO: Deleting Job.batch foo took: 15.984411ms
Dec 14 09:38:07.441: INFO: Terminating Job.batch foo pods took: 100.184458ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:38:48.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8576" for this suite.

• [SLOW TEST:43.113 seconds]
[sig-apps] Job
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":226,"skipped":3509,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:38:48.254: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9890
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:38:48.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-9890'
Dec 14 09:38:48.681: INFO: stderr: ""
Dec 14 09:38:48.681: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Dec 14 09:38:48.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-9890'
Dec 14 09:38:48.889: INFO: stderr: ""
Dec 14 09:38:48.889: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Dec 14 09:38:49.892: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:38:49.892: INFO: Found 1 / 1
Dec 14 09:38:49.892: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 14 09:38:49.894: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:38:49.894: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 09:38:49.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 describe pod agnhost-master-z8cbw --namespace=kubectl-9890'
Dec 14 09:38:49.984: INFO: stderr: ""
Dec 14 09:38:49.984: INFO: stdout: "Name:         agnhost-master-z8cbw\nNamespace:    kubectl-9890\nPriority:     0\nNode:         k8s-2/10.20.20.5\nStart Time:   Sat, 14 Dec 2019 09:38:48 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           10.33.1.50\nIPs:\n  IP:           10.33.1.50\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   containerd://051402f20afe21da8ddf8d3645d0de8c3bd20bfe6df42ed95bfc64d93de3745f\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 14 Dec 2019 09:38:49 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rwv2z (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-rwv2z:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rwv2z\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-9890/agnhost-master-z8cbw to k8s-2\n  Normal  Pulled     0s    kubelet, k8s-2     Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    0s    kubelet, k8s-2     Created container agnhost-master\n  Normal  Started    0s    kubelet, k8s-2     Started container agnhost-master\n"
Dec 14 09:38:49.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 describe rc agnhost-master --namespace=kubectl-9890'
Dec 14 09:38:50.076: INFO: stderr: ""
Dec 14 09:38:50.076: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-9890\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  InjectionSkipped  2s    linkerd-proxy-injector  Linkerd sidecar proxy injection skipped: neither the namespace nor the pod have the annotation \"linkerd.io/inject:enabled\"\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-master-z8cbw\n"
Dec 14 09:38:50.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 describe service agnhost-master --namespace=kubectl-9890'
Dec 14 09:38:50.159: INFO: stderr: ""
Dec 14 09:38:50.159: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-9890\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.32.0.233\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.33.1.50:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 14 09:38:50.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 describe node k8s-1'
Dec 14 09:38:50.279: INFO: stderr: ""
Dec 14 09:38:50.279: INFO: stdout: "Name:               k8s-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"c2:ce:b7:1a:b4:95\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.20.20.4\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 14 Dec 2019 08:37:04 +0000\nTaints:             node-role.kubernetes.io/master=true:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-1\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 14 Dec 2019 09:38:47 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 14 Dec 2019 09:38:06 +0000   Sat, 14 Dec 2019 08:36:58 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 14 Dec 2019 09:38:06 +0000   Sat, 14 Dec 2019 08:36:58 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 14 Dec 2019 09:38:06 +0000   Sat, 14 Dec 2019 08:36:58 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 14 Dec 2019 09:38:06 +0000   Sat, 14 Dec 2019 08:37:14 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.20.20.4\n  Hostname:    k8s-1\nCapacity:\n  cpu:                1\n  ephemeral-storage:  50633164Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3941368Ki\n  pods:               110\nAllocatable:\n  cpu:                1\n  ephemeral-storage:  46663523866\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3838968Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ace31c5368e84fcd954c2fee7d3d4ae5\n  System UUID:                ACE31C53-68E8-4FCD-954C-2FEE7D3D4AE5\n  Boot ID:                    0e61f066-f6cb-49fb-8151-63311b0b038f\n  Kernel Version:             4.15.0-38-generic\n  OS Image:                   Ubuntu 18.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.2.10\n  Kubelet Version:            v1.17.0\n  Kube-Proxy Version:         v1.17.0\nPodCIDR:                      10.33.0.0/24\nPodCIDRs:                     10.33.0.0/24\nNon-terminated Pods:          (3 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 kube-flannel-ds-amd64-hpqg7                                100m (10%)    100m (10%)  50Mi (1%)        50Mi (1%)      60m\n  sonobuoy                    sonobuoy-e2e-job-79255fb84e8440fd                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         51m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-59631ba8c0bd4cad-wl5r4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         51m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                100m (10%)  100m (10%)\n  memory             50Mi (1%)   50Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Dec 14 09:38:50.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 describe namespace kubectl-9890'
Dec 14 09:38:50.410: INFO: stderr: ""
Dec 14 09:38:50.410: INFO: stdout: "Name:         kubectl-9890\nLabels:       e2e-framework=kubectl\n              e2e-run=cceeb2be-c65b-4aa6-a86e-5ad15a0f559d\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:38:50.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9890" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":227,"skipped":3528,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:38:50.415: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2556
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-kvnh
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 09:38:50.557: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-kvnh" in namespace "subpath-2556" to be "success or failure"
Dec 14 09:38:50.564: INFO: Pod "pod-subpath-test-downwardapi-kvnh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370698ms
Dec 14 09:38:52.566: INFO: Pod "pod-subpath-test-downwardapi-kvnh": Phase="Running", Reason="", readiness=true. Elapsed: 2.008422817s
Dec 14 09:38:54.568: INFO: Pod "pod-subpath-test-downwardapi-kvnh": Phase="Running", Reason="", readiness=true. Elapsed: 4.010744123s
Dec 14 09:38:56.570: INFO: Pod "pod-subpath-test-downwardapi-kvnh": Phase="Running", Reason="", readiness=true. Elapsed: 6.013352177s
Dec 14 09:38:58.573: INFO: Pod "pod-subpath-test-downwardapi-kvnh": Phase="Running", Reason="", readiness=true. Elapsed: 8.015886684s
Dec 14 09:39:00.575: INFO: Pod "pod-subpath-test-downwardapi-kvnh": Phase="Running", Reason="", readiness=true. Elapsed: 10.018229802s
Dec 14 09:39:02.578: INFO: Pod "pod-subpath-test-downwardapi-kvnh": Phase="Running", Reason="", readiness=true. Elapsed: 12.021095543s
Dec 14 09:39:04.580: INFO: Pod "pod-subpath-test-downwardapi-kvnh": Phase="Running", Reason="", readiness=true. Elapsed: 14.023177945s
Dec 14 09:39:06.583: INFO: Pod "pod-subpath-test-downwardapi-kvnh": Phase="Running", Reason="", readiness=true. Elapsed: 16.025426028s
Dec 14 09:39:08.585: INFO: Pod "pod-subpath-test-downwardapi-kvnh": Phase="Running", Reason="", readiness=true. Elapsed: 18.02753798s
Dec 14 09:39:10.587: INFO: Pod "pod-subpath-test-downwardapi-kvnh": Phase="Running", Reason="", readiness=true. Elapsed: 20.030092422s
Dec 14 09:39:12.590: INFO: Pod "pod-subpath-test-downwardapi-kvnh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.032375081s
STEP: Saw pod success
Dec 14 09:39:12.590: INFO: Pod "pod-subpath-test-downwardapi-kvnh" satisfied condition "success or failure"
Dec 14 09:39:12.591: INFO: Trying to get logs from node k8s-2 pod pod-subpath-test-downwardapi-kvnh container test-container-subpath-downwardapi-kvnh: <nil>
STEP: delete the pod
Dec 14 09:39:12.604: INFO: Waiting for pod pod-subpath-test-downwardapi-kvnh to disappear
Dec 14 09:39:12.609: INFO: Pod pod-subpath-test-downwardapi-kvnh no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-kvnh
Dec 14 09:39:12.609: INFO: Deleting pod "pod-subpath-test-downwardapi-kvnh" in namespace "subpath-2556"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:39:12.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2556" for this suite.

• [SLOW TEST:22.202 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":228,"skipped":3609,"failed":0}
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:39:12.617: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3296
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-62749a22-1c1a-4a4d-80c8-15e9f92f3632
STEP: Creating a pod to test consume configMaps
Dec 14 09:39:12.762: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-776c1a25-ffd6-4245-844f-7598f5c6213c" in namespace "projected-3296" to be "success or failure"
Dec 14 09:39:12.773: INFO: Pod "pod-projected-configmaps-776c1a25-ffd6-4245-844f-7598f5c6213c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.052627ms
Dec 14 09:39:14.775: INFO: Pod "pod-projected-configmaps-776c1a25-ffd6-4245-844f-7598f5c6213c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013634722s
STEP: Saw pod success
Dec 14 09:39:14.775: INFO: Pod "pod-projected-configmaps-776c1a25-ffd6-4245-844f-7598f5c6213c" satisfied condition "success or failure"
Dec 14 09:39:14.777: INFO: Trying to get logs from node k8s-2 pod pod-projected-configmaps-776c1a25-ffd6-4245-844f-7598f5c6213c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 09:39:14.790: INFO: Waiting for pod pod-projected-configmaps-776c1a25-ffd6-4245-844f-7598f5c6213c to disappear
Dec 14 09:39:14.795: INFO: Pod pod-projected-configmaps-776c1a25-ffd6-4245-844f-7598f5c6213c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:39:14.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3296" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":229,"skipped":3609,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:39:14.802: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 09:39:14.945: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de70f6fb-da4c-454e-9a8f-30be8c445901" in namespace "projected-6883" to be "success or failure"
Dec 14 09:39:14.950: INFO: Pod "downwardapi-volume-de70f6fb-da4c-454e-9a8f-30be8c445901": Phase="Pending", Reason="", readiness=false. Elapsed: 5.630144ms
Dec 14 09:39:16.954: INFO: Pod "downwardapi-volume-de70f6fb-da4c-454e-9a8f-30be8c445901": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008969694s
STEP: Saw pod success
Dec 14 09:39:16.954: INFO: Pod "downwardapi-volume-de70f6fb-da4c-454e-9a8f-30be8c445901" satisfied condition "success or failure"
Dec 14 09:39:16.957: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-de70f6fb-da4c-454e-9a8f-30be8c445901 container client-container: <nil>
STEP: delete the pod
Dec 14 09:39:16.979: INFO: Waiting for pod downwardapi-volume-de70f6fb-da4c-454e-9a8f-30be8c445901 to disappear
Dec 14 09:39:16.983: INFO: Pod downwardapi-volume-de70f6fb-da4c-454e-9a8f-30be8c445901 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:39:16.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6883" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":230,"skipped":3630,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:39:16.993: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:39:17.144: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 14 09:39:17.153: INFO: Number of nodes with available pods: 0
Dec 14 09:39:17.153: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 14 09:39:17.171: INFO: Number of nodes with available pods: 0
Dec 14 09:39:17.171: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:39:18.174: INFO: Number of nodes with available pods: 0
Dec 14 09:39:18.174: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:39:19.174: INFO: Number of nodes with available pods: 1
Dec 14 09:39:19.174: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 14 09:39:19.186: INFO: Number of nodes with available pods: 1
Dec 14 09:39:19.186: INFO: Number of running nodes: 0, number of available pods: 1
Dec 14 09:39:20.189: INFO: Number of nodes with available pods: 0
Dec 14 09:39:20.189: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 14 09:39:20.197: INFO: Number of nodes with available pods: 0
Dec 14 09:39:20.197: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:39:21.200: INFO: Number of nodes with available pods: 0
Dec 14 09:39:21.200: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:39:22.200: INFO: Number of nodes with available pods: 0
Dec 14 09:39:22.200: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:39:23.200: INFO: Number of nodes with available pods: 0
Dec 14 09:39:23.200: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:39:24.200: INFO: Number of nodes with available pods: 1
Dec 14 09:39:24.200: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4566, will wait for the garbage collector to delete the pods
Dec 14 09:39:24.259: INFO: Deleting DaemonSet.extensions daemon-set took: 4.334541ms
Dec 14 09:39:24.360: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.164375ms
Dec 14 09:39:38.262: INFO: Number of nodes with available pods: 0
Dec 14 09:39:38.262: INFO: Number of running nodes: 0, number of available pods: 0
Dec 14 09:39:38.263: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4566/daemonsets","resourceVersion":"22345"},"items":null}

Dec 14 09:39:38.265: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4566/pods","resourceVersion":"22345"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:39:38.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4566" for this suite.

• [SLOW TEST:21.290 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":231,"skipped":3641,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:39:38.283: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1877
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 14 09:39:38.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6541'
Dec 14 09:39:38.500: INFO: stderr: ""
Dec 14 09:39:38.500: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 14 09:39:43.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pod e2e-test-httpd-pod --namespace=kubectl-6541 -o json'
Dec 14 09:39:43.627: INFO: stderr: ""
Dec 14 09:39:43.627: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-12-14T09:39:38Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6541\",\n        \"resourceVersion\": \"22361\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6541/pods/e2e-test-httpd-pod\",\n        \"uid\": \"e5ffb3c8-9e59-486f-9801-94f37aede464\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-xk7ln\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-xk7ln\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-xk7ln\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-14T09:39:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-14T09:39:39Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-14T09:39:39Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-14T09:39:38Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://b4b8440f6784b028de445baa5851b845a0e01b97f5c5d8cc3c007e682d6652af\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-14T09:39:39Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.20.20.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.33.1.56\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.33.1.56\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-14T09:39:38Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 14 09:39:43.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 replace -f - --namespace=kubectl-6541'
Dec 14 09:39:43.800: INFO: stderr: ""
Dec 14 09:39:43.800: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1882
Dec 14 09:39:43.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete pods e2e-test-httpd-pod --namespace=kubectl-6541'
Dec 14 09:39:48.189: INFO: stderr: ""
Dec 14 09:39:48.190: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:39:48.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6541" for this suite.

• [SLOW TEST:9.924 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1873
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":232,"skipped":3642,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:39:48.210: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9743
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 09:39:49.076: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 09:39:52.093: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:39:52.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9743" for this suite.
STEP: Destroying namespace "webhook-9743-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":233,"skipped":3654,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:39:52.267: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1198
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 14 09:39:54.935: INFO: Successfully updated pod "pod-update-0a244a57-998d-44da-a009-27d76c197521"
STEP: verifying the updated pod is in kubernetes
Dec 14 09:39:54.939: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:39:54.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1198" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":234,"skipped":3674,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:39:54.945: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-44684a26-d6d1-4b7d-908e-b71d9ccbaae5
STEP: Creating a pod to test consume secrets
Dec 14 09:39:55.088: INFO: Waiting up to 5m0s for pod "pod-secrets-dd4d7766-260d-4dc0-9999-4d96e9488b12" in namespace "secrets-1322" to be "success or failure"
Dec 14 09:39:55.100: INFO: Pod "pod-secrets-dd4d7766-260d-4dc0-9999-4d96e9488b12": Phase="Pending", Reason="", readiness=false. Elapsed: 12.226155ms
Dec 14 09:39:57.102: INFO: Pod "pod-secrets-dd4d7766-260d-4dc0-9999-4d96e9488b12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014451704s
Dec 14 09:39:59.105: INFO: Pod "pod-secrets-dd4d7766-260d-4dc0-9999-4d96e9488b12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016728133s
STEP: Saw pod success
Dec 14 09:39:59.105: INFO: Pod "pod-secrets-dd4d7766-260d-4dc0-9999-4d96e9488b12" satisfied condition "success or failure"
Dec 14 09:39:59.106: INFO: Trying to get logs from node k8s-2 pod pod-secrets-dd4d7766-260d-4dc0-9999-4d96e9488b12 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 09:39:59.118: INFO: Waiting for pod pod-secrets-dd4d7766-260d-4dc0-9999-4d96e9488b12 to disappear
Dec 14 09:39:59.120: INFO: Pod pod-secrets-dd4d7766-260d-4dc0-9999-4d96e9488b12 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:39:59.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1322" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":235,"skipped":3683,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:39:59.128: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3701
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3701.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3701.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3701.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3701.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3701.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3701.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3701.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3701.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3701.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3701.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 09:40:01.300: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local from pod dns-3701/dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0: the server could not find the requested resource (get pods dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0)
Dec 14 09:40:01.302: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local from pod dns-3701/dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0: the server could not find the requested resource (get pods dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0)
Dec 14 09:40:01.304: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3701.svc.cluster.local from pod dns-3701/dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0: the server could not find the requested resource (get pods dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0)
Dec 14 09:40:01.307: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3701.svc.cluster.local from pod dns-3701/dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0: the server could not find the requested resource (get pods dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0)
Dec 14 09:40:01.315: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local from pod dns-3701/dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0: the server could not find the requested resource (get pods dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0)
Dec 14 09:40:01.317: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local from pod dns-3701/dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0: the server could not find the requested resource (get pods dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0)
Dec 14 09:40:01.319: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3701.svc.cluster.local from pod dns-3701/dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0: the server could not find the requested resource (get pods dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0)
Dec 14 09:40:01.321: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3701.svc.cluster.local from pod dns-3701/dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0: the server could not find the requested resource (get pods dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0)
Dec 14 09:40:01.326: INFO: Lookups using dns-3701/dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3701.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3701.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3701.svc.cluster.local jessie_udp@dns-test-service-2.dns-3701.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3701.svc.cluster.local]

Dec 14 09:40:06.353: INFO: DNS probes using dns-3701/dns-test-906a5db9-683f-498f-a3de-ff2878d4e4d0 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:40:06.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3701" for this suite.

• [SLOW TEST:7.269 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":236,"skipped":3713,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:40:06.397: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1215
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 14 09:40:06.550: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:06.556: INFO: Number of nodes with available pods: 0
Dec 14 09:40:06.556: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:40:07.700: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:07.819: INFO: Number of nodes with available pods: 1
Dec 14 09:40:07.819: INFO: Node k8s-3 is running more than one daemon pod
Dec 14 09:40:08.559: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:08.561: INFO: Number of nodes with available pods: 2
Dec 14 09:40:08.561: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 14 09:40:08.572: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:08.573: INFO: Number of nodes with available pods: 1
Dec 14 09:40:08.573: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:40:09.577: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:09.579: INFO: Number of nodes with available pods: 1
Dec 14 09:40:09.579: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:40:10.576: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:10.578: INFO: Number of nodes with available pods: 1
Dec 14 09:40:10.579: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:40:11.579: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:11.587: INFO: Number of nodes with available pods: 1
Dec 14 09:40:11.588: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:40:12.576: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:12.578: INFO: Number of nodes with available pods: 1
Dec 14 09:40:12.578: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:40:13.576: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:13.578: INFO: Number of nodes with available pods: 1
Dec 14 09:40:13.578: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:40:14.576: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:14.578: INFO: Number of nodes with available pods: 1
Dec 14 09:40:14.578: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:40:15.576: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:15.578: INFO: Number of nodes with available pods: 1
Dec 14 09:40:15.578: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:40:16.576: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:16.579: INFO: Number of nodes with available pods: 1
Dec 14 09:40:16.579: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:40:17.576: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:17.578: INFO: Number of nodes with available pods: 1
Dec 14 09:40:17.578: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:40:18.576: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:18.578: INFO: Number of nodes with available pods: 1
Dec 14 09:40:18.578: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:40:19.576: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:40:19.579: INFO: Number of nodes with available pods: 2
Dec 14 09:40:19.579: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1215, will wait for the garbage collector to delete the pods
Dec 14 09:40:19.638: INFO: Deleting DaemonSet.extensions daemon-set took: 6.175036ms
Dec 14 09:40:20.639: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.00020635s
Dec 14 09:40:28.341: INFO: Number of nodes with available pods: 0
Dec 14 09:40:28.341: INFO: Number of running nodes: 0, number of available pods: 0
Dec 14 09:40:28.342: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1215/daemonsets","resourceVersion":"22754"},"items":null}

Dec 14 09:40:28.345: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1215/pods","resourceVersion":"22754"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:40:28.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1215" for this suite.

• [SLOW TEST:21.978 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":237,"skipped":3714,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:40:28.377: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1708
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Dec 14 09:40:31.046: INFO: Successfully updated pod "labelsupdateceb566a3-8ac3-4317-a918-1874ea167dd2"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:40:35.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1708" for this suite.

• [SLOW TEST:6.691 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":238,"skipped":3757,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:40:35.068: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-42dc5c2b-8658-4f58-92be-f51648553689 in namespace container-probe-9611
Dec 14 09:40:37.217: INFO: Started pod liveness-42dc5c2b-8658-4f58-92be-f51648553689 in namespace container-probe-9611
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 09:40:37.219: INFO: Initial restart count of pod liveness-42dc5c2b-8658-4f58-92be-f51648553689 is 0
Dec 14 09:40:51.240: INFO: Restart count of pod container-probe-9611/liveness-42dc5c2b-8658-4f58-92be-f51648553689 is now 1 (14.020690412s elapsed)
Dec 14 09:41:11.260: INFO: Restart count of pod container-probe-9611/liveness-42dc5c2b-8658-4f58-92be-f51648553689 is now 2 (34.040888386s elapsed)
Dec 14 09:41:31.282: INFO: Restart count of pod container-probe-9611/liveness-42dc5c2b-8658-4f58-92be-f51648553689 is now 3 (54.063308966s elapsed)
Dec 14 09:41:51.310: INFO: Restart count of pod container-probe-9611/liveness-42dc5c2b-8658-4f58-92be-f51648553689 is now 4 (1m14.091237095s elapsed)
Dec 14 09:43:03.396: INFO: Restart count of pod container-probe-9611/liveness-42dc5c2b-8658-4f58-92be-f51648553689 is now 5 (2m26.176744807s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:43:03.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9611" for this suite.

• [SLOW TEST:148.342 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":239,"skipped":3764,"failed":0}
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:43:03.411: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9074
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 14 09:43:05.581: INFO: &Pod{ObjectMeta:{send-events-84a5942c-8c75-459b-a56e-567466f58469  events-9074 /api/v1/namespaces/events-9074/pods/send-events-84a5942c-8c75-459b-a56e-567466f58469 0d4480dd-5923-41d7-a6af-3afca0f0eec6 23167 0 2019-12-14 09:43:03 +0000 UTC <nil> <nil> map[name:foo time:542916514] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vzrhg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vzrhg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vzrhg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:43:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:43:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:43:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:43:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.5,PodIP:10.33.1.66,StartTime:2019-12-14 09:43:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-14 09:43:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://95b578f2815020bb6f0041717f2d623e676583d86cc44d3e416c597e74bc6e74,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.33.1.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec 14 09:43:07.583: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 14 09:43:09.586: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:43:09.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9074" for this suite.

• [SLOW TEST:6.197 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":240,"skipped":3765,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:43:09.609: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5128
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 14 09:43:09.737: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 14 09:43:24.108: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:43:28.986: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:43:46.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5128" for this suite.

• [SLOW TEST:36.844 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":241,"skipped":3768,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:43:46.453: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Dec 14 09:43:46.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-646'
Dec 14 09:43:46.946: INFO: stderr: ""
Dec 14 09:43:46.946: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Dec 14 09:43:47.950: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:43:47.950: INFO: Found 1 / 1
Dec 14 09:43:47.950: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 14 09:43:47.952: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:43:47.952: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 09:43:47.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 patch pod agnhost-master-cfmmf --namespace=kubectl-646 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 14 09:43:48.045: INFO: stderr: ""
Dec 14 09:43:48.046: INFO: stdout: "pod/agnhost-master-cfmmf patched\n"
STEP: checking annotations
Dec 14 09:43:48.050: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:43:48.050: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:43:48.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-646" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":242,"skipped":3775,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:43:48.056: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:43:48.198: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c896996b-e67f-4676-8f94-afb06baff484" in namespace "security-context-test-804" to be "success or failure"
Dec 14 09:43:48.204: INFO: Pod "alpine-nnp-false-c896996b-e67f-4676-8f94-afb06baff484": Phase="Pending", Reason="", readiness=false. Elapsed: 5.702323ms
Dec 14 09:43:50.206: INFO: Pod "alpine-nnp-false-c896996b-e67f-4676-8f94-afb06baff484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007703227s
Dec 14 09:43:52.209: INFO: Pod "alpine-nnp-false-c896996b-e67f-4676-8f94-afb06baff484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010231822s
Dec 14 09:43:52.209: INFO: Pod "alpine-nnp-false-c896996b-e67f-4676-8f94-afb06baff484" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:43:52.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-804" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":243,"skipped":3787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:43:52.221: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5197
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Dec 14 09:43:52.411: INFO: Waiting up to 5m0s for pod "downward-api-b58c227f-9e1b-49e7-866d-6fa543549990" in namespace "downward-api-5197" to be "success or failure"
Dec 14 09:43:52.420: INFO: Pod "downward-api-b58c227f-9e1b-49e7-866d-6fa543549990": Phase="Pending", Reason="", readiness=false. Elapsed: 9.40119ms
Dec 14 09:43:54.422: INFO: Pod "downward-api-b58c227f-9e1b-49e7-866d-6fa543549990": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011301447s
STEP: Saw pod success
Dec 14 09:43:54.422: INFO: Pod "downward-api-b58c227f-9e1b-49e7-866d-6fa543549990" satisfied condition "success or failure"
Dec 14 09:43:54.424: INFO: Trying to get logs from node k8s-2 pod downward-api-b58c227f-9e1b-49e7-866d-6fa543549990 container dapi-container: <nil>
STEP: delete the pod
Dec 14 09:43:54.438: INFO: Waiting for pod downward-api-b58c227f-9e1b-49e7-866d-6fa543549990 to disappear
Dec 14 09:43:54.439: INFO: Pod downward-api-b58c227f-9e1b-49e7-866d-6fa543549990 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:43:54.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5197" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":244,"skipped":3827,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:43:54.445: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9364
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-5373b0c2-d8e3-4a0d-80c4-12c46bbc3be4
STEP: Creating a pod to test consume secrets
Dec 14 09:43:54.586: INFO: Waiting up to 5m0s for pod "pod-secrets-f7231e2d-7873-4414-a0b2-fcb1bd2195f9" in namespace "secrets-9364" to be "success or failure"
Dec 14 09:43:54.592: INFO: Pod "pod-secrets-f7231e2d-7873-4414-a0b2-fcb1bd2195f9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.095171ms
Dec 14 09:43:56.594: INFO: Pod "pod-secrets-f7231e2d-7873-4414-a0b2-fcb1bd2195f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008582596s
STEP: Saw pod success
Dec 14 09:43:56.595: INFO: Pod "pod-secrets-f7231e2d-7873-4414-a0b2-fcb1bd2195f9" satisfied condition "success or failure"
Dec 14 09:43:56.596: INFO: Trying to get logs from node k8s-2 pod pod-secrets-f7231e2d-7873-4414-a0b2-fcb1bd2195f9 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 09:43:56.610: INFO: Waiting for pod pod-secrets-f7231e2d-7873-4414-a0b2-fcb1bd2195f9 to disappear
Dec 14 09:43:56.611: INFO: Pod pod-secrets-f7231e2d-7873-4414-a0b2-fcb1bd2195f9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:43:56.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9364" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":245,"skipped":3833,"failed":0}
SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:43:56.617: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5031
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 14 09:43:56.763: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 14 09:44:01.766: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:44:02.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5031" for this suite.

• [SLOW TEST:6.169 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":246,"skipped":3836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:44:02.788: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7418
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:44:02.922: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:44:03.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7418" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":247,"skipped":3858,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:44:04.332: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8898
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:45:04.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8898" for this suite.

• [SLOW TEST:60.190 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":248,"skipped":3875,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:45:04.522: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1362
STEP: creating the pod
Dec 14 09:45:04.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 create -f - --namespace=kubectl-8246'
Dec 14 09:45:04.971: INFO: stderr: ""
Dec 14 09:45:04.971: INFO: stdout: "pod/pause created\n"
Dec 14 09:45:04.971: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 14 09:45:04.971: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8246" to be "running and ready"
Dec 14 09:45:04.977: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.196327ms
Dec 14 09:45:07.063: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.092310805s
Dec 14 09:45:07.063: INFO: Pod "pause" satisfied condition "running and ready"
Dec 14 09:45:07.063: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 14 09:45:07.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 label pods pause testing-label=testing-label-value --namespace=kubectl-8246'
Dec 14 09:45:07.391: INFO: stderr: ""
Dec 14 09:45:07.391: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 14 09:45:07.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pod pause -L testing-label --namespace=kubectl-8246'
Dec 14 09:45:07.598: INFO: stderr: ""
Dec 14 09:45:07.598: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 14 09:45:07.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 label pods pause testing-label- --namespace=kubectl-8246'
Dec 14 09:45:07.857: INFO: stderr: ""
Dec 14 09:45:07.857: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 14 09:45:07.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pod pause -L testing-label --namespace=kubectl-8246'
Dec 14 09:45:07.971: INFO: stderr: ""
Dec 14 09:45:07.971: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1369
STEP: using delete to clean up resources
Dec 14 09:45:07.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete --grace-period=0 --force -f - --namespace=kubectl-8246'
Dec 14 09:45:08.056: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 09:45:08.056: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 14 09:45:08.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get rc,svc -l name=pause --no-headers --namespace=kubectl-8246'
Dec 14 09:45:08.140: INFO: stderr: "No resources found in kubectl-8246 namespace.\n"
Dec 14 09:45:08.140: INFO: stdout: ""
Dec 14 09:45:08.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 get pods -l name=pause --namespace=kubectl-8246 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 09:45:08.214: INFO: stderr: ""
Dec 14 09:45:08.214: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:45:08.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8246" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":249,"skipped":3896,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:45:08.220: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4418
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-4418
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 09:45:08.350: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 14 09:45:30.436: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.33.1.75 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4418 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:45:30.436: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:45:31.562: INFO: Found all expected endpoints: [netserver-0]
Dec 14 09:45:31.565: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.33.2.74 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4418 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:45:31.566: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:45:32.695: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:45:32.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4418" for this suite.

• [SLOW TEST:24.482 seconds]
[sig-network] Networking
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":250,"skipped":3931,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:45:32.703: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:45:32.832: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 14 09:45:32.839: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 09:45:37.841: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 14 09:45:37.841: INFO: Creating deployment "test-rolling-update-deployment"
Dec 14 09:45:37.847: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 14 09:45:37.860: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 14 09:45:39.866: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 14 09:45:39.876: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711913537, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711913537, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711913537, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711913537, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 09:45:41.878: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Dec 14 09:45:41.884: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-416 /apis/apps/v1/namespaces/deployment-416/deployments/test-rolling-update-deployment 71c6184c-bdf9-4545-bb60-a7483d736a8d 23916 1 2019-12-14 09:45:37 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0024affd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-14 09:45:37 +0000 UTC,LastTransitionTime:2019-12-14 09:45:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2019-12-14 09:45:39 +0000 UTC,LastTransitionTime:2019-12-14 09:45:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 09:45:41.886: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-416 /apis/apps/v1/namespaces/deployment-416/replicasets/test-rolling-update-deployment-67cf4f6444 dac9b5bf-c4e4-446e-80cc-e747de28ce49 23905 1 2019-12-14 09:45:37 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 71c6184c-bdf9-4545-bb60-a7483d736a8d 0xc0063bc757 0xc0063bc758}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0063bc828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 09:45:41.886: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 14 09:45:41.886: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-416 /apis/apps/v1/namespaces/deployment-416/replicasets/test-rolling-update-controller 67646eba-c079-49cb-a456-2679af634461 23915 2 2019-12-14 09:45:32 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 71c6184c-bdf9-4545-bb60-a7483d736a8d 0xc0063bc3d7 0xc0063bc3d8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0063bc548 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 09:45:41.888: INFO: Pod "test-rolling-update-deployment-67cf4f6444-95wmb" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-95wmb test-rolling-update-deployment-67cf4f6444- deployment-416 /api/v1/namespaces/deployment-416/pods/test-rolling-update-deployment-67cf4f6444-95wmb f69a1565-3c6e-4f41-9e40-7452415d276f 23904 0 2019-12-14 09:45:37 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 dac9b5bf-c4e4-446e-80cc-e747de28ce49 0xc0063bd017 0xc0063bd018}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kdkh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kdkh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kdkh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:45:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:45:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:45:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-14 09:45:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.20.20.5,PodIP:10.33.1.79,StartTime:2019-12-14 09:45:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-14 09:45:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://bad63076d9a624537d30cbc13f87cb6a73ddc7cae2c27447ac08a769675eef54,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.33.1.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:45:41.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-416" for this suite.

• [SLOW TEST:9.191 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":251,"skipped":3951,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:45:41.896: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-9869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Dec 14 09:45:42.025: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 09:46:42.043: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:46:42.045: INFO: Starting informer...
STEP: Starting pod...
Dec 14 09:46:42.262: INFO: Pod is running on k8s-2. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec 14 09:46:42.283: INFO: Pod wasn't evicted. Proceeding
Dec 14 09:46:42.283: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec 14 09:47:57.333: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:47:57.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9869" for this suite.

• [SLOW TEST:135.445 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":252,"skipped":3998,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:47:57.341: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8845
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-f7mr
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 09:47:57.487: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-f7mr" in namespace "subpath-8845" to be "success or failure"
Dec 14 09:47:57.493: INFO: Pod "pod-subpath-test-projected-f7mr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028322ms
Dec 14 09:47:59.495: INFO: Pod "pod-subpath-test-projected-f7mr": Phase="Running", Reason="", readiness=true. Elapsed: 2.008390386s
Dec 14 09:48:01.497: INFO: Pod "pod-subpath-test-projected-f7mr": Phase="Running", Reason="", readiness=true. Elapsed: 4.010373392s
Dec 14 09:48:03.500: INFO: Pod "pod-subpath-test-projected-f7mr": Phase="Running", Reason="", readiness=true. Elapsed: 6.013073771s
Dec 14 09:48:05.502: INFO: Pod "pod-subpath-test-projected-f7mr": Phase="Running", Reason="", readiness=true. Elapsed: 8.015430453s
Dec 14 09:48:07.505: INFO: Pod "pod-subpath-test-projected-f7mr": Phase="Running", Reason="", readiness=true. Elapsed: 10.017929697s
Dec 14 09:48:09.507: INFO: Pod "pod-subpath-test-projected-f7mr": Phase="Running", Reason="", readiness=true. Elapsed: 12.019950661s
Dec 14 09:48:11.509: INFO: Pod "pod-subpath-test-projected-f7mr": Phase="Running", Reason="", readiness=true. Elapsed: 14.021722774s
Dec 14 09:48:13.511: INFO: Pod "pod-subpath-test-projected-f7mr": Phase="Running", Reason="", readiness=true. Elapsed: 16.023795698s
Dec 14 09:48:15.513: INFO: Pod "pod-subpath-test-projected-f7mr": Phase="Running", Reason="", readiness=true. Elapsed: 18.026061812s
Dec 14 09:48:17.515: INFO: Pod "pod-subpath-test-projected-f7mr": Phase="Running", Reason="", readiness=true. Elapsed: 20.028409897s
Dec 14 09:48:19.518: INFO: Pod "pod-subpath-test-projected-f7mr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.030859891s
STEP: Saw pod success
Dec 14 09:48:19.518: INFO: Pod "pod-subpath-test-projected-f7mr" satisfied condition "success or failure"
Dec 14 09:48:19.520: INFO: Trying to get logs from node k8s-2 pod pod-subpath-test-projected-f7mr container test-container-subpath-projected-f7mr: <nil>
STEP: delete the pod
Dec 14 09:48:19.532: INFO: Waiting for pod pod-subpath-test-projected-f7mr to disappear
Dec 14 09:48:19.535: INFO: Pod pod-subpath-test-projected-f7mr no longer exists
STEP: Deleting pod pod-subpath-test-projected-f7mr
Dec 14 09:48:19.535: INFO: Deleting pod "pod-subpath-test-projected-f7mr" in namespace "subpath-8845"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:48:19.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8845" for this suite.

• [SLOW TEST:22.201 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":253,"skipped":3998,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:48:19.543: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9683
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-9683
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 09:48:19.693: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 14 09:48:39.771: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.33.1.84:8080/dial?request=hostname&protocol=udp&host=10.33.1.83&port=8081&tries=1'] Namespace:pod-network-test-9683 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:48:39.771: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:48:39.904: INFO: Waiting for responses: map[]
Dec 14 09:48:39.907: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.33.1.84:8080/dial?request=hostname&protocol=udp&host=10.33.2.75&port=8081&tries=1'] Namespace:pod-network-test-9683 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:48:39.907: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:48:40.034: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:48:40.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9683" for this suite.

• [SLOW TEST:20.498 seconds]
[sig-network] Networking
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":254,"skipped":4036,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:48:40.042: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2422
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 09:48:40.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f487f5a4-b74f-48ec-a369-d53766cf09a2" in namespace "downward-api-2422" to be "success or failure"
Dec 14 09:48:40.190: INFO: Pod "downwardapi-volume-f487f5a4-b74f-48ec-a369-d53766cf09a2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.23581ms
Dec 14 09:48:42.192: INFO: Pod "downwardapi-volume-f487f5a4-b74f-48ec-a369-d53766cf09a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008935639s
STEP: Saw pod success
Dec 14 09:48:42.192: INFO: Pod "downwardapi-volume-f487f5a4-b74f-48ec-a369-d53766cf09a2" satisfied condition "success or failure"
Dec 14 09:48:42.194: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-f487f5a4-b74f-48ec-a369-d53766cf09a2 container client-container: <nil>
STEP: delete the pod
Dec 14 09:48:42.205: INFO: Waiting for pod downwardapi-volume-f487f5a4-b74f-48ec-a369-d53766cf09a2 to disappear
Dec 14 09:48:42.208: INFO: Pod downwardapi-volume-f487f5a4-b74f-48ec-a369-d53766cf09a2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:48:42.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2422" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":255,"skipped":4042,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:48:42.214: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-6368
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 09:48:42.343: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 14 09:49:06.417: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.33.1.87:8080/dial?request=hostname&protocol=http&host=10.33.1.86&port=8080&tries=1'] Namespace:pod-network-test-6368 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:49:06.417: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:49:06.544: INFO: Waiting for responses: map[]
Dec 14 09:49:06.548: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.33.1.87:8080/dial?request=hostname&protocol=http&host=10.33.2.76&port=8080&tries=1'] Namespace:pod-network-test-6368 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:49:06.548: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:49:06.672: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:49:06.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6368" for this suite.

• [SLOW TEST:24.465 seconds]
[sig-network] Networking
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":256,"skipped":4049,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:49:06.680: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5524
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Dec 14 09:49:06.814: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:49:31.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5524" for this suite.

• [SLOW TEST:24.503 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":257,"skipped":4093,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:49:31.185: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8324
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 14 09:49:31.345: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:49:31.349: INFO: Number of nodes with available pods: 0
Dec 14 09:49:31.349: INFO: Node k8s-2 is running more than one daemon pod
Dec 14 09:49:32.352: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:49:32.354: INFO: Number of nodes with available pods: 1
Dec 14 09:49:32.354: INFO: Node k8s-3 is running more than one daemon pod
Dec 14 09:49:33.352: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:49:33.353: INFO: Number of nodes with available pods: 1
Dec 14 09:49:33.353: INFO: Node k8s-3 is running more than one daemon pod
Dec 14 09:49:34.352: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:49:34.354: INFO: Number of nodes with available pods: 2
Dec 14 09:49:34.354: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 14 09:49:34.370: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:49:34.374: INFO: Number of nodes with available pods: 1
Dec 14 09:49:34.374: INFO: Node k8s-3 is running more than one daemon pod
Dec 14 09:49:35.377: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:49:35.381: INFO: Number of nodes with available pods: 1
Dec 14 09:49:35.381: INFO: Node k8s-3 is running more than one daemon pod
Dec 14 09:49:36.377: INFO: DaemonSet pods can't tolerate node k8s-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 14 09:49:36.379: INFO: Number of nodes with available pods: 2
Dec 14 09:49:36.379: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8324, will wait for the garbage collector to delete the pods
Dec 14 09:49:36.440: INFO: Deleting DaemonSet.extensions daemon-set took: 5.416686ms
Dec 14 09:49:37.440: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000211288s
Dec 14 09:49:48.342: INFO: Number of nodes with available pods: 0
Dec 14 09:49:48.342: INFO: Number of running nodes: 0, number of available pods: 0
Dec 14 09:49:48.344: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8324/daemonsets","resourceVersion":"24816"},"items":null}

Dec 14 09:49:48.345: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8324/pods","resourceVersion":"24816"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:49:48.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8324" for this suite.

• [SLOW TEST:17.171 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":258,"skipped":4120,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:49:48.356: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7624
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 14 09:49:48.546: INFO: Waiting up to 5m0s for pod "pod-e87391c4-a433-48fa-b63b-fe7c98e2afaa" in namespace "emptydir-7624" to be "success or failure"
Dec 14 09:49:48.556: INFO: Pod "pod-e87391c4-a433-48fa-b63b-fe7c98e2afaa": Phase="Pending", Reason="", readiness=false. Elapsed: 9.504739ms
Dec 14 09:49:50.558: INFO: Pod "pod-e87391c4-a433-48fa-b63b-fe7c98e2afaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012006649s
STEP: Saw pod success
Dec 14 09:49:50.558: INFO: Pod "pod-e87391c4-a433-48fa-b63b-fe7c98e2afaa" satisfied condition "success or failure"
Dec 14 09:49:50.560: INFO: Trying to get logs from node k8s-2 pod pod-e87391c4-a433-48fa-b63b-fe7c98e2afaa container test-container: <nil>
STEP: delete the pod
Dec 14 09:49:50.575: INFO: Waiting for pod pod-e87391c4-a433-48fa-b63b-fe7c98e2afaa to disappear
Dec 14 09:49:50.577: INFO: Pod pod-e87391c4-a433-48fa-b63b-fe7c98e2afaa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:49:50.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7624" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":259,"skipped":4128,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:49:50.583: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6454
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-13d5a6f1-5442-4a8e-a851-64c74a17dc88
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-13d5a6f1-5442-4a8e-a851-64c74a17dc88
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:49:54.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6454" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":260,"skipped":4134,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:49:54.809: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2475
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 14 09:49:54.949: INFO: Waiting up to 5m0s for pod "pod-44403692-74fb-4124-a168-5e2515fbc6eb" in namespace "emptydir-2475" to be "success or failure"
Dec 14 09:49:54.959: INFO: Pod "pod-44403692-74fb-4124-a168-5e2515fbc6eb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.65571ms
Dec 14 09:49:56.962: INFO: Pod "pod-44403692-74fb-4124-a168-5e2515fbc6eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013070159s
Dec 14 09:49:58.964: INFO: Pod "pod-44403692-74fb-4124-a168-5e2515fbc6eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015199689s
STEP: Saw pod success
Dec 14 09:49:58.964: INFO: Pod "pod-44403692-74fb-4124-a168-5e2515fbc6eb" satisfied condition "success or failure"
Dec 14 09:49:58.966: INFO: Trying to get logs from node k8s-2 pod pod-44403692-74fb-4124-a168-5e2515fbc6eb container test-container: <nil>
STEP: delete the pod
Dec 14 09:49:58.980: INFO: Waiting for pod pod-44403692-74fb-4124-a168-5e2515fbc6eb to disappear
Dec 14 09:49:58.982: INFO: Pod pod-44403692-74fb-4124-a168-5e2515fbc6eb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:49:58.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2475" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":261,"skipped":4137,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:49:58.989: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1951
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-0e9456bd-d4b4-41eb-bf8b-5265c46b1ab2
STEP: Creating a pod to test consume secrets
Dec 14 09:49:59.181: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3a8df878-8c39-4296-a1c8-c04e886197dd" in namespace "projected-1951" to be "success or failure"
Dec 14 09:49:59.187: INFO: Pod "pod-projected-secrets-3a8df878-8c39-4296-a1c8-c04e886197dd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.77098ms
Dec 14 09:50:01.190: INFO: Pod "pod-projected-secrets-3a8df878-8c39-4296-a1c8-c04e886197dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008901518s
STEP: Saw pod success
Dec 14 09:50:01.190: INFO: Pod "pod-projected-secrets-3a8df878-8c39-4296-a1c8-c04e886197dd" satisfied condition "success or failure"
Dec 14 09:50:01.192: INFO: Trying to get logs from node k8s-2 pod pod-projected-secrets-3a8df878-8c39-4296-a1c8-c04e886197dd container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 09:50:01.206: INFO: Waiting for pod pod-projected-secrets-3a8df878-8c39-4296-a1c8-c04e886197dd to disappear
Dec 14 09:50:01.208: INFO: Pod pod-projected-secrets-3a8df878-8c39-4296-a1c8-c04e886197dd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:50:01.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1951" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":262,"skipped":4142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:50:01.221: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4675
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4675.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4675.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4675.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4675.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4675.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4675.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4675.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4675.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4675.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4675.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4675.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4675.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4675.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 33.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.33_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4675.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4675.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4675.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4675.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4675.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4675.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4675.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4675.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4675.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4675.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4675.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4675.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4675.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 33.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.33_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 09:50:03.435: INFO: Unable to read wheezy_udp@dns-test-service.dns-4675.svc.cluster.local from pod dns-4675/dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf: the server could not find the requested resource (get pods dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf)
Dec 14 09:50:03.437: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4675.svc.cluster.local from pod dns-4675/dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf: the server could not find the requested resource (get pods dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf)
Dec 14 09:50:03.439: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4675.svc.cluster.local from pod dns-4675/dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf: the server could not find the requested resource (get pods dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf)
Dec 14 09:50:03.441: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4675.svc.cluster.local from pod dns-4675/dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf: the server could not find the requested resource (get pods dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf)
Dec 14 09:50:03.455: INFO: Unable to read jessie_udp@dns-test-service.dns-4675.svc.cluster.local from pod dns-4675/dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf: the server could not find the requested resource (get pods dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf)
Dec 14 09:50:03.457: INFO: Unable to read jessie_tcp@dns-test-service.dns-4675.svc.cluster.local from pod dns-4675/dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf: the server could not find the requested resource (get pods dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf)
Dec 14 09:50:03.459: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4675.svc.cluster.local from pod dns-4675/dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf: the server could not find the requested resource (get pods dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf)
Dec 14 09:50:03.462: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4675.svc.cluster.local from pod dns-4675/dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf: the server could not find the requested resource (get pods dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf)
Dec 14 09:50:03.474: INFO: Lookups using dns-4675/dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf failed for: [wheezy_udp@dns-test-service.dns-4675.svc.cluster.local wheezy_tcp@dns-test-service.dns-4675.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4675.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4675.svc.cluster.local jessie_udp@dns-test-service.dns-4675.svc.cluster.local jessie_tcp@dns-test-service.dns-4675.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4675.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4675.svc.cluster.local]

Dec 14 09:50:08.515: INFO: DNS probes using dns-4675/dns-test-bca30943-9f64-46a6-8247-3f897d9f08cf succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:50:08.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4675" for this suite.

• [SLOW TEST:7.392 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":263,"skipped":4170,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:50:08.614: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-4173
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 14 09:50:12.791: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4173 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:50:12.791: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:50:12.929: INFO: Exec stderr: ""
Dec 14 09:50:12.929: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4173 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:50:12.929: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:50:13.056: INFO: Exec stderr: ""
Dec 14 09:50:13.056: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4173 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:50:13.056: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:50:13.179: INFO: Exec stderr: ""
Dec 14 09:50:13.179: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4173 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:50:13.179: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:50:13.301: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 14 09:50:13.301: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4173 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:50:13.301: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:50:13.424: INFO: Exec stderr: ""
Dec 14 09:50:13.424: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4173 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:50:13.424: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:50:13.552: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 14 09:50:13.552: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4173 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:50:13.552: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:50:13.672: INFO: Exec stderr: ""
Dec 14 09:50:13.672: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4173 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:50:13.672: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:50:13.793: INFO: Exec stderr: ""
Dec 14 09:50:13.794: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4173 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:50:13.794: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:50:13.932: INFO: Exec stderr: ""
Dec 14 09:50:13.932: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4173 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 09:50:13.932: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
Dec 14 09:50:14.052: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:50:14.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4173" for this suite.

• [SLOW TEST:5.447 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":264,"skipped":4183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:50:14.063: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 14 09:50:14.211: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e77703d7-ef65-4dbe-822d-ae2bfcfcaf5a" in namespace "projected-8804" to be "success or failure"
Dec 14 09:50:14.218: INFO: Pod "downwardapi-volume-e77703d7-ef65-4dbe-822d-ae2bfcfcaf5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.590152ms
Dec 14 09:50:16.220: INFO: Pod "downwardapi-volume-e77703d7-ef65-4dbe-822d-ae2bfcfcaf5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008716234s
STEP: Saw pod success
Dec 14 09:50:16.220: INFO: Pod "downwardapi-volume-e77703d7-ef65-4dbe-822d-ae2bfcfcaf5a" satisfied condition "success or failure"
Dec 14 09:50:16.222: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-e77703d7-ef65-4dbe-822d-ae2bfcfcaf5a container client-container: <nil>
STEP: delete the pod
Dec 14 09:50:16.235: INFO: Waiting for pod downwardapi-volume-e77703d7-ef65-4dbe-822d-ae2bfcfcaf5a to disappear
Dec 14 09:50:16.237: INFO: Pod downwardapi-volume-e77703d7-ef65-4dbe-822d-ae2bfcfcaf5a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:50:16.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8804" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":265,"skipped":4205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:50:16.246: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3997
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:50:16.376: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 14 09:50:21.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-3997 create -f -'
Dec 14 09:50:22.015: INFO: stderr: ""
Dec 14 09:50:22.015: INFO: stdout: "e2e-test-crd-publish-openapi-2412-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 14 09:50:22.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-3997 delete e2e-test-crd-publish-openapi-2412-crds test-cr'
Dec 14 09:50:22.099: INFO: stderr: ""
Dec 14 09:50:22.099: INFO: stdout: "e2e-test-crd-publish-openapi-2412-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 14 09:50:22.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-3997 apply -f -'
Dec 14 09:50:22.264: INFO: stderr: ""
Dec 14 09:50:22.264: INFO: stdout: "e2e-test-crd-publish-openapi-2412-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 14 09:50:22.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 --namespace=crd-publish-openapi-3997 delete e2e-test-crd-publish-openapi-2412-crds test-cr'
Dec 14 09:50:22.351: INFO: stderr: ""
Dec 14 09:50:22.351: INFO: stdout: "e2e-test-crd-publish-openapi-2412-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 14 09:50:22.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 explain e2e-test-crd-publish-openapi-2412-crds'
Dec 14 09:50:22.513: INFO: stderr: ""
Dec 14 09:50:22.513: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2412-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:50:24.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3997" for this suite.

• [SLOW TEST:8.553 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":266,"skipped":4306,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:50:24.800: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Dec 14 09:50:24.929: INFO: PodSpec: initContainers in spec.initContainers
Dec 14 09:51:08.401: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-77e80642-f8a2-47f8-90b5-1f15e0a8958c", GenerateName:"", Namespace:"init-container-7869", SelfLink:"/api/v1/namespaces/init-container-7869/pods/pod-init-77e80642-f8a2-47f8-90b5-1f15e0a8958c", UID:"4df6323c-9109-4e21-a3ef-9139a63cd6bf", ResourceVersion:"25321", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63711913824, loc:(*time.Location)(0x7d421e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"929656330"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-8nzj8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0033ec000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8nzj8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8nzj8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8nzj8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00306ce08), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00790ef60), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00306ceb0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00306ced0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00306ced8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00306cedc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711913824, loc:(*time.Location)(0x7d421e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711913824, loc:(*time.Location)(0x7d421e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711913824, loc:(*time.Location)(0x7d421e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711913824, loc:(*time.Location)(0x7d421e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.20.20.5", PodIP:"10.33.1.96", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.33.1.96"}}, StartTime:(*v1.Time)(0xc0043c8e40), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0043c8e80), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00365bf10)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://d82dbc4b4b6d0221c83d459ada5b37165c20f86d5e60b74a50d83d0508736601", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0043c8ec0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0043c8e60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00306cf6f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:51:08.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7869" for this suite.

• [SLOW TEST:43.609 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":267,"skipped":4311,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:51:08.410: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4899
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4742
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3812
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:51:37.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4899" for this suite.
STEP: Destroying namespace "nsdeletetest-4742" for this suite.
Dec 14 09:51:37.827: INFO: Namespace nsdeletetest-4742 was already deleted
STEP: Destroying namespace "nsdeletetest-3812" for this suite.

• [SLOW TEST:29.419 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":268,"skipped":4330,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:51:37.831: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6945
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:51:54.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6945" for this suite.

• [SLOW TEST:17.160 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":269,"skipped":4349,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:51:54.991: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2813
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-2813/secret-test-54510d54-e739-4f45-980a-3b577f7bb95e
STEP: Creating a pod to test consume secrets
Dec 14 09:51:55.131: INFO: Waiting up to 5m0s for pod "pod-configmaps-70e32826-78ac-42b4-aaf4-dbee08608cbc" in namespace "secrets-2813" to be "success or failure"
Dec 14 09:51:55.143: INFO: Pod "pod-configmaps-70e32826-78ac-42b4-aaf4-dbee08608cbc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.533321ms
Dec 14 09:51:57.146: INFO: Pod "pod-configmaps-70e32826-78ac-42b4-aaf4-dbee08608cbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015090071s
STEP: Saw pod success
Dec 14 09:51:57.146: INFO: Pod "pod-configmaps-70e32826-78ac-42b4-aaf4-dbee08608cbc" satisfied condition "success or failure"
Dec 14 09:51:57.148: INFO: Trying to get logs from node k8s-2 pod pod-configmaps-70e32826-78ac-42b4-aaf4-dbee08608cbc container env-test: <nil>
STEP: delete the pod
Dec 14 09:51:57.167: INFO: Waiting for pod pod-configmaps-70e32826-78ac-42b4-aaf4-dbee08608cbc to disappear
Dec 14 09:51:57.171: INFO: Pod pod-configmaps-70e32826-78ac-42b4-aaf4-dbee08608cbc no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:51:57.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2813" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":270,"skipped":4356,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:51:57.178: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9941
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 14 09:52:01.373: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 09:52:01.379: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 09:52:03.379: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 09:52:03.381: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 09:52:05.379: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 09:52:05.381: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:52:05.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9941" for this suite.

• [SLOW TEST:8.212 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":271,"skipped":4363,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:52:05.392: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8121
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Dec 14 09:52:05.572: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:52:07.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8121" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":272,"skipped":4377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:52:07.489: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7146
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-00401b0b-c064-45d4-a35d-5738926b979b
STEP: Creating a pod to test consume configMaps
Dec 14 09:52:07.626: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-73ba5786-df98-436b-afda-32582e108c08" in namespace "projected-7146" to be "success or failure"
Dec 14 09:52:07.632: INFO: Pod "pod-projected-configmaps-73ba5786-df98-436b-afda-32582e108c08": Phase="Pending", Reason="", readiness=false. Elapsed: 5.487959ms
Dec 14 09:52:09.634: INFO: Pod "pod-projected-configmaps-73ba5786-df98-436b-afda-32582e108c08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007883944s
STEP: Saw pod success
Dec 14 09:52:09.635: INFO: Pod "pod-projected-configmaps-73ba5786-df98-436b-afda-32582e108c08" satisfied condition "success or failure"
Dec 14 09:52:09.636: INFO: Trying to get logs from node k8s-2 pod pod-projected-configmaps-73ba5786-df98-436b-afda-32582e108c08 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 09:52:09.648: INFO: Waiting for pod pod-projected-configmaps-73ba5786-df98-436b-afda-32582e108c08 to disappear
Dec 14 09:52:09.651: INFO: Pod pod-projected-configmaps-73ba5786-df98-436b-afda-32582e108c08 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:52:09.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7146" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":273,"skipped":4434,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:52:09.672: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7043
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Dec 14 09:52:12.358: INFO: Successfully updated pod "labelsupdate517a1e98-861a-416f-b834-4a4b2e7fcbd5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:52:14.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7043" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":274,"skipped":4437,"failed":0}
SSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:52:14.375: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-6240
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Dec 14 09:52:14.516: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6240" to be "success or failure"
Dec 14 09:52:14.522: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.947519ms
Dec 14 09:52:16.525: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008215813s
STEP: Saw pod success
Dec 14 09:52:16.525: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec 14 09:52:16.527: INFO: Trying to get logs from node k8s-2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec 14 09:52:16.540: INFO: Waiting for pod pod-host-path-test to disappear
Dec 14 09:52:16.543: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:52:16.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6240" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":275,"skipped":4442,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:52:16.551: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8748
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:52:29.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8748" for this suite.

• [SLOW TEST:13.233 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":276,"skipped":4444,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:52:29.784: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Dec 14 09:52:29.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 api-versions'
Dec 14 09:52:29.985: INFO: stderr: ""
Dec 14 09:52:29.985: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauditregistration.k8s.io/v1alpha1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nlinkerd.io/v1alpha1\nlinkerd.io/v1alpha2\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1alpha1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nsplit.smi-spec.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1alpha1\nstorage.k8s.io/v1beta1\ntap.linkerd.io/v1alpha1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:52:29.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4159" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":277,"skipped":4449,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:52:29.991: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 14 09:52:30.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-154'
Dec 14 09:52:30.213: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 14 09:52:30.213: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Dec 14 09:52:30.233: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-9zp4s]
Dec 14 09:52:30.233: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-9zp4s" in namespace "kubectl-154" to be "running and ready"
Dec 14 09:52:30.240: INFO: Pod "e2e-test-httpd-rc-9zp4s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.171769ms
Dec 14 09:52:32.242: INFO: Pod "e2e-test-httpd-rc-9zp4s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008771641s
Dec 14 09:52:34.244: INFO: Pod "e2e-test-httpd-rc-9zp4s": Phase="Running", Reason="", readiness=true. Elapsed: 4.010613833s
Dec 14 09:52:34.244: INFO: Pod "e2e-test-httpd-rc-9zp4s" satisfied condition "running and ready"
Dec 14 09:52:34.244: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-9zp4s]
Dec 14 09:52:34.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 logs rc/e2e-test-httpd-rc --namespace=kubectl-154'
Dec 14 09:52:34.335: INFO: stderr: ""
Dec 14 09:52:34.335: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.33.1.105. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.33.1.105. Set the 'ServerName' directive globally to suppress this message\n[Sat Dec 14 09:52:31.695493 2019] [mpm_event:notice] [pid 1:tid 139966342179688] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Sat Dec 14 09:52:31.695593 2019] [core:notice] [pid 1:tid 139966342179688] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Dec 14 09:52:34.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-014205462 delete rc e2e-test-httpd-rc --namespace=kubectl-154'
Dec 14 09:52:34.421: INFO: stderr: ""
Dec 14 09:52:34.421: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:52:34.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-154" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":280,"completed":278,"skipped":4482,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:52:34.430: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-172
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-172
Dec 14 09:52:34.599: INFO: Found 0 stateful pods, waiting for 1
Dec 14 09:52:44.601: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Dec 14 09:52:44.619: INFO: Deleting all statefulset in ns statefulset-172
Dec 14 09:52:44.635: INFO: Scaling statefulset ss to 0
Dec 14 09:53:14.696: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 09:53:14.698: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:53:14.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-172" for this suite.

• [SLOW TEST:40.289 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":279,"skipped":4486,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 14 09:53:14.720: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3753
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 14 09:53:14.861: INFO: >>> kubeConfig: /tmp/kubeconfig-014205462
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 14 09:53:16.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3753" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":280,"skipped":4497,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSDec 14 09:53:16.793: INFO: Running AfterSuite actions on all nodes
Dec 14 09:53:16.793: INFO: Running AfterSuite actions on node 1
Dec 14 09:53:16.793: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4534,"failed":0}

Ran 280 of 4814 Specs in 3921.677 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4534 Skipped
PASS

Ginkgo ran 1 suite in 1h5m23.410313851s
Test Suite Passed
